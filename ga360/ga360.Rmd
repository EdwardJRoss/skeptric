---
title: "Do you have an over-confident interval?"
output: html_notebook
---

```{r}
# Import libraries
library(magrittr)
library(boot)
library(lubridate)
library(tidyverse)
library(ggrepel)
library(ggformula)
library(knitr)
```


Confidence intervals and statistical tests are a powerful (if somewhat mind-bending) tool for quantifying the risk of random chance in a result.
You could run an experiment and find that the UI changes increased the average visit time by 20% - that's great.
But did we just get lucky? With 80% confidence the average visit time changed somewhere between -5% and 30% - that's less convincing.

Often in business and digital products it's not important to get every decision right, but it's important to move in the right direction.
An 80% confidence level says that if we run many experiments, we expect the true value to be in that inverval in 80% of the experiments.
That's a useful thing to know - for changes with large implications we can increase our confidence and for less important changes we can carry a bigger risk of being wrong.

But how can we know that the real answer is in this interval 80% of the time?
One common way is to assume the samples fit on a bell curve and use a z-test (or t-test) - but this isn't always true, especially for long tailed data like visit time, page views and revenue.
This leads us to be overconfident - and take on more business risk than we assumed.

However we can do better - there's a powerful tool called the bias-corrected and accelerated (BCa) bootstrap interval that gives much more accurate confidence intervals.

In this article we'll use real web analytics data from the [Google Merchandise Store](https://www.kaggle.com/c/ga-customer-revenue-prediction/data) to show how much better the BCa bootstrap does than the normality assumption.

How does your A/B testing tool calculate confidence levels?


# Central Limit Theorem

The reason that z-tests are so commonly used is the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) which roughly says if you have a bunch of independent samples with similar variances the distribution of the *sample means* is approximately a normal distribution centred on the true mean of the data. Moreover if you quadruple your sample size then you halve the width of this normal distribution.

Let's illustrate this with [Google Analytics data from the Google Merchandise Store](https://www.kaggle.com/c/ga-customer-revenue-prediction/data).

We'll look at 3 common measures - the [bounce rate](https://support.google.com/analytics/answer/1009409?hl=en) (proportion of visitors who leave the site after viewing one page), the average [page views](https://support.google.com/analytics/answer/6086080?hl=en) per browsing session, and the [average time spent browsing per session](https://support.google.com/analytics/answer/1006253?hl=en)  (excluding bounced visits).

To make sure our samples our approximately independent we'll only examine the first session for each user (since the number of pages on a second visit is likely to be related to the number of pages on the first browsing session).

```{r}
# Import Data
# Downloaded from https://www.kaggle.com/c/ga-customer-revenue-prediction/data and converted into a single file with most relevant fields extracted
# Revenue has to be a double because it can overflow an R 32-bit integer
df <- read_csv('ga360.csv', col_types=cols(revenue = col_double())) %>%
    mutate(start_time = .POSIXct(start_time, tz="UTC"),
           # Transaction Revenue is multiplied by 10^6
           # See https://support.google.com/analytics/answer/3437719
           revenue = revenue/1e6)

# Get the first visit for each user
# This is pretty slow because there are so many users
df1 <- df %>%
    group_by(user) %>%
    arrange(start_time) %>%
    filter(row_number() == 1) %>%
    ungroup()

# The three metrics. I'll use the term visit and session interchangably
pages <- df1$pages
bounces <- pages <= 1
visit_time <- df1$visit_time[df1$visit_time > 0]
```

We've got a total of `r nrow(df1)` first browsing sessions. In an experiment we'd take a fixed size sample that would get a separate treatment and measure some statistic on the sample against a control group. Here we're just going to treat all the browsing sessions as a single group and take many samples to simulate the range of possible outcomes (a statistical ensemble).

```{r}
# Sample the vector v with samples of size n, n_rep times and apply statistic to each sample
sample_stat <- function(v, n, n_rep, statistic=mean)  {
    r <- replicate(n_rep, sample(v, n))
    apply(r, 2, statistic)
}

sample_at_sizes <- function(v, sample_sizes, n_rep) {
  tibble(sample_size = sample_sizes %>% rep(n_rep)) %>%
    rowwise %>%
    do(sample_size=.$sample_size,
     sample = sample(v, .$sample_size)) %>%
    unnest(sample_size) %>%
    # Calculate some common statistics
    mutate(mean = map(sample, mean),
           sd = map(sample, sd)) %>%
    unnest(mean, sd)
}

sample_sizes <- seq(10, 300, by=10)
n_rep <- 1000
filt_sample_sizes <- c(10, 20, 40, 80)



# This takes a while so cache the result to a file
# Should capture this in a function/macro rather than repeating 3 times
if (!file.exists('bounce.rds')) {
  bounce_samples <- sample_at_sizes(bounces, sample_sizes, n_rep)
  bounce_samples %>% write_rds('bounce.rds')
}

bounce_samples <- read_rds('bounce.rds') %>% mutate(pop_mean = mean(bounces), pop_sd=sd(bounces))

if (!file.exists('page.rds')) {
  page_samples <- sample_at_sizes(pages, sample_sizes, n_rep)
  page_samples %>% write_rds('page.rds')
}
page_samples <- read_rds('page.rds') %>% mutate(pop_mean = mean(pages), pop_sd=sd(pages))


if (!file.exists('visit_time.rds')) {
  visit_time_samples <- sample_at_sizes(visit_time, sample_sizes, n_rep)
  visit_time_samples %>% write_rds('visit_time.rds')
}
visit_time_samples <- read_rds('visit_time.rds')  %>% mutate(pop_mean = mean(visit_time), pop_sd=sd(visit_time))

plot_limit_histogram <- function(samples, sample_sizes, n_bins) {
  central_limits <- samples %>%
  filter(sample_size %in% sample_sizes) %>%
  mutate(min=min(mean), max=max(mean)) %>%
  group_by(sample_size, pop_mean, pop_sd, min, max) %>%
  # The number of (non-empty) bins in histogram
  summarise(num_bins=sum(hist(mean, n_bins, plot=FALSE)$counts > 0),
            n_rep=n()) %>%
  mutate(x=list(seq(min[1], max[1], length.out=graph_steps))) %>%
  unnest(x) %>%
  mutate(y= n_rep * (max-min) /num_bins *dnorm(x, pop_mean, pop_sd / sqrt(sample_size)))

  ggplot(samples %>% filter(sample_size %in% sample_sizes)) + 
  geom_histogram(aes(mean), bins=n_bins) +
  geom_line(data=central_limits, aes(x, y), linetype="dashed") +
  facet_wrap(.~sample_size) +
  xlab('Sample mean') +
  ylab('Frequency')
}
```


Bounces have a binary outcome, either someone bounces or they don't:

```{r}
df1 %>% mutate(bounce = pages == 1) %>% count(bounce) %>% mutate(proportion=scales::percent(n/sum(n)))
```

The bounce rate will follow a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) which converges to a normal distribution rapidly as the sample size increases.

```{r}
plot_limit_histogram(bounce_samples, filt_sample_sizes, n_bins=79)  +
  ggtitle('Sample means of bounce rate for different sample sizes')
```

The page data is long tailed: most visits only have a few page views, but a few visits have hundreds of page views.

```{r}
df1 %>% gf_histogram(~pages, bins=40) + scale_y_log10() + ggtitle('Most visits only have a few page views') + ylab('Frequency (log scale)')
```


```{r}
df1 %>% 
  count(pages) %>%
  arrange(desc(n)) %>% 
  mutate(Proportion = scales::percent(n/sum(n)),
         "Cumulative Proportion"=scales::percent(cumsum(n)/sum(n))) %>%
  head(10) %>%
  kable
```

Because of this the sample means often underestimate the true mean, and the sample means converge to a normal distribution more slowly; even at 80 it is significantly left skewed.

```{r}
plot_limit_histogram(page_samples, filt_sample_sizes, n_bins=80)  +
  ggtitle('Sample means of page views for different sample sizes')
```

Similar to page views, visit time is highly skewed:

```{r}
df1 %>% filter(visit_time > 0) %>% gf_histogram(~visit_time) + scale_y_log10() + ggtitle('Most visits have a short visit time') + ylab('Frequency (log scale)')
```

Consequently we see the sample means tend to undersetimate the actual mean.

```{r}
plot_limit_histogram(visit_time_samples, filt_sample_sizes, n_bins=80)  +
  ggtitle('Sample means of average visit time for different sample sizes')
```

## Standard Confidence Intervals

For long tailed data like page views and average visit times we've seen that the sample means tend to underestimate the true mean.

However to construct standard confidence intervals we also need to estimate the population standard deviation, which we also underestimate.


```{r}
visit_time_samples %>% filter(sample_size %in% c(10,20,40,80)) %>%
  gf_histogram(~ sd | sample_size, bins=40) %>%
  gf_vline(xintercept=sd(visit_time)) +
  annotate("text", x=sd(visit_time) * 1.02, y=130, label="Pop. s.d.", hjust=0) +
  ggtitle("Sample Standard Deviations of Average Visit Time underestimate") +
  xlab("Standard Devation (seconds)") +
  ylab("Frequency")
```

It's even worse because sample mean and standard deviation are correlated; when our sample doesn't contain any of the rare very high values we will get a low estimate of both the mean and the standard deviation and so be overconfident in our small value.

```{r}
visit_time_samples %>% 
  filter(sample_size %in% filt_sample_sizes) %>% 
  gf_hex(sd ~ mean | sample_size, bins=50) %>%
  gf_vline(xintercept=mean(visit_time)) %>%
  gf_hline(yintercept=sd(visit_time)) +
  ggtitle("Sample mean vs standard deviation for Average Visit Time", subtitle="Sample standard deviation correlated with sample mean") +
  xlab("Sample Mean") +
  ylab("Sample Standard Deviation")
```

```{r}
visit_time_samples %>%
  filter(sample_size %in% filt_sample_sizes) %>%
  mutate(p = pnorm(mean, mean=pop_mean, sd=sd / sqrt(sample_size))) %>%
  gf_histogram(~p | sample_size)
```


### Confidence Intervals

```{r}
n <- 10
n_rep <- 3
v <- pages
alpha <- 0.05

mu <- mean(v)

# Returns the percentage where true mean is below, within and above confidence interval
z_cov <- function(v, n, n_rep, alpha=0.05, statistic=mean) {
    r <- replicate(n_rep, sample(v, n))
    sample_mean <- apply(r, 2, statistic)
    sample_sd <- apply(r, 2, sd) / sqrt(n)
    ## below <- mean(v) < sample_mean - sample_sd * qnorm(1-alpha/2)
    ## above <- mean(v) > sample_mean + sample_sd * qnorm(1-alpha/2)
    ## c(mean(below), 1-mean(below) - mean(above), mean(above))
    c(z_cov = mean(abs(statistic(v) - sample_mean) <= sample_sd * qnorm(1-alpha/2)))
}


boot_cov <- function(v, n, n_rep, alpha=0.05, R=1000, statistic=mean) {
    r <- replicate(n, sample(v, n_rep))
    boots <- apply(r, 1, boot, R=R, statistic=function(r, i) statistic(r[i]))

    conf_int <- lapply(boots, function(x) boot.ci(x, conf=1 - alpha))

    true_value = statistic(v)

    ci_norm <- vapply(conf_int, function(ci) ci$normal[2:3], numeric(2))
    ci_basic <- vapply(conf_int, function(ci) ci$basic[4:5], numeric(2))
    ci_percent <- vapply(conf_int, function(ci) ci$percent[4:5], numeric(2))
    ci_bca <- vapply(conf_int, function(ci) ci$bca[4:5], numeric(2))

    c(norm_cov = mean(ci_norm[1,] <= true_value & true_value <= ci_norm[2,]),
      basic_cov=mean(ci_basic[1,] <= true_value & true_value <= ci_basic[2,]),
      percent_cov=mean(ci_percent[1,] <= true_value & true_value <= ci_percent[2,]),
      bca_cov=mean(ci_bca[1,] <= true_value & true_value <= ci_bca[2,]))
}
```

Comparison of methods
[medical](https://m.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf)
[Efron](https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214)

```{r}
sample_sizes <- seq(20, 100, 10)
conf <- c(0.6, 0.75, 0.95, 0.99)
n_rep <- 2000

param <- expand.grid(sample_size=sample_sizes, confidence=conf)


# 1 hour
start_time <- Sys.time()
page_conf <- param %>%
    group_by_all() %>%
    do(boot_cov(pages, .$sample_size, n_rep, 1-.$confidence, R=5000) %>% t %>% as.data.frame) %>%
    group_by_all() %>%
    do(z_cov(pages, .$sample_size, n_rep, 1-.$confidence) %>% t %>% as.data.frame) %>%
    ungroup
end_time <- Sys.time()

end_time - start_time


write_csv(page_cov, 'page_cov.csv')
```

```{r}
page_cov %>%
    gather(method, coverage, ends_with("cov")) %>%
    mutate(method = str_replace(method, "_cov$", "")) %>%
    mutate(label = if_else(sample_size == max(sample_size), as.character(method), NA_character_)) %>%
    gf_line(coverage/confidence ~ sample_size | confidence, group=~method, col=~method)
```

```{r}
# 1 hour
start_time <- Sys.time()
bounce_cov <- param %>%
    group_by_all() %>%
    do(boot_cov(bounces, .$sample_size, n_rep, 1-.$confidence) %>% t %>% as.data.frame) %>%
    group_by_all() %>%
    do(z_cov(bounces, .$sample_size, n_rep, 1-.$confidence) %>% t %>% as.data.frame) %>%
    ungroup
end_time <- Sys.time()

write_csv(bounce_cov, 'bounce_cov.csv')

end_time - start_time
```


```{r}
bounce_cov <- read_csv('bounce_cov.csv')

bounce_cov %>%
    gather(method, coverage, ends_with("cov")) %>%
    mutate(method = str_replace(method, "_cov$", "")) %>%
    gf_line(coverage/confidence ~ sample_size | confidence, colour=~method)
```




```{r}
# 1 hour
start_time <- Sys.time()
visit_time_cov <- param %>%
    group_by_all() %>%
    do(boot_cov(visit_time, .$sample_size, n_rep, 1-.$confidence) %>% t %>% as.data.frame) %>%
    group_by_all() %>%
    do(z_cov(visit_time, .$sample_size, n_rep, 1-.$confidence) %>% t %>% as.data.frame) %>%
    ungroup
end_time <- Sys.time()

write_csv(visit_time_cov, 'visit_time_cov.csv')

end_time - start_time
```

```{r}
visit_time_cov <- read_csv('visit_time_cov.csv')


visit_time_cov %>%
    gather(method, coverage, ends_with("cov")) %>%
    mutate(method = str_replace(method, "_cov$", "")) %>%
    gf_line(coverage - confidence ~ sample_size | confidence, colour=~method)
```
