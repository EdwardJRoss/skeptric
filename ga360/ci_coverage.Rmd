---
title: "Do you have an over-confident interval?"
output: html_notebook
---

```{r message=FALSE}
# Import libraries
library(magrittr)
library(boot)
library(lubridate)
library(tidyverse)
library(ggrepel)
library(ggformula)
library(knitr)
library(ggthemes)

# Import Data
# Downloaded from https://www.kaggle.com/c/ga-customer-revenue-prediction/data and converted into a single file with most relevant fields extracted
# See ga360.py
# Revenue has to be a double because it can overflow an R 32-bit integer
df <- read_csv('ga360.csv', col_types=cols(revenue = col_double())) %>%
    mutate(start_time = .POSIXct(start_time, tz="UTC"),
           # Transaction Revenue is multiplied by 10^6
           # See https://support.google.com/analytics/answer/3437719
           revenue = revenue/1e6)

# Get the first visit for each user
# This is pretty slow because there are so many users
df1 <- df %>%
    group_by(user) %>%
    arrange(start_time) %>%
    filter(row_number() == 1) %>%
    ungroup()

# The three metrics. I'll use the term visit and session interchangably
pages <- df1$pages
bounces <- pages <= 1
visit_time <- df1$visit_time[df1$visit_time > 0]


# Parameters
# The sizes at which to take samples
# Start at 20 because bounces can fail at 10 with BCa when a sample contains all 1s/0s
# (This is a very slight rigging)
sample_sizes <- seq(20, 300, by=10)
# Sizes to show samples in plots
filt_sample_sizes <- c(20, 40, 60, 80)

# Confidence levels
conf <- c(0.5, 0.6, 0.8, 0.9, 0.95, 0.99)
# Number of samples to take at each sample_size x confidence
n_rep <- 1000
# Number of bootstrap samples to use
boot_rep <- 1000



# Create individual functions for data pipeline to make it easier to change
# processing at a conceptual level.


# Sample the vector v with samples of size n, n_rep times and apply statistic to each sample
sample_stat <- function(v, n, n_rep, statistic=mean)  {
    r <- replicate(n_rep, sample(v, n))
    apply(r, 2, statistic)
}

# Sample the vector v with samples of size n, n_rep times
sample_at_sizes <- function(v, sample_sizes, n_rep) {
  tibble(sample_size = sample_sizes %>% rep(n_rep)) %>%
    rowwise %>%
    do(sample_size=.$sample_size,
     sample = sample(v, .$sample_size)) %>%
    unnest(sample_size)
}

sample_add_stats <- function(samples) {
    # Calculate some common statistics
    samples %>%
    mutate(mean = map(sample, mean),
           sd = map(sample, sd)) %>%
    unnest(mean, sd)
}

# Get a function that calculates the BCa Bootstrap on a sample
# to calculate statistic at each confidence level in conf.
# BCa will occasionally have "endpoint" warnings - we don't need to read them 1000 times
boot_ci_get <- function(R, statistic=mean, conf=c(0.95), warn=FALSE) {
  function(v) {
    boot_samples <- boot(v, 
                         function(d, x) statistic(d[x]),
                         R=R)
    if (warn) {
      ci <- boot.ci(boot_samples, type="bca", conf=conf)
    } else {
      ci <- suppressWarnings(boot.ci(boot_samples, type="bca", conf=conf))
    }
    as_tibble(ci$bca) %>% 
      rename(boot_ci_min="V3", boot_ci_max="V4") %>%
      select(-V1, -V2)
  }
}
  

# Add bootstrap ci for sample
sample_add_boot_ci <- function(samples, R=1000, conf=0.95, statistic=mean) {
  samples %>%
  mutate(bootci = map(sample, 
                        boot_ci_get(R=R, conf=conf, statistic=statistic)))
}



# This takes a while so sample and cache the result to a file
write_samples <- function(fname, v, sample_sizes, n_rep, conf, R, overwrite=FALSE) {
  if (overwrite || !file.exists(fname)) {
    sample_at_sizes(v, sample_sizes, n_rep) %>%
      sample_add_stats() %>%
      sample_add_boot_ci(conf=conf, R=boot_rep) %>% 
      write_rds(fname)
  }
}


write_samples(fname='pages.rds', v=pages, n_rep=n_rep, conf=conf, R=boot_rep, sample_sizes=sample_sizes)
write_samples(fname='bounces.rds', v=bounces, n_rep=n_rep, conf=conf, R=boot_rep, sample_sizes=sample_sizes)
write_samples(fname='visit_time.rds', v=visit_time, n_rep=n_rep, conf=conf, R=boot_rep, sample_sizes=sample_sizes)

page_samples <- read_rds('pages.rds') %>% mutate(pop_mean = mean(pages), pop_sd=sd(pages))
bounces_samples <- read_rds('bounces.rds') %>% mutate(pop_mean = mean(bounces), pop_sd=sd(bounces))
visit_time_samples <- read_rds('visit_time.rds') %>% mutate(pop_mean = mean(visit_time), pop_sd=sd(visit_time))

# Add normal confidence interval and whether in bounds
add_ci_bounds <- function(samples) {
  samples %>%
  unnest(bootci) %>%
  mutate(norm_ci_min = mean - sd * qnorm(1-(1-conf)/2) / sqrt(sample_size),
         norm_ci_max = mean + sd * qnorm(1-(1-conf)/2) / sqrt(sample_size),
         norm_bound_below = norm_ci_min <= pop_mean,
         norm_bound_above = pop_mean <= norm_ci_max,
         boot_bound_below = boot_ci_min <= pop_mean,
         boot_bound_above = pop_mean <= boot_ci_max,
         boot_bound = boot_bound_below & boot_bound_above,
         norm_bound = norm_bound_below & norm_bound_above)
}

sample_coverage <- function(sample) {
  sample %>% 
  filter(sample_size > 10) %>%
  add_ci_bounds %>%
  group_by(sample_size, conf) %>%
  summarise(normal = mean(norm_bound),
            bca_bootstrap = mean(boot_bound)) %>%
  gather("method", "coverage", normal, bca_bootstrap)
}

plot_coverage <- function(sample) {
  sample %>%
  ggplot(aes(sample_size, coverage - conf, colour=method)) +
  geom_line() +
  facet_wrap(~conf, labeller=as_labeller(function(var) paste0("Confidence ", var))) +
  labs(colour="Method",
       y="Coverage in Excess of Confidence Level",
       x="Sample Size") +
  theme_hc() + scale_colour_hc(labels=c(normal="Normal Confidence Interval", bca_bootstrap="BCa Bootstrap Confidence Interval"))
}

```





Confidence intervals are powerful, if somewhat mind-bending, tool for quantifying the risk of random chance in a result.
You could run an experiment and find that a websites user interface changes increased the average visit time by 20% - that sounds great.
But did we just get lucky? If there's and 80% chance average visit time changed somewhere between -5% and 30% - that's less convincing.

Often it's not important to get every decision right, but it's important to move in the right direction.
An 80% confidence level says that if we run many experiments, we expect the true value to be in that inverval in 80% of the experiments.
That's a useful thing to know - for changes with large implications we can increase our confidence and for less important changes we can carry a bigger risk of being wrong.

But how can we know that the real answer is in this interval 80% of the time?
One common way is to assume the samples fit on a bell curve and use a z-test (or t-test) - but this isn't always true, especially for long tailed data like visit time, page views and revenue.
This leads us to be overconfident - and take on more business risk than we assumed.

However we can do better - there's a powerful tool called the BCa bootstrap interval that gives much more accurate confidence intervals.
I recommend using BCa bootstrap for confidence intervals for any experiments over the normal intervals, especially for skewed data.

Let's compare the two with real [web analytics data](https://www.kaggle.com/c/ga-customer-revenue-prediction/data) from the [Google Merchandise Store](https://www.kaggle.com/c/ga-customer-revenue-prediction/data).
We'll look at common engagement metrics for the first browsing sessions for each user accross `r nrow(df1)` unique users.

# Interval Coverage

When a test has 80% confidence it should contain the true value 80% of the time.
The proportion of confidence intervals that actually contain the true value the coverage - ideally this should be the same as the confidence level.
For each sample size and confidence level we estimate the coverage by taking `r n_rep` samples and calculating the proportion that contain the true mean.
We'll evaluate how good the coverage is by calculating the difference from the confidence level - a perfect measure would be zero.

For the web metrics we look at here the BCa Bootstrap always has coverage at least as good as the normal approximation, and often much better.
However for long tailed data (like average page views and average visit time) both overestimate their confidence - so it's often better to set a confidence level a bit higher than you really need.

For the [bounce rate](https://support.google.com/analytics/answer/1009409?hl=en) (the proportion of visitors who leave the site after viewing only one page) both metrics do pretty well for sample sizes over 100. However at confidence levels of 80% and below they can be off by 3-5 percentage points even with sample sizes near 300.


```{r}
bounces_samples %>%
  sample_coverage %>%
  plot_coverage +
  ggtitle("Bounce Rate CI Coverage Good by either method at high confidence levels",
          subtitle="Bounce Rate: Excess Confidence Interval Coverage versus Sample Size")
```

For the average [page views](https://support.google.com/analytics/answer/6086080?hl=en) the two methods do similarly at low confidence levels, but the BCa Bootstrap has much better coverage at confidence levels about 90%, especially at small sample sizes. In both cases they have much lower coverage than the confidence level: up to 4 percentage points for sample size 100.

```{r}
page_samples %>%
  sample_coverage %>%
  plot_coverage +
  ggtitle("Page Views: Bootstrap CI Coverage dominates Normal",
          subtitle="Average Page Vies: Excess Confidence Interval Coverage versus Sample Size")
```


For the [average time spent browsing per session](https://support.google.com/analytics/answer/1006253?hl=en) (excluding bounced visits)

```{r}
visit_time_samples %>%
  sample_coverage %>%
  plot_coverage +
  ggtitle("Page Views: Bootstrap CI Coverage dominates Normal",
          subtitle="Average Page Vies: Excess Confidence Interval Coverage versus Sample Size")

```

# Central Limit Theorem

What's going on?
Why do normal intervals work at all, and why are they so much less accurate at large sample sizes.

Normal confidence intervals are justified by the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) which says that if you have a bunch of independent samples with similar variances the distribution of the *sample means* is approximately a normal distribution centred on the true mean of the data.
Moreover the width of the sample mean distribution is the width of the population distribution divided by the square root of the sample size ($\sigma/\sqrt{n}$).

Let's illustrate the central limit theorem for our metrics above.
Because we're only looking at the first browsing session for each user, our samples are approximately independent.

Bounces have a binary outcome - either someone bounced or they didn't.

```{r}
table(bounces) %>% as_tibble %>% mutate(percent = scales::percent(n/sum(n)))
```

Consequently our samples or bounces are [binomially distributed](https://en.wikipedia.org/wiki/Binomial_distribution) and converge to a normal distribution very quickly - which explains why the normal intervals worked so well.

```{r}
plot_limit_histogram <- function(samples, sample_sizes, n_bins, graph_steps=1000) {
  central_limits <- samples %>%
  filter(sample_size %in% sample_sizes) %>%
  mutate(min=min(mean), max=max(mean)) %>%
  group_by(sample_size, pop_mean, pop_sd, min, max) %>%
  # The number of (non-empty) bins in histogram
  summarise(num_bins=sum(hist(mean, n_bins, plot=FALSE)$counts > 0),
            n_rep=n()) %>%
  mutate(x=list(seq(min[1], max[1], length.out=graph_steps))) %>%
  unnest(x) %>%
  mutate(y= n_rep * (max-min) /num_bins *dnorm(x, pop_mean, pop_sd / sqrt(sample_size)))

  ggplot(samples %>% filter(sample_size %in% sample_sizes)) + 
  geom_histogram(aes(mean), bins=n_bins) +
  geom_line(data=central_limits, aes(x, y), linetype="dashed") +
  facet_wrap(~sample_size, labeller=as_labeller(function(var) paste0("Sample Size ", var))) +
  xlab('Sample mean') +
  ylab('Frequency') +
    theme_hc()
}
```


```{r}
plot_limit_histogram(bounces_samples, filt_sample_sizes, 33) +
  ggtitle("Bounces approximately normal at all sample sizes",
          subtitle="Count of bounce rates of 1000 samples at each sample size")
```

However page views tends to be left-skewed and converges much more slowly to the normal distribution.
This is why the normal confidence intervals tended to be too low.

```{r}
plot_limit_histogram(page_samples, filt_sample_sizes, 90) +
  ggtitle("Page views slowly converges to normal distribution",
          subtitle="Histograms of average page views of 1000 samples at each sample size") +
  theme_hc()
```


The sample means are left-skwered because the number of page views in each session is extremely left-skewed; most users bounce and only have one page view, but some users have many (hundreds) of page views.

```{r}
df1 %>%
  gf_histogram(~pages, binwidth=1) + 
  scale_y_log10() +
  theme_hc() + 
  xlab("Page Views") +
  ylab("Frequency (log scale)")
  ggtitle("Vast majority of users only have a few pages per session",
          subtitle="Count (log scale) of Sessions with Page Views")
```

The majority of visits only have a few page views; so most samples will be composed of these and not contain the extreme outliers.
This explains why the confidence intervals tended to be too low - they underestimate the impact of unseen outliers.

```{r}
df1 %>% 
  count(pages, sort=TRUE) %>% 
  mutate(percent = scales::percent(n/sum(n)),
         cumulative_percent=scales::percent(cumsum(n)/sum(n))) %>%
  head(10)
```

It's worth pausing to consider whether we actually want to measure average page views here.
The few sessions with large numbers of page views will have a large impact on the average and may not represent what we're trying to measure.
For example if we were measuring engagement we may be more interested in the proportion of visits with at least 3 page views, or in different page view ranges (e.g. 0-2, 3-7, 7+).
However if for example total page views were directly related to cost or revenue (e.g. from banner advertising) then the average is actually interesting.


Average visit time


## Standard Confidence Intervals

For long tailed data like page views and average visit times we've seen that the sample means tend to underestimate the true mean.

However to construct standard confidence intervals we also need to estimate the population standard deviation which is also underestimated at small sample sizes.
This means that normal intervals consistently underestimate the width of the confidence interval.

```{r}
visit_time_samples %>%
  filter(sample_size %in% filt_sample_sizes) %>%
  gf_histogram(~ sd | sample_size, bins=70) %>%
  gf_vline(xintercept=sd(visit_time)) +
  annotate("text", x=sd(visit_time) * 1.02, y=100, label="Pop. s.d.", hjust=0) +
  ggtitle("Sample Standard Deviations of Average Visit Time underestimate") +
  xlab("Standard Devation (seconds)") +
  ylab("Frequency")  +
  theme_hc()
```





It's even worse because sample mean and standard deviation are correlated; when the sample doesn't contain any of the rare very high values we will get a low estimate of both the mean and the standard deviation and so be overconfident in our small value.

```{r}
visit_time_samples %>% 
  filter(sample_size %in% filt_sample_sizes) %>% 
  gf_hex(sd ~ mean | sample_size, bins=20) %>%
  gf_vline(xintercept=mean(visit_time)) %>%
  gf_hline(yintercept=sd(visit_time)) +
  ggtitle("Sample mean vs standard deviation for Average Visit Time", subtitle="Sample standard deviation correlated with sample mean") +
  xlab("Sample Mean") +
  ylab("Sample Standard Deviation") +
  theme_hc()
```

### Bootstrap Confidence Intervals

Non-parametric Bootstrap Confidence Intervals take a different approach - they involve taking random subsamples with replacement from the data itself to produce estimates of uncertainty.

There are a variety of methods of estimating the confidence intervals; for example to estimate the 95th percentile confidence interval you could use the 2.5th and 97.5th percentiles of the subsample means.

The most effective method is Brad Efron's [BCa (Bias-Corrected and Accelerated) bootstrap](http://hal.case.edu/~robrien/Efron87Better%20Bootstrap%20Confidence%20Intervals.pdf) - it automatically transforms the data to correct for bias and skew.
In fact to deal with issues with long tailed data like we saw with page views and average visit time, statisticians sometimes concoct transforms that will make the distribution more normal.
The advantage of the BCa Bootstrap is that it is *automatic* and *robust* - without additional effort it works well on a wide range of datasets.
In some sense it is better for statistical testing than normal approximations because it makes less assumptions and works on smaller sample sizes.

For more details see [Bootstrap Confidence Intervals (DiCiccio, Efron)](https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214), or this [practical guide for medical students](https://m.tau.ac.il/~saharon/Boot/10.1.1.133.8405.pdf). I tried a few other bootstrap methods with this data (percentile, basic and normal) and found that the BCa had better coverage than all of them.

The BCa Bootstrap seems like a great choice for constructing confidence intervals.
While it's a little more opaque and requires more computation time than normal intervals, it gives better results on a wide range of data.
