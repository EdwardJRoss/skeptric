{
  "hash": "1e19fc8ff295ad4382c6a03ed614e2db",
  "result": {
    "markdown": "---\ntitle: \"Estimating Bias in a Coin with Bayes Rule\"\nauthor: \"Edward Ross\"\ndate: \"2020-05-08T20:00:20+10:00\"\nfeature_image: \"/images/posterior_8_flips.png\"\noutput: html_document\n---\n\n\n\n\nI wanted to work through an example of applying Bayes rule to update model paremeters based on toy data\nThis example comes from [Kruschke's *Doing Bayesian Data Analysis*](https://doingbayesiandataanalysis.blogspot.com/), Section 5.3.\n\nThe model is that we have a coin and we're trying to estimate the bias in the coin, that is the probability that it will come up heads when flipped.\nFor simplicity we assume the bias, `theta` is a multiple of 0.1.\nWe take a triangular prior centred at 0.5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple model; parameter can only be a multiple of 0.1\ntheta <- seq(0, 1, by=0.1)\n\n\n# Prior is a triangle function\nprior <- c(0, 0.04, 0.08, 0.12, 0.16, 0.2, 0.16, 0.12, 0.08, 0.04, 0)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/triangle_prio-1.png){width=672}\n:::\n:::\n\n\n## Impact of seeing a head\n\nLet's consider how the model distribution changes by Bayes' rule in the case we flip the coin once and see a head.\n\nThe likelihood of getting a head given `theta`, is just `theta` (because it's the head bias by definition)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlikelihood <- theta\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThen Bayes' rule says the posterior is proportional to the prior and the likelihood.\nWe can ignore the constant of proportionality by normalising it to 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Posterior given the head\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe posterior has moved markedly to the right, it's more likely the coin is head biased now.\nThe expected of bias has changed from 0.5 with the prior distribution, to 0.58 after incorporating the data of having seen one heads.\nThe most likely value for `theta` is still 0.5.\n\n\n# Second flip of the coin\n\nAfter our first flip our new prior is the posterior given the first flip was heads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprior <- posterior\n```\n:::\n\n\nSuppose we flipped another heads, then we can apply Bayes' rule again to get the posterior given two heads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThe most likely value of `theta` is now 0.7, and the expectation value is 0.637931.\n\n\n## What if we had head then tails?\n\nSuppose the second flip gave tails instead of heads.\nThen our likelihood of tails is the likelihood of not flipping heads, `1 - theta`.\nThen we can get the posterior by applying Bayes' rule as before\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlikelihood <- 1 - theta\n\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nExpectation value is 0.5, as you would expect by symmetry.\nNotice that the distribution is less spread around 0.5 than our original prior.\n\n# After 8 coin flips\n\nWhat would our posterior look like after 8 coin flips?\nLet's look at each possible result of 8 coin flips, from 0 heads and 8 tails to 8 heads and 0 tails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_flips <- 8\nheads <- seq(0, num_flips)\ntails <- num_flips - heads\n```\n:::\n\n\nIt would also be useful to contrast how the posterior is to if we had assumed a uniform prior.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuniform_prior <- rep(1/11, 11)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nLet's combine all the scenarios in a dataframe, calculating the posterior after the 8 flips.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <-\n  tibble(heads=rep(heads, each=length(theta)),\n       tails=rep(tails, each=length(theta)),\n       theta=rep(theta, num_flips+1),\n       uniform_prior = rep(uniform_prior, num_flips+1),\n       triangle_prior=rep(prior, num_flips + 1)) %>%\n  mutate(label = paste0(heads, \" heads, \", tails, \" tails\"),\n         likelihood = (theta ^ heads) * ((1-theta) ^ tails)) %>%\n  gather(\"prior_model\", \"prior\", ends_with('_prior')) %>%\n  group_by(heads, prior_model) %>%\n  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>%\n  ungroup()\n```\n:::\n\n\nThe likelihood is `theta^heads * (1-theta)^tails`.\nNotice how the most likely value for `theta` depends on the outcomes.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAfter 8 flips the posterior distribution for `theta` changes much more depending on the outcome.\nNotice that the uniform prior moves more quickly with more data, and the triangular prior is more confident when the outcomes are close to equal.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThis is just a simple example but I found running through the calculations helps me understand the idea of estimating parameter distributions rather than just estimating the most likely parameter.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}