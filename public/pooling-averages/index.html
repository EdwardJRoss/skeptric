<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-12-20">

<title>skeptric - Better than Average Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">skeptric</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/EdwardJRoss"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Better than Average Statistics</h1>
  <div class="quarto-categories">
    <div class="quarto-category">statistics</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 20, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="better-than-average-statistics" class="level1">
<h1>Better than Average Statistics</h1>
<p>Averaging by group are the standard tool for summarising and comparing groups. At SEEK we use averages to inform our customers on the job market and facilitate a fair and efficient employment marketplace. We inform employers and job seekers salary expectations for a role by showing the typical salary range advertised in the market. We help people find potential career paths in <a href="https://www.seek.com.au/career-advice">SEEK Career Advice</a> using average career transitions other people have made. We help employers with their talent and sourcing strategies by publishing the average supply of jobs and applications in <a href="https://www.seek.com.au/about/news/seek-employment-reports/">SEEK Employment Reports</a>.</p>
<p>There are more accurate ways to calculate group averages than adding up the values and dividing by the count in each group separately. Using information about how the groups are related can produce better estimates, informing better decisions. Even when the groups are independent, shrinking the averages closer together leads to a more accurate overall estimate by reducing the variance.</p>
<p>Averages can be improved by shrinking the estimates on small groups towards the other groups. In a group with few data it’s much more likely that the group estimate will be far from the true average. Estimates on small groups are unstable; halving the group size doubles the sample variance. Adjusting estimates to be closer to the centre of the entire dataset reduces the variance substantially at the cost of increasing bias. In many applications there is a long tail of small groups and the overall accuracy can be improved substantially.</p>
<p>A simple way to reduce variance is to estimate small groups with the overall average across all data points. Groups below some threshold size are estimated with the overall average. Groups above the threshold size are estimated by their independent group average.</p>
<p>A more effective way to improve estimates is to treat the groups averages as random variables drawn from a common distribution. Each data point from one group informs the common distribution, which updates the estimates of other groups. The estimates for smaller groups, where there is less information, end up closer to the centre of this distribution. This leads to substantially better estimates, and is generally a better default way to calculate group averages.</p>
<p>More complex models may produce better estimates, but are more costly to develop and maintain. Reformulating calculating averages on groups as a machine learning problem enables more sophisticated approaches. Related data sources can inform relationships between the groups which can be incorporated into the model. These methods can tease out subtle relationships that enable more fine grained comparisons between groups. However they can also easily overfit to noise and produce worse estimates, especially when data is scarce.</p>
<p>The best method to calculate averages on groups depends on the situation. When there is low variance in the groups the simplest model of calculating individual group averages works well. When there are groups with a high variance relative to the variance between groups then shrinking will lead to better estimates. The complexity worth having to achieve better accuracy depends on how much that accuracy will inform better outcomes.</p>
<p>These different methods can be illustrated with an example from SEEK’s marketplace.</p>
<section id="show-me-the-money" class="level2">
<h2 class="anchored" data-anchor-id="show-me-the-money">Show me the money</h2>
<p>When an employer posts a job ad on SEEK they can choose whether to show the salary range for the role. For some kinds of roles it is very common to show the salary (such as $20 - $30 an hour), and for other kinds it is not. We can calculate the proportion of employers who show salary for each role.</p>
<p>Below is 12 roles, taken from the thousands of role titles on SEEK, with the percentage of employers who show the salary. For example choosing a random employer who hires dentists, and then choosing a random dentist ad on SEEK from that employer, there is a 13% chance the salary will be shown. The population salary shown is an estimate, but it is within a few percentage points of the true probability because of the large number of advertisers on SEEK. We’ve taken a small sample of 450 job ads on SEEK, and want to use them to estimate the population salary shown.</p>
<table class="table">
<colgroup>
<col style="width: 28%">
<col style="width: 29%">
<col style="width: 14%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Role Title</th>
<th>Population Salary Shown</th>
<th>Sample Ads</th>
<th>Sample Ads with Salary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dentist</td>
<td>13%</td>
<td>30</td>
<td>4</td>
</tr>
<tr class="even">
<td>Enrolled Nurse</td>
<td>21%</td>
<td>7</td>
<td>1</td>
</tr>
<tr class="odd">
<td>Dental Hygienist</td>
<td>22%</td>
<td>2</td>
<td>1</td>
</tr>
<tr class="even">
<td>Registered Nurse</td>
<td>22%</td>
<td>47</td>
<td>9</td>
</tr>
<tr class="odd">
<td>General Practitioner</td>
<td>23%</td>
<td>20</td>
<td>7</td>
</tr>
<tr class="even">
<td>Medical Receptionist</td>
<td>27%</td>
<td>130</td>
<td>39</td>
</tr>
<tr class="odd">
<td>Dental Assistant</td>
<td>28%</td>
<td>105</td>
<td>41</td>
</tr>
<tr class="even">
<td>Dental Receptionist</td>
<td>29%</td>
<td>27</td>
<td>6</td>
</tr>
<tr class="odd">
<td>Chiropractor</td>
<td>31%</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Physiotherapist</td>
<td>38%</td>
<td>50</td>
<td>23</td>
</tr>
<tr class="odd">
<td>Occupational Therapist</td>
<td>40%</td>
<td>28</td>
<td>16</td>
</tr>
<tr class="even">
<td>Chiropractic Assistant</td>
<td>43%</td>
<td>4</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>The sample has 30 Dentist job ads, from different employers, 4 of which show the salary. The simple average 4/30 is around 13% which happens to be very close to the whole population probability. For an enrolled nurse the simple average of 1/7 is 14% which is substantially lower than the probability of 21%. Because the sample is so small the estimates have high variation.</p>
<p>To compare the accuracy of different estimates requires a quantitative metric. We will use the Root Mean Squared Error (RMSE) for each group because it is easy to interpret as the typical percentage point error. The <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss">logistic loss</a> (or equivalently binary cross entropy) is more appropriate for estimating proportions, but in this example it does not change the conclusions. For some applications the error should be weighted, for example by volume, but we will treat each group as equally important.</p>
<p>In this example there are only 12 groups, but in many applications real at SEEK we have tens of thousands or more. The techniques here work much better the more groups there are, but 12 is enough to show the impact and make it easy to follow. These group estimates have very high variance which makes these techniques particularly effective. In contrast when estimating average salary, which may vary by 20% within a role, but between roles can vary from $20 an hour to over $100 an hour, there is little benefit in shrinking the estimates.</p>
</section>
</section>
<section id="baseline-averages" class="level1">
<h1>Baseline Averages</h1>
<p>In any machine learning problem it is good to start with a simple baseline. The best constant model is the overall proportion of ads with salary shown. That is the 149 ads with salary shown, divided by 450 ads in the sample, which is 33%.</p>
<table class="table">
<colgroup>
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 31%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Role Title</th>
<th>Overall Sample Average</th>
<th>Population Salary Shown</th>
<th>Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dentist</td>
<td>33%</td>
<td>13%</td>
<td>20%</td>
</tr>
<tr class="even">
<td>Enrolled Nurse</td>
<td>33%</td>
<td>21%</td>
<td>12%</td>
</tr>
<tr class="odd">
<td>Dental Hygienist</td>
<td>33%</td>
<td>22%</td>
<td>11%</td>
</tr>
<tr class="even">
<td>Registered Nurse</td>
<td>33%</td>
<td>22%</td>
<td>11%</td>
</tr>
<tr class="odd">
<td>General Practitioner</td>
<td>33%</td>
<td>23%</td>
<td>10%</td>
</tr>
<tr class="even">
<td>Medical Receptionist</td>
<td>33%</td>
<td>27%</td>
<td>6%</td>
</tr>
<tr class="odd">
<td>Dental Assistant</td>
<td>33%</td>
<td>28%</td>
<td>5%</td>
</tr>
<tr class="even">
<td>Dental Receptionist</td>
<td>33%</td>
<td>29%</td>
<td>4%</td>
</tr>
<tr class="odd">
<td>Chiropractor</td>
<td>33%</td>
<td>31%</td>
<td>2%</td>
</tr>
<tr class="even">
<td>Physiotherapist</td>
<td>33%</td>
<td>38%</td>
<td>-5%</td>
</tr>
<tr class="odd">
<td>Occupational Therapist</td>
<td>33%</td>
<td>40%</td>
<td>-7%</td>
</tr>
<tr class="even">
<td>Chiropractic Assistant</td>
<td>33%</td>
<td>43%</td>
<td>-10%</td>
</tr>
</tbody>
</table>
<p>Squaring the errors, calculating the mean and taking the square root gives an RMSE of 9.8%. This estimate is about 10 percentage points from the population values.</p>
<p>The other obvious baseline is calculating the average for each group individually. For Chiropractor there are no ads in the sample, and so there is no average to calculate. A reasonable estimate is the overall sample average of 33%; it’s a better guess than 0%, 50% or 100%.</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 21%">
<col style="width: 35%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th>Role Title</th>
<th>Group Average</th>
<th>Population Salary Shown</th>
<th>Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Dentist</td>
<td>13%</td>
<td>13%</td>
<td>0%</td>
</tr>
<tr class="even">
<td>Enrolled Nurse</td>
<td>14%</td>
<td>21%</td>
<td>-7%</td>
</tr>
<tr class="odd">
<td>Dental Hygienist</td>
<td>50%</td>
<td>22%</td>
<td>28%</td>
</tr>
<tr class="even">
<td>Registered Nurse</td>
<td>19%</td>
<td>22%</td>
<td>-3%</td>
</tr>
<tr class="odd">
<td>General Practitioner</td>
<td>35%</td>
<td>23%</td>
<td>12%</td>
</tr>
<tr class="even">
<td>Medical Receptionist</td>
<td>30%</td>
<td>27%</td>
<td>3%</td>
</tr>
<tr class="odd">
<td>Dental Assistant</td>
<td>39%</td>
<td>28%</td>
<td>11%</td>
</tr>
<tr class="even">
<td>Dental Receptionist</td>
<td>22%</td>
<td>29%</td>
<td>-7%</td>
</tr>
<tr class="odd">
<td>Chiropractor</td>
<td><em>33%</em></td>
<td>31%</td>
<td><em>2%</em></td>
</tr>
<tr class="even">
<td>Physiotherapist</td>
<td>46%</td>
<td>38%</td>
<td>8%</td>
</tr>
<tr class="odd">
<td>Occupational Therapist</td>
<td>57%</td>
<td>40%</td>
<td>17%</td>
</tr>
<tr class="even">
<td>Chiropractic Assistant</td>
<td>50%</td>
<td>43%</td>
<td>7%</td>
</tr>
</tbody>
</table>
<p>This gives an RMSE of 11.4%, worse than the overall average. Because many of the samples are small the variance in the estimates is very high leading to larger errors. In particular for Dental Hygienist there are only 2 ads, 1 with salary and so the sample estimate of 50% is much too high.</p>
<p>The two baselines are different extremes of the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a>. The overall average is the lowest variance estimate, all predictions are the same, but the group estimates are biased away from their true values to the overall average. The group average is the lowest bias estimate, but the variance is high and it gives a worse estimate than the overall average. The group average makes its largest errors on smaller groups, whereas the overall average makes errors independent of the size of the group. A better estimate would be somewhere between the two extremes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/group_vs_overall_average_squared_error.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Squared error of group average vs overall average</figcaption><p></p>
</figure>
</div>
</section>
<section id="fallback-method" class="level1">
<h1>Fallback Method</h1>
<p>The fallback method switches from the overall average for small groups to the group average for large groups. When calculating the group average we imputed the average for a Chiropractor, with no ads, as the overall average. For other small groups, like Dental Hygienist with only 2 ads, the overall average is a better estimate as well. Assigning any group below some threshold size to the overall average reduces the largest part of the variance.</p>
<p>The threshold could be chosen through statistical heuristics. Ideally the variance within groups would be balanced against the variance between groups. The variance between the groups can be estimated as the standard deviation of the group averages, which is 15 percentage points. To estimate the binomial sample with standard deviation <span class="math inline">\(\sigma\)</span> requires a sample size of <span class="math inline">\(n=\frac{p(1-p)}{\sigma^2}\)</span>. Using the overall average <span class="math inline">\(p=33\%\)</span> and <span class="math inline">\(\sigma=15 {\rm\ p.p}\)</span> which gives a sample size of 10. Including the small groups in the calculation of standard deviation inflates the standard deviation, which reduces the sample size, so we may want to adjust it to be larger. Alternatively cross-validation could be used to estimate the threshold. In this case it’s not too sensitive and 10 works reasonably well.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/fallback_average_squared_error.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Comparing squared error of Fallback with Group Average and Overall Average</figcaption><p></p>
</figure>
</div>
<p>The RMSE of the fallback average is 8.1%, substantially lower than the overall average RMSE of 9.8% and the group average of 11.4%.</p>
<p>The change in the estimate across the threshold is very sharp. For groups below the threshold size all the specific data from that group is ignored. For groups above the threshold size all the data from other groups is ignored. A better approach would be to interpolate between the two extremes.</p>
</section>
<section id="partial-pooling-model" class="level1">
<h1>Partial Pooling Model</h1>
<p>A weighted average can balance the information from within each group and the information from other groups. In each group we have <span class="math inline">\(n\)</span> ads, <span class="math inline">\(k\)</span> of which have salary shown, giving a group proportion of <span class="math inline">\(p=k/n\)</span>. For a group with no data, such as Chiropractor, the proportion should be <span class="math inline">\(P\)</span> close to the overall average. For groups with some data we could interpolate between the group proportion, and the prior proportion <span class="math inline">\(P\)</span> by using some effective number of ads <span class="math inline">\(N\)</span>. Then the estimate for the group is the weighted average:</p>
<p><span class="math display">\[\frac{N}{N+n} P + \frac{n}{N+n} p\]</span></p>
<p>When there are no ads the estimated average is <span class="math inline">\(P\)</span>, and as the number of ads increases it gets closer to the group average, <span class="math inline">\(p\)</span>.</p>
<p>The values for <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> can be estimated by a statistical model. The probability that a role has salary shown is assumed to be randomly drawn from a statistical distribution. In particular we assume it is a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> peaked at <span class="math inline">\(P\)</span>, with strength <span class="math inline">\(N+2\)</span>. For a given <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> we can simulate 12 random draws for each role’s probability of showing the salary, and then for each of the 450 jobs in the sample randomly choose whether the salary is shown with the group probability. Doing this very many times we could look at what percentage of draws had exactly our sample data, which gives the probability of the data given <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span>. A good estimate of <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> is the one for which the actual observed data is most likely.</p>
<p>The most likely values of <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> can be calculated efficiently. The strategy of directly calculating the likelihood of the data is computationally infeasible, even for this small dataset. However for each <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span> it is <a href="https://skeptric.com/pooling-proportions-empirical-bayes/">possible to calculate</a> the probability as a closed form expression. This can be efficiently numerically optimised even for a very large number of groups. For this data below is a plot showing the likelihood for some different values of <span class="math inline">\(N\)</span> and <span class="math inline">\(P\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/likelihood_heirarchical_pooling.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Likelihood at different values of N and P</figcaption><p></p>
</figure>
</div>
<p>This gives most likely values of <span class="math inline">\(P=30\%\)</span> and <span class="math inline">\(N=100\)</span>. The most likely probability is slightly different to the overall average of 33%. The overall average is the centre of the groups weighted by size, whereas <span class="math inline">\(P\)</span> is the centre of the groups weighted equally. Partial pooling then shrinks the estimates towards <span class="math inline">\(P=30\%\)</span>, weighting this prior estimate by <span class="math inline">\(N=100\)</span>. A Dental Assistant, with 105 ads, shrinks almost halfway between the group average and 30%. A Dental Hygienist with 2 ads shrinks almost all the way to 30%.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/partial_pooling.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Partial Pooling shrinks group averages towards the centre</figcaption><p></p>
</figure>
</div>
<p>The RMSE of Partial Pooling is 7.3%, a full 2.5 percentage points lower than the overall average, and 0.8 percentage points lower than the fallback method. It does particularly well for roles with many ads where it always beats the worst of the group average and the overall average.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/partial_pooling_squared_error.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Partial Pooling has lower mean error than Group or Overall Average</figcaption><p></p>
</figure>
</div>
<p>This partial pooling technique is a robust and efficient way to calculate group averages, and is a better default baseline. It was popularised by Efron and Morris in the 1970s, for example in <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1975.10479864">Data Analysis Using Stein’s Estimator and its Generalizations</a>. They showed that even when the groups are truly <em>independent</em> it produces better estimates than the separate group average, at least when the groups are the same size and there are at least 4 of them. Some good introductions to these techniques are George Casella’s <a href="https://www.jstor.org/stable/2682801">An Introduction to Empirical Bayes Data Analysis</a>, David Robinson’s <a href="http://varianceexplained.org/r/empirical-bayes-book/">Introduction to Empirical Bayes</a>, and Chapter 7 of Efron and Hastie’s <a href="https://web.stanford.edu/~hastie/CASI/">Computer Age of Statistical Inference</a>.</p>
<p>This modelling approach relates the groups to each other, but treats them all the same way. But we know that a Dentist and a Dental Assistant have a different kind of relationship to a Dentist and a Physiotherapist. There isn’t much information in this data to inform that relationship. To model these kinds of relations requires using additional data.</p>
</section>
<section id="clustering" class="level1">
<h1>Clustering</h1>
<p>Whether a role has a salary shown depends partly on the policies of the employer. Two roles that are commonly posted by the same kinds of employer may have a similar probability of showing the salary. This similarity can be represented as a matrix of how likely an employer is to have advertised for one kind of role, given they have advertised for another.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/role_cross_advertise.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Heatmap showing likelihood of a employer advertising a job for a role given they have advertised a job for another role</figcaption><p></p>
</figure>
</div>
<p>The roles can be clustered into groups of similar roles, which can be used in other models. In the cross-advertising matrix above the dark boxes show clusters formed by k-means with 5 groups. The averages could be calculated directly on each cluster, instead of each group. The fallback model can be extended to use the clusters as another layer between the individual groups and the overall average. The partial pooling model could pool the role titles towards each cluster, and optionally the clusters to their centre.</p>
<p>Using clustering does not produce significantly better estimates than partial pooling on this dataset. This could be because the underlying cross-advertising data doesn’t align with the outcome of showing salary, and we need to use a different dataset. It could be because the particular numerical representation and clustering algorithm used does not extract the relationship. Maybe a different model on the clusters would do better. There are many ways to model this, and it is easy to overfit this small dataset.</p>
<p>Clustering puts hard boundaries between groups in different clusters. The clusters are simple and interpretable, but the relationship between groups in different clusters is lost. For example Registered Nurses are somewhat likely to be employed in the same organisation as a General Practitioner, but they are in completely separate clusters. An alternative is to model on the quantitative relationships directly.</p>
</section>
<section id="embeddings" class="level1">
<h1>Embeddings</h1>
<p>The similarity matrix of roles can be used directly as features in another machine learning method. Each discrete group is embedded as a vector in some space. These embeddings can be used as continuous predictors in a variety of models from linear and logistic regression, to boosted trees and neural networks. The use of external data to form these categorical embeddings is as a form of transfer learning. The cross-advertising matrix is a form of embedding, and can be visualised by projecting into a two dimensional space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/embedding_cross_advertising.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Two dimensional embedding of roles based on cross-advertisement</figcaption><p></p>
</figure>
</div>
<p>For this example these embeddings do not seem to produce better estimates. There are even more choices of model for embeddings than for clustering. A penalised logistic regression worked very poorly in this example, doing worse than the overall average. Perhaps there is a model that would work better, but with so few groups there is unlikely to be significant benefit. In examples with many more groups we have seen examples that produce better estimates.</p>
</section>
<section id="averages-as-machine-learning" class="level1">
<h1>Averages as Machine Learning</h1>
<p>Through this example we have gone from calculating individual group averages, through to fitting a complex model on categorical embeddings. Some methods have significantly improved the estimates on this dataset, but others have made them much worse. In real problems we don’t know what the actual population averages are, and have to be careful with evaluating different methods. Considering calculating group averages as a machine learning problem gives a general framework for producing and evaluating estimates.</p>
<p>Calculating a group average is a regression problem where all the predictors are categorical. Typical measures of central tendency such as the mean, median and proportion can be reframed as estimating a value measured with independent random errors from a statistical error distribution. Equivalently this is finding the model parameters that minimise the related loss function, the negative log likelihood of the error distribution.</p>
<table class="table">
<thead>
<tr class="header">
<th>Measure of Centrality</th>
<th>Statistical Error Distribution</th>
<th>Loss Function</th>
<th>Loss at Average Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean</td>
<td>Normal</td>
<td>Mean Squared Error</td>
<td>Standard Deviation</td>
</tr>
<tr class="even">
<td>Median</td>
<td><a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace</a></td>
<td>Mean Absolute Error</td>
<td><a href="https://en.wikipedia.org/wiki/Average_absolute_deviation#Mean_absolute_deviation_around_the_median">MAD median</a></td>
</tr>
<tr class="odd">
<td>Proportion</td>
<td>Binomial</td>
<td><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross Entropy</a></td>
<td></td>
</tr>
</tbody>
</table>
<p>Treating all of the categorical predictors as independent will overfit the small groups when their variance is high. Calculating the proportion of jobs that showed salary for each group led to very poor estimates on small groups. Falling back to the overall average on small groups reduced the error by decreasing variance significantly for a small increase in bias; this is similar to the common practice of putting small groups together into an “Other” category.</p>
<p>Partial pooling is a general technique that reduces the variance on small groups by setting an appropriate prior probability on the groups. This can be efficiently calculated using Empirical Bayes methods, and generally outperforms other simple methods of calculating averages. This should be considered a standard baseline for estimating group averages.</p>
<p>When there is additional related data more complex models can be built. Additional data can be used to cluster or embed the groups, and these can be used as features in more complex models. However this opens up many choices in what data to use, how to represent it, and what model to use. This can be more costly to interpret, and easily overfit small datasets.</p>
<p>Better estimates of group averages enable more granular comparisons. At SEEK we need to understand the employment market across roles, locations, customer segments, and time. The full cross product of these factors is very sparse, and simple averages produce very noisy estimates. Improving our estimates of quantities across these groups helps us make better decisions, and evaluate the decisions we’ve made.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>