[
  {
    "objectID": "chompjs/index.html",
    "href": "chompjs/index.html",
    "title": "Chompjs for parsing tricky Javascript Objects",
    "section": "",
    "text": "Chompjs converts a javascript string into something that json.loads. It’s a little less strict than demjson; for example {\"key\": undefined} will be converted by chompjs.parse_js_object to {\"key\": \"undefined\"} (contrast with demjson {\"key\": demjson.undefined} which preserves the type). However it’s much faster, about 20x on the tests I’ve done, which makes it a much better drop-in replacement for json.loads on messy data."
  },
  {
    "objectID": "unique-counts-sql/index.html",
    "href": "unique-counts-sql/index.html",
    "title": "Checking for Uniques in SQL",
    "section": "",
    "text": "The easiest way is to compare the count of a column with the distinct count. They will be the same only if the count is unique.\nselect count(user_id) as num_user,\n       count(distinct user_id) as num_distinct_user\nfrom users\nIt’s generally good practice to check if there are any nulls in the column as well. Checking whether two numbers is the same requires a little mental effort, so I get the computer to do the calculation and just check if the number is 0.\nselect count(user_id) as num_user,\n       count(*) - count(user_id) as num_missing_user\n       count(user_id) - count(distinct user_id) as num_duplicate_user\nfrom users\nIf I’m ok with a small proportion number of rows missing or being repeated it can be more meaningful to report them as a ratio of the number of rows rather than an absolute number. If there are no missing/duplicate rows then the last two columns will be 1. I have taken to using 1e0 in divions because Presto rounds integer division, in some databases you could simply use count(*) in the denominator.\nselect count(*) as n, \n       count(user_id)/sum(1e0) as prop_not_missing,\n       count(distinct user_id)/sum(1e0) as prop_distinct\nfrom users\nAnother useful strategy is to find some examples of duplicate rows; they can be useful for diagnosing what’s going wrong. If the rows are distinct then this query will return no results, otherwise it will give the most frequently repeated rows.\nselect user_id, count(*) as n\nfrom users\ngroup by user_id\nhaving count(*) > 1\norder by n desc\nlimit 10\nOne benefit of this approach is it can be extended to multiple columns. For example we might have a table that summarises user activity by day that is keyed on user_id and action_date.\nselect user_id, action_date, count(*) as n\nfrom daily_users\ngroup by user_id, action_date\nhaving count(*) > 1\norder by n desc\nlimit 10\nWe can build this into a single row summary as before by putting this into a subquery.\nselect sum(n) as num_user_days,\n       sum(n) - count(*) as num_duplicate_user_days,\n       sum(case\n             when user_id is null \n               or action_date is null\n             then n\n           end) as num_missing_user_days\nfrom (\n    select user_id, action_date, count(*) as n\n    from daily_users\n    group by user_id, action_date\n)\nIt’s easy to create ratios as before.\nselect sum(n) as num_user_days,\n       sum(n) / sum(1e0) as prop_unique_user_days,\n       sum(case\n             when user_id is not null \n               and action_date is not null\n             then n\n           end)  / sum(n*1e0) as prop_not_missing_user_days\nfrom (\n    select user_id, action_date, count(*) as n\n    from daily_users\n    group by user_id, action_date\n)\nYou can even combine this to also get the most frequent duplicate in a single query for diagnosis.\nselect sum(n) as num_user_days,\n       sum(n) / sum(1e0) as prop_unique_user_days,\n       sum(case\n             when user_id is not null \n               and action_date is not null\n             then n\n           end)  / sum(n*1e0) as prop_not_missing_user_days\n       max(case when rn = 1 then user_id end) as most_common_pair_user,\n       max(case when rn = 1 then action_date end) as most_common_pair_action,\n       max(n) as n_most_common_pair,\n       max(n)/sum(1e0) as prop_most_common_pair\nfrom (\n    select user_id, action_date, n,\n    from (\n           row_number() over (order by n desc) as rn\n        from (\n            select user_id, action_date, count(*) as n\n            from daily_users\n            group by user_id, action_date\n        )\n    )\n)\nBy this stage it becomes moderately complex to maintain. You could put it as an SQL template in your programming language of choice, or build it as a dbplyr function."
  },
  {
    "objectID": "presto-athena-emacs/index.html",
    "href": "presto-athena-emacs/index.html",
    "title": "Presto and Athena CLI in Emacs",
    "section": "",
    "text": "After considering the alternatives I ended up using Mastering Emacs guide to hack together my own comint modes for Presto and Athena and it works pretty well. These use the Presto CLI and Athena CLI to interact with the databases; you will need to have them set up. To connect with emacs you need to add the files to your load-path, and require the package (presto or athena), and configure the path to the binary and the arguments.\n(add-to-list 'load-path (expand-file-name \"lisp\" user-emacs-directory))\n(require 'presto)\n;; Configure path and arguments\n(setq presto-cli-file-path \"~/bin/presto\")\n;; Here put whatever arguments you would use to connect through the CLI\n(setq presto-cli-arguments '(\"--server\"\n                                \"localhost:8889\"\n                                \"--catalog\"\n                                \"hive\")\nThen to use it in Emacs you just invoke M-x run-presto to start a presto session in a buffer called *Presto*. If you’re in a buffer with cursor over some SQL you can send it to be evaluated using M-x presto-send. The rest of this post covers the rationale and the details.\n\nWays to integrate SQL into Emacs\nThe best way to use SQL from Emacs is sql-mode; there are good guides on using it to manage multiple database connection configurations and easily send chunks of SQL from a file to the database. However I really struggled to add a Presto backend and found the source documentation on adding a product didn’t work for me. More recently someone else has created a sql-mode backend for presto and I will look to migrate to that.\nAn alternative is Emacs EDBI, which uses Perl’s DBI (DataBase Interface) to connect to databases. This means there is a very large number of databases available and you can connect via ODBC drivers (which your database likely has for your platform). However installing Perl dependencies is a bit of trouble, and the mode has a detailed workflow and keybindings which I don’t find intuitive (especially as an Emacs evil user). I think it could be a good solution if you spend some time configuring it to be comfortable for you, but I haven’t invested in it.\nSo I resolved to make my own comint mode following Mastering Emacs comint mode guide. Comint mode allows integrating a command line interface through Emacs, which is very flexible and convenient to use. This worked pretty well with Presto and Emacs\n\n\nSetting up Command Line Interfaces\nPresto comes with a good CLI written in Java. You should be able to download it and just run it with Java; try presto --help to see all the configuration options.\nThe story for Athena is much worse; Amazon doesn’t give you a nice CLI just a crummy web GUI and an onerous interface through AWS CLI (you have to do your own polling). There is an unmaintained Athena CLI that works well enough, however it’s dependencies are broken. Make sure you first pip install cmd2 == 0.8.0 (and maybe tabulate == 0.8.3) before trying to run it or it will be broken. Once it’s installed run athena --help to see all the configuration options; in particular you will need to set a --bucket as a S3 staging bucket for all your query results. Note that this uses boto3 to connect to AWS and so you should use boto3 configuration for AWS. In particular the environment variables like AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY or AWS_PROFILE are useful for making sure you can connect with the right role.\nMake sure you can run your CLI from the terminal before you try to integrate with emacs.\n\n\nIntegrating the CLIs into Emacs\nOnce you can connect and execute queries from the command line you want to encode that into Emacs by defining the path to the program and the arguments. Here’s an example for Presto:\n(defvar presto-cli-file-path \"~/bin/presto\"\n  \"Path to the program used by `run-presto'.\")\n\n(defvar presto-cli-arguments '(\"--server\"\n                                  \"localhost:8889\"\n                                  \"--catalog\"\n                                  \"hive\")\n  \"Commandline arguments to pass to `presto-cli'.\")\nThen we create a run-presto function to start the process:\n(defun run-presto ()\n  \"Run an inferior instance of `presto-cli' inside Emacs.\"\n  (interactive)\n  (let* ((presto-program presto-cli-file-path)\n         (buffer (comint-check-proc \"Presto\")))\n    ;; pop to the \"*Presto*\" buffer if the process is dead, the\n    ;; buffer is missing or it's got the wrong mode.\n    (pop-to-buffer-same-window\n     (if (or buffer (not (derived-mode-p 'presto-mode))\n             (comint-check-proc (current-buffer)))\n         (get-buffer-create (or buffer \"*Presto*\"))\n       (current-buffer)))\n    ;; create the comint process if there is no buffer.\n    (unless buffer\n      (apply 'make-comint-in-buffer \"Presto\" buffer\n             presto-program '() presto-cli-arguments)\n      (presto-mode))))\nThis starts the new process in presto-mode which we need to define as a derived mode of comint-mode. See the Mastering Emacs guide or the source for details.\n\n\nSetting the Pager\nBy default less is used as the pager which doesn’t work well in Emacs (you’ll see WARNING: terminal is not fully functional messages).\nOne solution is to turn off the pager and let Emacs terminal handle all the buffering, using the special environment variables for Presto and Athena CLIs.\n(setenv \"PRESTO_PAGER\" \"\")\n(setenv \"ATHENA_PAGER\" \"\")\nUnfortunately I found Emacs comint gets really slow for large results (which terminals handle well), and so if I forgot a limit on a query I could lock up Emacs fetching results. So I used head to only get the top 1000 results; unfortunately there’s no clear sign if the results have been truncated, but for me the tradeoff was worth it.\n(setenv \"PRESTO_PAGER\" \"head -n 1000\")\n(setenv \"ATHENA_PAGER\" \"head -n 1000\")\nI will still sometimes find it’s slow if I have really long rows; but I don’t know how to improve that.\n\n\nSending text to Presto/Athena\nOften I like to work in an SQL file and interactively send queries to Presto/Athena. While I can copy and paste the query into the *Presto* or *Athena* buffer, it’s much nicer to do it with a single command.\nIt’s straightforward to send it using comint-send-region (which handles some magic in how newlines are sent to the comint process).\n(defun presto-send ()\n  \"Send the current region or paragraph to Presto process.\"\n  (interactive)\n  (let ((beg-end (if (use-region-p)\n                    (cons (region-beginning) (region-end))\n                    (er/paragraph-extents))))\n    (comint-send-region \"*Presto*\" (car beg-end) (cdr beg-end))))\nThe er/paragraph-extents function will grab the paragraph under the cursor to send, so if your query is separated by newlines and your cursor is anywhere in it this will do the right thing.\n(defun er/paragraph-extents ()\n  \"Return a cons cell with beginning and end of paragraph.\"\n  (save-excursion\n    (forward-paragraph)\n    (let ((end (point)))\n      (backward-paragraph)\n      (cons (point) end))))\n\n\nQuality of life improvements\nThat’s the bare minimum to use it, but I made some changes to make it more usable.\nI copied the Presto keywords and added a font-lock to highlight them. This makes it easier to spot spelling mistakes with keywords in the buffer\nPresto CLI emits some ANSI control sequences to move the cursor which Emacs doesn’t understand, so I followed oleksandrmanzyuk’s guide to filtering them out.\n\n\nPain points\nWhile this works alright there are still a few pain points.\nIt would be nice to bind presto-send to a key, but I don’t know what mode to do it with because not all SQL is presto. The sql-mode has a nice way of solving this (and more) with the idea of attaching a process to an editing buffer, but this is pretty hard to implement. The nicest solution would probably be to integrate with sql-mode.\nBy default long lines are wrapped; I always invoke toggle-truncate-lines the first time that happens but that should be baked into the mode by default.\nThere is some weird behaviour with scrolling through command history. The first time I invoke comint-previous-input with M-p the cursor moves back one space. If I press it again I get the error message not-at-command-line. So I then move the cursor right and invoke comint-previous-input again and it retrieves the 2nd to last input. I can then get the last input by invoking comint-next-input with M-n. I haven’t worked out how to work around this so I’ve got used to this, and tend to send commands rather than using comint more often. Also history is lost between invocations.\nPresto will show the progress of the query as it executes. In the terminal this will only happen in a single paragraph as the cursor is moved around with ANSI control sequences, and the text is overwritten on update. However Emacs can’t deal with these (in fact these are the ones that we filtered out above), and so we will get a new paragraph for every update. For a long running query the comint buffer can fill up with these updates. It should be possible to remove this noise in the comint mode, but I haven’t worked out how.\nPresto processes can’t be killed with (comint-interrupt-subjob) (C-c C-c), but Athena processes can. When I make a mistake in Presto I’ll delete the *Presto* buffer, kill the job in the Presto UI, and then run-presto again. When AWS credentials expire I have to delete the *Athena* buffer, update the credentials and run-athena again.\nSometimes I want to export the output into another program for sharing. My workflow is to copy the results into a scratch buffer. I’ll then regexp-replace +| + with ,, and then remove the leading whitespace and delete the line that seperates the header (which works fine unless the row could contain a pipe!). Then I can copy this CSV into another program. It would be nice to have an integrated way of doing this (this is one thing JetBrains Datagrip seems more convenient for).\nHowever it’s good enough to be useful for everyday work."
  },
  {
    "objectID": "price-hysteresis/index.html",
    "href": "price-hysteresis/index.html",
    "title": "Price Hysteresis",
    "section": "",
    "text": "Hysteresis is where the value of a quantity depends on how you got there. Pricing hysteresis is about that the quantity of goods sold isn’t dependent just on the price today, but on previous prices too. The study A Fine is a Price imposed a small fine for late pick up on a test group of childcare centres on a group that didn’t previously have them. This greatly increased the number of parents that picked up their children late; introduction of a fee changed the implicit social contract (although other explanations are explored in the paper). However even after they removed the fine the number of parents that picked up their children late stayed higher in the test group. That is even though at the end of the study the fee was zero in both centres, the rate of late pick ups is much higher in the group that at one point had a fine - price hysteresis.\nIn the book Confessions of the Pricing Man Hermann Simon has examples of where decreasing the price leads to hysteresis. One example is the “employee pricing for everyone” scheme from General Motors, and later Ford, where they greatly reduced the price of cars for a promotional period. Because the customer lifetime for a car is relatively fixed the growth they get is largely borrowed from future sales, people who would have eventually bought one of those cars. When they return the price to the previous level the sales will be lower than they were before.\nIn the same book Hermann Simon talks about how changing the price can permanently affect its positioning. A large part of the value of prestige cars is that they are hard to obtain, and so manufacturers such as Jaguar do not allow discounting of excess stock, because this would lower the brand value and the resale value. Even when new models come onto the market the fact that previous models were discounted would impact the price. This is seen in retail with high-low pricing strategies; a substantial number of customers wait for the lower price, expecting it to come from previous behaviour.\nBuying behaviour is not just determined by price, but also depends on expectations; whether that’s a social trust that can be eroded by acting unfairly for example by pushing a price too high or negotiating too hard, or whether that’s the expectation that prices will be reduced in the future changing the buying behaviour and even the perceived value. These kinds of effects are significant; if prices have been fairly stable, any sort of price change, especially upward, requires a lot of customer change management and carries some risk. These impacts need to be considered when thinking about estimating demand curves; it’s a lot easier if customers are already used to price changes, which seems to be the norm in increasingly many industries."
  },
  {
    "objectID": "casper-2-to-3/index.html",
    "href": "casper-2-to-3/index.html",
    "title": "Hugo Casper 2 to 3",
    "section": "",
    "text": "I cloned the repository, and changed the theme in my config.toml to theme = \"hugo-casper3\". The article images weren’t showing because the Casper 3 theme uses feature_image instead of image and requires a leading slash in the path (which was optional in 2). I fixed both of these on my mmark articles using:\nfind . -name '*.mmark' -exec sed -i -E 's|^image( ?[=:] ?)\"/?images|feature_image\\1\"/images|' {} \\;\n\nTesting features\nTo make sure the conversion went well I created a list of articles using different features:\n\n\n\nArticle\nImage\nCode\nFile\nList\nQuote\nTable\nLaTeX\nRMarkdown\n\n\n\n\nExtracting Skills with Conjugations\n✓\n✓\n✓\n✓\n\n✓\n\n\n\n\nJobPosting Schema\n\n✓\n\n\n✓\n✓\n\n\n\n\nReal Roots of Polynomials\n✓\n\n\n✓\n\n\n✓\n\n\n\nBlogdown\n✓\n✓\n\n\n✓\n\n\n\n\n\n\nThe result of my tests:\n\nImages, files and links are working\nSyntax highlighting now works\nQuotes work as before, lists look better (the CSS was broken before)\nLaTeX was broken\nRMarkdown files lost their images\n\nI’ve also lost a lot of features from the list page such as pagination, contact details and a cover image. The theme doesn’t look well supported, and I will investigate other themes in the future. But this is a start and I can work on remediating LaTeX and RMarkdown."
  },
  {
    "objectID": "checking/index.html",
    "href": "checking/index.html",
    "title": "Checking your Work",
    "section": "",
    "text": "When you get to the end of a long analysis it seems like a time to relax and be glad the hard work is over. But the fourth step of How to Solve It is to “Review and Extend”. There are typically lots of ways you can check your result makes sense. Is there another way you can calculate it? How does it fit with other things you know about?\nThis is why it can be very handy to keep a bunch of basic numbers about your domain at hand. How many users does your website get each week? How many emails do you send out, what percentage of them are opened? These often allow you to do a quick Fermi Estimate to check an answer is in the right ballpark, by relating it to one of these other things.\nThinking through how it to relates to other things can also help you gain insight. If the pageviews went up but weekly users went down, are you getting more churn in your infrequent users? What would this mean about your conversions? What sources of traffic are they asssociated with?\nUltimately the best way to have reliable output is to monitor your input and transform with well small and well tested steps; tracking invariants with property based testing. But this kind of system can be very slow to build and you can lose the intuition of how the parts relate to the whole. Checking a result by relating it to other known quantities is a very effective technique (and the only reason people can use an unstructured tool like Excel for analyisis)."
  },
  {
    "objectID": "can-must-should/index.html",
    "href": "can-must-should/index.html",
    "title": "Can I? Must I? Should I?",
    "section": "",
    "text": "The first question that comes is normally “Can”. Can it be? This leads to looking for evidence that confirms the idea.\nA different perspective is to re-frame the question to “Must”. Must it be? This challenges the default setting, and starts asking for evidence that rejects the idea.\nA third perspective is re-framing the question to “Should”. Should it be? This question starts to challenge the moral and the rational parts of the brain.\nWhen you’ve got a great new idea try to test it by re-framing with “can”, “must”, “should”."
  },
  {
    "objectID": "leaving-keepass/index.html",
    "href": "leaving-keepass/index.html",
    "title": "Moving Away From Keepass",
    "section": "",
    "text": "One of the most frequent ways people get their accounts hacked is by password reuse. Their email and password is revealed in some online breach of a website, and then these credentials can be used on other websites. However it’s really hard to remember lots of strong passwords. A password manager does exactly that; generate and store strong passwords for lots of sites.\nA password manager is also a huge target; if you can infiltrate it you can access all their passwords. For someone who is a very likely target for sophisticated hackers a password manager is a bad idea - it’s a liability, and they should take the effort to do something like diceware. However for the rest of us a password manager is likely worth the cost since it’s so convenient you’re likely to actually use it.\nI’ve been using KeePass and it’s derivatives (such as KeePassDroid and KeePassXC) for years. It’s open source, been audited to be relatively secure, and everything is stored in a local file which makes it easy to backup and you know who has control. However it doesn’t have a native way to automatically fill passwords into pages or sync across devices, so I am looking for an alternative.\nI was using the Kee Firefox Addon to automatically fill passwords, but in a recent version it’s stopped supporting KeePass and become it’s own management system that I don’t trust or want to use. Automatic filling is both convenient, but it’s also important for security; it prevents you copy pasting into a forged website if you don’t carefully check the domain (which can happen with email phishing attacks).\nI was using Syncthing to sync the Keepass database between devices, but it seems like Syncthing is unexpectedly opening Tor connections (without being configured to do so) which makes me really suspicious and makes it a no go to use in work devices. Manually syncing is difficult and error prone, and I have to fall back to typing in passwords from my mobile app which is painfully slow.\nSo I’m giving up control of open source to find a managed service to handle this for me. Giving all my passwords to someone else makes me a bit nervous; but I have to rely on the commercial reputation they would lose in a breach to give my confidence that my secrets are relatively secure (for my use case). Luckily now there’s a whole heap of these services from vendor specific solutions like Firefox Sync (which has questionable security regarding encryption) to general password managers like 1Password, LastPass, Bitwarden and Dashlane. I’m going to look through them and see what seems best for my use case, and start switching to it."
  },
  {
    "objectID": "geometry-of-division-rings/index.html",
    "href": "geometry-of-division-rings/index.html",
    "title": "Geometry of division rings",
    "section": "",
    "text": "Interestingly it’s just as possible to go the other way, if we’re careful about what we mean by a geometry. I will loosely follow Artin’s book Geometric Algebra. In particular we have the undefined terms of point, line and the undefined relation of lies on. Then, for a fixed positive integer, the axioms are:\n\nGiven two distinct points there is a unique line that both points lie on\nEach line has at least three points which lie on it\nGiven a line and a point not on that line there exists a unique line lying on the plane containing them that the point lies on and no point of the first line lies on.\nAll points are spanned by d+1 points and no fewer.\n\n\nThere are obviously a couple of definitions wanting. A linear manifold is a collection of points such that given any pair of distinct points in the collection, every point that lies on the line is also in the collection. The span of a set of points is the smallest linear manifold containing each of the points (that such exists follows from the fact the collection of all points is a linear manifold, and the intersection of two linear manifolds is a linear manifold). A plane is a set that is spanned by 3 points and no fewer.\nWe can define a dilation to be a mapping of points onto points such that the image of all points that lie on a given line lie on a line (that is a transformation that preserves lying on a line). We define a translation to be an injective dilation with no fixed points, or the identity.\nAny line containing a point and the translation (or more generally an injective dilation) of the point is called a trace of the translation. A scalar multiplication is a group automorphism of the translations such that each trace of a translation is a trace of its image.\nGiven two scalar multiplications \\(\\alpha\\) and \\(\\beta\\) we define their sum on a translation T by \\((\\alpha + \\beta)(T) = \\alpha(T) \\beta(T)\\) and their product by \\(\\alpha \\times \\beta (T) = \\alpha (\\beta T)\\) , define 0 to be the scalar multiplication sending all translations to the identity, and 1 to be the identity scalar multiplication.\nTheorem: The scalar multiplications form a division ring under the multiplications given above if and only if \\(d > 2\\) .\nThen we can go ahead and choose any point, which we call the origin, and d other points which, with the origin, span the space. We denote the d translations from the origin to the other points by \\(v_1, \\ldots, v_d\\) . Then any vector can be uniquely written \\((k_1, \\ldots, k_d) = k_1 (v_1) + \\ldots k_d(v_d)\\) for unique \\(k_1, \\ldots k_d\\) in the space. Now given there is a unique translation from the origin to any point, we can identify a vector with its action on the origin. Thus we obtain a coordinitisation of the space; a correspondence with \\(K^d\\) for some division ring K. This is of course not cannonical; our choice of the d + 1 points were arbitrary.\nIt is interesting to note how such geometric axioms (although carefully chosen) correspond so exactly with the algebraic notions of division ring and vector space.\nWhat about d = 1 and 2? For d=1 there is no hope, since axiom 3 is trivial and we could just take the line with n points for n not a power of a prime number. Since there is no division ring with n elements the theorem could not be true.\nThe case d = 2 turns out to be quite interesting. In dimensions at least 3 we can prove Desargues theorem. This has two parts:\n\nLet p, q, r be parallel lines, and let P, P’ be distinct points on p, Q, Q’ be distinct points on q and R, R’ be distinct points on R. If PQ is parallel to P’Q’ and QR is parallel to Q’R’ then PR is parallel to P’R’.\n\nThis is shown in the figure below (where line segments are lines). To prove this one assumes first the lines lie in different planes, and then to prove in the plane projects into the plane.\n\n\n\nParallel form of Desargues Theorem\n\n\nArtin proves that this is equivalent to for every pair of distinct points there exists a translation from one to the other. This underlies the vector structure.\n2.(X) Let p, q, r be three lines which all meet in a point X and P, P’ lie on p, Q, Q’ lie on q and R, R’ lie on r. If PQ is parallel to P’Q’ and QR is parallel to Q’R’ then PR is parallel to P’R’.\nA diagram is shown below. If it is true for one point X then it can be shown to be true for all points. Again the proof in dimensions at least 3 is done by first proving the case where the 3 lines are not coplanar and then projecting them into the same plane.\n\n\n\nProjective form of Desargues Theorem\n\n\nGeometrically Artin shows this is equivalent to: Given three collinear, distinct points P, Q, R there exists a dilation with fixed point P mapping Q onto R. This is essentially saying we can get to any point by a scaling operation, and underlies the scalar multiplication structure.\nOne may ask whether this axiom necessarily holds in dimension 2. It doesn’t. An interesting counterexample is the octonionic plane (the octonions are a non-associative (but alternative) division algebra over the reals). Because the octonions are non-associative you can’t really do linear algebra over them; in particular consider a potential line through an origin \\(\\{a\\mathbf{v}| a \\in \\mathbb{O}\\}\\) . Now consider the ‘line’ through the origin and another point on this ‘line’, \\(\\{b(a \\mathbf{v})| b \\in \\mathbb{O}\\}\\) because \\(b(a \\mathbf{v}) \\neq (ba) \\mathbf{v}\\) in general this line would have different points to the original.\nFor a plethora of examples of non-Desarguesian planes see this review.\nReflecting back there are a couple of interesting things to note about this construction. Axiom 3 is inherently 2-dimensional, so all the geometry of a d-dimensional affine space is determined by the geometry of its planes. Notice how the structure of 2 and 3 dimensions completely determines the structure of higher dimensions; this may have something to do with our familiarity with 2 and 3 dimensions in the choice of our axioms.\nAxiom 3 can be replaced by a projective equivalent, such as\n3P. (Veblen and Young) Given a triangle (that is three non-collinear points) any line that intersects two sides of the triangle (a side of a triangle is the line between two of the points, excluding the points themselves) intersects the third.\nWith any of these replacements all the appropriately projectivised statements above are true; in particular Desargues’ theorem has a more elegant statement. We can recover the affine space by choosing a point at infinity; an interesting question is whether the fields constructed from two different points at infinity are canonically isomorphic.\nIt’s also remarkable there is a geometric proposition that is satisfied if and only if the space is commutative, Pappus’ theorem.\nAbove all the geometric approach shows the space for geometry (as I’ve argued before) is the affine/projective plane itself, and not its group of transformations, the vector space. From an algebraic perspective that is to say the geometric element is not the vector space itself but the set on which the vector space acts transitively and freely (it can be represented by the same underlying set as the vector space, but does not have the same algebraic structure)."
  },
  {
    "objectID": "data-models/index.html",
    "href": "data-models/index.html",
    "title": "Data Models",
    "section": "",
    "text": "A typical example of where analysis can help is trying to increase revenue of a digitally sold product. Maybe you could increase marketing activity to attract new customers - but which acquisition channels should you focus on? Or you could make it easier to make a purchase - but where are potential customers getting stuck? Maybe you could cross-sell existing customers, but what products should you recommend to a customer? A good answer to these questions can contribute significantly to revenue, but they require good behavioural information.\nDigital products make it really easy to gather data. You can just check your server logs to see the activity of your website. If you keep these logs, and have a way of tracking acquisition channels, you’ve got all the data you need to answer these questions.\nHowever it’s really hard to go from logs of events to answer these kinds of questions because the data model doesn’t match your product. In all of the examples above there’s some notion of conversion opportunity; when a prospective customer comes to make a purchase do they make one? You can almost certainly identify a purchase because it will trigger some sort of delivery process, but how can you identify a prospective customer and tie it to the purchase? You can come up with a rule to identify individuals, and link it to the purchase event. But you’re really interested in acquisition channels, you’ll remove individuals that have been to your website before. And you’ll need to find how they arrived at the site, identifying the first page.\nThis is all possible, but there’s a lot of implementation work to create each piece. But there are lots of questions to ask and many of them have these similar concepts. You’ll end up writing the same things over and over, especially if you’re using SQL which doesn’t have language abstractions. Over time you’ll spend a lot of time rebuilding the same abstractions from the event data.\nHowever you could build a view of the data that fits these questions much more closely, the Google Analytics Schema is a good example of this. It’s got a notion of a visitorID to identify an individual and a session or “visit” which represents a browsing session. Each session has a trafficSource identifying how the visit was acquired, and eCommerceActions which summarise the purchase behaviour. It’s even got a “newVisits” field to distinguish new versus returning visits. A lot of the questions we asked before are now a very simple query because the data is presented conceptually at a level closer to the questions.\nWhen questions are easier to answer that enables asking deeper questions. We can start digging into the conversion funnel, identifying key events on the path to purchase to better understand the friction. Once the low-lying fruit is gone we can start segmenting customers based on their behaviour to understand how we can better serve specific groups of customers. We can also start understanding how products relate to better drive cross-selling. However again asking questions about funnels, identifying groups of behaviour or digging up related products starts becoming tedious. We can materialise these concepts and then lift the views to incorporate these concepts to make it easier to answer these questions, and start asking deeper questions again.\nIt’s definitely important to keep the event level data separately from these views. You can always rebuild the concepts, but it’s not always easy to extract the raw event data. The views by their nature implement business rules, and as the product changes some of the concepts will become outdated. It’s really easy to get stuck with a bad data model, but if you keep the raw data and the methodology of building views then it’s at least possible to build a new version and phase out the old one.\nThere’s obviously a cost to building new concepts and views, and doing it too much can leave a mess of data (instead of a mess of code). However in my experience the problem is that people are unwilling (or unable) to create new views, more than that they are creating too many. There is an exception in the intermediate tables someone created but no one knows how it was made; which is why the transform logic needs to be stored as a version controlled asset. Now that data storage and processing is much cheaper this should become the norm rather than the exception."
  },
  {
    "objectID": "flatten-object-python/index.html",
    "href": "flatten-object-python/index.html",
    "title": "Flattening Nested Objects in Python",
    "section": "",
    "text": "Here is a simple recursive function flatten_object to do this:\nfrom collections.abc import Iterable\nimport types\nfrom typing import Any, Dict\n\ndef flatten_object(nested: Any, sep: str=\"_\", prefix=\"\") -> Dict[str, Any]:\n    \"\"\"Flattens nested dictionaries and iterables\n\n    The key to a leaf (something is not list-like or a dictionary)\n    is the accessors to that leaf from the root separated by sep\n    prefixed with prefix.\n\n    If flattening results in a duplicate key raises a ValueError.\n\n    For example:\n      flatten_object([{'a': {'b': 'c'}}, [1]],\n                     prefix='nest_') == {'nest_0_a_b': 'c', 'nest_1_0': 1}\n    \"\"\"\n    ans = {}\n\n    def flatten(x, name=()):\n        if isinstance(x, dict):\n            for k,v in x.items():\n                flatten(v, name + (str(k),))\n        elif isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for i, v in enumerate(x):\n                flatten(v, name + (str(i),))\n        else:\n            key = sep.join(name)\n            if key in ans:\n                raise ValueError(f\"Duplicate key {key}\")\n            ans[prefix + sep.join(name)] = x\n\n    flatten(nested)\n    return ans\nIt is possible for keys be ambiguous, as in the case of dictionaries with mixed type keys or containing the separator as keys. Explicitly consider {'1': 'a', 1: 'b'} and {'a_b': 0, 'a': {'b': 1}}. There’s no universal way to handle these cases, and so the function raises a ValueError when it occurs. Also note that the function drops empty lists or dictionaries: flatten_object({'a': []}) == {}, so quite different objects could have the same flattened form.\nHowever I’ve found this a convenient way to quickly analyse nested data in Pandas, by flattening each of a list of such nested objects and passing the result to pandas.DataFrame. Then when I refine the code I can build a more specific extractor, or use an extraction DSL."
  },
  {
    "objectID": "australian-birth-check/index.html",
    "href": "australian-birth-check/index.html",
    "title": "Checking Australian Births Estimates",
    "section": "",
    "text": "My estimate was 25 million times 0.8 children per person lifetime divided by lifetime of 80 years.\nThe actual total fertility rate is 1.74 per woman, giving a birth rate of around half this of 0.87 per person which is significantly higher than I estimated. Even if I use a rate of 0.9, then my estimate is 280,000, about 10% too low.\nThe key assumption was smoothing out the person lifetime; that we’ll get one eightieth of the population. Women in Australia tend to have children in an age range of about 20-40 (and later in more recent times). Looking at the ABS Australian Demographics people of this age range are about 29% of the population, rather than 25% based on a smooth age assumtpion. This is an uplift of 1.16 times, which is slightly too much, but illustrates a closer look at demographics is likely to explain the difference."
  },
  {
    "objectID": "language-through-prism/index.html",
    "href": "language-through-prism/index.html",
    "title": "Language Through Prism",
    "section": "",
    "text": "The Tasks\nThere are three tasks they evaluate on, in order to evaluate performance at different scales:\n\nPart of Speech Tagging (word level) on Penn Treebank\nDialogue speech act classification (utterance-level) on Switchboard Dialog Speech Acts corpus\nTopic Classification (document level) on the 20 Newsgroups dataset\nMasked Language Modelling on Wikitext-103\n\nThese are all quite old and well known tasks in their class.\n\n\nProbing contextual word embeddings\nBERT can give contextual word embeddings; that is an embedding for each word that takes in the surrounding context. The word embeddings can then be used for the tasks by training a softmax classifier, that is a very shallow neural net.\nConcretely given the (contextual) word vectors \\(v_i\\), then we train a classifier \\(\\rm{softmax}\\left(W \\sum_{i \\in C} \\frac{1}{\\lvert C \\rvert}v_i\\right)\\), where W is a weight matrix of embedding dimension x number of classes, to be fitted using cross-entropy loss. Note that the vectors have to be averaged to the appropriate level for the task (the sum over context C, in the previous equation); for Part of Speech tagging (and Masked Language Modelling) we just use each word vector directly, for dialogue speech we average the vectors over each utterance and for topic classification we average over the whole document. I’m actually assuming that the vectors are averaged; it’s not discussed explicitly in the work and there’s no code provided to see what was done; but this seems like the most likely way to handle it.\nThe idea introduced in the paper is to try to try a classifier after passing the vectors through a bandpass filter at different frequencies (identically across each dimension) to see how they would do at different topics. For example a low pass filter will capture most of the document level variation of the embeddings, and a high pass filter will capture most of the word level variation (taking out the average context).\n\n\n\nDiagram of Bandpassed Embeddings\n\n\nThe results turn out nicely;\n\nthe high frequency vectors (1-2 token period) best predict part of speech\nthe mid frequency vectors (8-32 token period) best predict dialog\nthe low frequency vectors (256+ token period) best predict document classification, doing better than the original word embeddings\n\n\n\n\nResults of Probing Prism\n\n\nThe last result is particularly interesting; by throwing away information we get a better result. But assuming it is just an average at the document level, really by taking the low frequency we’re getting a better average by smoothing out some of the local noise. In fact a frequency 0 vector would in fact just be the average across each dimension, for every word.\n\n\nUsing Spectral Filters During Training\nThe second approach they take is to apply spectral filters to BERT. They take the hidden state in the last layer and divide it into 5 equal segments and apply 5 different band filters (from low to high) on those segments. They then add a classification layer and train the model on each of the tasks above, comparing it with BERT trained in the same way.\n\n\n\nDiagram of Prism Model\n\n\nThey find that for tasks above the word level that adding the Prism layer gives results better than BERT:\n\n\n\nResults of Prism experiments\n\n\nHowever I find it strange all these results are far below State of the Art:\n\nPart of Speech Tagging on Penn Treebank above 97% accuracy\nDialogue Act Classification on Switchboard Corpus above 82% accuracy\nTopic Classification on 20 Newsgroups above 88% accuracy\n\nI’m wondering if finetuning the language model would result in better scores. I think they didn’t do this because the masked task is local, as shown in the first section, and may lose information from the higher layers. However this casts doubt in my mind whether the prism layer would be useful in increasing performance in real systems.\nAnother thing they showed is the prism model was better at predicting missing words further away (but worse at predicting closer missing words). I would like to see a deeper analysis of this before I draw conclusions - is it just guessing common words more often?\n\n\n\nPrism model predicts missing words better further away\n\n\n\n\nDiscussion\nI’m not convinced this approach produces better models, that requires more evidence. But it seems like an interesting tool for analysing models and text. When looking at the activations relevant to a prediction you get to analyse it at different scales, so you could say whether it was driven by word, paragraph or document level features without explicitly modelling these. You could also look at the regions at this level relevant to it.\nThe better long range masked prediction is also potentially interesting. The best generative language models like BERT, GPT-2 and GPT-3 tend to be fine for short sentences, but tend to lose coherence after a couple of sentences. If we could force the models to focus on longer range dependencies (i.e. lower frequencies) maybe we could generate language with more coherence at the paragraph and document level. But I’m not sure how."
  },
  {
    "objectID": "extract-skills-3-conjugations/index.html",
    "href": "extract-skills-3-conjugations/index.html",
    "title": "Extracting Skills from Job Ads: Part 3 Conjugations",
    "section": "",
    "text": "In the previous post I extracted skills written in phrases like “experience in telesales” using spaCy’s dependency parse, but it wouldn’t extract many types of experience from a job ad. Here we will extend these rules to extract lists of skills (for example extracting “telesales” and “callcentre” from “experience in telesales or receptionist”, which will let us analyse which experiences are related.\nYou can see the Jupyter notebook for the full analysis.\n\nExpanding Conjugations\nWe can use spaCy’s dependency parse to extract conjugations\n\n\n\nConjugation Dependency Parse\n\n\nTo extract the conjugations (blue lines in the diagram) of a term we look for children with a dependency conj, and then recursively look for conjugations in their children.\ndef get_conjugations(tok):\n    new = [tok]\n    while new:\n        tok = new.pop()\n        yield tok\n        for child in tok.children:\n            if child.dep_ == 'conj':\n                new.append(child)\nFor each conjugation we want to extract the phrase (the green terms); a rough way to do this is to extract the longest sequence of nouns/adjectives left of the term.\ndef get_left_span(tok, label='', include=True):\n    offset = 1 if include else 0\n    idx = tok.i\n    while idx > tok.left_edge.i:\n        if tok.doc[idx - 1].pos_ in ('NOUN', 'PROPN', 'ADJ', 'X'):\n            idx -= 1\n        else:\n            break\n    return label, idx, tok.i+offset\nThen we can modify our previous rule to handle conjugations by iterating over conjugations (the last 2 lines):\ndef extract_adp_conj_experience(doc):\n    for tok in doc:\n        if tok.lower_ == 'experience':                             # red text\n            for child in tok.rights:\n                if child.dep_ == 'prep':                           # red arrow\n                    for obj in child.children:\n                        if obj.dep_ == 'pobj':                     # orange arrow\n                            for conj in get_conjugations(obj):     # blue arrows\n                                yield get_left_span(conj, label)   # green text\nWhile this works pretty well for the phrase “experience of Pioneer or Miser software” it will only extract the term “Miser software”.\n\n\n\nParse tree: “experience of Pioneer or Miser software”\n\n\nHowever if we rewrite the sentence to “experience of Pioneer software of Miser software” then it will extract both “Miser software” and “Pioneer software”.\n\n\n\nParse tree: “experience of Pioneer software or Miser software”\n\n\nThis kind of pattern is pretty common (e.g. sales or service environment), and we would get better results if we could implement these rewrite rules but I haven’t tried to yet.\n\n\nAnalysing the results\nThis allows us to extract a list of skills like in the previous post, but now we can also look at which terms commonly co-occur to find related skills by ranking. For example the top related skills for “sales” are “customer service”, “marketing”, and “business development”. For common skills this works pretty well:\n\n\n\n\n\n\n\n\n\nCore Skill\nClosest Skill\nSecond Closest Skill\nThird Closest Skill\n\n\n\n\nsales\ncustomer service\nmarketing\nbusiness development\n\n\nproject management\ndesign\ndelivery\ndevelopment\n\n\nSQL\nOracle\nSAS\nJava\n\n\nmanufacturing environment\naerospace industry\nautomotive industry\nstatistical process\n\n\nplanning\nmanaging\ndelivering\nmanagement\n\n\ntesting\ndevelopment\ndesign\nmaintenance\n\n\nmarketing\nsales\nadvertising\nPR\n\n\nanalysis\ndesign\ndevelopment\nreporting\n\n\nJava\nC++\nC\nSQL\n\n\nsoftware development\n.NET\ncommercial environment\ndifferent methodologies\n\n\ncustomer service\nsales\nretail\nhospitality\n\n\nadministration\nconfiguration\nmaintenance\nsystem design\n\n\nCSS\nHTML\nJavaScript\nPHP\n\n\nrecruitment\ntraining\nsales\nsales environment\n\n\nExcel\nWord\nPowerPoint\nOutlook\n\n\nSAP\nExcel\nOracle\nHyperion\n\n\nwriting\nediting\nmaintaining\nreviewing\n\n\nWindows\nLinux\nActive Directory\ndevelopment\n\n\nPython\nPerl\nRuby\nJava\n\n\n\nThis is very informative, for example:\n\nif you want a career in marketing it’s useful to have sales skills, which are close to customer service skills, which are often found in retail and hospitality\nThe backend programming languages (Java, C++, C) cluster together, separately from the frontend languages (CSS, HTML, JavaScript, PHP)\nExcel often ends up in a list of Windows Office technologies; but is especially useful for people who are using SAP\n\nHowever for some skills noise terms start to occur, for example “teaching” is most closely related to “training”, “UK” and “years”. This is because we’re extracting skills in a very specific way, and so we’re missing many other ways skills could be encoded in the job ad. Another consequence of our extraction method is we get related skills that are phrased in the same way because they often occur together in a list, for example “planning”, “managing” and “delivering”. This is good because it mitigates there being multiple ways a skill could be written; admistration, administrating, admin, and Administration could all the same thing.\nThere’s a lot more we could do here to look at the network of related skills, or disambiguate broad skills like “design” based on their context, if we could retrieve more skills from a job ad. Unfortunately it rapidly becomes much more difficult to write rules to extract skills phrased in different ways. In particular this job ad data has had some formatting removed (like lists) that makes it even harder to use a rule based approach. In a follow up series we will investigate using the rule based extraction to help seed a predictive model to extract skills."
  },
  {
    "objectID": "property-based-testing/index.html",
    "href": "property-based-testing/index.html",
    "title": "Property Based Testing - A thousand test cases in a single line",
    "section": "",
    "text": "Property based testing lets you specify rules that a function being tested will satisfy over a wide range of inputs. This specifies how to thoroughly test a function without coming up with a detailed set of test cases.\nFor example instead of writing a specific test case like sort([1, 3, 2]) == [1, 2, 3], you could state that the input and output of sort should contain exactly the same elements for any valid input."
  },
  {
    "objectID": "property-based-testing/index.html#specifying-more-complete-tests-with-less-code",
    "href": "property-based-testing/index.html#specifying-more-complete-tests-with-less-code",
    "title": "Property Based Testing - A thousand test cases in a single line",
    "section": "Specifying more complete tests with less code",
    "text": "Specifying more complete tests with less code\nWriting simple, disjoint test cases that each test exactly one thing and cover most likely failure modes is difficult. Often there are simple properties that can succinctly replace a large number of tests. For example a banking application may have a complex implementation spread accross a codebase, and it’s hard to test all the possible failure modes. However a simple property is that the amount in an account should balance against the deposits and withdrawals. This declarative property could be tested against a multitude of possible concurrent transactions to ensure the code is working correctly.\nSome common types of properties include:\n\nApplying the function twice gives the same result as applying it once (idempotency). This test on unicode canonicalisation would help prevent an account hijacking vulnerability at Spotify.\nIt gives the same result as a simple brute-force solution. For example finding the shortest path in a graph by enumeration.\nThe output takes a specific form; like the output of the sort function is in non-descending order.\n\nProperties are complementary to individually specified test cases. Cases can be clearer and make better documentation, and some functions don’t have properties that are easy to check and cover everything you want to test. However when properties exist they can be used to test on automatically generated data more complex than you would write in a test case."
  },
  {
    "objectID": "property-based-testing/index.html#generating-test-cases",
    "href": "property-based-testing/index.html#generating-test-cases",
    "title": "Property Based Testing - A thousand test cases in a single line",
    "section": "Generating test cases",
    "text": "Generating test cases\nOnce we’ve specified some properties we need some input data to test whether the function satisfies these properties. There are a few choices for test data:\n\nRandom data from a specified input distribution. This is how Haskell’s QuickCheck works\nEnumerating over all cases from simplest to most complex - for example starting with an empty list and trying longer lists. This is how Haskell’s SmallCheck works and is good for expensive properties.\nUsing data samples from production workloads.\nGetting examples of likely edge cases (for example rare unicode characters, extreme floating point numbers). This is close to classic unit testing."
  },
  {
    "objectID": "property-based-testing/index.html#example-of-property-based-testing",
    "href": "property-based-testing/index.html#example-of-property-based-testing",
    "title": "Property Based Testing - A thousand test cases in a single line",
    "section": "Example of Property Based Testing",
    "text": "Example of Property Based Testing\nI recently needed to find the minimum number of padding elements I would need to add to a list to divide it into sublists of a pre-defined length.\nTo do this I wanted a helper function of signature in pseudo-python of:\npad_divide(numerator: int > 0, denominator: int > 0) ->  (pad, div): Tuple[int, int]\nBy definition pad_divide must satisfy the properties:\n\npad and div are integers\nnumerator + pad == denominator * div\n0 <= pad < denominator (since if pad >= denominator then pad - denominator would also be an acceptable padding length).\n\nIn fact these properties uniquely define the function (we could inefficiently solve it by searching through all pad sizes). Writing these tests up front, and testing them on all numerators and denominators between 1 and 30 helped me get to the right solution more quickly."
  },
  {
    "objectID": "property-based-testing/index.html#implementing-property-based-testing",
    "href": "property-based-testing/index.html#implementing-property-based-testing",
    "title": "Property Based Testing - A thousand test cases in a single line",
    "section": "Implementing Property Based Testing",
    "text": "Implementing Property Based Testing\nFor simple cases (like above) it’s possible to implement property based testing by hand; but if you want to use it extensively (and do clever things like find the simplest failing case) there are libraries for many languages:\n\nThis type of testing originated in Haskell with QuickCheck and later SmallCheck.\nPython has Hypothesis\nR has Hedgehog\nScala has ScalaCheck\nJavaScript has jsverify\n\nTo understand more of how these are implemented the original papers for QuickCheck and SmallCheck are illuminating."
  },
  {
    "objectID": "data-tests-sql/index.html",
    "href": "data-tests-sql/index.html",
    "title": "Data Tests with SQL",
    "section": "",
    "text": "The right tests will depend on your scenario, but there are a lot of common ones in SQL. Checking that a column is non-null, or that the values are unique or that one column is contained in another. Ideally these would be validated in the database itself, but often are not for implementation reasons, and sometimes it’s good enough to be only true most of the time (say, 98% of the time). Other examples are checking daily counts are within a certain range, checking the values a column can take, or checking that the values are withing some range (for example dates are not in the future).\nWhen writing tests the key is to keep them as simple and obviously correct as possible. The easiest way to do this is to keep each test a separate query (which may seem inefficient, but is worth the savings in debugging time). It’s also useful to build up intermediate tables or views to keep the logic clearer and make debugging easier.\nIf you’re really interested in making sure your report is correct make sure you do these checks in the same transaction with enough isolation; in particular that reads are repeatable. Otherwise you may be testing different data to the data you use."
  },
  {
    "objectID": "gold-or-bills/index.html",
    "href": "gold-or-bills/index.html",
    "title": "Gold or Bills",
    "section": "",
    "text": "As a bank robber sitting in the vault planning your getaway, do you fill your suitcase with gold bars or $100 bills? Assume first that how much you can carry is a fixed weight. Then redo your analysis assuming that how much you can carry is a fixed volume.\n\nAs I estimated in suitcase of money the mass of a paper note is about 1 gram, and the volume is about 1 cm³. So the mass density of an $100 note is $100 per gram, and the volume density is $100 per cubic centimetre.\nAs I estimated in Value of Gold the volume density of gold is about $2000 per cubic centimetre. So clearly for a fixed volume you are much better off filling a suitcase with gold bars.\nHowever gold is quite dense, with a density of about 20 grams per cubic centimetre (about twice the “typical” amount of 10 grams per cubic centimetre for many metals). So the mass density is about $100 per gram, similar to a money note. I actually overestimated the value density slightly, so in fact for a fixed weight you would be better off with paper money.\nNote that a small suitcase of money weighs around 50 kilograms. Gold is about 20 times denser, so would weigh 1 tonne. In practice weight is more likely to be the limiting factor and you’re better off with a suitcase full of money."
  },
  {
    "objectID": "orderly-life/index.html",
    "href": "orderly-life/index.html",
    "title": "Orderly Life for Original Work",
    "section": "",
    "text": "It’s hard to find the energy and focus to be creative when your life is a mess. Before you can be productive you need to sleep well, eat well, exercise well and have good routines and social supports. See here for more on the origin of this quote."
  },
  {
    "objectID": "book-title-ner-outline/index.html",
    "href": "book-title-ner-outline/index.html",
    "title": "Side Project Outline: Book Title NER",
    "section": "",
    "text": "It turns out there’s already there’s also MapFilterFold that searches “Ask HN” threads on Hacker News for book titles. It sounds like this list is semi-manually curated and results are manually validated. The site is designed very well; there are categories on the front page, and the detail page has co-recommendations as well as the threads it was recommended in and extracts specific to that book (often the thread has lots of other text). So I can select the “Programming” category, click into the top result, “The Pragmatic Programmer”, and see it’s often recommended with “Clean Code”, “Refactoring”, “The Effective Engineer”, and “Thinking in Systems”, and go down to read that people found the “prototype” and “tracer bullet” ideas useful.\nThere are also sites that extract books using Amazon links; Hacker News Books and Books Reddit for HN and Reddit respectively. This is a high precision strategy, but low recall since most mentions won’t have an Amazon link. For example Martin Fowler’s Refactoring gets an entry in Hacker News Books for the first edition and second edition with 20 and 5 comments respectively. Searching for “refactoring fowler” in Hacker News comments currently gives me 260 results; there’s a lot of comments missing.\nThe real reason I want to do this though is I’ve been looking for a good Natural Language Processing project and this fits the bill. The traditional way of running this kind of project is to carefully building a model and specifications, writing detailed annotation guidelines, and spending a lot of time manually annotating corpora (or outsourcing to something like Amazon’s Mechanical Turk) to train a machine learning model. This is a lot of upfront planning work to get this pipeline right, and if you discover while training the model that the specification wasn’t right you have to throw out a lot of annotations. Matthew Honnibal, of SpaCy fame, has a PyData talk on shortening this cycle and iterating the data and model together. I’d love to see if I can make this kind of rapid prototyping approach can work.\nThe first tool in speeding up this process is “weak supervision”, popularised by Snorkel in their 2016 Data Programming paper, of using noisy labelling heuristics to bootstrap a machine learning model. Amazon links to books is a good high precision heuristic to find a book. However there are many others like string matching on a big list of book names (a gazetteer), or looking for language patterns like the book <NNP> or <NNP> by <PER>, or looking for “Ask HN” threads with “Book” in the title. Then we can aggregate these noisy labels, typically using generative models to estimate the correlation between functions and give a probabilistic label. These are then used as the input to a machine learning model. I want to try this with Snorkel and skweak which generalises to NER using HMM and integrates with SpaCy.\nThe second tool is reducing the data required using transfer learning. Until around 5 years ago the standard way of training a NER model was to train a randomly initialised CRF (Conditional Random Field) model, typically with some hand tuned features, at the end of a pipeline consisting of tokenization, prediction Parts of Speech and finding Lemmas using something like Stanford Core NLP. The problem is that you needed a lot of data to train the CRF from scratch and if your text was unlike the “standard texts” (usually news stories) the pipeline may perform badly reducing the quality of the data to the model. In this case you may need to reannotate parts of speech and tweak the tokenization to improve performance. An emerging approach is to use pretrained dense vector representations of text, especially using Transformer models, which already represent similar words close together (as opposed to one hot encoding models which needs to learn these relations from the data). Then a neural network is trained on this representation which, given the pretraining, requires a lot less data to get to a good accuracy. I’m interested in trying this and want to look into Stanza, flairNLP, SpaCy and BERT/RoBERTa models.\nThe third tool is efficient labelling tools using active learning. The traditional approach of annotating data is to take each document and label it separately. However dragging NER spans over text, especially with a lot of categories, is very time consuming for every single example. Also a lot of these aren’t particularly informative; I’m sure I’d end up labelling SICP many times when it could be found by a simple model. A better approach is to correct model predictions which is faster, especially those the model is uncertain about which will give more model improvement. This approach is called active learning and is avaiable in labelling tools such as Label Studio and Prodigy.\nThe other reason this is technically interesting is further techniques could be used to extract more information. The books and comments could be clustered, or the comments could be summarized, or searched using Question Answering techniques. These techniques are a lot better performing and more accessible now and I’d love to see how they could be used for effect.\nThe approach I’m going to take is to quickly deliver an end-to-end baseline and iterate on it. I could spend a lot of time exploring these different options on a task, but maybe that’s not even the right problem. I want to learn how to use these techniques to create value, and by iteratively building something I can focus on what I think will improve the outcome the most. This is a learning project, not a business, and the outcome is something I find interesting, but I don’t want to spend a whole month trying to optimise an F1-score.\nAs a starting point I’ve exported all 2021 Hacker News data from BigQuery using Kaggle. I’ve also found a large dataset of books on Amazon and there are many potential datasets for book names such as open library. The next step will be to do some exploratory data analysis on the data and find the quickest way to get some good book data out.\nUpdate 2022-09: I came across Hacker News Readings from a Show HN which apparently does this, but unfortunately there’s no code or data available. It’s a good strong baseline though."
  },
  {
    "objectID": "emacs-tempfile-hugo/index.html",
    "href": "emacs-tempfile-hugo/index.html",
    "title": "Hugo Readdir Error with Emacs",
    "section": "",
    "text": "ERROR 2021/10/18 19:36:03 process: readAndProcessContent: walk:\nReaddir: decorate: lstat\n/home/user/skeptric/content/user@machine.2139:12345 no such file or directory\nOften I can work around it by editing and saving the file I’ve been editing again, but I’ll have to restart the Hugo process. However the underlying cause is lockfiles in Emacs, and the easiest fix is to run the elisp (in 24.3 and above):\n(setq create-lockfiles nil)\n\nExplanation\nIf I got the above error when editing a file called about.md I’d normally be able to find a symlink like .#about.md -> user@machine.2761:12345. This turns out to be emacs system for file locks. The .#<filename> is symlinked to <user>@<host>.<pid>:<time since boot>. Then when another user tries to to open the file in emacs it will then complain about the file being locked. You can turn this off by setting create-lockfiles to nil.\nHugo tries to follow these symlinks and realising they’re not going to a real file complains. Unfortunately I can’t find any way to disable this behaviour of Hugo, and since I’m not in a multiuser environment it seems the only solution is to turn off Emacs lockfiles."
  },
  {
    "objectID": "search-in-site/index.html",
    "href": "search-in-site/index.html",
    "title": "Searching within a Website",
    "section": "",
    "text": "If I want to search for articles about jobs just in this website I can type: site:skeptric.com job into either Google or Bing. I find this really handy because I have over 150 articles and I often forget what I wrote. For this website I could also search with Github (e.g. repo:EdwardJRoss/skeptric job), or just download and search with grep; but that’s generally less convenient.\nAnother useful example is the Australian Beaurau of Statistics. They run lots of useful surveys, but it’s really hard to find them through navigating the website. However if I search for site:abs.gov.au motor vehicle I can immediately find survey and census data on Motor Vehicles.\nThere are many other useful advanced search operators, like to get results with a specific filetype, language, location or words in the title. Google has a partial list but you can access much more through their advanced search. Bing has more comprehensive documentation about its search operators.\nHowever I’ve found they don’t always function as I would expect; for example if I want to search for Jupyter notebooks on RoBERTa I would try a search like roberta ext:ipynb. This works perfectly in Google, but in Bing returns HTML articles about people named Roberta. It’s useful to try both when you’re looking for something very specific. If you want to do a broad shallow search of what’s in the internet or do some analysis, then the Common Crawl columnar index is a better tool."
  },
  {
    "objectID": "common-crawl-job-ads/index.html",
    "href": "common-crawl-job-ads/index.html",
    "title": "Extracting Job Ads from Common Crawl",
    "section": "",
    "text": "I’ve been using data from the Adzuna Job Salary Predictions Kaggle Competition to extract skills, find near duplicate job ads and understand seniority of job titles. But the dataset has heavily processed ad text which makes it harder to do natural language processing on. Instead I’m going to find job ads in Common Crawl’s, a dataset containing over a billion webpages each month.\nThe Common Crawl data is much better because it’s longitudinal over several years, international, broad and continually being updated. The downside is that it’s not a nicely formatted CSV; you have to find the job ads in the billions of pages, download them and extract and integrate them. But by being thoughtful about our approach we can get a good sample fairly easily."
  },
  {
    "objectID": "common-crawl-job-ads/index.html#json-linked-data",
    "href": "common-crawl-job-ads/index.html#json-linked-data",
    "title": "Extracting Job Ads from Common Crawl",
    "section": "JSON Linked Data",
    "text": "JSON Linked Data\nSometimes the data would occur in a <script type='application/ld+json'> object. This is JSON Linked Data which contains a structured representation of the job ad. It’s frequently put in job ads because Google promotes it in search results.\nI used the extruct library to extract this. When I first ran it I was getting the error rdflib.plugin.PluginException: No plugin registered for (json-ld, <class 'rdflib.serializer.Serializer'>). However a comment on a github issue let me to a solution.\nfrom rdflib.plugin import register, Serializer, Parser\nregister('json-ld', Serializer, 'rdflib_jsonld.serializer', 'JsonLDSerializer')\nregister('json-ld', Parser, 'rdflib_jsonld.parser', 'JsonLDParser')\nThen it was straightforward to extract the job ad as a structured object:\n[data for data in extruct.extract(objs[0].content)['json-ld'] if data['@type'] == 'JobPosting']\nThe best thing is it’s a standard format that’s used accross many websites and can contain a lot of information like the title, location, salary, industry, skills and job ad details.\n\n\n\nExtracted LD Json\n\n\nSometimes it’s contained in microdata instead, which is also captured by extruct.\n[x for x in extruct.extract(objs[0].content)['microdata'] if x['type'] == 'http://schema.org/JobPosting']"
  },
  {
    "objectID": "common-crawl-job-ads/index.html#javascript-objects",
    "href": "common-crawl-job-ads/index.html#javascript-objects",
    "title": "Extracting Job Ads from Common Crawl",
    "section": "Javascript objects",
    "text": "Javascript objects\nSometimes there’s no Job Posting JSON Linked Data or Microdata on the page, but there is a Javascript object containing all the relevant information. Then if you can consistently find that object you can extract the structured data from it.\nI wrote a simple parser to find the entire extent of a Javascript object by counting curly brace depth (ignoring quotes):\ndef get_object(text):\n    depth = 0\n    inquote = False\n    escape = False\n    for idx, char in enumerate(text):\n        if escape:\n            escape = False\n            continue\n        if char == '\"':\n            inquote = not inquote\n        if  char == '\\\\':\n            escape = True\n        if (not inquote) and char == '{':\n            depth += 1\n        if (not inquote) and char == '}':\n            depth -= 1\n            if depth <= 0:\n                break\n    return text[:idx+1]\nThen once you have the Javascript string you can parse it into a Python object. For simple objects json.loads will do the trick, but if it’s more Javascript than JSON then you may need demjson.\nimport demjson\n# obj_start_text is the text before the opening brace\ndef extract(text):\n    start_idx = text.find(obj_start_text) + len(obj_start_text)\n    object_text = get_object(text)\n    return demjson.decode(object_text)"
  },
  {
    "objectID": "common-crawl-job-ads/index.html#searching-html-with-beautifulsoup",
    "href": "common-crawl-job-ads/index.html#searching-html-with-beautifulsoup",
    "title": "Extracting Job Ads from Common Crawl",
    "section": "Searching HTML with BeautifulSoup",
    "text": "Searching HTML with BeautifulSoup\nIf you can’t get the data from an API or a structured object you will have to parse the HTML. This tended to be common with government job boards.\nFinding the right patterns is a bit of an art and it’s worth inspecting the page with developer tools to look for interesting tags or properties to find what you need. I find using CSS Selectors with Beautiful Soup’s select method, or XPath using lxml (or html5-parser) tend to be quite effective.\nYou’ll probably need to do some specific text search and clean up as well to fix the documents."
  },
  {
    "objectID": "mere-exposure/index.html",
    "href": "mere-exposure/index.html",
    "title": "Mere Exposure",
    "section": "",
    "text": "This bolsters the idea that repetition is important in getting people to agree with a message. It also sits in the broader theory of cognitive fluency, that people like what they are more familiar with. This is why if you want people to like a presentation you should use the house style and language they are familiar with, because it’s more pleasant to them. This is covered in chapter 5 of Thinking Fast and Slow, Cognitive Ease."
  },
  {
    "objectID": "editing-hugo-themes/index.html",
    "href": "editing-hugo-themes/index.html",
    "title": "Learning Hugo by Editing Themes",
    "section": "",
    "text": "One of the hardest parts of learning something new is motivation. This is why one of the best ways to learn programming is editing code; it’s goal driven so motivation is built in. I’ve successfully used this to start learning how to write Hugo themes.\nNow that I’ve got a reasonable collection of posts, over 250, I would like to understand what content people are actually accessing on this website to get an idea of what would be useful. Because I’m lazy the easiest way to do this is with Google Analytics. Unfortunately the theme I’m using doesn’t support Google Analytics, and I can’t inject it into an existing partial like I did to include diagrams in Hugo pages.\nSo I ended up forking the Hugo Casper 3 theme to create my own version where I could inject these tags. I wanted to inject an appropriate partial hook so that I could put in the Google Analytics script; the existing site-header.html that I used to inject Mathjax and Mermaid.js isn’t included on the list page. There’s an HTML template in layouts/_default/baseof.html that seems to contain the HTML template used to create every page, which would be appropriate. This was as simple as adding the partial at the end of the head:\nThen I added a dummy layouts/partials/header-scripts.html in the theme:\nThen in my website, which has the theme as a submodule, I overwrite layouts/partials/header-scripts.html with my Google Analytics tags (and also migrate my Mathjax and Mermaid code over to it). Once I update the submodule in git it all works."
  },
  {
    "objectID": "editing-hugo-themes/index.html#why-not-write-your-own-theme",
    "href": "editing-hugo-themes/index.html#why-not-write-your-own-theme",
    "title": "Learning Hugo by Editing Themes",
    "section": "Why not write your own theme?",
    "text": "Why not write your own theme?\nThe best thing about editing the theme is it opens the door to a lot of customisation. It’s easier than writing from scratch, and allows more flexibility than any theme.\nI could write a theme from scratch, but that’s a lot of work. Not only would I have to understand HTML and CSS (and design) well enough to make a good website, I would need to understand Hugo well enough to do it. I don’t have the motivation to learn this whole framework from scratch; it’s surprisingly complex for something that seems simple."
  },
  {
    "objectID": "editing-hugo-themes/index.html#why-not-use-an-existing-theme",
    "href": "editing-hugo-themes/index.html#why-not-use-an-existing-theme",
    "title": "Learning Hugo by Editing Themes",
    "section": "Why not use an existing theme?",
    "text": "Why not use an existing theme?\nOne of the appeals of Hugo is the vast array of themes you can just drop in. But unfortunately there’s no consistency built into features in Hugo. It was difficult when I ported from Casper 2 to Casper 3 because the parameters were slightly different (and in fact I just realised today my social links had been broken since the upgrade 3 months ago). For example Casper 2 uses image for a post image, others use feature_image (and others don’t support it), Casper 2 used githubName to link to a github repository Casper 3 uses github and requires the full URL.\nMoreover I couldn’t find a single theme that did everything I wanted. The closest was academic, but it was slow to render and required a lot of configuration which has its own steep learning curve."
  },
  {
    "objectID": "editing-hugo-themes/index.html#editing-themes",
    "href": "editing-hugo-themes/index.html#editing-themes",
    "title": "Learning Hugo by Editing Themes",
    "section": "Editing Themes",
    "text": "Editing Themes\nThe wonderful thing about open source code is you can build on it. Now I’ve opened the door to editing a theme the Hugo themes turns from a menu to a buffet. I can take elements from other themes and integrate them into the theme I’m working on.\nI can now start to look into making improvements to this blog, and slowly learning features of Hugo to solve real problems. I’ve already sped up page loads by moving Mermaid.js to the end of the body instead of in the head. There’s more potential improvements by only loading the script on pages that use it (or pre-building the images), optimising image sizes, paginating the index page and so on."
  },
  {
    "objectID": "pagination-wayback-cdx/index.html",
    "href": "pagination-wayback-cdx/index.html",
    "title": "Pagination in Internet Archive’s Wayback Machine with CDX",
    "section": "",
    "text": "I’ve been trying to use pagination with the Internet Archive’s CDX for the Wayback Machine but have been getting lots of empty results. The reason is that filters are applied after pagination, and so when using tight filters almost all records will be empty."
  },
  {
    "objectID": "pagination-wayback-cdx/index.html#using-the-cdx-api",
    "href": "pagination-wayback-cdx/index.html#using-the-cdx-api",
    "title": "Pagination in Internet Archive’s Wayback Machine with CDX",
    "section": "Using the CDX API",
    "text": "Using the CDX API\nFollowing the documentation it’s easy enough to make a query (and similar, but not the same, as Common Crawl’s CDX Index). Here’s some logic to get captures from en.wikipedia.org made on 2021-11-01.\nIA_CDX_URL = 'http://web.archive.org/cdx/search/cdx'\nDEFAULT_PARAMS = {\n                     'url':'https://en.wikipedia.org/*', \n                     'from':'20211101', \n                     'to':'20211101', \n                     'output':'json',\n}\ndef cdx_request(**params):\n    return requests.get(IA_CDX_URL, params={**DEFAULT_PARAMS, **params})\nThe returned data is a JSON array of arrays; the first array being the header and the following being the data.\n'[[\"urlkey\",\"timestamp\",\"original\",\"mimetype\",\"statuscode\",\"digest\",\"length\"],\n[\"org,wikipedia,en)/\", \"20211101002442\", \"http://en.wikipedia.org/\", \"warc/revisit\", \"-\", \"3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\", \"615\"],\n[\"org,wikipedia,en)/\", \"20211101002443\", \"https://en.wikipedia.org/\", \"warc/revisit\", \"-\", \"3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\", \"935\"],\n[\"org,wikipedia,en)/\", \"20211101022822\", \"http://en.wikipedia.org/\", \"unk\", \"301\", \"3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\", \"740\"],\n[\"org,wikipedia,en)/\", \"20211101022822\", \"https://en.wikipedia.org/\", \"text/html\", \"301\", \"3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\", \"1066\"],\n...\nIn this case we’ve got 1574 rows plus the header."
  },
  {
    "objectID": "pagination-wayback-cdx/index.html#adding-pagination",
    "href": "pagination-wayback-cdx/index.html#adding-pagination",
    "title": "Pagination in Internet Archive’s Wayback Machine with CDX",
    "section": "Adding Pagination",
    "text": "Adding Pagination\nAccording to the documentation we can also use pagination (which uses a different index).\n\nTo determine the number of pages, add the showNumPages=true param. This is a special query that will return a single number indicating the number of pages\n\nAnd if we call that:\nnum_pages_request = cdx_request(showNumPages=True)\nnum_pages_request.content\n# b'11326\\n'\nThis seems like way too many pages given there are only a few thousand results, and if we start looking at the pages they seem empty:\npage_0_request = cdx_request(page=0)\npage_0_request.content\n# b'[]\\n'\nThe reason is that the pagination is done before the date filter; if we change the dates we get the same result (note that we can’t use from as a named argument because it’s a keyword):\nnum_pages_request = cdx_request(**{\"from\":'201801', \"showNumPages\":\"true\"})\nnum_pages_request.content\n# b'11326\\n'\nSo pagination is only really useful if you want to get almost all the results. In this case looking for data from a particular day is like finding a needle in the haystack; I suspect there is data in one of the 11326 pages but it will take a long time to find the right one.\nIn general if you only want Wayback Machine snapshots from a limited period time with CDX you’re better off ignoring the pagination entirely and just getting all the results in one query (as long as it’s under the internal limit of 150,000)."
  },
  {
    "objectID": "4-analyst-competencies/index.html",
    "href": "4-analyst-competencies/index.html",
    "title": "Four Competencies of an Effective Analyst",
    "section": "",
    "text": "These make up the four competencies of an effective analyst.\n\nProblem solving, logical thinking, reasoning and statistics\nSubject matter expertise and business acumen\nAdept with technology, information systems and programming\nCommunication, stakeholder management and influence\n\nI really like the 4-way Venn diagram of Stephen Kolassa for illustrating this; he calls it a “The Perfect Data Scientist” but it’s getting to the same point.\n\n\n\nData Scientist Requires Communication, Statistics, Programming and Business\n\n\nIt’s very difficult to get someone adept at all of these things, and when building an analytics team it will be most effective with people that have complementary skills collaborating. For an analytics professional it makes sense to build out some strength in each of these areas.\n\nQuantitative problem solving\nThe bread and butter of analytics is understanding what impact actions have on metrics of interest. This requires having a model of the system, knowing the interactions and understanding measurement. The models can range from informal sketches to fully specified statistical models. Interactions in the measurement can be managed by understanding the order of magnitude of effects or through statistical testing or causality modelling.\nSometimes these techniques can be overemphasised and the focus can be more on the cool new modelling technique (say, deep reinforcement learning) than how effectively it solves the problem. In practice a core of simple techniques can be effective and efficient for a wide range of problems. Sometimes for highly valuable problems more advanced techniques can pay huge dividends, but they are often time consuming to implement, harder to communicate and maintain and don’t always work out.\n\n\nSubject Matter Expertise\nSome of the best analysts are the people who have a wealth of experience in their area. This is a very specific kind of skill, subject expertise in digital marketing won’t help you in novel drug discovery. This is part of what makes it valuable; the only way to get it is to spend a long time working in the area.\nFor problem solving you need at least an informal model of the system, understanding how measurements are collected and being familiar with issues that will occur. These come naturally with deep experience in the area; understanding deeply how things work (and how to differentiate what’s important from what isn’t) and having been exposed to what goes wrong.\nTo be able to communicate and influence it’s important to speak the same language as your stakeholders and know what matters to them. Subject expertise helps immensely because you know the industry terminology and what people worry about. This makes it easier to build trust and communicate insights.\nAny business will have people that are experts in the area you’re analysing. The best way to gain expertise is to talk with them frequently to understand whether your models make sense and that you can be understood.\n\n\nTechnology\nTechnology enables complex analyses, handling large or diverse datasets, automating workflows and delivering bespoke solutions. At minimum you need to be able to access and analyse relevant data, and even when it’s collected manually this normally requires some technology. Generally there’s some amount of data cleaning, integrating different datasets and performing calculations and creating visualisations.\nSpecific technologies are often disproportionately represented in job ads for analysts. Typically you can learn to use different systems, especially ones that built specifically for analysts. Some are specific to problem solving techniques, like Stan, or to a particular domain, like Google Analytics or Adobe Analytics to web and mobile applications. When communicating with other people you may be better off using technologies that are familiar to them; Excel is popular.\nBuilding bespoke technology systems can automate time-consuming processes and make whole new things possible. However maintaining these systems is often expensive and requires continual investment. In the worst case you could become a full time developer operating and maintaining a software system.\nTechnology makes new things possible, but it’s often best to use existing specific tools or leveraging other technical expertise than building your own solutions. However even using these tools and interacting with technical experts requires being adept with technology and being able to think about systems.\n\n\nCommunication and influence\nAnalytics is using quantitative information to inform which decision will best deliver desired outcomes. Even if you have the most rigorous and specific analyses, if you can’t persuade the decision maker to act based on them they are worthless. To understand what problems to solve you need to understand what options the decision maker has and what outcomes are desired.\nCommunication can help guide how to solve problems. I’ve sometimes come up with a complex metric or analysis that fits the problem well, but is very hard to communicate with my stakeholders. Often there’s a simpler framing that results in the same decision but is easier to understand. Being easy to understand is highly valuable; it will be much clearer if it makes sense. A decision is often made based on a number of factors outside the analysis, and the decision maker needs to understand how the analysis fits in the bigger picture.\nCommunicating with data is a particular skill set. Knowing how to present data and emphasise what matters to your stakeholders can be all the difference in influencing a decision. I really like Cole Nussbaumer Knaflic’s Storytelling with Data as a starting point here.\nEven in a more product delivery role you will still need to persuade your users to follow the recommendation of your system, and persuade people in your company that your system adds value. In both of these cases understanding their objectives and being able to clearly communicate what value you’re delivering will make it much more effective.\n\n\nBringing it all together\nThese competencies are very broad and you could spend a lifetime learning any one of them. Whether you’re building systems or influencing decisions understanding how all these pieces fit in your objectives, and how to mitigate the deficiencies is important. You can always work with experts in statistics, business, technology or communication to get things done. But you’ll have to have enough overlap to understand what actually needs to be done and convey why it is important."
  },
  {
    "objectID": "writing_dataframes_to_s3/index.html",
    "href": "writing_dataframes_to_s3/index.html",
    "title": "Writing Pandas Dataframes to S3",
    "section": "",
    "text": "# df is a pandas dataframe\ndf.to_csv(f's3://{bucket}/{key}')\nUnder the hood Pandas uses fsspec which lets you work easily with remote filesystems, and abstracts over s3fs for Amazon S3 and gcfs for Google Cloud Storage (and other backends such as (S)FTP, SSH or HDFS). In particular s3fs is very handy for doing simple file operations in S3 because boto is often quite subtly complex to use.\nSometimes managing access credentials can be difficult, s3fs uses botocore credentials, trying first environment variables, then configuration files, then IAM metadata. But you can also specify an AWS Profile manually, and you can pass this (and other arguments) through pandas using the storage_options keyword argument:\n# df is a pandas dataframe\ndf.to_parquet(f's3://{bucket}/{key}', storage_options={'profile': aws_profile}))\nOne useful alternative is to create AWS Athena tables over the dataframes, so you can access them with SQL. The fastest way to do this is with AWS Data Wrangler, although PyAthena is also a good option."
  },
  {
    "objectID": "wsl2-xserver/index.html",
    "href": "wsl2-xserver/index.html",
    "title": "Running an X server with WSL2",
    "section": "",
    "text": "Note\n\n\n\nWindows now provides built-in support for GUI apps in WSL2 which is considerably easier to use and setup than the instructions below.\nOnly follow the below tutorial if for some reason the WSL GUI experience doesn’t meet your needs.\n\n\nAs outlined in a StackOverflow answer there are three steps to getting this working:\n\nEnable WSL to access the X server on Windows Firewall\nEnable public access from an X11 Server\nExport the appropriate display variables from Linux\n\n\nAllow WSL Access via Windows Firewall\nWSL2 runs in a Virtual Machine, so network traffic looks like it’s coming from another machine (as opposed to WSL1 where network traffic was local). This means that when we’re trying to forward X from WSL2 to an X Server running in Windows it has to pass through the firewall. We’ll enable this traffic to pass through the firewall, as described in cascadium. This is straightforward but requires a few steps.\nPress the Windows Start Button and select “Firewall & Network Protection” (typing until it comes up). Then select “Advanded Settings”.\n\n\n\nWindows Network Advanced Settings\n\n\nYou may be asked at an Administrator prompt to allow this application to make changes to the system; allow it. Right click on “Inbound Rules” and select the option “Add Rule…”\n\n\n\nAdd firewall rule\n\n\nWe’re going to open up Port 6000 for the X Server to communicate on. Under Rule Type select port and then click next. Select TCP Port 6000 and click next. Then click next through windows (allowing the connection and applying to all profiles) until the last screen, and then give it a reasonable name like “WSL2 X Access” and then Finish.\n\n\n\nOpening Port 6000\n\n\nThis opens up the port to the whole internet, which is a security risk. We now need to narrow the scope so it’s only open to WSL2.\nFind the Rule you just created in the list of Inbound Rules, right click and select “Properties” Click the “Scope” tab, and under “Remote IP Addresses” click Add entering “172.16.0.0/12” under port. Then click Add again entering “192.168.0.0/16” under port. These are local subnets that may be used by WSL2.\n\n\n\nWSL Subnet\n\n\nNow WSL2, and only WSL2, is able to send network traffic to Windows on port 6000, which we need for X.\n\n\nEnable public access from an X11 Server\nFor the X11 Server it appears as if the traffic is coming from elsewhere so we have to enable public access. Using VcXsrv via XLaunch this means checking the “Disable access control” on the “Extra settings” page.\n\n\n\nDisable Access Control\n\n\nAlternatively you can do this with the -ac command line parameter to the vcxsrv executable. Assuming it’s installed in Program Files you could invoke it from Powershell as follows:\n& 'C:\\Program Files\\VcXsrv\\vcxsrv.exe' :0 -multiwindow -wgl -ac -silent-dup-error\nThis sets up a multiwindow display using Windows GL with disabled access control, and doesn’t raise an error if this already exists (so it’s safe to run multiple times).\n\n\nExport appropriate display variables\nFinally you need to export the DISPLAY to the corresponding port from Windows, and set LIBGL_ALWAYS_INDIRECT. You can add this to your .bashrc or similar.\nexport DISPLAY=$(awk '/nameserver / {print $2; exit}' /etc/resolv.conf 2>/dev/null):0\nexport LIBGL_ALWAYS_INDIRECT=1\nNow you should be able to run applications.\n\n\nMaking the process smoother\nI used this for Emacs 26 under WSL2, and it works perfect after disabling double buffering.\nOnce it’s configured you can run it directly from Powershell running bash -c 'source ~/.bashrc; emacs'. Note that if you exported the display variables in a local config file like .bashrc.local you have to source that file instead.\nYou could put this all together into a single shortcut that starts the display and executes the application:\nC:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe\n  -windowstyle hidden\n  -command & 'C:\\Program Files\\VcXsrv\\vcxsrv.exe' :0\n    -multiwindow -wgl -ac -silent-dup-error;\n  \"&{ bash -c \\\"source ~/.localrc; cd; emacs\\\" }\"\nAlternatively you could even launch a Window Manager and run all your linux applications from there."
  },
  {
    "objectID": "hn-asin/index.html",
    "href": "hn-asin/index.html",
    "title": "Finding ASINs in HackerNews",
    "section": "",
    "text": "One way to extract books is using Amazon links. People often refer to a book with a link to Amazon, and each Amazon product has a 10 character ASIN (Amazon Standard Identification Number); for books this is the same as its ISBN-10.\nI spent some time iterating on a regular expression to find these. I would take a sample of 10,000 posts, look at the top results and check them. Eventually I settled on amazon\\.[^\"> ]*/dp/([A-Z0-9]{10})\\W; this works across different TLDs (e.g. .com, .co.uk, .ca), captures most examples and has few false positives.\nThere were only under 200 ASINs that have occured more than once in 2021.\n\n\n\nFrequency\nDistinct ASINs\n\n\n\n\n1\n2243\n\n\n2\n158\n\n\n3\n40\n\n\n4\n7\n\n\n5\n4\n\n\n7\n1\n\n\n6\n1\n\n\n\nI checked the 10 most common ASINs on Amazon and they were all books. I also checked another random 10 and most were books, but there were also other products. Looking through the text it was pretty easy to see which were books and which were electronics. I started looking for patterns in the text for more ideas on how to extract books and filter out electronics. But then I realised I was effectively doing annotation work that I didn’t capture; instead I exported it to a CSV to enable more analysis and annotation.\nIf you want to see the gory details check out the notebook."
  },
  {
    "objectID": "embed-behaviour/index.html",
    "href": "embed-behaviour/index.html",
    "title": "Using Behaviour to Understand Items",
    "section": "",
    "text": "Using behavioural information can greatly improve modelling on the tabular data in your database. In natural language processing there a large number of tasks like predicting whether a sentence is saying something good or bad (sentiment analysis), identifying people and places in a document (Named Entity Recognition) and translating text from one language to another (machine translation). In recent years performance on all these tasks have increased dramatically by firt pretraining a language model before training on the specific task. A language model predicts the next word (or letters) that will come part way through a sentence, like the next word suggestions on your phone’s keyboard (or write with transformer to see a state of the art example). By becoming really good at predicting the next word a language model gets really good representations of language, which allows it to better determine what a sentence is saying, whether these words are someone’s name and how to produce grammatical output in a translation. Recently the same idea has started to show promising results in computer vision, where it is called self-supervised learning. So if you’ve got a large database filled with behavioural information then you’re only going to get the best results on building on that knowledge.\nCreating tasks that embed categorical items is a powerful way to use behavioural data, and enables use of the embeddings in other tasks. I’ve already written about building categorical embeddings; in short instead of treating items as independent it lets you put them as points in some vector space where similar items are close together. Suppose you’re trying to gauge interest on some very niche category of item; you don’t have much direct information on the category and so can’t directly measure it. But in an embedding you can use information about interest of related items to infer how interest is changing for this item.\nThinking about a website like eBay there are lots of different ways you can use behaviour to understand items. Uou could use people’s intent behaviour on items to understand both of them. For example people who view, watch (save), and buy many of the same items are likely similar in some way and products that are viewed, saved and bought by many of the same people are similar. But this doesn’t work for all sorts of purchases; maybe you need to filter it by search behaviour. Items that are likely to be clicked on by the same search term are likely to be similar in a more contextual sense. Items that are often posted together by the same person are similar in a different sense. Items that contain similar text descriptions or a similar photograph are similar in yet another way. You could do this for individual items, categories of items, or aspects of items. You could even use this to make categories of items.\nIt would be great to demonstrate this effect in the open, but it’s hard to find good datasets. A good real example of this is the 3rd place solution to the Kaggle Rossman Stores Sales Data competition, where they solved it with a neural network and got informative embeddings of locations. The Google Analytics sample dataset is a good candidate, containing user behaviour in the Google Merchandise store. There’s an ecommerce behavioural dataset on Kaggle which also could be good. Or using a big public forum like pushshift.io’s export of Reddit, Hacker News or Stack Exchange could be used by looking at people that comment in the same threads. In each of these cases it would be worth nominating a task to show improvement on, like predicting revenue form Google Analytics or closed questions on Stack Overflow."
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "",
    "text": "Chapter 4 of the fastai book covers how to build a Neural Network for distinguishing 3s and 7s on MNIST from scratch. We’re going to do a similar thing but instead of building the neural network from the ground up we’re going to use fastai’s layered API to build it top down. We’ll start with the high level API to train a dense neural network in a few lines. Then we’ll redo the problem going deeper and deeper into the API. At the very core it’s mainly PyTorch, and we’ll have a pure PyTorch implementation like in the book. Then we’ll start rebuilding the abstractions from scratch to get a high level API like we started with.\nInstead of using the MNIST digits we’ll use Fashion MNIST, which contains little black and white images of different types of clothing. This is a bit harder and a convolutional neural network would perform better here (as demonstrated in v3 of fastai course). But to keep things simple we’ll use a dense neural network.\nThis post was generated with a Jupyter notebook. You can also view this notebook on Kaggle or download the Jupyter notebook."
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#what-did-we-just-do",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#what-did-we-just-do",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "What did we just do?",
    "text": "What did we just do?\nLet’s go back through those 5 lines slowly to see what was going on.\n\n1. Import\nThe first line imports all the libraries we need for tabular analysis.\nThis includes specific fastai libraries, as well as general utilities such as Pandas, numpy and PyTorch, and much more\nfrom fastai.tabular.all import *\nIf you want to see exactly what was imported you can look into the module or the source code.\nimport fastai.tabular.all\nL(dir(fastai.tabular.all))\n(#846) ['APScoreBinary','APScoreMulti','AccumMetric','ActivationStats','Adam','AdaptiveAvgPool','AdaptiveConcatPool1d','AdaptiveConcatPool2d','ArrayBase','ArrayImage'...]\nThis includes standard imports like “pandas as pd”\nfastai.tabular.all.pd\n<module 'pandas' from '/opt/conda/lib/python3.7/site-packages/pandas/__init__.py'>\n\n\n2. Data\nWe read in the data from Pandas as a CSV, letting Pandas know that the label column is categorical.\ndf = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv', dtype={'label':'category'})\nThe dataframe contains a label column giving the kind of image, and then 784 columns for the pixel value from 0-255.\ndf\n\n\n\n\n\n\n\n\n\nlabel\n\n\npixel1\n\n\npixel2\n\n\npixel3\n\n\npixel4\n\n\npixel5\n\n\npixel6\n\n\npixel7\n\n\npixel8\n\n\npixel9\n\n\n…\n\n\npixel775\n\n\npixel776\n\n\npixel777\n\n\npixel778\n\n\npixel779\n\n\npixel780\n\n\npixel781\n\n\npixel782\n\n\npixel783\n\n\npixel784\n\n\n\n\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n6\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n5\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n30\n\n\n43\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n59995\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59996\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n73\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59997\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n160\n\n\n162\n\n\n163\n\n\n135\n\n\n94\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59998\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59999\n\n\n7\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n60000 rows × 785 columns\n\n\nA histogram of the pixels shows they are mostly 0, with values up to 255.\n_ = plt.hist(df.filter(like='pixel', axis=1).to_numpy().reshape(-1))\n\n\n\npng\n\n\nFrom a singe row of the dataframe we can read the label, and the pixels\nlabel, *pixels = df.iloc[0]\n\nlabel, len(pixels)\n('2', 784)\nThe 784 pixels are actually 28 rows of the image, each containing 28 columns. If we rearrange them we can plot it as an image.\nimage_array = np.array(pixels).reshape(28, 28)\n_ = plt.imshow(image_array, cmap='Greys')\n\n\n\npng\n\n\nAll we are seeing here are the pixel intensities from 0 (white) to 255 (black) on a grid.\nfig, ax = plt.subplots(figsize=(12,12))\nim = ax.imshow(image_array, cmap=\"Greys\")\n\nfor i in range(image_array.shape[0]):\n    for j in range(image_array.shape[1]):\n        text = ax.text(j, i, image_array[i, j], ha=\"center\", va=\"center\", color=\"magenta\")\n\n\n\npng\n\n\nThe labels are categorical codes for different types of clothing.\nWe can copy the label description and convert it into a Python dictionary.\nlabels_txt = \"\"\"\nLabel   Description\n0   T-shirt/top\n1   Trouser\n2   Pullover\n3   Dress\n4   Coat\n5   Sandal\n6   Shirt\n7   Sneaker\n8   Bag\n9   Ankle boot\n\"\"\".strip()\n\nlabels = dict([row.split('\\t') for row in labels_txt.split('\\n')[1:]])\nlabels\n{'0': 'T-shirt/top',\n '1': 'Trouser',\n '2': 'Pullover',\n '3': 'Dress',\n '4': 'Coat',\n '5': 'Sandal',\n '6': 'Shirt',\n '7': 'Sneaker',\n '8': 'Bag',\n '9': 'Ankle boot'}\nThe image above is of a Pullover\nlabel, labels[label]\n('2', 'Pullover')\nWe’ve got 6000 images of each type.\ndf.label.map(labels).value_counts()\nT-shirt/top    6000\nTrouser        6000\nPullover       6000\nDress          6000\nCoat           6000\nSandal         6000\nShirt          6000\nSneaker        6000\nBag            6000\nAnkle boot     6000\nName: label, dtype: int64\n\n\n3. Dataloader\nNow we have our raw data we need a way to pass that into the model in a way it understands. We do this with a DataLoader reading from the dataframe. We need to tell it:\n\ndf: the dataframe to read from\ny_names: the name of the column containing the outcome variable, here label\nbs: the batch size, how many rows to feed to the model each time. We use 4096 because the data and models are small\nprocs: any preprocessing steps to do, here we use Normalize to map them from 0-255 to a more reasonable range.\ncont_names: The name of the continuous columns\n\nNote that before we didn’t pass cont_names and it automatically detected them; however it can reorder the columns so we specify it here for clarity.\ndls = TabularDataLoaders.from_df(df, y_names='label', bs=4096, procs=[Normalize], cont_names=list(df.columns[1:]))\nThis data loader can then produce the pixel arrays for a subset of rows, and the outcome labels on demand. Note these are the values before using procs.\ndls.show_batch()\n\n\n\n\n\n\n\npixel1\n\n\npixel2\n\n\npixel3\n\n\npixel4\n\n\npixel5\n\n\npixel6\n\n\npixel7\n\n\npixel8\n\n\npixel9\n\n\npixel10\n\n\npixel11\n\n\npixel12\n\n\npixel13\n\n\npixel14\n\n\npixel15\n\n\npixel16\n\n\npixel17\n\n\npixel18\n\n\npixel19\n\n\npixel20\n\n\npixel21\n\n\npixel22\n\n\npixel23\n\n\npixel24\n\n\npixel25\n\n\npixel26\n\n\npixel27\n\n\npixel28\n\n\npixel29\n\n\npixel30\n\n\npixel31\n\n\npixel32\n\n\npixel33\n\n\npixel34\n\n\npixel35\n\n\npixel36\n\n\npixel37\n\n\npixel38\n\n\npixel39\n\n\npixel40\n\n\npixel41\n\n\npixel42\n\n\npixel43\n\n\npixel44\n\n\npixel45\n\n\npixel46\n\n\npixel47\n\n\npixel48\n\n\npixel49\n\n\npixel50\n\n\npixel51\n\n\npixel52\n\n\npixel53\n\n\npixel54\n\n\npixel55\n\n\npixel56\n\n\npixel57\n\n\npixel58\n\n\npixel59\n\n\npixel60\n\n\npixel61\n\n\npixel62\n\n\npixel63\n\n\npixel64\n\n\npixel65\n\n\npixel66\n\n\npixel67\n\n\npixel68\n\n\npixel69\n\n\npixel70\n\n\npixel71\n\n\npixel72\n\n\npixel73\n\n\npixel74\n\n\npixel75\n\n\npixel76\n\n\npixel77\n\n\npixel78\n\n\npixel79\n\n\npixel80\n\n\npixel81\n\n\npixel82\n\n\npixel83\n\n\npixel84\n\n\npixel85\n\n\npixel86\n\n\npixel87\n\n\npixel88\n\n\npixel89\n\n\npixel90\n\n\npixel91\n\n\npixel92\n\n\npixel93\n\n\npixel94\n\n\npixel95\n\n\npixel96\n\n\npixel97\n\n\npixel98\n\n\npixel99\n\n\npixel100\n\n\npixel101\n\n\npixel102\n\n\npixel103\n\n\npixel104\n\n\npixel105\n\n\npixel106\n\n\npixel107\n\n\npixel108\n\n\npixel109\n\n\npixel110\n\n\npixel111\n\n\npixel112\n\n\npixel113\n\n\npixel114\n\n\npixel115\n\n\npixel116\n\n\npixel117\n\n\npixel118\n\n\npixel119\n\n\npixel120\n\n\npixel121\n\n\npixel122\n\n\npixel123\n\n\npixel124\n\n\npixel125\n\n\npixel126\n\n\npixel127\n\n\npixel128\n\n\npixel129\n\n\npixel130\n\n\npixel131\n\n\npixel132\n\n\npixel133\n\n\npixel134\n\n\npixel135\n\n\npixel136\n\n\npixel137\n\n\npixel138\n\n\npixel139\n\n\npixel140\n\n\npixel141\n\n\npixel142\n\n\npixel143\n\n\npixel144\n\n\npixel145\n\n\npixel146\n\n\npixel147\n\n\npixel148\n\n\npixel149\n\n\npixel150\n\n\npixel151\n\n\npixel152\n\n\npixel153\n\n\npixel154\n\n\npixel155\n\n\npixel156\n\n\npixel157\n\n\npixel158\n\n\npixel159\n\n\npixel160\n\n\npixel161\n\n\npixel162\n\n\npixel163\n\n\npixel164\n\n\npixel165\n\n\npixel166\n\n\npixel167\n\n\npixel168\n\n\npixel169\n\n\npixel170\n\n\npixel171\n\n\npixel172\n\n\npixel173\n\n\npixel174\n\n\npixel175\n\n\npixel176\n\n\npixel177\n\n\npixel178\n\n\npixel179\n\n\npixel180\n\n\npixel181\n\n\npixel182\n\n\npixel183\n\n\npixel184\n\n\npixel185\n\n\npixel186\n\n\npixel187\n\n\npixel188\n\n\npixel189\n\n\npixel190\n\n\npixel191\n\n\npixel192\n\n\npixel193\n\n\npixel194\n\n\npixel195\n\n\npixel196\n\n\npixel197\n\n\npixel198\n\n\npixel199\n\n\npixel200\n\n\npixel201\n\n\npixel202\n\n\npixel203\n\n\npixel204\n\n\npixel205\n\n\npixel206\n\n\npixel207\n\n\npixel208\n\n\npixel209\n\n\npixel210\n\n\npixel211\n\n\npixel212\n\n\npixel213\n\n\npixel214\n\n\npixel215\n\n\npixel216\n\n\npixel217\n\n\npixel218\n\n\npixel219\n\n\npixel220\n\n\npixel221\n\n\npixel222\n\n\npixel223\n\n\npixel224\n\n\npixel225\n\n\npixel226\n\n\npixel227\n\n\npixel228\n\n\npixel229\n\n\npixel230\n\n\npixel231\n\n\npixel232\n\n\npixel233\n\n\npixel234\n\n\npixel235\n\n\npixel236\n\n\npixel237\n\n\npixel238\n\n\npixel239\n\n\npixel240\n\n\npixel241\n\n\npixel242\n\n\npixel243\n\n\npixel244\n\n\npixel245\n\n\npixel246\n\n\npixel247\n\n\npixel248\n\n\npixel249\n\n\npixel250\n\n\npixel251\n\n\npixel252\n\n\npixel253\n\n\npixel254\n\n\npixel255\n\n\npixel256\n\n\npixel257\n\n\npixel258\n\n\npixel259\n\n\npixel260\n\n\npixel261\n\n\npixel262\n\n\npixel263\n\n\npixel264\n\n\npixel265\n\n\npixel266\n\n\npixel267\n\n\npixel268\n\n\npixel269\n\n\npixel270\n\n\npixel271\n\n\npixel272\n\n\npixel273\n\n\npixel274\n\n\npixel275\n\n\npixel276\n\n\npixel277\n\n\npixel278\n\n\npixel279\n\n\npixel280\n\n\npixel281\n\n\npixel282\n\n\npixel283\n\n\npixel284\n\n\npixel285\n\n\npixel286\n\n\npixel287\n\n\npixel288\n\n\npixel289\n\n\npixel290\n\n\npixel291\n\n\npixel292\n\n\npixel293\n\n\npixel294\n\n\npixel295\n\n\npixel296\n\n\npixel297\n\n\npixel298\n\n\npixel299\n\n\npixel300\n\n\npixel301\n\n\npixel302\n\n\npixel303\n\n\npixel304\n\n\npixel305\n\n\npixel306\n\n\npixel307\n\n\npixel308\n\n\npixel309\n\n\npixel310\n\n\npixel311\n\n\npixel312\n\n\npixel313\n\n\npixel314\n\n\npixel315\n\n\npixel316\n\n\npixel317\n\n\npixel318\n\n\npixel319\n\n\npixel320\n\n\npixel321\n\n\npixel322\n\n\npixel323\n\n\npixel324\n\n\npixel325\n\n\npixel326\n\n\npixel327\n\n\npixel328\n\n\npixel329\n\n\npixel330\n\n\npixel331\n\n\npixel332\n\n\npixel333\n\n\npixel334\n\n\npixel335\n\n\npixel336\n\n\npixel337\n\n\npixel338\n\n\npixel339\n\n\npixel340\n\n\npixel341\n\n\npixel342\n\n\npixel343\n\n\npixel344\n\n\npixel345\n\n\npixel346\n\n\npixel347\n\n\npixel348\n\n\npixel349\n\n\npixel350\n\n\npixel351\n\n\npixel352\n\n\npixel353\n\n\npixel354\n\n\npixel355\n\n\npixel356\n\n\npixel357\n\n\npixel358\n\n\npixel359\n\n\npixel360\n\n\npixel361\n\n\npixel362\n\n\npixel363\n\n\npixel364\n\n\npixel365\n\n\npixel366\n\n\npixel367\n\n\npixel368\n\n\npixel369\n\n\npixel370\n\n\npixel371\n\n\npixel372\n\n\npixel373\n\n\npixel374\n\n\npixel375\n\n\npixel376\n\n\npixel377\n\n\npixel378\n\n\npixel379\n\n\npixel380\n\n\npixel381\n\n\npixel382\n\n\npixel383\n\n\npixel384\n\n\npixel385\n\n\npixel386\n\n\npixel387\n\n\npixel388\n\n\npixel389\n\n\npixel390\n\n\npixel391\n\n\npixel392\n\n\npixel393\n\n\npixel394\n\n\npixel395\n\n\npixel396\n\n\npixel397\n\n\npixel398\n\n\npixel399\n\n\npixel400\n\n\npixel401\n\n\npixel402\n\n\npixel403\n\n\npixel404\n\n\npixel405\n\n\npixel406\n\n\npixel407\n\n\npixel408\n\n\npixel409\n\n\npixel410\n\n\npixel411\n\n\npixel412\n\n\npixel413\n\n\npixel414\n\n\npixel415\n\n\npixel416\n\n\npixel417\n\n\npixel418\n\n\npixel419\n\n\npixel420\n\n\npixel421\n\n\npixel422\n\n\npixel423\n\n\npixel424\n\n\npixel425\n\n\npixel426\n\n\npixel427\n\n\npixel428\n\n\npixel429\n\n\npixel430\n\n\npixel431\n\n\npixel432\n\n\npixel433\n\n\npixel434\n\n\npixel435\n\n\npixel436\n\n\npixel437\n\n\npixel438\n\n\npixel439\n\n\npixel440\n\n\npixel441\n\n\npixel442\n\n\npixel443\n\n\npixel444\n\n\npixel445\n\n\npixel446\n\n\npixel447\n\n\npixel448\n\n\npixel449\n\n\npixel450\n\n\npixel451\n\n\npixel452\n\n\npixel453\n\n\npixel454\n\n\npixel455\n\n\npixel456\n\n\npixel457\n\n\npixel458\n\n\npixel459\n\n\npixel460\n\n\npixel461\n\n\npixel462\n\n\npixel463\n\n\npixel464\n\n\npixel465\n\n\npixel466\n\n\npixel467\n\n\npixel468\n\n\npixel469\n\n\npixel470\n\n\npixel471\n\n\npixel472\n\n\npixel473\n\n\npixel474\n\n\npixel475\n\n\npixel476\n\n\npixel477\n\n\npixel478\n\n\npixel479\n\n\npixel480\n\n\npixel481\n\n\npixel482\n\n\npixel483\n\n\npixel484\n\n\npixel485\n\n\npixel486\n\n\npixel487\n\n\npixel488\n\n\npixel489\n\n\npixel490\n\n\npixel491\n\n\npixel492\n\n\npixel493\n\n\npixel494\n\n\npixel495\n\n\npixel496\n\n\npixel497\n\n\npixel498\n\n\npixel499\n\n\npixel500\n\n\npixel501\n\n\npixel502\n\n\npixel503\n\n\npixel504\n\n\npixel505\n\n\npixel506\n\n\npixel507\n\n\npixel508\n\n\npixel509\n\n\npixel510\n\n\npixel511\n\n\npixel512\n\n\npixel513\n\n\npixel514\n\n\npixel515\n\n\npixel516\n\n\npixel517\n\n\npixel518\n\n\npixel519\n\n\npixel520\n\n\npixel521\n\n\npixel522\n\n\npixel523\n\n\npixel524\n\n\npixel525\n\n\npixel526\n\n\npixel527\n\n\npixel528\n\n\npixel529\n\n\npixel530\n\n\npixel531\n\n\npixel532\n\n\npixel533\n\n\npixel534\n\n\npixel535\n\n\npixel536\n\n\npixel537\n\n\npixel538\n\n\npixel539\n\n\npixel540\n\n\npixel541\n\n\npixel542\n\n\npixel543\n\n\npixel544\n\n\npixel545\n\n\npixel546\n\n\npixel547\n\n\npixel548\n\n\npixel549\n\n\npixel550\n\n\npixel551\n\n\npixel552\n\n\npixel553\n\n\npixel554\n\n\npixel555\n\n\npixel556\n\n\npixel557\n\n\npixel558\n\n\npixel559\n\n\npixel560\n\n\npixel561\n\n\npixel562\n\n\npixel563\n\n\npixel564\n\n\npixel565\n\n\npixel566\n\n\npixel567\n\n\npixel568\n\n\npixel569\n\n\npixel570\n\n\npixel571\n\n\npixel572\n\n\npixel573\n\n\npixel574\n\n\npixel575\n\n\npixel576\n\n\npixel577\n\n\npixel578\n\n\npixel579\n\n\npixel580\n\n\npixel581\n\n\npixel582\n\n\npixel583\n\n\npixel584\n\n\npixel585\n\n\npixel586\n\n\npixel587\n\n\npixel588\n\n\npixel589\n\n\npixel590\n\n\npixel591\n\n\npixel592\n\n\npixel593\n\n\npixel594\n\n\npixel595\n\n\npixel596\n\n\npixel597\n\n\npixel598\n\n\npixel599\n\n\npixel600\n\n\npixel601\n\n\npixel602\n\n\npixel603\n\n\npixel604\n\n\npixel605\n\n\npixel606\n\n\npixel607\n\n\npixel608\n\n\npixel609\n\n\npixel610\n\n\npixel611\n\n\npixel612\n\n\npixel613\n\n\npixel614\n\n\npixel615\n\n\npixel616\n\n\npixel617\n\n\npixel618\n\n\npixel619\n\n\npixel620\n\n\npixel621\n\n\npixel622\n\n\npixel623\n\n\npixel624\n\n\npixel625\n\n\npixel626\n\n\npixel627\n\n\npixel628\n\n\npixel629\n\n\npixel630\n\n\npixel631\n\n\npixel632\n\n\npixel633\n\n\npixel634\n\n\npixel635\n\n\npixel636\n\n\npixel637\n\n\npixel638\n\n\npixel639\n\n\npixel640\n\n\npixel641\n\n\npixel642\n\n\npixel643\n\n\npixel644\n\n\npixel645\n\n\npixel646\n\n\npixel647\n\n\npixel648\n\n\npixel649\n\n\npixel650\n\n\npixel651\n\n\npixel652\n\n\npixel653\n\n\npixel654\n\n\npixel655\n\n\npixel656\n\n\npixel657\n\n\npixel658\n\n\npixel659\n\n\npixel660\n\n\npixel661\n\n\npixel662\n\n\npixel663\n\n\npixel664\n\n\npixel665\n\n\npixel666\n\n\npixel667\n\n\npixel668\n\n\npixel669\n\n\npixel670\n\n\npixel671\n\n\npixel672\n\n\npixel673\n\n\npixel674\n\n\npixel675\n\n\npixel676\n\n\npixel677\n\n\npixel678\n\n\npixel679\n\n\npixel680\n\n\npixel681\n\n\npixel682\n\n\npixel683\n\n\npixel684\n\n\npixel685\n\n\npixel686\n\n\npixel687\n\n\npixel688\n\n\npixel689\n\n\npixel690\n\n\npixel691\n\n\npixel692\n\n\npixel693\n\n\npixel694\n\n\npixel695\n\n\npixel696\n\n\npixel697\n\n\npixel698\n\n\npixel699\n\n\npixel700\n\n\npixel701\n\n\npixel702\n\n\npixel703\n\n\npixel704\n\n\npixel705\n\n\npixel706\n\n\npixel707\n\n\npixel708\n\n\npixel709\n\n\npixel710\n\n\npixel711\n\n\npixel712\n\n\npixel713\n\n\npixel714\n\n\npixel715\n\n\npixel716\n\n\npixel717\n\n\npixel718\n\n\npixel719\n\n\npixel720\n\n\npixel721\n\n\npixel722\n\n\npixel723\n\n\npixel724\n\n\npixel725\n\n\npixel726\n\n\npixel727\n\n\npixel728\n\n\npixel729\n\n\npixel730\n\n\npixel731\n\n\npixel732\n\n\npixel733\n\n\npixel734\n\n\npixel735\n\n\npixel736\n\n\npixel737\n\n\npixel738\n\n\npixel739\n\n\npixel740\n\n\npixel741\n\n\npixel742\n\n\npixel743\n\n\npixel744\n\n\npixel745\n\n\npixel746\n\n\npixel747\n\n\npixel748\n\n\npixel749\n\n\npixel750\n\n\npixel751\n\n\npixel752\n\n\npixel753\n\n\npixel754\n\n\npixel755\n\n\npixel756\n\n\npixel757\n\n\npixel758\n\n\npixel759\n\n\npixel760\n\n\npixel761\n\n\npixel762\n\n\npixel763\n\n\npixel764\n\n\npixel765\n\n\npixel766\n\n\npixel767\n\n\npixel768\n\n\npixel769\n\n\npixel770\n\n\npixel771\n\n\npixel772\n\n\npixel773\n\n\npixel774\n\n\npixel775\n\n\npixel776\n\n\npixel777\n\n\npixel778\n\n\npixel779\n\n\npixel780\n\n\npixel781\n\n\npixel782\n\n\npixel783\n\n\npixel784\n\n\nlabel\n\n\n\n\n\n\n0\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n9.682471e-07\n\n\n-0.000002\n\n\n-5.505288e-07\n\n\n4.053924e-07\n\n\n4.035032e-07\n\n\n2.956210e-07\n\n\n4.005417e-07\n\n\n-0.000001\n\n\n5.516976e-08\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n-0.000001\n\n\n7.044741e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-7.585758e-07\n\n\n-0.000006\n\n\n0.000006\n\n\n0.000002\n\n\n-1.279166e-07\n\n\n0.000001\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000003\n\n\n-4.594104e-07\n\n\n0.000004\n\n\n0.000005\n\n\n0.000001\n\n\n-4.233165e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n8.386749e-07\n\n\n0.000002\n\n\n0.000005\n\n\n0.000004\n\n\n-0.000003\n\n\n-0.000003\n\n\n-0.000003\n\n\n-0.000004\n\n\n-6.836428e-07\n\n\n-3.299791e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n5.257434e-07\n\n\n8.999998e+00\n\n\n6.600000e+01\n\n\n9.500000e+01\n\n\n9.700000e+01\n\n\n1.000000e+01\n\n\n-4.949160e-09\n\n\n1.000000e+00\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000002\n\n\n2.000001\n\n\n0.000003\n\n\n10.000000\n\n\n1.880000e+02\n\n\n2.170000e+02\n\n\n238.000000\n\n\n250.000003\n\n\n251.000002\n\n\n2.550000e+02\n\n\n218.000003\n\n\n2.290000e+02\n\n\n2.550000e+02\n\n\n2.550000e+02\n\n\n2.360000e+02\n\n\n2.550000e+02\n\n\n1.420000e+02\n\n\n-1.021340e-08\n\n\n2.000000e+00\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000005\n\n\n9.999999\n\n\n6.513797e-07\n\n\n70.999999\n\n\n255.000005\n\n\n236.999996\n\n\n236.000001\n\n\n236.999995\n\n\n229.000004\n\n\n225.000003\n\n\n234.999999\n\n\n2.310000e+02\n\n\n224.000001\n\n\n2.240000e+02\n\n\n2.190000e+02\n\n\n2.440000e+02\n\n\n1.430000e+02\n\n\n1.621610e-07\n\n\n1.000000e+00\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-8.460442e-08\n\n\n0.000003\n\n\n6.000000e+00\n\n\n-0.000002\n\n\n58.000001\n\n\n246.999997\n\n\n228.000003\n\n\n232.999999\n\n\n233.999998\n\n\n227.999995\n\n\n227.000000\n\n\n232.000000\n\n\n220.999996\n\n\n2.280000e+02\n\n\n239.000001\n\n\n2.310000e+02\n\n\n2.450000e+02\n\n\n1.570000e+02\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n-0.000001\n\n\n0.000001\n\n\n-0.000004\n\n\n3.999998e+00\n\n\n-0.000004\n\n\n1.440000e+02\n\n\n248.999999\n\n\n228.000004\n\n\n241.999999\n\n\n240.999997\n\n\n240.000003\n\n\n237.999997\n\n\n242.999999\n\n\n238.000004\n\n\n234.000004\n\n\n240.000001\n\n\n230.999996\n\n\n2.460000e+02\n\n\n2.150000e+02\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n7.406876e-07\n\n\n-0.000003\n\n\n-9.455209e-07\n\n\n0.000004\n\n\n0.000004\n\n\n-0.000002\n\n\n154.999998\n\n\n246.999996\n\n\n217.000003\n\n\n230.000003\n\n\n228.000001\n\n\n228.000003\n\n\n229.000003\n\n\n223.999996\n\n\n2.300000e+02\n\n\n229.999996\n\n\n237.999998\n\n\n220.000002\n\n\n244.999993\n\n\n2.220000e+02\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n9.804652e-07\n\n\n0.000001\n\n\n-0.000003\n\n\n-0.000005\n\n\n-0.000004\n\n\n-0.000004\n\n\n183.999997\n\n\n250.000004\n\n\n2.260000e+02\n\n\n232.000004\n\n\n228.999998\n\n\n228.000002\n\n\n226.999997\n\n\n221.999995\n\n\n229.000000\n\n\n234.000001\n\n\n231.999997\n\n\n228.000004\n\n\n2.410000e+02\n\n\n2.440000e+02\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000002\n\n\n254.999999\n\n\n241.000002\n\n\n229.000004\n\n\n233.999998\n\n\n235.999997\n\n\n236.000002\n\n\n233.999998\n\n\n231.000000\n\n\n2.350000e+02\n\n\n239.000004\n\n\n234.999997\n\n\n2.370000e+02\n\n\n230.999995\n\n\n255.000001\n\n\n1.000000e+02\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n1.999999\n\n\n0.000005\n\n\n29.000004\n\n\n254.999996\n\n\n229.999997\n\n\n237.000003\n\n\n234.000000\n\n\n236.999997\n\n\n238.000002\n\n\n236.000000\n\n\n235.000003\n\n\n235.999998\n\n\n232.000001\n\n\n236.999995\n\n\n237.000003\n\n\n229.000001\n\n\n2.390000e+02\n\n\n226.999998\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n0.000002\n\n\n-0.000004\n\n\n176.999998\n\n\n249.999996\n\n\n225.999996\n\n\n2.370000e+02\n\n\n233.000003\n\n\n234.999999\n\n\n237.000003\n\n\n234.999996\n\n\n233.000005\n\n\n236.000004\n\n\n235.999998\n\n\n235.000000\n\n\n233.000000\n\n\n230.000003\n\n\n226.000004\n\n\n255.000000\n\n\n2.700000e+01\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n0.000001\n\n\n3.000000\n\n\n0.000001\n\n\n24.999996\n\n\n254.999996\n\n\n230.999999\n\n\n245.000004\n\n\n234.999997\n\n\n234.000001\n\n\n232.000003\n\n\n235.999999\n\n\n235.999998\n\n\n233.999996\n\n\n2.370000e+02\n\n\n235.999995\n\n\n235.999998\n\n\n230.000004\n\n\n227.999997\n\n\n2.210000e+02\n\n\n254.999991\n\n\n9.300000e+01\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n0.000002\n\n\n0.000005\n\n\n-0.000001\n\n\n0.000003\n\n\n201.999999\n\n\n243.999999\n\n\n226.000003\n\n\n2.350000e+02\n\n\n237.000004\n\n\n234.000001\n\n\n230.999999\n\n\n233.999997\n\n\n235.000000\n\n\n232.000001\n\n\n234.999997\n\n\n2.350000e+02\n\n\n2.300000e+02\n\n\n226.000000\n\n\n224.999997\n\n\n2.210000e+02\n\n\n255.000004\n\n\n1.510000e+02\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n1.000003\n\n\n1.999999e+00\n\n\n0.000004\n\n\n0.000005\n\n\n1.070000e+02\n\n\n254.999997\n\n\n231.999999\n\n\n232.999999\n\n\n231.000002\n\n\n233.999996\n\n\n236.000002\n\n\n232.999996\n\n\n233.000004\n\n\n231.000002\n\n\n229.999999\n\n\n234.000000\n\n\n232.999996\n\n\n2.280000e+02\n\n\n229.999995\n\n\n231.999996\n\n\n2.230000e+02\n\n\n2.550000e+02\n\n\n1.730000e+02\n\n\n-2.135472e-07\n\n\n1.000000e+00\n\n\n5.000000e+00\n\n\n8.000000\n\n\n-5.332055e-07\n\n\n5.139264e-07\n\n\n-0.000002\n\n\n-0.000004\n\n\n74.000001\n\n\n254.999999\n\n\n231.999998\n\n\n229.000002\n\n\n233.000003\n\n\n232.999999\n\n\n232.999998\n\n\n234.999997\n\n\n232.999999\n\n\n232.000003\n\n\n232.999998\n\n\n2.280000e+02\n\n\n223.000004\n\n\n2.330000e+02\n\n\n228.000000\n\n\n227.000001\n\n\n229.000000\n\n\n220.000000\n\n\n2.550000e+02\n\n\n1.340000e+02\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n0.000002\n\n\n68.999999\n\n\n197.999998\n\n\n254.999996\n\n\n237.000001\n\n\n229.000001\n\n\n231.000004\n\n\n233.000000\n\n\n231.999996\n\n\n233.999999\n\n\n2.340000e+02\n\n\n234.000004\n\n\n230.999996\n\n\n231.000002\n\n\n2.280000e+02\n\n\n2.310000e+02\n\n\n2.230000e+02\n\n\n227.999996\n\n\n231.999995\n\n\n2.380000e+02\n\n\n2.330000e+02\n\n\n255.000009\n\n\n2.000000e+00\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n74.999998\n\n\n1.330000e+02\n\n\n196.000003\n\n\n245.999997\n\n\n254.999999\n\n\n244.000001\n\n\n227.999995\n\n\n230.999998\n\n\n231.999996\n\n\n233.000002\n\n\n237.000000\n\n\n232.999998\n\n\n234.000001\n\n\n233.999996\n\n\n232.999998\n\n\n233.000000\n\n\n230.000000\n\n\n222.000002\n\n\n227.000002\n\n\n250.999998\n\n\n246.000005\n\n\n2.320000e+02\n\n\n2.200000e+02\n\n\n232.999997\n\n\n1.860000e+02\n\n\n-3.012789e-07\n\n\n6.600000e+01\n\n\n2.280000e+02\n\n\n2.410000e+02\n\n\n2.420000e+02\n\n\n2.370000e+02\n\n\n2.350000e+02\n\n\n224.999999\n\n\n211.999995\n\n\n2.300000e+02\n\n\n232.000002\n\n\n233.000002\n\n\n235.000004\n\n\n235.999999\n\n\n232.000002\n\n\n234.000000\n\n\n2.340000e+02\n\n\n235.999999\n\n\n226.000000\n\n\n227.000003\n\n\n254.999995\n\n\n254.999998\n\n\n197.999995\n\n\n215.000005\n\n\n202.000000\n\n\n186.999999\n\n\n2.360000e+02\n\n\n140.000000\n\n\n4.011770e-07\n\n\n1.750000e+02\n\n\n2.470000e+02\n\n\n2.140000e+02\n\n\n224.000000\n\n\n222.000002\n\n\n221.000005\n\n\n2.280000e+02\n\n\n233.000004\n\n\n2.310000e+02\n\n\n2.330000e+02\n\n\n231.000003\n\n\n231.000000\n\n\n229.999996\n\n\n230.999999\n\n\n234.999996\n\n\n236.000004\n\n\n227.000001\n\n\n235.999997\n\n\n255.000001\n\n\n217.000005\n\n\n17.000001\n\n\n-2.610268e-07\n\n\n204.999999\n\n\n220.000001\n\n\n204.999997\n\n\n218.000003\n\n\n7.500000e+01\n\n\n-1.202886e-08\n\n\n9.600000e+01\n\n\n2.540000e+02\n\n\n2.380000e+02\n\n\n2.210000e+02\n\n\n2.290000e+02\n\n\n222.999997\n\n\n225.999995\n\n\n2.320000e+02\n\n\n2.300000e+02\n\n\n231.000001\n\n\n2.310000e+02\n\n\n2.350000e+02\n\n\n236.000004\n\n\n237.999995\n\n\n224.000005\n\n\n230.000000\n\n\n2.410000e+02\n\n\n254.999998\n\n\n78.000001\n\n\n0.000004\n\n\n6.923674e-07\n\n\n0.000001\n\n\n2.170000e+02\n\n\n2.070000e+02\n\n\n1.980000e+02\n\n\n211.999995\n\n\n2.700000e+01\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n9.100000e+01\n\n\n2.550000e+02\n\n\n2.520000e+02\n\n\n243.999992\n\n\n235.000002\n\n\n2.190000e+02\n\n\n2.200000e+02\n\n\n224.000004\n\n\n2.270000e+02\n\n\n232.000003\n\n\n235.000003\n\n\n224.999995\n\n\n2.230000e+02\n\n\n234.000001\n\n\n254.999996\n\n\n2.000000e+02\n\n\n0.000005\n\n\n-0.000004\n\n\n-0.000004\n\n\n4.021377e-07\n\n\n0.000002\n\n\n187.000001\n\n\n216.000000\n\n\n2.200000e+02\n\n\n2.150000e+02\n\n\n5.000000e+00\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n9.100000e+01\n\n\n199.999998\n\n\n2.550000e+02\n\n\n255.000003\n\n\n2.550000e+02\n\n\n253.999999\n\n\n245.000000\n\n\n233.999998\n\n\n235.999999\n\n\n243.999996\n\n\n254.999996\n\n\n255.000000\n\n\n1.170000e+02\n\n\n0.000003\n\n\n-0.000003\n\n\n0.999999\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n209.999999\n\n\n2.060000e+02\n\n\n1.810000e+02\n\n\n1.470000e+02\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n7.000000e+00\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n0.000002\n\n\n49.000000\n\n\n1.160000e+02\n\n\n1.810000e+02\n\n\n208.999997\n\n\n2.260000e+02\n\n\n223.000000\n\n\n205.999999\n\n\n139.999999\n\n\n-0.000001\n\n\n8.076585e-07\n\n\n-0.000005\n\n\n3.000001\n\n\n0.000005\n\n\n-1.852046e-07\n\n\n6.976350e-08\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n-0.000002\n\n\n3.933536e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n-0.000005\n\n\n-5.581073e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n-3.593068e-07\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n9\n\n\n\n\n1\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n3.400000e+01\n\n\n4.500000e+01\n\n\n1.650000e+02\n\n\n1.480000e+02\n\n\n34.000000\n\n\n-5.505288e-07\n\n\n4.053924e-07\n\n\n4.035032e-07\n\n\n2.956210e-07\n\n\n6.300000e+01\n\n\n190.000002\n\n\n1.390000e+02\n\n\n2.600000e+01\n\n\n2.800000e+01\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n2.000000e+00\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n1.070000e+02\n\n\n1.310000e+02\n\n\n6.400000e+01\n\n\n151.000002\n\n\n2.290000e+02\n\n\n226.000002\n\n\n219.999994\n\n\n2.410000e+02\n\n\n236.999998\n\n\n243.000004\n\n\n255.000000\n\n\n1.950000e+02\n\n\n92.000000\n\n\n9.000000e+01\n\n\n1.030000e+02\n\n\n9.000000e+01\n\n\n8.000000e+00\n\n\n-8.071513e-09\n\n\n1.000000e+00\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n1.040000e+02\n\n\n9.000000e+01\n\n\n5.200000e+01\n\n\n61.000000\n\n\n76.000000\n\n\n172.000000\n\n\n183.999998\n\n\n197.000003\n\n\n191.000000\n\n\n1.880000e+02\n\n\n169.000002\n\n\n160.000000\n\n\n128.999999\n\n\n7.000000e+01\n\n\n88.000001\n\n\n52.000000\n\n\n8.500000e+01\n\n\n7.700000e+01\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.700000e+01\n\n\n8.900000e+01\n\n\n5.700000e+01\n\n\n5.700000e+01\n\n\n7.000000e+01\n\n\n59.000001\n\n\n127.000000\n\n\n184.999999\n\n\n182.000001\n\n\n179.000002\n\n\n168.999998\n\n\n163.000000\n\n\n1.710000e+02\n\n\n8.900000e+01\n\n\n85.000000\n\n\n77.000000\n\n\n5.900000e+01\n\n\n5.700000e+01\n\n\n8.100000e+01\n\n\n8.000000e+00\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n5.000000e+01\n\n\n6.700000e+01\n\n\n6.400000e+01\n\n\n65.000000\n\n\n65.000000\n\n\n78.000000\n\n\n69.000000\n\n\n139.000001\n\n\n215.000005\n\n\n1.780000e+02\n\n\n1.960000e+02\n\n\n177.000000\n\n\n81.999999\n\n\n83.000001\n\n\n8.200000e+01\n\n\n65.999999\n\n\n5.900000e+01\n\n\n6.000000e+01\n\n\n8.800000e+01\n\n\n4.100000e+01\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n7.200000e+01\n\n\n69.000000\n\n\n67.000001\n\n\n64.000000\n\n\n67.000000\n\n\n58.999999\n\n\n66.999999\n\n\n5.800000e+01\n\n\n91.000000\n\n\n177.000000\n\n\n116.000000\n\n\n35.000001\n\n\n52.999998\n\n\n71.000001\n\n\n58.000002\n\n\n63.000000\n\n\n5.700000e+01\n\n\n72.000000\n\n\n6.400000e+01\n\n\n7.300000e+01\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.000000e+00\n\n\n8.400000e+01\n\n\n60.000001\n\n\n69.000000\n\n\n60.000000\n\n\n6.100000e+01\n\n\n56.000001\n\n\n5.200000e+01\n\n\n68.999999\n\n\n55.999999\n\n\n76.000000\n\n\n58.000000\n\n\n40.000001\n\n\n56.999998\n\n\n54.000001\n\n\n45.000001\n\n\n51.000001\n\n\n46.000000\n\n\n6.500000e+01\n\n\n61.000000\n\n\n9.700000e+01\n\n\n2.000000e+00\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n7.100000e+01\n\n\n5.400000e+01\n\n\n56.000000\n\n\n72.000000\n\n\n82.000000\n\n\n66.000000\n\n\n48.000000\n\n\n5.200000e+01\n\n\n52.999997\n\n\n7.600000e+01\n\n\n106.000000\n\n\n48.000000\n\n\n60.999999\n\n\n73.000000\n\n\n73.000000\n\n\n61.000000\n\n\n78.000000\n\n\n71.999999\n\n\n63.999999\n\n\n56.999999\n\n\n82.000000\n\n\n1.290000e+02\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n1.540000e+02\n\n\n1.260000e+02\n\n\n16.000000\n\n\n7.200000e+01\n\n\n67.000000\n\n\n4.100000e+01\n\n\n73.000000\n\n\n61.000001\n\n\n57.000001\n\n\n63.000002\n\n\n97.000000\n\n\n65.000002\n\n\n60.999998\n\n\n71.000000\n\n\n61.000002\n\n\n70.000001\n\n\n77.999999\n\n\n5.400000e+01\n\n\n108.000000\n\n\n83.000000\n\n\n144.000001\n\n\n117.000001\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n1.450000e+02\n\n\n1.470000e+02\n\n\n1.500000e+02\n\n\n122.000001\n\n\n106.000000\n\n\n106.000000\n\n\n51.999998\n\n\n55.999999\n\n\n59.999999\n\n\n81.999999\n\n\n6.500000e+01\n\n\n55.999997\n\n\n69.000001\n\n\n52.999997\n\n\n70.000001\n\n\n61.000002\n\n\n94.000000\n\n\n205.999996\n\n\n168.000003\n\n\n127.000002\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n9.500000e+01\n\n\n188.000003\n\n\n190.999996\n\n\n98.000000\n\n\n70.000000\n\n\n49.999998\n\n\n52.999998\n\n\n57.000000\n\n\n58.000001\n\n\n52.000002\n\n\n52.000001\n\n\n76.000000\n\n\n52.999998\n\n\n53.999996\n\n\n64.000001\n\n\n7.700000e+01\n\n\n161.999997\n\n\n85.000000\n\n\n1.135173e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n31.999998\n\n\n60.000001\n\n\n63.999999\n\n\n51.999997\n\n\n53.000000\n\n\n50.000003\n\n\n61.000000\n\n\n51.000002\n\n\n50.999997\n\n\n75.000002\n\n\n69.999999\n\n\n60.999997\n\n\n86.000000\n\n\n19.999996\n\n\n-0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n1.000002\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n2.000000\n\n\n0.000001\n\n\n40.999999\n\n\n73.000000\n\n\n64.000000\n\n\n53.000000\n\n\n47.999999\n\n\n52.000004\n\n\n58.000004\n\n\n5.200000e+01\n\n\n52.999999\n\n\n56.999997\n\n\n64.000002\n\n\n53.000000\n\n\n77.000002\n\n\n43.999999\n\n\n-0.000005\n\n\n3.000005\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n32.000000\n\n\n69.000001\n\n\n47.000001\n\n\n58.000002\n\n\n51.999996\n\n\n52.999996\n\n\n53.999996\n\n\n52.000001\n\n\n56.000004\n\n\n56.999997\n\n\n58.999996\n\n\n51.000000\n\n\n66.000002\n\n\n3.900000e+01\n\n\n-0.000001\n\n\n1.000001\n\n\n0.000004\n\n\n0.000002\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n1.999999e+00\n\n\n0.000002\n\n\n26.000003\n\n\n59.999999\n\n\n47.999997\n\n\n58.000000\n\n\n52.999997\n\n\n49.999999\n\n\n5.200000e+01\n\n\n50.999996\n\n\n56.999999\n\n\n62.999997\n\n\n57.000003\n\n\n57.999998\n\n\n73.000001\n\n\n56.999999\n\n\n7.423447e-07\n\n\n9.999991e-01\n\n\n0.000005\n\n\n0.000002\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n1.999998\n\n\n-5.223541e-09\n\n\n26.000001\n\n\n53.000002\n\n\n5.000000e+01\n\n\n63.000003\n\n\n57.000000\n\n\n53.000003\n\n\n60.999999\n\n\n56.999998\n\n\n55.999997\n\n\n64.000004\n\n\n56.999997\n\n\n60.999998\n\n\n69.000000\n\n\n64.999998\n\n\n0.000005\n\n\n1.000005e+00\n\n\n-0.000005\n\n\n0.000002\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n2.000003e+00\n\n\n-0.000002\n\n\n32.000000\n\n\n50.000000\n\n\n66.000002\n\n\n76.000002\n\n\n60.999997\n\n\n55.999997\n\n\n58.999998\n\n\n57.000004\n\n\n59.999999\n\n\n64.999999\n\n\n51.000002\n\n\n63.999998\n\n\n6.100000e+01\n\n\n59.000000\n\n\n-4.653743e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n51.000002\n\n\n47.000002\n\n\n69.000002\n\n\n66.999997\n\n\n58.999998\n\n\n56.000002\n\n\n59.999997\n\n\n58.000000\n\n\n6.100000e+01\n\n\n63.000000\n\n\n52.999998\n\n\n66.000002\n\n\n6.400000e+01\n\n\n5.800000e+01\n\n\n-6.241514e-07\n\n\n-0.000005\n\n\n1.000003\n\n\n5.651978e-07\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.352320e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n59.999999\n\n\n61.000001\n\n\n77.000000\n\n\n71.999997\n\n\n64.000003\n\n\n57.999997\n\n\n62.999996\n\n\n58.999998\n\n\n65.999999\n\n\n63.000000\n\n\n58.000000\n\n\n67.000003\n\n\n72.000000\n\n\n71.999999\n\n\n11.000003\n\n\n-0.000005\n\n\n9.999988e-01\n\n\n-3.174695e-07\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n-9.513843e-07\n\n\n1.000001e+00\n\n\n2.894738e-08\n\n\n0.000004\n\n\n76.000000\n\n\n5.800000e+01\n\n\n81.999998\n\n\n82.999998\n\n\n68.999996\n\n\n63.000002\n\n\n66.000002\n\n\n64.000001\n\n\n7.200000e+01\n\n\n66.000002\n\n\n64.000004\n\n\n65.000000\n\n\n76.000001\n\n\n87.999999\n\n\n32.000002\n\n\n-0.000002\n\n\n2.999998\n\n\n-0.000002\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n-0.000002\n\n\n2.000001\n\n\n-0.000003\n\n\n1.100000e+01\n\n\n92.000000\n\n\n6.400000e+01\n\n\n8.500000e+01\n\n\n84.999998\n\n\n78.999999\n\n\n71.999999\n\n\n69.000002\n\n\n72.999999\n\n\n85.999999\n\n\n73.000002\n\n\n70.999998\n\n\n67.000000\n\n\n83.999999\n\n\n98.000000\n\n\n2.800000e+01\n\n\n0.000003\n\n\n3.000001\n\n\n-0.000002\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n-2.873870e-07\n\n\n1.999999e+00\n\n\n0.000001\n\n\n15.999998\n\n\n9.800000e+01\n\n\n8.500000e+01\n\n\n88.000001\n\n\n8.100000e+01\n\n\n8.800000e+01\n\n\n79.000001\n\n\n72.999999\n\n\n83.000000\n\n\n91.000001\n\n\n7.800000e+01\n\n\n81.000000\n\n\n84.000001\n\n\n88.000000\n\n\n1.220000e+02\n\n\n35.000001\n\n\n-8.325330e-07\n\n\n2.000000e+00\n\n\n-1.849482e-07\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n1.000002\n\n\n-0.000001\n\n\n3.300000e+01\n\n\n1.350000e+02\n\n\n96.000000\n\n\n8.900000e+01\n\n\n89.999999\n\n\n96.000000\n\n\n83.999999\n\n\n9.000000e+01\n\n\n97.999999\n\n\n99.999999\n\n\n9.000000e+01\n\n\n90.000000\n\n\n102.000000\n\n\n72.999999\n\n\n1.350000e+02\n\n\n52.000001\n\n\n0.000002\n\n\n2.000001\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n2.000002\n\n\n9.323434e-07\n\n\n101.000000\n\n\n1.290000e+02\n\n\n110.000000\n\n\n115.000000\n\n\n97.999999\n\n\n96.000001\n\n\n91.000000\n\n\n95.000001\n\n\n101.000000\n\n\n1.030000e+02\n\n\n97.000000\n\n\n92.000000\n\n\n104.000000\n\n\n101.000000\n\n\n113.000001\n\n\n59.000000\n\n\n-0.000001\n\n\n3.000000e+00\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n2.999998e+00\n\n\n0.000002\n\n\n46.000000\n\n\n1.560000e+02\n\n\n9.400000e+01\n\n\n117.000000\n\n\n1.170000e+02\n\n\n127.000000\n\n\n112.000000\n\n\n112.000000\n\n\n128.000000\n\n\n1.230000e+02\n\n\n133.000000\n\n\n121.000001\n\n\n102.000000\n\n\n9.200000e+01\n\n\n1.500000e+02\n\n\n51.000000\n\n\n-0.000002\n\n\n3.000002\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n1.270000e+02\n\n\n160.000000\n\n\n1.040000e+02\n\n\n58.999999\n\n\n7.300000e+01\n\n\n85.000000\n\n\n82.000000\n\n\n95.000000\n\n\n92.000000\n\n\n86.000000\n\n\n90.000000\n\n\n1.060000e+02\n\n\n165.999997\n\n\n112.999998\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n1.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n109.999998\n\n\n222.000005\n\n\n213.999997\n\n\n181.000000\n\n\n163.000000\n\n\n169.000001\n\n\n175.999999\n\n\n169.000001\n\n\n171.999999\n\n\n188.000002\n\n\n187.000002\n\n\n82.999999\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n6.100000e+01\n\n\n1.260000e+02\n\n\n1.420000e+02\n\n\n1.620000e+02\n\n\n1.830000e+02\n\n\n171.000000\n\n\n1.450000e+02\n\n\n5.400000e+01\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n0\n\n\n\n\n2\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n6.900000e+01\n\n\n4.800000e+01\n\n\n9.682471e-07\n\n\n3.000002\n\n\n-5.505288e-07\n\n\n4.053924e-07\n\n\n4.035032e-07\n\n\n2.956210e-07\n\n\n4.005417e-07\n\n\n-0.000001\n\n\n5.516976e-08\n\n\n1.300000e+02\n\n\n3.200000e+01\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n1.770000e+02\n\n\n173.000001\n\n\n7.044741e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-7.585758e-07\n\n\n0.999997\n\n\n0.000006\n\n\n1.999996\n\n\n-1.279166e-07\n\n\n0.000001\n\n\n2.040000e+02\n\n\n7.800000e+01\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n127.999997\n\n\n184.999995\n\n\n0.000002\n\n\n0.000001\n\n\n0.999996\n\n\n1.000004\n\n\n-4.594104e-07\n\n\n0.000004\n\n\n5.000005\n\n\n0.000001\n\n\n5.100000e+01\n\n\n200.999995\n\n\n-0.000002\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n7.300000e+01\n\n\n212.000004\n\n\n17.999997\n\n\n0.000004\n\n\n0.999997\n\n\n-0.000003\n\n\n-0.000003\n\n\n-0.000004\n\n\n-6.836428e-07\n\n\n-3.299791e-07\n\n\n121.999999\n\n\n189.000002\n\n\n5.257434e-07\n\n\n-3.949514e-07\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n44.000000\n\n\n219.999997\n\n\n97.000000\n\n\n0.000003\n\n\n2.000004\n\n\n9.626116e-07\n\n\n6.564152e-07\n\n\n1.000003\n\n\n0.999998\n\n\n-0.000005\n\n\n1.770000e+02\n\n\n166.000002\n\n\n5.942894e-07\n\n\n9.570876e-07\n\n\n7.753735e-07\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n41.999999\n\n\n206.000005\n\n\n197.999999\n\n\n6.513797e-07\n\n\n-0.000001\n\n\n0.000005\n\n\n-0.000004\n\n\n0.000004\n\n\n-0.000001\n\n\n45.999999\n\n\n209.000003\n\n\n170.999998\n\n\n2.016850e-07\n\n\n-0.000002\n\n\n-4.557509e-07\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n4.900000e+01\n\n\n224.000005\n\n\n1.940000e+02\n\n\n162.000001\n\n\n20.000002\n\n\n-0.000001\n\n\n-0.000004\n\n\n-0.000005\n\n\n47.999998\n\n\n203.000001\n\n\n195.999999\n\n\n194.999998\n\n\n0.000005\n\n\n-7.583148e-07\n\n\n-0.000002\n\n\n8.919840e-07\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n-0.000001\n\n\n57.000000\n\n\n221.999997\n\n\n1.800000e+02\n\n\n202.999997\n\n\n2.150000e+02\n\n\n194.999999\n\n\n194.999999\n\n\n207.000000\n\n\n213.000002\n\n\n190.000000\n\n\n183.000001\n\n\n222.999998\n\n\n1.999996\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n7.406876e-07\n\n\n-0.000003\n\n\n1.130000e+02\n\n\n223.000002\n\n\n166.000000\n\n\n165.000001\n\n\n173.999998\n\n\n185.000001\n\n\n186.999998\n\n\n181.000001\n\n\n176.000001\n\n\n168.999999\n\n\n182.000001\n\n\n222.000000\n\n\n8.100000e+01\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n9.804652e-07\n\n\n0.000001\n\n\n165.999999\n\n\n243.000000\n\n\n233.000002\n\n\n223.999995\n\n\n183.000001\n\n\n186.000002\n\n\n1.800000e+02\n\n\n191.999998\n\n\n187.000002\n\n\n191.000000\n\n\n189.000001\n\n\n232.000000\n\n\n119.000000\n\n\n-0.000006\n\n\n0.000005\n\n\n0.000002\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n0.000002\n\n\n0.000003\n\n\n129.000000\n\n\n230.000005\n\n\n231.000002\n\n\n233.999995\n\n\n222.000001\n\n\n215.000002\n\n\n223.999995\n\n\n215.000001\n\n\n216.000002\n\n\n225.000002\n\n\n221.000001\n\n\n255.000004\n\n\n4.500000e+01\n\n\n-0.000003\n\n\n-0.000002\n\n\n1.135173e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n108.000000\n\n\n221.000000\n\n\n213.000004\n\n\n223.000003\n\n\n215.000002\n\n\n220.999997\n\n\n230.999996\n\n\n216.999998\n\n\n230.000001\n\n\n188.000000\n\n\n222.000000\n\n\n239.000005\n\n\n15.000004\n\n\n-0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000001\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n71.000000\n\n\n250.999996\n\n\n229.000000\n\n\n189.999998\n\n\n192.000001\n\n\n200.999998\n\n\n1.990000e+02\n\n\n195.000001\n\n\n199.999999\n\n\n200.000002\n\n\n214.000000\n\n\n216.000002\n\n\n-0.000003\n\n\n-0.000005\n\n\n-0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n0.000001\n\n\n36.000001\n\n\n235.999998\n\n\n191.000001\n\n\n186.999999\n\n\n182.999999\n\n\n176.000000\n\n\n174.000000\n\n\n174.000001\n\n\n176.999999\n\n\n179.999999\n\n\n215.999998\n\n\n194.000001\n\n\n5.151848e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n0.000004\n\n\n0.000002\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n0.000002\n\n\n0.000005\n\n\n10.999995\n\n\n231.999995\n\n\n191.999999\n\n\n189.000001\n\n\n185.999999\n\n\n1.880000e+02\n\n\n185.999999\n\n\n183.000000\n\n\n186.000000\n\n\n182.999999\n\n\n213.000002\n\n\n191.999998\n\n\n0.000001\n\n\n7.423447e-07\n\n\n1.327465e-07\n\n\n0.000005\n\n\n0.000002\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-5.223541e-09\n\n\n0.000004\n\n\n1.999999\n\n\n2.340000e+02\n\n\n195.999998\n\n\n187.000000\n\n\n185.000001\n\n\n180.999999\n\n\n180.999998\n\n\n181.000001\n\n\n182.000001\n\n\n183.000001\n\n\n206.000000\n\n\n191.000001\n\n\n0.000002\n\n\n0.000005\n\n\n4.826824e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n5.139264e-07\n\n\n-0.000002\n\n\n-0.000004\n\n\n40.999999\n\n\n225.999998\n\n\n183.000001\n\n\n189.000000\n\n\n184.999999\n\n\n181.000000\n\n\n182.000001\n\n\n181.000001\n\n\n181.000001\n\n\n185.000000\n\n\n188.999999\n\n\n2.230000e+02\n\n\n11.000000\n\n\n-4.653743e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n0.000005\n\n\n186.000001\n\n\n207.999998\n\n\n181.000001\n\n\n186.000000\n\n\n181.999999\n\n\n181.000000\n\n\n181.000001\n\n\n1.800000e+02\n\n\n182.000000\n\n\n179.000000\n\n\n180.000000\n\n\n2.120000e+02\n\n\n1.350000e+02\n\n\n-6.241514e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n5.651978e-07\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.352320e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000005\n\n\n144.000000\n\n\n220.999998\n\n\n182.000001\n\n\n183.000000\n\n\n180.000001\n\n\n181.999999\n\n\n178.999999\n\n\n179.999999\n\n\n176.999999\n\n\n188.000001\n\n\n222.999998\n\n\n151.999999\n\n\n0.999998\n\n\n0.000004\n\n\n-0.000005\n\n\n-2.677840e-07\n\n\n-3.174695e-07\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n-9.513843e-07\n\n\n-2.911069e-07\n\n\n2.894738e-08\n\n\n0.000004\n\n\n0.000005\n\n\n-3.351748e-07\n\n\n93.000001\n\n\n225.999998\n\n\n185.000000\n\n\n181.000001\n\n\n180.999999\n\n\n180.000001\n\n\n1.770000e+02\n\n\n191.000000\n\n\n213.000000\n\n\n67.000001\n\n\n0.000005\n\n\n0.000003\n\n\n0.000005\n\n\n-0.000002\n\n\n-0.000001\n\n\n-0.000002\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000003\n\n\n3.360567e-07\n\n\n0.000004\n\n\n9.999977e-01\n\n\n2.258955e-07\n\n\n84.000001\n\n\n226.000001\n\n\n180.000000\n\n\n182.000001\n\n\n181.999998\n\n\n177.999999\n\n\n226.000001\n\n\n43.999998\n\n\n-0.000004\n\n\n1.000000\n\n\n-0.000001\n\n\n-2.610268e-07\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n-2.873870e-07\n\n\n5.500636e-07\n\n\n0.000001\n\n\n-0.000002\n\n\n6.275720e-07\n\n\n4.274649e-07\n\n\n0.000001\n\n\n5.958969e-07\n\n\n1.540000e+02\n\n\n213.000001\n\n\n178.000001\n\n\n172.999998\n\n\n216.000001\n\n\n1.180000e+02\n\n\n0.000001\n\n\n0.000001\n\n\n0.999996\n\n\n6.923674e-07\n\n\n0.000001\n\n\n-8.325330e-07\n\n\n-5.767398e-07\n\n\n-1.849482e-07\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n0.000001\n\n\n-0.000001\n\n\n-4.551377e-07\n\n\n7.378563e-07\n\n\n-0.000002\n\n\n1.000002e+00\n\n\n0.000003\n\n\n29.000002\n\n\n221.999998\n\n\n1.800000e+02\n\n\n182.000000\n\n\n223.000000\n\n\n1.000002e+00\n\n\n0.000005\n\n\n-0.000004\n\n\n-0.000004\n\n\n1.000000e+00\n\n\n0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n0.000001\n\n\n9.323434e-07\n\n\n0.000002\n\n\n8.744090e-07\n\n\n0.000002\n\n\n1.000005\n\n\n0.000003\n\n\n0.000005\n\n\n197.999999\n\n\n187.000003\n\n\n194.000002\n\n\n1.530000e+02\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000005\n\n\n0.000002\n\n\n1.000002\n\n\n0.000001\n\n\n-0.000001\n\n\n3.297868e-07\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n0.000002\n\n\n-0.000001\n\n\n8.809801e-07\n\n\n1.723858e-07\n\n\n0.000001\n\n\n-7.570235e-07\n\n\n0.000001\n\n\n135.000000\n\n\n196.999999\n\n\n199.000002\n\n\n8.800000e+01\n\n\n-0.000005\n\n\n1.000000\n\n\n0.000005\n\n\n-1.852046e-07\n\n\n1.000000e+00\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n0.999999\n\n\n3.933536e-07\n\n\n58.999999\n\n\n199.000005\n\n\n197.000002\n\n\n51.000001\n\n\n0.000002\n\n\n0.999998\n\n\n-5.581073e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n2.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n201.000001\n\n\n223.999998\n\n\n26.999997\n\n\n-0.000003\n\n\n2.999998\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n1.000000e+00\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n9.900000e+01\n\n\n1.280000e+02\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n1.000001e+00\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n0\n\n\n\n\n3\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n9.682471e-07\n\n\n-0.000002\n\n\n-5.505288e-07\n\n\n4.053924e-07\n\n\n4.035032e-07\n\n\n2.956210e-07\n\n\n4.005417e-07\n\n\n-0.000001\n\n\n5.516976e-08\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n-0.000001\n\n\n7.044741e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-7.585758e-07\n\n\n-0.000006\n\n\n0.000006\n\n\n0.000002\n\n\n-1.279166e-07\n\n\n0.000001\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n1.000000\n\n\n-0.000003\n\n\n0.000003\n\n\n1.290000e+02\n\n\n197.000002\n\n\n181.000000\n\n\n207.000005\n\n\n2.370000e+02\n\n\n219.000005\n\n\n180.999997\n\n\n1.610000e+02\n\n\n1.110000e+02\n\n\n9.700000e+01\n\n\n7.500000e+01\n\n\n4.400000e+01\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n8.386749e-07\n\n\n0.000002\n\n\n0.000005\n\n\n2.000005\n\n\n-0.000003\n\n\n-0.000003\n\n\n203.999995\n\n\n214.000003\n\n\n1.880000e+02\n\n\n2.080000e+02\n\n\n247.000002\n\n\n217.000003\n\n\n2.300000e+02\n\n\n2.220000e+02\n\n\n2.220000e+02\n\n\n2.400000e+02\n\n\n2.170000e+02\n\n\n2.520000e+02\n\n\n9.000000e+00\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000005\n\n\n2.999999\n\n\n0.000002\n\n\n9.626116e-07\n\n\n2.170000e+02\n\n\n206.000004\n\n\n209.000005\n\n\n213.000000\n\n\n2.010000e+02\n\n\n174.000002\n\n\n2.000000e+02\n\n\n1.990000e+02\n\n\n1.940000e+02\n\n\n2.030000e+02\n\n\n2.010000e+02\n\n\n1.860000e+02\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000005\n\n\n-0.000001\n\n\n3.999999e+00\n\n\n-0.000001\n\n\n3.999997\n\n\n193.000001\n\n\n200.000002\n\n\n201.000002\n\n\n197.000000\n\n\n199.000002\n\n\n185.000003\n\n\n2.300000e+02\n\n\n189.999995\n\n\n1.960000e+02\n\n\n1.980000e+02\n\n\n2.100000e+02\n\n\n2.060000e+02\n\n\n1.880000e+02\n\n\n2.200000e+01\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-8.460442e-08\n\n\n0.000003\n\n\n-9.104431e-07\n\n\n4.999997\n\n\n0.000005\n\n\n13.000001\n\n\n194.999999\n\n\n198.999998\n\n\n202.000002\n\n\n219.000000\n\n\n229.999995\n\n\n197.000001\n\n\n198.000001\n\n\n1.650000e+02\n\n\n218.000001\n\n\n2.160000e+02\n\n\n2.120000e+02\n\n\n2.210000e+02\n\n\n2.250000e+02\n\n\n8.100000e+01\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n-0.000001\n\n\n0.000001\n\n\n-0.000004\n\n\n1.262333e-07\n\n\n4.999995\n\n\n-4.162347e-07\n\n\n14.999996\n\n\n237.000002\n\n\n200.000002\n\n\n196.000000\n\n\n190.000000\n\n\n192.999998\n\n\n253.000005\n\n\n223.999996\n\n\n223.000005\n\n\n226.000000\n\n\n216.999997\n\n\n2.100000e+02\n\n\n1.840000e+02\n\n\n1.900000e+01\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n7.406876e-07\n\n\n-0.000003\n\n\n-9.455209e-07\n\n\n0.000004\n\n\n0.000004\n\n\n5.000001\n\n\n-0.000002\n\n\n14.000000\n\n\n223.999995\n\n\n201.000001\n\n\n205.999998\n\n\n215.999998\n\n\n186.000002\n\n\n188.000000\n\n\n2.120000e+02\n\n\n192.999997\n\n\n194.000003\n\n\n191.999997\n\n\n219.999991\n\n\n1.210000e+02\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n9.804652e-07\n\n\n0.000001\n\n\n-0.000003\n\n\n-0.000005\n\n\n-0.000004\n\n\n2.999995\n\n\n-0.000002\n\n\n14.000003\n\n\n2.260000e+02\n\n\n199.000000\n\n\n209.000001\n\n\n214.999999\n\n\n213.000000\n\n\n199.000001\n\n\n200.000000\n\n\n202.999995\n\n\n203.000000\n\n\n189.999996\n\n\n2.170000e+02\n\n\n1.420000e+02\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n0.000003\n\n\n-0.000003\n\n\n1.000002\n\n\n-0.000004\n\n\n4.999998\n\n\n211.000002\n\n\n210.999999\n\n\n211.999998\n\n\n197.000001\n\n\n204.000001\n\n\n203.000002\n\n\n2.000000e+02\n\n\n201.999999\n\n\n200.000002\n\n\n1.920000e+02\n\n\n202.000001\n\n\n206.000000\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000004\n\n\n0.000005\n\n\n-0.000002\n\n\n-0.000002\n\n\n144.000000\n\n\n210.000002\n\n\n189.000001\n\n\n233.000002\n\n\n252.999998\n\n\n228.999998\n\n\n222.999998\n\n\n214.999998\n\n\n212.000005\n\n\n204.999999\n\n\n210.000003\n\n\n196.000004\n\n\n2.090000e+02\n\n\n62.000001\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n0.000002\n\n\n1.999995\n\n\n0.000001\n\n\n0.000002\n\n\n222.999996\n\n\n1.890000e+02\n\n\n196.000002\n\n\n164.000000\n\n\n146.000000\n\n\n221.000000\n\n\n180.000002\n\n\n199.999999\n\n\n207.999997\n\n\n207.000001\n\n\n201.999999\n\n\n196.000002\n\n\n205.000000\n\n\n163.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n0.000001\n\n\n0.000003\n\n\n0.000001\n\n\n4.000004\n\n\n0.000002\n\n\n49.999998\n\n\n233.000001\n\n\n178.999998\n\n\n218.000001\n\n\n190.999999\n\n\n155.000000\n\n\n205.000002\n\n\n217.000000\n\n\n2.270000e+02\n\n\n185.000000\n\n\n201.000000\n\n\n203.000004\n\n\n200.000001\n\n\n2.000000e+02\n\n\n200.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n0.000002\n\n\n0.000005\n\n\n0.999998\n\n\n0.000003\n\n\n-0.000001\n\n\n-0.000003\n\n\n154.000000\n\n\n2.110000e+02\n\n\n199.000000\n\n\n216.999998\n\n\n190.000001\n\n\n228.000001\n\n\n208.000000\n\n\n201.999998\n\n\n231.000001\n\n\n2.050000e+02\n\n\n1.880000e+02\n\n\n193.999997\n\n\n200.000003\n\n\n2.000000e+02\n\n\n212.000002\n\n\n9.000000e+00\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n9.999992e-01\n\n\n1.999999\n\n\n1.999998\n\n\n2.999998e+00\n\n\n0.000004\n\n\n0.000005\n\n\n-1.976124e-07\n\n\n44.999999\n\n\n161.000000\n\n\n177.000000\n\n\n145.000000\n\n\n220.000000\n\n\n222.999999\n\n\n141.000000\n\n\n223.999999\n\n\n218.999999\n\n\n223.999996\n\n\n215.000002\n\n\n205.000002\n\n\n2.250000e+02\n\n\n206.999995\n\n\n207.999997\n\n\n1.970000e+02\n\n\n2.240000e+02\n\n\n3.900000e+01\n\n\n-2.135472e-07\n\n\n2.000001e+00\n\n\n2.999999e+00\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n5.139264e-07\n\n\n-0.000002\n\n\n-0.000004\n\n\n-0.000002\n\n\n101.000001\n\n\n174.000000\n\n\n165.000000\n\n\n171.000001\n\n\n174.999999\n\n\n195.999998\n\n\n232.000004\n\n\n237.000001\n\n\n222.999999\n\n\n174.999999\n\n\n1.800000e+02\n\n\n171.000001\n\n\n1.960000e+02\n\n\n192.000003\n\n\n186.999997\n\n\n189.999995\n\n\n191.000004\n\n\n2.160000e+02\n\n\n1.700000e+01\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n6.000001\n\n\n2.000002\n\n\n29.000001\n\n\n84.000000\n\n\n150.999999\n\n\n147.000000\n\n\n113.999999\n\n\n164.000000\n\n\n196.000001\n\n\n211.999998\n\n\n207.000000\n\n\n2.040000e+02\n\n\n208.000000\n\n\n204.999999\n\n\n190.000001\n\n\n1.930000e+02\n\n\n1.960000e+02\n\n\n1.850000e+02\n\n\n180.000000\n\n\n182.000000\n\n\n1.840000e+02\n\n\n1.840000e+02\n\n\n216.000002\n\n\n1.400000e+01\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n101.000000\n\n\n1.340000e+02\n\n\n144.000002\n\n\n163.999999\n\n\n161.999999\n\n\n144.000000\n\n\n109.000000\n\n\n113.000000\n\n\n169.000000\n\n\n193.999998\n\n\n204.000002\n\n\n209.999999\n\n\n212.000002\n\n\n203.000001\n\n\n199.000001\n\n\n188.999999\n\n\n176.999999\n\n\n178.999998\n\n\n188.000002\n\n\n204.000003\n\n\n205.000005\n\n\n2.010000e+02\n\n\n1.990000e+02\n\n\n190.000001\n\n\n2.190000e+02\n\n\n1.600000e+01\n\n\n-1.447285e-07\n\n\n9.400000e+01\n\n\n1.930000e+02\n\n\n1.260000e+02\n\n\n6.400000e+01\n\n\n3.200000e+01\n\n\n11.000002\n\n\n90.000000\n\n\n1.520000e+02\n\n\n170.999999\n\n\n191.000000\n\n\n199.000000\n\n\n204.000002\n\n\n209.000002\n\n\n207.999998\n\n\n1.900000e+02\n\n\n186.999999\n\n\n180.000001\n\n\n214.000002\n\n\n233.000001\n\n\n204.000005\n\n\n182.000002\n\n\n176.999998\n\n\n175.000000\n\n\n178.000005\n\n\n1.760000e+02\n\n\n215.000007\n\n\n1.700000e+01\n\n\n2.200000e+01\n\n\n1.870000e+02\n\n\n2.160000e+02\n\n\n215.999999\n\n\n201.999998\n\n\n199.999997\n\n\n1.890000e+02\n\n\n195.000001\n\n\n1.960000e+02\n\n\n1.960000e+02\n\n\n195.999998\n\n\n199.999999\n\n\n204.000001\n\n\n199.999998\n\n\n204.999999\n\n\n196.000001\n\n\n215.999998\n\n\n255.000000\n\n\n143.000000\n\n\n90.000000\n\n\n190.000005\n\n\n1.630000e+02\n\n\n171.000000\n\n\n175.999995\n\n\n179.999997\n\n\n180.000004\n\n\n2.110000e+02\n\n\n1.900000e+01\n\n\n1.090000e+02\n\n\n1.960000e+02\n\n\n1.720000e+02\n\n\n1.960000e+02\n\n\n2.080000e+02\n\n\n214.999998\n\n\n222.999998\n\n\n2.100000e+02\n\n\n2.050000e+02\n\n\n202.999998\n\n\n2.010000e+02\n\n\n2.060000e+02\n\n\n210.999999\n\n\n214.000003\n\n\n200.000002\n\n\n219.000001\n\n\n1.870000e+02\n\n\n31.000004\n\n\n0.000001\n\n\n27.000001\n\n\n2.280000e+02\n\n\n165.000001\n\n\n1.790000e+02\n\n\n1.810000e+02\n\n\n1.800000e+02\n\n\n179.000000\n\n\n2.060000e+02\n\n\n2.900000e+01\n\n\n4.600000e+01\n\n\n1.900000e+02\n\n\n2.140000e+02\n\n\n1.950000e+02\n\n\n178.999996\n\n\n187.999999\n\n\n1.930000e+02\n\n\n2.020000e+02\n\n\n205.000001\n\n\n2.120000e+02\n\n\n210.000000\n\n\n209.000001\n\n\n199.999999\n\n\n1.830000e+02\n\n\n197.999999\n\n\n109.000000\n\n\n9.842208e-07\n\n\n0.000005\n\n\n-0.000004\n\n\n44.000002\n\n\n1.880000e+02\n\n\n168.000004\n\n\n176.999996\n\n\n164.999996\n\n\n1.750000e+02\n\n\n1.760000e+02\n\n\n1.760000e+02\n\n\n2.500000e+01\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n9.100000e+01\n\n\n1.930000e+02\n\n\n210.999997\n\n\n2.050000e+02\n\n\n202.000003\n\n\n2.010000e+02\n\n\n199.000001\n\n\n188.000001\n\n\n189.999997\n\n\n191.000001\n\n\n183.000000\n\n\n211.000004\n\n\n152.000000\n\n\n8.867231e-07\n\n\n0.000003\n\n\n4.999995\n\n\n0.000005\n\n\n12.999999\n\n\n198.999999\n\n\n190.000003\n\n\n192.999998\n\n\n1.970000e+02\n\n\n1.990000e+02\n\n\n2.030000e+02\n\n\n2.120000e+02\n\n\n5.900000e+01\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n2.200000e+01\n\n\n62.000000\n\n\n152.000000\n\n\n2.050000e+02\n\n\n1.860000e+02\n\n\n232.000002\n\n\n2.290000e+02\n\n\n182.999998\n\n\n214.999995\n\n\n164.000003\n\n\n7.999998\n\n\n8.076585e-07\n\n\n2.000003\n\n\n-0.000006\n\n\n0.000005\n\n\n9.999992e-01\n\n\n1.840000e+02\n\n\n137.000002\n\n\n155.000001\n\n\n165.000005\n\n\n136.000003\n\n\n1.290000e+02\n\n\n1.130000e+02\n\n\n1.800000e+01\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.000000e+00\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n20.000000\n\n\n3.933536e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n-0.000005\n\n\n-5.581073e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n-3.593068e-07\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n9\n\n\n\n\n4\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n1.930000e+02\n\n\n222.000000\n\n\n2.050000e+02\n\n\n1.790000e+02\n\n\n1.970000e+02\n\n\n1.700000e+02\n\n\n1.770000e+02\n\n\n201.000007\n\n\n1.480000e+02\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n-0.000001\n\n\n2.550000e+02\n\n\n236.999996\n\n\n240.999995\n\n\n2.390000e+02\n\n\n219.000004\n\n\n206.000000\n\n\n207.000000\n\n\n2.100000e+02\n\n\n221.000003\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n0.000002\n\n\n24.000000\n\n\n255.000005\n\n\n227.000002\n\n\n227.000003\n\n\n232.000001\n\n\n2.140000e+02\n\n\n202.999998\n\n\n204.999995\n\n\n181.000003\n\n\n2.250000e+02\n\n\n0.000002\n\n\n-0.000002\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n8.386749e-07\n\n\n93.000000\n\n\n254.999997\n\n\n214.000001\n\n\n212.999998\n\n\n211.000002\n\n\n219.999995\n\n\n214.000003\n\n\n2.320000e+02\n\n\n2.150000e+02\n\n\n235.999998\n\n\n66.000000\n\n\n5.257434e-07\n\n\n-3.949514e-07\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n153.000000\n\n\n254.999998\n\n\n205.000000\n\n\n195.000002\n\n\n1.950000e+02\n\n\n2.180000e+02\n\n\n227.000002\n\n\n233.999997\n\n\n197.999999\n\n\n1.900000e+02\n\n\n162.000002\n\n\n5.942894e-07\n\n\n9.570876e-07\n\n\n7.753735e-07\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000002\n\n\n178.000001\n\n\n253.000005\n\n\n1.980000e+02\n\n\n161.999999\n\n\n215.000002\n\n\n241.000000\n\n\n228.000000\n\n\n237.999996\n\n\n207.000001\n\n\n166.000002\n\n\n194.000003\n\n\n2.016850e-07\n\n\n-0.000002\n\n\n-4.557509e-07\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-8.460442e-08\n\n\n188.999996\n\n\n2.510000e+02\n\n\n196.999998\n\n\n170.000000\n\n\n232.000002\n\n\n205.000002\n\n\n251.999998\n\n\n238.999999\n\n\n211.999997\n\n\n166.999999\n\n\n206.000000\n\n\n0.000005\n\n\n-7.583148e-07\n\n\n-0.000002\n\n\n8.919840e-07\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n-0.000001\n\n\n0.000001\n\n\n188.000003\n\n\n2.490000e+02\n\n\n199.000002\n\n\n1.860000e+02\n\n\n255.000003\n\n\n79.000000\n\n\n254.999997\n\n\n240.000002\n\n\n214.999997\n\n\n177.000001\n\n\n201.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n7.406876e-07\n\n\n-0.000003\n\n\n-9.455209e-07\n\n\n184.999998\n\n\n246.999997\n\n\n204.000001\n\n\n197.999999\n\n\n255.000000\n\n\n27.000001\n\n\n246.999998\n\n\n245.999998\n\n\n223.000005\n\n\n192.999999\n\n\n188.999998\n\n\n3.125923e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n9.804652e-07\n\n\n0.000001\n\n\n-0.000003\n\n\n155.000002\n\n\n253.999999\n\n\n206.000004\n\n\n209.000003\n\n\n255.000000\n\n\n-6.622733e-07\n\n\n230.999997\n\n\n255.000000\n\n\n221.000004\n\n\n213.000000\n\n\n171.999998\n\n\n-0.000002\n\n\n-0.000006\n\n\n0.000005\n\n\n0.000002\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n99.000000\n\n\n254.999997\n\n\n205.999997\n\n\n218.999996\n\n\n252.000002\n\n\n-0.000003\n\n\n179.000000\n\n\n255.000001\n\n\n219.000002\n\n\n227.000003\n\n\n148.000000\n\n\n8.404825e-07\n\n\n-0.000003\n\n\n-0.000002\n\n\n1.135173e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n43.000000\n\n\n254.999995\n\n\n209.000002\n\n\n217.999997\n\n\n227.999996\n\n\n0.000004\n\n\n96.000000\n\n\n254.999999\n\n\n220.000001\n\n\n243.000002\n\n\n127.000000\n\n\n-0.000005\n\n\n-0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000001\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n0.000002\n\n\n251.000003\n\n\n217.000004\n\n\n221.000001\n\n\n187.999999\n\n\n-5.688553e-07\n\n\n47.000001\n\n\n255.000002\n\n\n220.000001\n\n\n251.999996\n\n\n103.999999\n\n\n-0.000003\n\n\n-0.000005\n\n\n-0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n0.000001\n\n\n0.000003\n\n\n0.000001\n\n\n218.000002\n\n\n222.999999\n\n\n225.000003\n\n\n150.000000\n\n\n0.000002\n\n\n26.999997\n\n\n255.000003\n\n\n216.000001\n\n\n253.999997\n\n\n76.000002\n\n\n5.151848e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n0.000004\n\n\n0.000002\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n0.000002\n\n\n0.000005\n\n\n-0.000001\n\n\n0.000003\n\n\n169.000001\n\n\n229.000002\n\n\n230.000000\n\n\n1.200000e+02\n\n\n-0.000004\n\n\n5.999993\n\n\n255.000003\n\n\n218.000002\n\n\n250.999996\n\n\n59.000000\n\n\n0.000001\n\n\n7.423447e-07\n\n\n1.327465e-07\n\n\n0.000005\n\n\n0.000002\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-5.223541e-09\n\n\n0.000004\n\n\n0.000005\n\n\n-1.976124e-07\n\n\n138.000000\n\n\n233.999999\n\n\n228.999997\n\n\n94.999998\n\n\n0.000001\n\n\n0.000002\n\n\n255.000000\n\n\n221.000001\n\n\n221.999998\n\n\n63.000001\n\n\n0.000002\n\n\n0.000005\n\n\n4.826824e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n5.139264e-07\n\n\n-0.000002\n\n\n-0.000004\n\n\n-0.000002\n\n\n-0.000003\n\n\n146.000000\n\n\n232.000000\n\n\n232.000002\n\n\n96.000000\n\n\n-0.000004\n\n\n-0.000006\n\n\n242.000001\n\n\n226.999999\n\n\n222.000000\n\n\n6.300000e+01\n\n\n-0.000005\n\n\n-4.653743e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000005\n\n\n119.000000\n\n\n232.000000\n\n\n227.999997\n\n\n75.000000\n\n\n-0.000001\n\n\n-5.077659e-07\n\n\n208.000000\n\n\n232.000004\n\n\n212.999999\n\n\n2.900000e+01\n\n\n3.296967e-07\n\n\n-6.241514e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n5.651978e-07\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.352320e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000005\n\n\n-0.000004\n\n\n0.000001\n\n\n139.000000\n\n\n235.000003\n\n\n214.999998\n\n\n18.000001\n\n\n0.000003\n\n\n-0.000008\n\n\n169.000000\n\n\n234.999997\n\n\n211.999998\n\n\n12.000004\n\n\n-0.000003\n\n\n0.000004\n\n\n-0.000005\n\n\n-2.677840e-07\n\n\n-3.174695e-07\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n-9.513843e-07\n\n\n-2.911069e-07\n\n\n2.894738e-08\n\n\n0.000004\n\n\n0.000005\n\n\n-3.351748e-07\n\n\n-0.000003\n\n\n158.000000\n\n\n236.000003\n\n\n213.999998\n\n\n14.000003\n\n\n-0.000002\n\n\n-9.949699e-07\n\n\n176.000001\n\n\n234.999997\n\n\n215.000001\n\n\n32.000002\n\n\n0.000003\n\n\n0.000005\n\n\n-0.000002\n\n\n-0.000001\n\n\n-0.000002\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000003\n\n\n3.360567e-07\n\n\n0.000004\n\n\n1.282302e-07\n\n\n2.258955e-07\n\n\n152.000000\n\n\n232.000001\n\n\n227.000003\n\n\n66.000000\n\n\n-0.000002\n\n\n0.000002\n\n\n203.000000\n\n\n234.999997\n\n\n217.000002\n\n\n35.000002\n\n\n-0.000001\n\n\n-2.610268e-07\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n-2.873870e-07\n\n\n5.500636e-07\n\n\n0.000001\n\n\n-0.000002\n\n\n6.275720e-07\n\n\n4.274649e-07\n\n\n0.000001\n\n\n1.450000e+02\n\n\n2.340000e+02\n\n\n229.999998\n\n\n90.000000\n\n\n-0.000002\n\n\n0.000002\n\n\n2.080000e+02\n\n\n235.000001\n\n\n212.999995\n\n\n8.999999\n\n\n6.923674e-07\n\n\n0.000001\n\n\n-8.325330e-07\n\n\n-5.767398e-07\n\n\n-1.849482e-07\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n0.000001\n\n\n-0.000001\n\n\n-4.551377e-07\n\n\n7.378563e-07\n\n\n-0.000002\n\n\n3.950150e-07\n\n\n119.000000\n\n\n236.999998\n\n\n229.000001\n\n\n7.600000e+01\n\n\n0.000003\n\n\n-0.000005\n\n\n1.670000e+02\n\n\n241.000000\n\n\n208.000003\n\n\n-0.000004\n\n\n4.021377e-07\n\n\n0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n0.000001\n\n\n9.323434e-07\n\n\n0.000002\n\n\n8.744090e-07\n\n\n0.000002\n\n\n0.000005\n\n\n76.000002\n\n\n239.000004\n\n\n218.999999\n\n\n19.000002\n\n\n0.000005\n\n\n8.867231e-07\n\n\n93.000000\n\n\n244.999997\n\n\n206.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000001\n\n\n3.297868e-07\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n0.000002\n\n\n-0.000001\n\n\n8.809801e-07\n\n\n1.723858e-07\n\n\n0.000001\n\n\n4.900000e+01\n\n\n239.000004\n\n\n203.000003\n\n\n-0.000001\n\n\n-0.000001\n\n\n8.076585e-07\n\n\n31.000002\n\n\n238.000000\n\n\n205.000002\n\n\n-1.852046e-07\n\n\n6.976350e-08\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n40.000001\n\n\n2.390000e+02\n\n\n191.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n18.000001\n\n\n228.000000\n\n\n2.010000e+02\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n29.000000\n\n\n245.999998\n\n\n198.999995\n\n\n0.000002\n\n\n-0.000003\n\n\n0.000001\n\n\n9.000003\n\n\n239.000002\n\n\n219.999997\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n1.600000e+02\n\n\n1.340000e+02\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n1.380000e+02\n\n\n128.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n1\n\n\n\n\n5\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n9.682471e-07\n\n\n38.000000\n\n\n3.000000e+01\n\n\n6.000002e+00\n\n\n1.600000e+01\n\n\n3.900000e+01\n\n\n2.500000e+01\n\n\n-0.000001\n\n\n5.516976e-08\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n7.000000\n\n\n3.000000e+01\n\n\n68.000001\n\n\n64.999999\n\n\n7.500000e+01\n\n\n39.999998\n\n\n67.000000\n\n\n54.999999\n\n\n3.000000e+00\n\n\n0.000001\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n3.000000e+00\n\n\n3.900000e+01\n\n\n75.999999\n\n\n56.000000\n\n\n9.000000\n\n\n132.999999\n\n\n161.999998\n\n\n151.000001\n\n\n1.520000e+02\n\n\n164.000000\n\n\n72.999999\n\n\n22.999999\n\n\n4.800000e+01\n\n\n61.000000\n\n\n12.000000\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n1.000000e+00\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n1.600000e+01\n\n\n6.800000e+01\n\n\n6.300000e+01\n\n\n3.800000e+01\n\n\n24.999998\n\n\n0.000005\n\n\n109.000000\n\n\n241.000002\n\n\n240.000000\n\n\n255.000003\n\n\n117.000000\n\n\n2.000000e+01\n\n\n1.880000e+02\n\n\n47.000001\n\n\n30.000001\n\n\n5.400000e+01\n\n\n5.500000e+01\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.400000e+01\n\n\n3.800000e+01\n\n\n20.000000\n\n\n54.000000\n\n\n58.999999\n\n\n100.000000\n\n\n0.000003\n\n\n138.000000\n\n\n1.840000e+02\n\n\n1.860000e+02\n\n\n0.000004\n\n\n0.000004\n\n\n10.999999\n\n\n1.050000e+02\n\n\n42.000001\n\n\n1.900000e+01\n\n\n4.100000e+01\n\n\n3.000001e+00\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n41.000000\n\n\n33.000000\n\n\n35.000001\n\n\n59.000000\n\n\n41.999999\n\n\n22.999999\n\n\n9.999996e+00\n\n\n-0.000001\n\n\n101.000000\n\n\n56.000002\n\n\n0.000004\n\n\n19.000002\n\n\n8.000001\n\n\n34.999998\n\n\n41.000000\n\n\n1.600000e+01\n\n\n41.000001\n\n\n2.400000e+01\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n1.000000e+00\n\n\n45.000000\n\n\n36.999999\n\n\n34.000001\n\n\n2.200000e+01\n\n\n24.000000\n\n\n2.600000e+01\n\n\n-0.000002\n\n\n0.000005\n\n\n-0.000001\n\n\n25.000001\n\n\n15.999998\n\n\n7.000003\n\n\n14.000005\n\n\n95.000000\n\n\n12.999997\n\n\n16.999999\n\n\n3.200000e+01\n\n\n34.000001\n\n\n8.919840e-07\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.000000e+00\n\n\n44.000000\n\n\n34.000000\n\n\n42.000000\n\n\n28.999999\n\n\n51.999999\n\n\n1.370000e+02\n\n\n136.999999\n\n\n1.110000e+02\n\n\n49.999997\n\n\n17.000002\n\n\n26.999997\n\n\n23.999996\n\n\n72.000000\n\n\n124.000000\n\n\n5.000002\n\n\n28.000002\n\n\n30.999998\n\n\n37.000000\n\n\n1.000000\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n1.400000e+01\n\n\n49.000000\n\n\n2.500000e+01\n\n\n48.999999\n\n\n3.900000e+01\n\n\n26.000002\n\n\n39.999999\n\n\n81.000000\n\n\n117.000000\n\n\n125.000000\n\n\n95.000000\n\n\n54.000002\n\n\n17.999999\n\n\n42.999996\n\n\n55.000001\n\n\n2.999996\n\n\n3.000000e+01\n\n\n26.999999\n\n\n33.000001\n\n\n9.000000\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n2.600000e+01\n\n\n4.900000e+01\n\n\n8.000000e+00\n\n\n141.000003\n\n\n75.000000\n\n\n11.000000\n\n\n14.000002\n\n\n6.000004\n\n\n14.000001\n\n\n35.000000\n\n\n7.200000e+01\n\n\n93.999999\n\n\n13.000003\n\n\n72.999998\n\n\n8.999999\n\n\n81.000001\n\n\n126.000000\n\n\n4.999995\n\n\n34.000002\n\n\n19.000001\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n3.100000e+01\n\n\n5.100000e+01\n\n\n13.999998\n\n\n159.999997\n\n\n88.000000\n\n\n13.000001\n\n\n20.000000\n\n\n25.000000\n\n\n17.000003\n\n\n19.000002\n\n\n11.000003\n\n\n38.000001\n\n\n133.000000\n\n\n69.000003\n\n\n0.000005\n\n\n159.000000\n\n\n1.380000e+02\n\n\n1.000002\n\n\n33.000002\n\n\n2.600000e+01\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n3.400000e+01\n\n\n4.700000e+01\n\n\n45.000000\n\n\n99.999999\n\n\n23.000000\n\n\n71.000000\n\n\n31.000002\n\n\n17.999999\n\n\n41.999996\n\n\n75.999998\n\n\n13.000002\n\n\n23.999999\n\n\n82.000002\n\n\n59.000001\n\n\n22.000000\n\n\n37.999999\n\n\n82.999999\n\n\n12.999999\n\n\n32.999999\n\n\n28.999998\n\n\n-0.000001\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n41.000000\n\n\n39.999999\n\n\n72.000000\n\n\n68.000000\n\n\n5.000001\n\n\n100.000000\n\n\n50.000000\n\n\n23.000004\n\n\n32.999996\n\n\n95.000000\n\n\n9.999981e-01\n\n\n67.999997\n\n\n62.999997\n\n\n66.999996\n\n\n24.999999\n\n\n16.000001\n\n\n93.999999\n\n\n22.000005\n\n\n33.999999\n\n\n33.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n42.000000\n\n\n42.000001\n\n\n8.500000e+01\n\n\n57.000001\n\n\n72.000001\n\n\n49.000002\n\n\n33.999996\n\n\n27.999999\n\n\n37.999997\n\n\n122.000000\n\n\n0.000002\n\n\n105.999999\n\n\n82.000001\n\n\n12.999995\n\n\n94.999998\n\n\n0.000004\n\n\n8.900000e+01\n\n\n35.999998\n\n\n38.000000\n\n\n32.999998\n\n\n3.999999\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n7.000000e+00\n\n\n40.000000\n\n\n3.600000e+01\n\n\n97.000000\n\n\n54.000000\n\n\n45.999999\n\n\n27.000003\n\n\n36.000000\n\n\n29.999999\n\n\n40.999997\n\n\n1.110000e+02\n\n\n-0.000004\n\n\n119.000000\n\n\n56.000002\n\n\n3.000001\n\n\n79.000004\n\n\n46.000002\n\n\n62.999998\n\n\n6.800000e+01\n\n\n3.800000e+01\n\n\n24.999999\n\n\n9.000003\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n1.600000e+01\n\n\n36.999999\n\n\n35.000001\n\n\n1.140000e+02\n\n\n30.000001\n\n\n26.000002\n\n\n3.000000e+01\n\n\n32.999997\n\n\n22.000002\n\n\n37.000002\n\n\n110.000000\n\n\n0.000001\n\n\n125.999999\n\n\n32.999996\n\n\n16.000001\n\n\n29.000002\n\n\n53.000004\n\n\n55.999998\n\n\n90.999999\n\n\n3.700000e+01\n\n\n22.000000\n\n\n16.000000\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n22.000000\n\n\n3.500000e+01\n\n\n3.900000e+01\n\n\n131.000000\n\n\n2.000004\n\n\n40.999999\n\n\n52.000000\n\n\n50.999997\n\n\n33.999998\n\n\n40.999997\n\n\n104.999999\n\n\n0.999995\n\n\n136.000000\n\n\n19.000007\n\n\n28.000002\n\n\n34.000002\n\n\n1.800000e+01\n\n\n35.999998\n\n\n1.160000e+02\n\n\n31.000001\n\n\n23.000002\n\n\n18.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n2.200000e+01\n\n\n33.000000\n\n\n54.000001\n\n\n137.999999\n\n\n0.000005\n\n\n47.000002\n\n\n63.999998\n\n\n41.000000\n\n\n28.000001\n\n\n56.999998\n\n\n94.999998\n\n\n43.999996\n\n\n1.070000e+02\n\n\n38.000001\n\n\n45.000003\n\n\n42.999999\n\n\n2.600000e+01\n\n\n8.000003e+00\n\n\n1.440000e+02\n\n\n30.000003\n\n\n22.000002\n\n\n1.900000e+01\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n2.700000e+01\n\n\n23.999998\n\n\n62.000001\n\n\n130.000001\n\n\n-0.000005\n\n\n22.999996\n\n\n50.999996\n\n\n43.000002\n\n\n35.000004\n\n\n42.000002\n\n\n19.999999\n\n\n58.999998\n\n\n18.000000\n\n\n39.999998\n\n\n42.000000\n\n\n24.999998\n\n\n26.000004\n\n\n-0.000003\n\n\n142.999999\n\n\n34.000001\n\n\n2.200000e+01\n\n\n2.400000e+01\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n2.900000e+01\n\n\n2.400000e+01\n\n\n7.200000e+01\n\n\n109.999999\n\n\n2.000004\n\n\n5.800000e+01\n\n\n59.000000\n\n\n59.999996\n\n\n58.000004\n\n\n73.999996\n\n\n52.999996\n\n\n83.999998\n\n\n4.500000e+01\n\n\n52.000000\n\n\n55.999999\n\n\n37.000002\n\n\n39.999998\n\n\n6.000001\n\n\n120.000001\n\n\n41.000001\n\n\n17.999997\n\n\n20.000000\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n27.000000\n\n\n18.999999\n\n\n75.000000\n\n\n9.700000e+01\n\n\n0.000004\n\n\n4.500000e+01\n\n\n4.300000e+01\n\n\n60.000004\n\n\n62.999998\n\n\n77.000001\n\n\n58.000001\n\n\n120.000001\n\n\n98.999999\n\n\n29.000004\n\n\n47.999997\n\n\n36.000001\n\n\n29.000001\n\n\n6.999998\n\n\n1.180000e+02\n\n\n42.000001\n\n\n20.000002\n\n\n24.000000\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n6.200000e+01\n\n\n3.600000e+01\n\n\n64.000000\n\n\n81.000000\n\n\n6.275720e-07\n\n\n4.274649e-07\n\n\n0.000001\n\n\n5.958969e-07\n\n\n6.911157e-07\n\n\n-0.000003\n\n\n-0.000004\n\n\n9.999997\n\n\n1.999998\n\n\n-3.494023e-08\n\n\n0.000001\n\n\n0.000001\n\n\n0.000004\n\n\n6.923674e-07\n\n\n100.000000\n\n\n3.700000e+01\n\n\n5.800000e+01\n\n\n5.100000e+01\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n8.300000e+01\n\n\n35.000001\n\n\n58.000000\n\n\n9.100000e+01\n\n\n7.378563e-07\n\n\n-0.000002\n\n\n3.950150e-07\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000004\n\n\n-4.699235e-07\n\n\n0.000003\n\n\n-0.000005\n\n\n9.842208e-07\n\n\n0.000005\n\n\n-0.000004\n\n\n-0.000004\n\n\n4.021377e-07\n\n\n100.000000\n\n\n27.999998\n\n\n70.000000\n\n\n6.000000e+01\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n8.100000e+01\n\n\n36.000000\n\n\n5.500000e+01\n\n\n95.000000\n\n\n8.744090e-07\n\n\n0.000002\n\n\n0.000005\n\n\n0.000003\n\n\n0.000005\n\n\n-0.000001\n\n\n0.000004\n\n\n1.000001\n\n\n8.867231e-07\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n96.000000\n\n\n29.999999\n\n\n7.500000e+01\n\n\n5.800000e+01\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n6.900000e+01\n\n\n4.100000e+01\n\n\n51.000000\n\n\n102.000001\n\n\n8.809801e-07\n\n\n1.723858e-07\n\n\n0.000001\n\n\n-7.570235e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-0.000001\n\n\n-0.000001\n\n\n8.076585e-07\n\n\n-0.000005\n\n\n-0.000006\n\n\n0.000005\n\n\n-1.852046e-07\n\n\n6.976350e-08\n\n\n88.000001\n\n\n28.000000\n\n\n73.999999\n\n\n52.999999\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n6.900000e+01\n\n\n38.000000\n\n\n46.000000\n\n\n106.000000\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n-0.000002\n\n\n3.933536e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n-0.000005\n\n\n1.000001e+00\n\n\n0.000002\n\n\n-0.000002\n\n\n90.999999\n\n\n1.900000e+01\n\n\n79.999999\n\n\n5.300000e+01\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n7.600000e+01\n\n\n5.000000e+01\n\n\n6.000000e+01\n\n\n106.000001\n\n\n6.000001e+00\n\n\n-0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000001\n\n\n1.000001\n\n\n-0.000002\n\n\n3.000000\n\n\n99.000002\n\n\n43.000000\n\n\n99.999998\n\n\n6.000000e+01\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n3.800000e+01\n\n\n1.600000e+01\n\n\n3.743793e-07\n\n\n1.100000e+01\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n9.999998e-01\n\n\n9.522988e-07\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n-3.593068e-07\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n3.000001e+00\n\n\n-6.984834e-07\n\n\n2.100000e+01\n\n\n9.000000e+00\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n4\n\n\n\n\n6\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n1.250000e+02\n\n\n114.000003\n\n\n1.090000e+02\n\n\n1.060000e+02\n\n\n1.110000e+02\n\n\n1.200000e+02\n\n\n1.130000e+02\n\n\n131.999999\n\n\n9.900000e+01\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n4.000002\n\n\n2.550000e+02\n\n\n237.999997\n\n\n254.999999\n\n\n2.550000e+02\n\n\n255.000004\n\n\n255.000004\n\n\n255.000000\n\n\n2.460000e+02\n\n\n255.000007\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n0.000002\n\n\n110.000001\n\n\n236.999999\n\n\n217.999997\n\n\n218.000003\n\n\n219.999999\n\n\n2.190000e+02\n\n\n215.999998\n\n\n218.000001\n\n\n216.999996\n\n\n2.550000e+02\n\n\n21.999999\n\n\n-0.000002\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n8.386749e-07\n\n\n219.000000\n\n\n232.999996\n\n\n218.999999\n\n\n224.999995\n\n\n224.999998\n\n\n224.000003\n\n\n223.000003\n\n\n2.240000e+02\n\n\n2.150000e+02\n\n\n255.000000\n\n\n110.000001\n\n\n5.257434e-07\n\n\n-3.949514e-07\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n253.999995\n\n\n225.000000\n\n\n217.000005\n\n\n220.000004\n\n\n2.220000e+02\n\n\n2.230000e+02\n\n\n220.999998\n\n\n223.000002\n\n\n211.000002\n\n\n2.520000e+02\n\n\n146.000002\n\n\n5.942894e-07\n\n\n9.570876e-07\n\n\n7.753735e-07\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000002\n\n\n250.000000\n\n\n223.000005\n\n\n2.180000e+02\n\n\n220.000001\n\n\n220.999995\n\n\n218.999997\n\n\n221.000002\n\n\n219.000003\n\n\n216.000004\n\n\n246.000001\n\n\n133.000000\n\n\n2.016850e-07\n\n\n-0.000002\n\n\n-4.557509e-07\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-8.460442e-08\n\n\n240.000000\n\n\n2.240000e+02\n\n\n220.000000\n\n\n221.999997\n\n\n226.000002\n\n\n227.000001\n\n\n218.999998\n\n\n221.000000\n\n\n215.000002\n\n\n243.000003\n\n\n140.999999\n\n\n0.000005\n\n\n-7.583148e-07\n\n\n-0.000002\n\n\n8.919840e-07\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n-0.000001\n\n\n0.000001\n\n\n251.000000\n\n\n2.270000e+02\n\n\n217.000000\n\n\n2.250000e+02\n\n\n240.000004\n\n\n232.999997\n\n\n225.999998\n\n\n218.999998\n\n\n214.999997\n\n\n239.000004\n\n\n160.000002\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n7.406876e-07\n\n\n-0.000003\n\n\n-9.455209e-07\n\n\n254.000002\n\n\n220.000004\n\n\n215.999999\n\n\n230.000000\n\n\n220.999998\n\n\n130.000000\n\n\n255.000002\n\n\n215.999997\n\n\n216.999998\n\n\n248.999996\n\n\n131.000000\n\n\n3.125923e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n9.804652e-07\n\n\n0.000001\n\n\n22.999998\n\n\n255.000003\n\n\n215.999997\n\n\n211.000002\n\n\n252.000001\n\n\n175.999998\n\n\n2.300000e+01\n\n\n254.999998\n\n\n217.999999\n\n\n214.000001\n\n\n255.000002\n\n\n102.000000\n\n\n-0.000002\n\n\n-0.000006\n\n\n0.000005\n\n\n0.000002\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n0.000002\n\n\n0.000003\n\n\n37.999998\n\n\n254.999999\n\n\n216.000001\n\n\n208.999997\n\n\n254.999999\n\n\n86.000000\n\n\n-0.000003\n\n\n255.000002\n\n\n222.000002\n\n\n217.999998\n\n\n255.000001\n\n\n74.999998\n\n\n8.404825e-07\n\n\n-0.000003\n\n\n-0.000002\n\n\n1.135173e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n37.000002\n\n\n255.000002\n\n\n214.000003\n\n\n211.999997\n\n\n254.999996\n\n\n19.000001\n\n\n0.000004\n\n\n254.000001\n\n\n228.000004\n\n\n217.000001\n\n\n254.999998\n\n\n54.000001\n\n\n-0.000005\n\n\n-0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000001\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n33.000001\n\n\n255.000001\n\n\n212.999995\n\n\n214.000002\n\n\n254.999999\n\n\n-0.000002\n\n\n-5.688553e-07\n\n\n220.000002\n\n\n233.999997\n\n\n218.000000\n\n\n255.000004\n\n\n12.999999\n\n\n-0.000003\n\n\n-0.000005\n\n\n-0.000002\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-7.203892e-07\n\n\n0.000001\n\n\n37.000002\n\n\n255.000000\n\n\n212.000002\n\n\n222.000002\n\n\n214.000000\n\n\n-0.000002\n\n\n0.000002\n\n\n186.999999\n\n\n236.000002\n\n\n219.999998\n\n\n246.000000\n\n\n0.000004\n\n\n5.151848e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n0.000004\n\n\n0.000002\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n0.000002\n\n\n0.000005\n\n\n55.000001\n\n\n255.000002\n\n\n209.000000\n\n\n227.000000\n\n\n193.000001\n\n\n4.167304e-07\n\n\n-0.000004\n\n\n160.000000\n\n\n239.000001\n\n\n220.999999\n\n\n238.000001\n\n\n0.000005\n\n\n0.000001\n\n\n7.423447e-07\n\n\n1.327465e-07\n\n\n0.000005\n\n\n0.000002\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n0.000002\n\n\n-5.223541e-09\n\n\n0.000004\n\n\n41.000002\n\n\n2.550000e+02\n\n\n211.000001\n\n\n229.000003\n\n\n189.000001\n\n\n0.000002\n\n\n0.000001\n\n\n140.000000\n\n\n239.999997\n\n\n222.999999\n\n\n233.000001\n\n\n-0.000001\n\n\n0.000002\n\n\n0.000005\n\n\n4.826824e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n5.139264e-07\n\n\n-0.000002\n\n\n-0.000004\n\n\n-0.000002\n\n\n254.000003\n\n\n216.000002\n\n\n227.999997\n\n\n215.000000\n\n\n-0.000004\n\n\n-0.000004\n\n\n113.000001\n\n\n241.000002\n\n\n223.999999\n\n\n221.000002\n\n\n-1.050891e-07\n\n\n-0.000005\n\n\n-4.653743e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n0.000005\n\n\n0.000002\n\n\n240.000004\n\n\n224.000001\n\n\n235.000004\n\n\n196.000001\n\n\n0.000004\n\n\n-0.000001\n\n\n7.200000e+01\n\n\n238.999997\n\n\n225.000001\n\n\n221.999998\n\n\n8.073550e-07\n\n\n3.296967e-07\n\n\n-6.241514e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n5.651978e-07\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.352320e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000005\n\n\n-0.000004\n\n\n176.999999\n\n\n231.999996\n\n\n229.000000\n\n\n200.999998\n\n\n-0.000004\n\n\n0.000003\n\n\n48.999999\n\n\n234.000003\n\n\n228.000003\n\n\n226.000003\n\n\n-0.000003\n\n\n-0.000003\n\n\n0.000004\n\n\n-0.000005\n\n\n-2.677840e-07\n\n\n-3.174695e-07\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n-9.513843e-07\n\n\n-2.911069e-07\n\n\n2.894738e-08\n\n\n0.000004\n\n\n0.000005\n\n\n-3.351748e-07\n\n\n112.000001\n\n\n236.999999\n\n\n217.000000\n\n\n255.000002\n\n\n-0.000003\n\n\n-0.000002\n\n\n5.000000e+01\n\n\n233.999996\n\n\n229.999998\n\n\n226.000004\n\n\n0.000005\n\n\n0.000003\n\n\n0.000005\n\n\n-0.000002\n\n\n-0.000001\n\n\n-0.000002\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000003\n\n\n3.360567e-07\n\n\n0.000004\n\n\n1.282302e-07\n\n\n6.400000e+01\n\n\n238.000001\n\n\n213.000000\n\n\n232.000001\n\n\n31.000001\n\n\n-0.000002\n\n\n62.000000\n\n\n240.000004\n\n\n227.000000\n\n\n213.000000\n\n\n-0.000004\n\n\n-0.000001\n\n\n-2.610268e-07\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n-2.873870e-07\n\n\n5.500636e-07\n\n\n0.000001\n\n\n-0.000002\n\n\n6.275720e-07\n\n\n4.274649e-07\n\n\n28.000004\n\n\n2.290000e+02\n\n\n2.160000e+02\n\n\n236.000004\n\n\n71.000000\n\n\n-0.000002\n\n\n59.000001\n\n\n2.390000e+02\n\n\n227.999996\n\n\n211.000002\n\n\n0.000004\n\n\n6.923674e-07\n\n\n0.000001\n\n\n-8.325330e-07\n\n\n-5.767398e-07\n\n\n-1.849482e-07\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n0.000001\n\n\n-0.000001\n\n\n-4.551377e-07\n\n\n7.378563e-07\n\n\n-0.000002\n\n\n3.950150e-07\n\n\n212.000002\n\n\n222.000000\n\n\n237.000004\n\n\n1.090000e+02\n\n\n0.000003\n\n\n51.000001\n\n\n2.370000e+02\n\n\n229.000001\n\n\n219.999996\n\n\n-0.000004\n\n\n4.021377e-07\n\n\n0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n0.000001\n\n\n9.323434e-07\n\n\n0.000002\n\n\n8.744090e-07\n\n\n0.000002\n\n\n0.000005\n\n\n207.999998\n\n\n227.999997\n\n\n236.000003\n\n\n158.000001\n\n\n0.000005\n\n\n5.600000e+01\n\n\n239.000004\n\n\n229.000003\n\n\n218.000000\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000001\n\n\n3.297868e-07\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n0.000002\n\n\n-0.000001\n\n\n8.809801e-07\n\n\n1.000002e+00\n\n\n0.000001\n\n\n1.510000e+02\n\n\n238.000000\n\n\n230.999997\n\n\n206.999996\n\n\n-0.000001\n\n\n5.100000e+01\n\n\n239.999999\n\n\n231.000004\n\n\n214.000003\n\n\n-1.852046e-07\n\n\n6.976350e-08\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n3.000001\n\n\n7.009453e-07\n\n\n103.000000\n\n\n2.360000e+02\n\n\n225.000002\n\n\n238.000004\n\n\n-0.000001\n\n\n41.000001\n\n\n234.000001\n\n\n230.999997\n\n\n2.050000e+02\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n3.000000\n\n\n0.000001\n\n\n40.000002\n\n\n242.999999\n\n\n223.999995\n\n\n254.999998\n\n\n-0.000003\n\n\n34.999997\n\n\n247.000003\n\n\n245.999994\n\n\n200.000003\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n3.000001e+00\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n1.720000e+02\n\n\n2.240000e+02\n\n\n1.390000e+02\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n1.050000e+02\n\n\n1.670000e+02\n\n\n27.000000\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n1\n\n\n\n\n7\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n9.682471e-07\n\n\n-0.000002\n\n\n-5.505288e-07\n\n\n4.053924e-07\n\n\n4.035032e-07\n\n\n2.956210e-07\n\n\n4.005417e-07\n\n\n-0.000001\n\n\n5.516976e-08\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n-8.031146e-08\n\n\n1.985495e-07\n\n\n-1.276844e-07\n\n\n-0.000001\n\n\n7.044741e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-7.585758e-07\n\n\n-0.000006\n\n\n0.000006\n\n\n0.000002\n\n\n-1.279166e-07\n\n\n0.000001\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n1.444539e-07\n\n\n-1.736492e-07\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n2.055711e-07\n\n\n4.611180e-07\n\n\n3.908779e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000003\n\n\n-4.594104e-07\n\n\n0.000004\n\n\n0.000005\n\n\n0.000001\n\n\n-4.233165e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n7.057845e-07\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n7.656082e-07\n\n\n-3.514349e-07\n\n\n-9.688781e-07\n\n\n8.386749e-07\n\n\n0.000002\n\n\n0.000005\n\n\n0.000004\n\n\n-0.000003\n\n\n-0.000003\n\n\n-0.000003\n\n\n-0.000004\n\n\n-6.836428e-07\n\n\n-3.299791e-07\n\n\n-0.000005\n\n\n0.000002\n\n\n5.257434e-07\n\n\n-3.949514e-07\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n2.273738e-07\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000002\n\n\n0.000005\n\n\n0.000003\n\n\n0.000002\n\n\n9.626116e-07\n\n\n6.564152e-07\n\n\n0.000004\n\n\n0.000004\n\n\n-0.000005\n\n\n2.960771e-07\n\n\n-0.000002\n\n\n5.942894e-07\n\n\n9.570876e-07\n\n\n7.753735e-07\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000005\n\n\n-0.000001\n\n\n6.513797e-07\n\n\n-0.000001\n\n\n0.000005\n\n\n-0.000004\n\n\n0.000004\n\n\n-0.000001\n\n\n0.000004\n\n\n0.000002\n\n\n0.000004\n\n\n2.016850e-07\n\n\n-0.000002\n\n\n-4.557509e-07\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n1.030000e+02\n\n\n6.700000e+01\n\n\n6.900000e+01\n\n\n68.000000\n\n\n60.000000\n\n\n56.000000\n\n\n5.400000e+01\n\n\n49.999999\n\n\n5.100000e+01\n\n\n51.000002\n\n\n49.000003\n\n\n55.000002\n\n\n59.000002\n\n\n55.999998\n\n\n58.000001\n\n\n60.000001\n\n\n71.000001\n\n\n72.000001\n\n\n74.000000\n\n\n7.500000e+01\n\n\n81.000000\n\n\n8.300000e+01\n\n\n7.900000e+01\n\n\n8.100000e+01\n\n\n1.220000e+02\n\n\n1.100000e+01\n\n\n-2.890140e-08\n\n\n5.900000e+01\n\n\n2.370000e+02\n\n\n2.280000e+02\n\n\n2.270000e+02\n\n\n254.999999\n\n\n255.000005\n\n\n254.999999\n\n\n255.000005\n\n\n252.999998\n\n\n2.530000e+02\n\n\n252.999997\n\n\n2.550000e+02\n\n\n255.000003\n\n\n254.999997\n\n\n254.999997\n\n\n254.999999\n\n\n254.999995\n\n\n255.000000\n\n\n254.999999\n\n\n254.999996\n\n\n254.999998\n\n\n254.999999\n\n\n255.000001\n\n\n2.550000e+02\n\n\n2.290000e+02\n\n\n2.480000e+02\n\n\n1.250000e+02\n\n\n2.976238e-08\n\n\n1.230000e+02\n\n\n2.380000e+02\n\n\n2.120000e+02\n\n\n2.140000e+02\n\n\n213.999998\n\n\n2.150000e+02\n\n\n214.999996\n\n\n2.130000e+02\n\n\n209.000005\n\n\n209.000001\n\n\n209.000004\n\n\n210.000003\n\n\n213.000005\n\n\n213.999999\n\n\n215.000003\n\n\n215.999997\n\n\n216.999998\n\n\n217.000003\n\n\n217.000002\n\n\n2.180000e+02\n\n\n217.999999\n\n\n217.999995\n\n\n218.999997\n\n\n218.000006\n\n\n2.180000e+02\n\n\n2.330000e+02\n\n\n1.480000e+02\n\n\n-2.152347e-08\n\n\n1.610000e+02\n\n\n2.370000e+02\n\n\n2.110000e+02\n\n\n2.180000e+02\n\n\n2.170000e+02\n\n\n2.190000e+02\n\n\n217.999998\n\n\n217.000003\n\n\n215.000001\n\n\n212.999998\n\n\n213.000001\n\n\n214.000000\n\n\n214.999997\n\n\n2.160000e+02\n\n\n215.999999\n\n\n217.999999\n\n\n217.999999\n\n\n220.000001\n\n\n219.999999\n\n\n221.000005\n\n\n221.999999\n\n\n220.999998\n\n\n222.000002\n\n\n2.250000e+02\n\n\n2.240000e+02\n\n\n2.350000e+02\n\n\n1.770000e+02\n\n\n3.391509e-08\n\n\n1.520000e+02\n\n\n2.400000e+02\n\n\n2.150000e+02\n\n\n2.190000e+02\n\n\n2.150000e+02\n\n\n217.999996\n\n\n217.999999\n\n\n217.000001\n\n\n214.000000\n\n\n211.999998\n\n\n210.999997\n\n\n210.999996\n\n\n213.000003\n\n\n216.000002\n\n\n216.000001\n\n\n218.000002\n\n\n217.999998\n\n\n219.000002\n\n\n219.000003\n\n\n2.200000e+02\n\n\n220.000005\n\n\n221.000002\n\n\n2.250000e+02\n\n\n226.000000\n\n\n221.000008\n\n\n2.320000e+02\n\n\n1.970000e+02\n\n\n-3.134657e-08\n\n\n1.000000e+02\n\n\n2.410000e+02\n\n\n2.210000e+02\n\n\n2.150000e+02\n\n\n2.190000e+02\n\n\n217.999998\n\n\n221.000004\n\n\n219.000001\n\n\n218.000002\n\n\n217.000002\n\n\n215.999997\n\n\n215.000002\n\n\n218.000000\n\n\n217.999998\n\n\n217.999998\n\n\n221.000000\n\n\n220.999999\n\n\n218.999999\n\n\n220.999997\n\n\n220.000001\n\n\n221.000002\n\n\n220.000004\n\n\n222.999999\n\n\n224.000002\n\n\n2.180000e+02\n\n\n255.000002\n\n\n1.500000e+02\n\n\n-1.048992e-07\n\n\n1.610000e+02\n\n\n2.290000e+02\n\n\n2.080000e+02\n\n\n231.999995\n\n\n212.000005\n\n\n219.000000\n\n\n219.000001\n\n\n217.999997\n\n\n218.999999\n\n\n217.999998\n\n\n218.000000\n\n\n217.000002\n\n\n219.000000\n\n\n2.190000e+02\n\n\n218.000000\n\n\n222.000001\n\n\n223.000000\n\n\n223.999999\n\n\n224.000001\n\n\n218.999998\n\n\n218.999997\n\n\n224.999997\n\n\n223.999995\n\n\n227.999995\n\n\n223.000005\n\n\n255.000000\n\n\n1.500000e+01\n\n\n-1.249524e-07\n\n\n2.550000e+02\n\n\n2.420000e+02\n\n\n2.040000e+02\n\n\n205.999994\n\n\n222.999998\n\n\n2.180000e+02\n\n\n222.000004\n\n\n220.999997\n\n\n217.999997\n\n\n218.000002\n\n\n218.000002\n\n\n216.000000\n\n\n217.000002\n\n\n218.000005\n\n\n218.000001\n\n\n218.999999\n\n\n222.000001\n\n\n220.000001\n\n\n216.000001\n\n\n2.170000e+02\n\n\n219.999997\n\n\n222.999995\n\n\n227.000001\n\n\n225.999996\n\n\n2.150000e+02\n\n\n254.999991\n\n\n3.500000e+01\n\n\n5.450883e-08\n\n\n2.250000e+02\n\n\n2.490000e+02\n\n\n2.380000e+02\n\n\n209.999997\n\n\n1.990000e+02\n\n\n220.000004\n\n\n225.999999\n\n\n222.999996\n\n\n222.000002\n\n\n221.000000\n\n\n220.000000\n\n\n219.999999\n\n\n2.180000e+02\n\n\n226.999999\n\n\n221.000001\n\n\n215.999999\n\n\n222.000002\n\n\n220.000000\n\n\n217.000000\n\n\n225.999998\n\n\n2.280000e+02\n\n\n2.230000e+02\n\n\n208.000003\n\n\n218.000004\n\n\n2.320000e+02\n\n\n255.000004\n\n\n6.200000e+01\n\n\n1.800000e+01\n\n\n2.370000e+02\n\n\n2.400000e+02\n\n\n2.430000e+02\n\n\n254.999998\n\n\n237.000000\n\n\n2.050000e+02\n\n\n206.999999\n\n\n219.000000\n\n\n2.250000e+02\n\n\n225.000000\n\n\n223.999998\n\n\n221.000000\n\n\n227.000003\n\n\n213.000002\n\n\n226.000002\n\n\n218.999998\n\n\n223.999999\n\n\n222.999998\n\n\n222.999998\n\n\n225.000005\n\n\n211.999997\n\n\n2.050000e+02\n\n\n230.999996\n\n\n251.999997\n\n\n2.400000e+02\n\n\n2.530000e+02\n\n\n7.000000e+01\n\n\n3.300000e+01\n\n\n2.410000e+02\n\n\n2.380000e+02\n\n\n237.999997\n\n\n2.400000e+02\n\n\n2.500000e+02\n\n\n252.999996\n\n\n234.000005\n\n\n214.999999\n\n\n213.000002\n\n\n216.000002\n\n\n219.999999\n\n\n221.999999\n\n\n235.999996\n\n\n99.000001\n\n\n210.999999\n\n\n226.000002\n\n\n223.999999\n\n\n224.000000\n\n\n2.150000e+02\n\n\n205.000002\n\n\n2.220000e+02\n\n\n246.999999\n\n\n254.999999\n\n\n244.000005\n\n\n236.000001\n\n\n2.550000e+02\n\n\n1.000000e+02\n\n\n2.900000e+01\n\n\n2.410000e+02\n\n\n2.340000e+02\n\n\n2.410000e+02\n\n\n239.000008\n\n\n238.999999\n\n\n237.999999\n\n\n243.999999\n\n\n250.000003\n\n\n236.000004\n\n\n221.000002\n\n\n210.999999\n\n\n213.000001\n\n\n225.999997\n\n\n223.999998\n\n\n2.240000e+02\n\n\n217.000000\n\n\n214.000001\n\n\n209.000001\n\n\n2.170000e+02\n\n\n2.430000e+02\n\n\n2.550000e+02\n\n\n245.000001\n\n\n237.999995\n\n\n2.430000e+02\n\n\n2.350000e+02\n\n\n252.999993\n\n\n1.120000e+02\n\n\n2.000000e+00\n\n\n2.300000e+02\n\n\n242.000000\n\n\n2.390000e+02\n\n\n235.999996\n\n\n242.000004\n\n\n238.000000\n\n\n237.000001\n\n\n238.000002\n\n\n245.000004\n\n\n242.000002\n\n\n236.000004\n\n\n227.000003\n\n\n215.000002\n\n\n215.999998\n\n\n208.000001\n\n\n216.999998\n\n\n222.999998\n\n\n242.000002\n\n\n249.000001\n\n\n248.000001\n\n\n243.999999\n\n\n240.999996\n\n\n2.350000e+02\n\n\n2.410000e+02\n\n\n233.999990\n\n\n2.530000e+02\n\n\n1.190000e+02\n\n\n-1.447285e-07\n\n\n1.890000e+02\n\n\n2.550000e+02\n\n\n2.320000e+02\n\n\n2.350000e+02\n\n\n2.340000e+02\n\n\n236.999997\n\n\n235.999997\n\n\n2.340000e+02\n\n\n234.999998\n\n\n238.999998\n\n\n241.999999\n\n\n242.000004\n\n\n241.000002\n\n\n236.999999\n\n\n2.410000e+02\n\n\n243.000004\n\n\n240.999998\n\n\n237.999996\n\n\n239.999997\n\n\n234.999998\n\n\n235.999999\n\n\n237.999996\n\n\n236.999996\n\n\n237.000008\n\n\n2.340000e+02\n\n\n252.999998\n\n\n5.000000e+01\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n2.060000e+02\n\n\n244.000003\n\n\n252.999992\n\n\n254.999998\n\n\n2.550000e+02\n\n\n254.999996\n\n\n2.550000e+02\n\n\n2.500000e+02\n\n\n243.999996\n\n\n238.000001\n\n\n235.000004\n\n\n232.000003\n\n\n230.999998\n\n\n230.000005\n\n\n235.000000\n\n\n238.999996\n\n\n248.999995\n\n\n254.000005\n\n\n254.999995\n\n\n2.550000e+02\n\n\n252.999996\n\n\n248.999999\n\n\n252.999990\n\n\n253.999994\n\n\n1.850000e+02\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n2.500000e+01\n\n\n7.700000e+01\n\n\n78.000000\n\n\n76.000000\n\n\n7.100000e+01\n\n\n7.000000e+01\n\n\n38.999999\n\n\n2.800000e+01\n\n\n1.500000e+01\n\n\n4.000001\n\n\n-0.000004\n\n\n-0.000002\n\n\n0.000002\n\n\n1.999997e+00\n\n\n14.000001\n\n\n51.000002\n\n\n55.000002\n\n\n7.700000e+01\n\n\n108.000000\n\n\n1.340000e+02\n\n\n1.530000e+02\n\n\n1.380000e+02\n\n\n76.999998\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n0.000001\n\n\n-0.000001\n\n\n-4.551377e-07\n\n\n7.378563e-07\n\n\n-0.000002\n\n\n3.950150e-07\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000004\n\n\n-4.699235e-07\n\n\n0.000003\n\n\n-0.000005\n\n\n9.842208e-07\n\n\n0.000005\n\n\n-0.000004\n\n\n-0.000004\n\n\n4.021377e-07\n\n\n0.000002\n\n\n0.000002\n\n\n0.000001\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n0.000001\n\n\n9.323434e-07\n\n\n0.000002\n\n\n8.744090e-07\n\n\n0.000002\n\n\n0.000005\n\n\n0.000003\n\n\n0.000005\n\n\n-0.000001\n\n\n0.000004\n\n\n0.000005\n\n\n8.867231e-07\n\n\n0.000003\n\n\n-0.000003\n\n\n0.000005\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-0.000001\n\n\n3.297868e-07\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n0.000002\n\n\n-0.000001\n\n\n8.809801e-07\n\n\n1.723858e-07\n\n\n0.000001\n\n\n-7.570235e-07\n\n\n0.000001\n\n\n0.000002\n\n\n-0.000001\n\n\n-0.000001\n\n\n8.076585e-07\n\n\n-0.000005\n\n\n-0.000006\n\n\n0.000005\n\n\n-1.852046e-07\n\n\n6.976350e-08\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000003\n\n\n-3.723668e-07\n\n\n-0.000002\n\n\n7.009453e-07\n\n\n-0.000002\n\n\n3.933536e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n0.000002\n\n\n-0.000005\n\n\n-5.581073e-07\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n2.139088e-07\n\n\n0.000002\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n-8.592667e-07\n\n\n0.000002\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n0.000002\n\n\n-0.000002\n\n\n-0.000002\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n3.743793e-07\n\n\n-6.565379e-07\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n-3.593068e-07\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n8\n\n\n\n\n8\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n-2.594887e-09\n\n\n-2.792910e-08\n\n\n-3.333367e-08\n\n\n8.642878e-08\n\n\n2.900000e+01\n\n\n1.570000e+02\n\n\n168.000001\n\n\n1.850000e+02\n\n\n1.820000e+02\n\n\n1.770000e+02\n\n\n1.740000e+02\n\n\n1.690000e+02\n\n\n181.000005\n\n\n1.640000e+02\n\n\n-2.068072e-07\n\n\n-9.467687e-08\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n4.300000e+01\n\n\n1.630000e+02\n\n\n1.880000e+02\n\n\n2.160000e+02\n\n\n255.000010\n\n\n2.550000e+02\n\n\n255.000006\n\n\n250.999996\n\n\n2.540000e+02\n\n\n255.000004\n\n\n255.000004\n\n\n255.000000\n\n\n2.500000e+02\n\n\n255.000007\n\n\n2.550000e+02\n\n\n1.750000e+02\n\n\n1.270000e+02\n\n\n1.600000e+01\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.690000e+02\n\n\n2.500000e+02\n\n\n2.550000e+02\n\n\n2.550000e+02\n\n\n252.999992\n\n\n240.000004\n\n\n234.999995\n\n\n230.999999\n\n\n231.999996\n\n\n228.999997\n\n\n2.300000e+02\n\n\n228.000002\n\n\n230.000004\n\n\n229.000001\n\n\n2.370000e+02\n\n\n245.999994\n\n\n255.000008\n\n\n2.550000e+02\n\n\n2.390000e+02\n\n\n2.110000e+02\n\n\n1.300000e+01\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n8.000000e+00\n\n\n2.160000e+02\n\n\n2.290000e+02\n\n\n2.420000e+02\n\n\n2.430000e+02\n\n\n2.410000e+02\n\n\n224.999998\n\n\n231.000003\n\n\n246.999995\n\n\n240.000002\n\n\n249.000000\n\n\n253.000005\n\n\n236.999995\n\n\n2.300000e+02\n\n\n2.370000e+02\n\n\n241.999999\n\n\n241.999997\n\n\n2.350000e+02\n\n\n2.330000e+02\n\n\n2.270000e+02\n\n\n2.500000e+02\n\n\n1.280000e+02\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n4.000000e+00\n\n\n2.520000e+02\n\n\n2.340000e+02\n\n\n2.300000e+02\n\n\n234.999991\n\n\n225.000000\n\n\n236.000002\n\n\n223.000004\n\n\n237.999998\n\n\n245.000000\n\n\n2.380000e+02\n\n\n2.330000e+02\n\n\n210.000000\n\n\n201.000002\n\n\n229.000004\n\n\n2.260000e+02\n\n\n232.999996\n\n\n2.450000e+02\n\n\n2.300000e+02\n\n\n2.260000e+02\n\n\n2.520000e+02\n\n\n1.080000e+02\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n3.100000e+01\n\n\n249.000004\n\n\n237.000009\n\n\n227.999999\n\n\n230.999999\n\n\n248.999997\n\n\n245.000005\n\n\n2.280000e+02\n\n\n241.000002\n\n\n246.000005\n\n\n230.999996\n\n\n194.999999\n\n\n222.999998\n\n\n254.999996\n\n\n227.000004\n\n\n225.000001\n\n\n2.230000e+02\n\n\n232.999995\n\n\n2.390000e+02\n\n\n1.890000e+02\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n181.000000\n\n\n238.999991\n\n\n204.999995\n\n\n2.160000e+02\n\n\n216.000002\n\n\n2.210000e+02\n\n\n225.000000\n\n\n214.000002\n\n\n225.000000\n\n\n229.999996\n\n\n242.999996\n\n\n238.999999\n\n\n223.999998\n\n\n208.000001\n\n\n206.000000\n\n\n204.000002\n\n\n2.160000e+02\n\n\n252.999997\n\n\n8.919840e-07\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n254.999999\n\n\n219.000005\n\n\n207.000000\n\n\n199.999999\n\n\n205.000005\n\n\n2.040000e+02\n\n\n202.000003\n\n\n2.210000e+02\n\n\n161.000000\n\n\n157.000001\n\n\n220.000003\n\n\n197.000000\n\n\n207.999998\n\n\n207.000001\n\n\n205.999999\n\n\n209.999999\n\n\n203.000005\n\n\n254.999999\n\n\n55.000000\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n5.900000e+01\n\n\n233.000005\n\n\n2.050000e+02\n\n\n206.000005\n\n\n2.040000e+02\n\n\n204.000003\n\n\n206.999997\n\n\n200.000005\n\n\n219.999996\n\n\n173.000000\n\n\n156.000000\n\n\n222.999996\n\n\n204.000002\n\n\n205.999998\n\n\n207.000002\n\n\n209.000001\n\n\n2.060000e+02\n\n\n207.000001\n\n\n233.000003\n\n\n137.000002\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n9.100000e+01\n\n\n2.340000e+02\n\n\n2.080000e+02\n\n\n202.000002\n\n\n207.999999\n\n\n207.999995\n\n\n209.000005\n\n\n206.000004\n\n\n218.999998\n\n\n196.000000\n\n\n1.910000e+02\n\n\n221.999999\n\n\n210.999998\n\n\n211.999999\n\n\n211.999998\n\n\n211.000003\n\n\n209.000002\n\n\n225.999999\n\n\n221.999998\n\n\n199.999996\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n1.210000e+02\n\n\n2.350000e+02\n\n\n226.999995\n\n\n224.999996\n\n\n205.999999\n\n\n211.000003\n\n\n210.000002\n\n\n209.999997\n\n\n220.999996\n\n\n197.000000\n\n\n188.999999\n\n\n224.000000\n\n\n214.000002\n\n\n213.000002\n\n\n214.000001\n\n\n214.000004\n\n\n2.180000e+02\n\n\n237.999999\n\n\n219.999995\n\n\n2.380000e+02\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n1.400000e+02\n\n\n2.390000e+02\n\n\n234.999997\n\n\n215.999995\n\n\n205.999996\n\n\n214.000001\n\n\n211.000005\n\n\n209.999997\n\n\n220.999996\n\n\n196.999998\n\n\n182.000002\n\n\n223.000000\n\n\n214.000001\n\n\n217.000001\n\n\n218.000000\n\n\n216.000005\n\n\n220.000001\n\n\n248.999995\n\n\n215.000003\n\n\n254.999999\n\n\n19.000002\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n182.999996\n\n\n237.000004\n\n\n229.000002\n\n\n211.000002\n\n\n215.999995\n\n\n213.999998\n\n\n212.000000\n\n\n212.000000\n\n\n219.000001\n\n\n201.999999\n\n\n1.810000e+02\n\n\n226.999997\n\n\n213.000001\n\n\n218.000000\n\n\n219.999999\n\n\n216.000002\n\n\n218.999998\n\n\n255.000001\n\n\n207.000001\n\n\n254.999996\n\n\n77.000000\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n221.000003\n\n\n227.000005\n\n\n2.310000e+02\n\n\n212.000006\n\n\n215.000003\n\n\n213.000004\n\n\n212.999998\n\n\n213.000001\n\n\n218.000000\n\n\n209.999998\n\n\n192.999998\n\n\n225.999999\n\n\n212.000000\n\n\n216.000001\n\n\n220.999999\n\n\n221.999999\n\n\n2.120000e+02\n\n\n251.000001\n\n\n211.000003\n\n\n250.999999\n\n\n112.000001\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n243.999994\n\n\n2.300000e+02\n\n\n232.000000\n\n\n212.999997\n\n\n219.999997\n\n\n211.999998\n\n\n214.000000\n\n\n214.000001\n\n\n217.000001\n\n\n2.170000e+02\n\n\n209.000000\n\n\n221.000001\n\n\n213.999999\n\n\n217.000000\n\n\n217.999999\n\n\n224.000000\n\n\n212.000002\n\n\n2.490000e+02\n\n\n2.170000e+02\n\n\n241.999995\n\n\n153.999998\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n236.999994\n\n\n234.000003\n\n\n2.210000e+02\n\n\n214.999999\n\n\n225.999999\n\n\n2.090000e+02\n\n\n215.000002\n\n\n214.000002\n\n\n220.000002\n\n\n205.000002\n\n\n190.000000\n\n\n226.999997\n\n\n212.000001\n\n\n217.999999\n\n\n213.000000\n\n\n219.999997\n\n\n216.999996\n\n\n246.999999\n\n\n2.200000e+02\n\n\n230.999996\n\n\n182.000004\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n2.490000e+02\n\n\n2.270000e+02\n\n\n235.999999\n\n\n217.999998\n\n\n230.999997\n\n\n210.999999\n\n\n216.000002\n\n\n215.000000\n\n\n220.000001\n\n\n209.999998\n\n\n184.000000\n\n\n227.000002\n\n\n214.999999\n\n\n216.999999\n\n\n216.000000\n\n\n2.230000e+02\n\n\n222.000000\n\n\n2.550000e+02\n\n\n225.000005\n\n\n217.000005\n\n\n200.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n3.200000e+01\n\n\n255.000007\n\n\n223.000000\n\n\n241.999997\n\n\n220.999998\n\n\n229.000005\n\n\n215.000002\n\n\n214.999998\n\n\n215.999999\n\n\n218.999999\n\n\n217.000002\n\n\n199.999998\n\n\n2.250000e+02\n\n\n213.999999\n\n\n214.000001\n\n\n214.999999\n\n\n2.270000e+02\n\n\n2.220000e+02\n\n\n2.480000e+02\n\n\n234.000003\n\n\n213.999996\n\n\n2.090000e+02\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.700000e+01\n\n\n254.999993\n\n\n221.999995\n\n\n240.000002\n\n\n221.999997\n\n\n227.000000\n\n\n215.999998\n\n\n216.000001\n\n\n217.000002\n\n\n220.000001\n\n\n215.000002\n\n\n201.000000\n\n\n225.999996\n\n\n213.999999\n\n\n216.999999\n\n\n213.000000\n\n\n222.000002\n\n\n223.999999\n\n\n240.999999\n\n\n233.999996\n\n\n2.190000e+02\n\n\n2.150000e+02\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n8.300000e+01\n\n\n2.550000e+02\n\n\n2.170000e+02\n\n\n236.999997\n\n\n221.000004\n\n\n2.250000e+02\n\n\n213.999999\n\n\n216.999999\n\n\n217.999999\n\n\n221.000000\n\n\n217.000001\n\n\n202.999998\n\n\n2.290000e+02\n\n\n217.999999\n\n\n219.000002\n\n\n218.000001\n\n\n218.000003\n\n\n224.000005\n\n\n234.999994\n\n\n229.999998\n\n\n215.999999\n\n\n220.999998\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n97.999999\n\n\n255.000009\n\n\n217.000004\n\n\n2.370000e+02\n\n\n219.000001\n\n\n2.220000e+02\n\n\n2.140000e+02\n\n\n215.999999\n\n\n217.000002\n\n\n222.000001\n\n\n214.000002\n\n\n196.999999\n\n\n226.000005\n\n\n214.999998\n\n\n215.999999\n\n\n219.999995\n\n\n217.000005\n\n\n217.000004\n\n\n2.370000e+02\n\n\n231.000005\n\n\n216.000003\n\n\n224.000004\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n9.800000e+01\n\n\n2.550000e+02\n\n\n219.000003\n\n\n238.999997\n\n\n2.140000e+02\n\n\n2.190000e+02\n\n\n218.000000\n\n\n2.200000e+02\n\n\n2.190000e+02\n\n\n223.000001\n\n\n216.000002\n\n\n200.000002\n\n\n231.999996\n\n\n2.180000e+02\n\n\n221.000002\n\n\n219.000000\n\n\n219.000004\n\n\n2.080000e+02\n\n\n239.999997\n\n\n2.320000e+02\n\n\n2.160000e+02\n\n\n2.240000e+02\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n1.020000e+02\n\n\n255.000008\n\n\n220.000000\n\n\n2.380000e+02\n\n\n2.120000e+02\n\n\n220.999996\n\n\n2.140000e+02\n\n\n216.999998\n\n\n219.000002\n\n\n224.999995\n\n\n2.170000e+02\n\n\n202.000000\n\n\n228.999995\n\n\n2.170000e+02\n\n\n219.999997\n\n\n219.000000\n\n\n219.000002\n\n\n2.090000e+02\n\n\n233.999996\n\n\n234.000003\n\n\n216.999995\n\n\n2.240000e+02\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n1.090000e+02\n\n\n254.999991\n\n\n2.210000e+02\n\n\n243.999996\n\n\n2.220000e+02\n\n\n215.000000\n\n\n222.000005\n\n\n227.000004\n\n\n226.000004\n\n\n225.999999\n\n\n212.999999\n\n\n201.000003\n\n\n2.210000e+02\n\n\n221.999999\n\n\n224.000005\n\n\n224.000005\n\n\n215.999996\n\n\n215.999996\n\n\n241.999996\n\n\n224.000002\n\n\n2.170000e+02\n\n\n2.200000e+02\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n1.040000e+02\n\n\n2.550000e+02\n\n\n220.000005\n\n\n241.000000\n\n\n2.430000e+02\n\n\n2.550000e+02\n\n\n251.000000\n\n\n2.540000e+02\n\n\n248.999996\n\n\n245.999998\n\n\n227.000003\n\n\n215.999995\n\n\n2.450000e+02\n\n\n249.000004\n\n\n252.000004\n\n\n249.999997\n\n\n2.510000e+02\n\n\n2.440000e+02\n\n\n245.999993\n\n\n224.999996\n\n\n217.000005\n\n\n226.999995\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n9.800000e+01\n\n\n236.999994\n\n\n211.000003\n\n\n244.999999\n\n\n2.050000e+02\n\n\n104.000001\n\n\n9.900000e+01\n\n\n87.000000\n\n\n8.600000e+01\n\n\n82.000001\n\n\n89.000000\n\n\n95.000000\n\n\n71.000000\n\n\n86.000000\n\n\n99.000000\n\n\n1.090000e+02\n\n\n91.000000\n\n\n179.999999\n\n\n251.000002\n\n\n2.210000e+02\n\n\n226.000007\n\n\n2.140000e+02\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n2.350000e+02\n\n\n2.430000e+02\n\n\n246.999997\n\n\n-7.577064e-07\n\n\n-0.000002\n\n\n0.000001\n\n\n0.000003\n\n\n0.000002\n\n\n-0.000002\n\n\n0.000002\n\n\n28.000000\n\n\n0.000001\n\n\n-0.000003\n\n\n0.000001\n\n\n-0.000002\n\n\n-0.000002\n\n\n-0.000001\n\n\n250.999997\n\n\n242.999990\n\n\n255.000007\n\n\n3.700000e+01\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n1.560000e+02\n\n\n2.370000e+02\n\n\n2.210000e+02\n\n\n6.560168e-08\n\n\n2.924545e-07\n\n\n3.999999e+00\n\n\n2.000000e+00\n\n\n1.000002e+00\n\n\n-6.002562e-07\n\n\n9.999983e-01\n\n\n3.000000e+00\n\n\n-0.000002\n\n\n1.999999e+00\n\n\n3.000001e+00\n\n\n0.999999\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n1.710000e+02\n\n\n2.410000e+02\n\n\n2.270000e+02\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n4\n\n\n\n\n9\n\n\n-1.628456e-11\n\n\n2.116344e-10\n\n\n-1.459979e-10\n\n\n2.426657e-09\n\n\n-2.665179e-11\n\n\n1.000000e+00\n\n\n1.000000e+00\n\n\n2.000000e+00\n\n\n8.642878e-08\n\n\n-4.371375e-07\n\n\n3.400000e+01\n\n\n89.999999\n\n\n1.280000e+02\n\n\n1.460000e+02\n\n\n1.560000e+02\n\n\n1.600000e+02\n\n\n1.310000e+02\n\n\n54.000000\n\n\n5.516976e-08\n\n\n-2.068072e-07\n\n\n4.000000e+00\n\n\n9.006170e-08\n\n\n-2.751660e-08\n\n\n-2.676011e-08\n\n\n1.534132e-09\n\n\n3.965276e-09\n\n\n4.171343e-09\n\n\n5.597892e-10\n\n\n-2.459002e-10\n\n\n1.018412e-09\n\n\n4.727461e-09\n\n\n4.497033e-09\n\n\n-4.694904e-09\n\n\n8.493365e-08\n\n\n2.000000e+00\n\n\n1.985495e-07\n\n\n4.999999e+00\n\n\n140.999998\n\n\n1.740000e+02\n\n\n213.999996\n\n\n211.000002\n\n\n2.240000e+02\n\n\n236.999998\n\n\n240.000004\n\n\n225.999999\n\n\n2.270000e+02\n\n\n177.000001\n\n\n-9.767781e-07\n\n\n-6.219337e-07\n\n\n9.999995e-01\n\n\n9.999998e-01\n\n\n-8.071513e-09\n\n\n-1.345314e-08\n\n\n-3.650504e-08\n\n\n3.064484e-09\n\n\n1.395653e-09\n\n\n-5.678459e-10\n\n\n-8.652231e-10\n\n\n6.095701e-09\n\n\n-2.268324e-08\n\n\n1.082306e-07\n\n\n1.000000e+00\n\n\n7.000001e+00\n\n\n3.908779e-07\n\n\n112.000002\n\n\n237.000000\n\n\n219.999999\n\n\n197.999999\n\n\n199.000000\n\n\n191.000000\n\n\n1.910000e+02\n\n\n180.000001\n\n\n205.999997\n\n\n231.000004\n\n\n2.500000e+02\n\n\n124.000000\n\n\n-0.000002\n\n\n5.000000e+00\n\n\n-1.318873e-07\n\n\n-1.672773e-07\n\n\n2.125031e-09\n\n\n-8.742525e-08\n\n\n-1.006425e-08\n\n\n-5.139340e-09\n\n\n2.001768e-09\n\n\n-9.364624e-10\n\n\n-1.518954e-08\n\n\n-4.168295e-08\n\n\n1.493545e-07\n\n\n9.999997e-01\n\n\n5.000001e+00\n\n\n-9.688781e-07\n\n\n2.600000e+01\n\n\n214.000000\n\n\n197.000001\n\n\n198.999997\n\n\n247.000001\n\n\n246.999999\n\n\n244.999999\n\n\n239.000005\n\n\n2.090000e+02\n\n\n2.010000e+02\n\n\n241.999999\n\n\n111.999999\n\n\n5.257434e-07\n\n\n5.000000e+00\n\n\n-1.805364e-07\n\n\n4.216997e-07\n\n\n-1.558985e-07\n\n\n-1.001893e-07\n\n\n-4.949160e-09\n\n\n1.981593e-08\n\n\n2.329049e-09\n\n\n-5.676910e-09\n\n\n-1.800060e-08\n\n\n-3.427830e-08\n\n\n-5.326159e-07\n\n\n9.999991e-01\n\n\n6.555034e-07\n\n\n-0.000001\n\n\n-0.000002\n\n\n0.000002\n\n\n199.000006\n\n\n217.000005\n\n\n247.999998\n\n\n2.120000e+02\n\n\n2.080000e+02\n\n\n248.999998\n\n\n225.000000\n\n\n177.999999\n\n\n6.400000e+01\n\n\n-0.000002\n\n\n5.942894e-07\n\n\n9.570876e-07\n\n\n7.753735e-07\n\n\n-8.642634e-07\n\n\n-4.548926e-07\n\n\n1.847391e-08\n\n\n-1.021340e-08\n\n\n1.077761e-08\n\n\n-2.561862e-09\n\n\n-4.695927e-08\n\n\n4.582200e-08\n\n\n-3.727119e-08\n\n\n1.100016e-07\n\n\n0.000002\n\n\n1.000002\n\n\n0.000001\n\n\n4.999999\n\n\n125.000001\n\n\n215.000005\n\n\n1.550000e+02\n\n\n219.000005\n\n\n235.999999\n\n\n242.999996\n\n\n255.000001\n\n\n188.000001\n\n\n161.000001\n\n\n148.000001\n\n\n91.000000\n\n\n2.100000e+01\n\n\n-0.000002\n\n\n9.999988e-01\n\n\n5.144796e-07\n\n\n-1.955981e-07\n\n\n1.345243e-07\n\n\n1.621610e-07\n\n\n1.430707e-08\n\n\n-4.618929e-09\n\n\n4.971934e-09\n\n\n6.497929e-08\n\n\n2.643760e-07\n\n\n-2.410009e-07\n\n\n2.999998\n\n\n0.000002\n\n\n30.000000\n\n\n1.950000e+02\n\n\n175.999999\n\n\n1.800000e+02\n\n\n205.999999\n\n\n199.999998\n\n\n227.000004\n\n\n253.999999\n\n\n185.000002\n\n\n168.000000\n\n\n138.000000\n\n\n181.000001\n\n\n200.000000\n\n\n204.000002\n\n\n3.800000e+01\n\n\n-0.000002\n\n\n2.999998e+00\n\n\n-6.656718e-07\n\n\n-5.450623e-07\n\n\n-4.032686e-07\n\n\n-1.347659e-08\n\n\n-2.890140e-08\n\n\n1.131168e-07\n\n\n-4.175154e-08\n\n\n-5.971974e-07\n\n\n3.715096e-07\n\n\n0.000001\n\n\n0.000001\n\n\n170.999996\n\n\n159.000002\n\n\n144.000002\n\n\n1.490000e+02\n\n\n155.000000\n\n\n1.530000e+02\n\n\n190.000003\n\n\n172.999999\n\n\n135.000001\n\n\n159.000000\n\n\n153.000000\n\n\n146.000000\n\n\n145.999999\n\n\n155.999999\n\n\n140.000002\n\n\n0.000002\n\n\n0.000001\n\n\n-7.762443e-07\n\n\n-6.404821e-07\n\n\n-6.205741e-07\n\n\n-9.954905e-08\n\n\n2.976238e-08\n\n\n-9.164984e-08\n\n\n-1.933697e-07\n\n\n-3.981918e-07\n\n\n4.334051e-07\n\n\n-0.000002\n\n\n1.100000e+01\n\n\n166.000000\n\n\n1.530000e+02\n\n\n156.999999\n\n\n159.000001\n\n\n152.000001\n\n\n137.000001\n\n\n178.999998\n\n\n178.000002\n\n\n143.000000\n\n\n162.000001\n\n\n152.000000\n\n\n159.000000\n\n\n158.000001\n\n\n1.520000e+02\n\n\n165.000002\n\n\n40.000002\n\n\n0.000001\n\n\n0.000002\n\n\n8.170825e-07\n\n\n-8.944622e-08\n\n\n9.377016e-08\n\n\n-2.152347e-08\n\n\n-9.805354e-08\n\n\n1.100760e-07\n\n\n-1.284166e-07\n\n\n-8.785177e-07\n\n\n-5.893633e-07\n\n\n5.400000e+01\n\n\n180.000005\n\n\n161.999997\n\n\n153.000000\n\n\n155.000002\n\n\n157.999998\n\n\n145.000000\n\n\n174.999998\n\n\n1.750000e+02\n\n\n143.000000\n\n\n157.000000\n\n\n153.000000\n\n\n152.000000\n\n\n155.000000\n\n\n161.000001\n\n\n168.000001\n\n\n55.000000\n\n\n0.000002\n\n\n-1.907800e-08\n\n\n-6.678754e-07\n\n\n3.626715e-07\n\n\n7.653776e-08\n\n\n3.391509e-08\n\n\n-1.837763e-07\n\n\n-1.002178e-07\n\n\n9.562319e-08\n\n\n5.449854e-07\n\n\n6.690875e-07\n\n\n94.000000\n\n\n184.000000\n\n\n170.000002\n\n\n162.999998\n\n\n152.000001\n\n\n156.000001\n\n\n144.000001\n\n\n166.999999\n\n\n179.999998\n\n\n145.000000\n\n\n157.000000\n\n\n153.000000\n\n\n147.000000\n\n\n178.999998\n\n\n1.660000e+02\n\n\n171.999999\n\n\n116.000000\n\n\n1.135173e-07\n\n\n-0.000001\n\n\n0.000001\n\n\n-7.649599e-07\n\n\n1.421396e-07\n\n\n-3.134657e-08\n\n\n-1.488990e-07\n\n\n-2.217518e-07\n\n\n-7.524475e-07\n\n\n-8.247956e-07\n\n\n2.124748e-07\n\n\n131.999997\n\n\n183.999999\n\n\n181.999997\n\n\n177.000001\n\n\n164.999999\n\n\n155.000000\n\n\n150.000001\n\n\n164.000001\n\n\n186.000000\n\n\n146.000000\n\n\n158.000000\n\n\n157.000000\n\n\n145.000000\n\n\n172.999999\n\n\n169.000001\n\n\n173.999998\n\n\n180.000001\n\n\n0.000002\n\n\n-0.000001\n\n\n-2.314611e-07\n\n\n-0.000002\n\n\n3.276677e-07\n\n\n-1.048992e-07\n\n\n-9.582211e-08\n\n\n-1.728624e-07\n\n\n4.348013e-07\n\n\n0.000001\n\n\n0.000002\n\n\n158.000000\n\n\n179.999998\n\n\n196.999997\n\n\n198.999996\n\n\n178.000001\n\n\n158.999999\n\n\n151.000000\n\n\n161.000001\n\n\n1.880000e+02\n\n\n146.000000\n\n\n157.000000\n\n\n157.000000\n\n\n157.000000\n\n\n187.000000\n\n\n169.000000\n\n\n169.999997\n\n\n215.999999\n\n\n0.000003\n\n\n-0.000002\n\n\n-0.000002\n\n\n0.000001\n\n\n-1.099796e-08\n\n\n-1.249524e-07\n\n\n8.047429e-10\n\n\n1.957992e-07\n\n\n-5.361049e-07\n\n\n0.000001\n\n\n0.000002\n\n\n1.710000e+02\n\n\n172.999998\n\n\n196.000004\n\n\n216.999997\n\n\n183.000002\n\n\n165.000001\n\n\n152.000000\n\n\n160.999999\n\n\n188.999998\n\n\n148.000000\n\n\n159.000000\n\n\n158.000000\n\n\n162.000000\n\n\n194.999999\n\n\n1.680000e+02\n\n\n164.000000\n\n\n212.000003\n\n\n14.999999\n\n\n0.000002\n\n\n3.101954e-07\n\n\n-0.000002\n\n\n-2.671779e-07\n\n\n5.450883e-08\n\n\n-1.799488e-07\n\n\n-2.672337e-07\n\n\n3.815783e-07\n\n\n0.000002\n\n\n-8.765240e-07\n\n\n185.000004\n\n\n167.999998\n\n\n188.999997\n\n\n164.999999\n\n\n186.999999\n\n\n160.000000\n\n\n154.000000\n\n\n1.560000e+02\n\n\n192.999999\n\n\n148.000000\n\n\n159.000000\n\n\n156.000000\n\n\n162.000000\n\n\n176.999999\n\n\n173.000002\n\n\n1.640000e+02\n\n\n1.920000e+02\n\n\n46.000002\n\n\n0.000002\n\n\n3.281200e-07\n\n\n-0.000002\n\n\n-1.307099e-07\n\n\n3.196277e-07\n\n\n5.418402e-08\n\n\n-2.917852e-07\n\n\n7.308653e-07\n\n\n-0.000002\n\n\n13.000001\n\n\n1.930000e+02\n\n\n167.999998\n\n\n184.000000\n\n\n1.170000e+02\n\n\n191.000002\n\n\n157.000000\n\n\n158.000000\n\n\n156.000000\n\n\n194.999998\n\n\n149.000000\n\n\n163.000000\n\n\n150.000000\n\n\n160.000000\n\n\n124.000001\n\n\n169.000001\n\n\n165.999999\n\n\n1.870000e+02\n\n\n70.000001\n\n\n0.000002\n\n\n3.892250e-07\n\n\n6.972123e-07\n\n\n-2.291508e-07\n\n\n-2.135472e-07\n\n\n-4.984975e-07\n\n\n-8.258425e-07\n\n\n0.000001\n\n\n-5.332055e-07\n\n\n3.300000e+01\n\n\n191.999997\n\n\n166.999997\n\n\n195.000000\n\n\n90.000001\n\n\n185.999999\n\n\n156.000000\n\n\n158.000000\n\n\n157.000000\n\n\n193.999999\n\n\n151.000000\n\n\n166.000000\n\n\n153.000000\n\n\n174.000000\n\n\n9.300000e+01\n\n\n158.000000\n\n\n1.880000e+02\n\n\n184.000000\n\n\n96.000000\n\n\n-0.000002\n\n\n0.000001\n\n\n-7.986391e-07\n\n\n-4.286650e-07\n\n\n-1.681014e-07\n\n\n6.363970e-07\n\n\n-6.975870e-07\n\n\n-5.423990e-07\n\n\n0.000001\n\n\n56.000001\n\n\n190.999996\n\n\n168.000003\n\n\n203.999996\n\n\n126.000000\n\n\n172.999999\n\n\n161.000000\n\n\n161.000000\n\n\n155.000000\n\n\n193.000001\n\n\n1.550000e+02\n\n\n163.000000\n\n\n156.000000\n\n\n170.000000\n\n\n1.090000e+02\n\n\n1.520000e+02\n\n\n2.060000e+02\n\n\n180.000000\n\n\n114.999999\n\n\n5.651978e-07\n\n\n-5.163183e-07\n\n\n0.000002\n\n\n-1.174552e-07\n\n\n-5.947502e-08\n\n\n-2.880678e-07\n\n\n-0.000001\n\n\n5.352320e-07\n\n\n0.000002\n\n\n77.000000\n\n\n189.000001\n\n\n173.000001\n\n\n208.000002\n\n\n157.000000\n\n\n174.000001\n\n\n162.000000\n\n\n163.000000\n\n\n154.000000\n\n\n192.999999\n\n\n157.000000\n\n\n163.000000\n\n\n159.000000\n\n\n172.000000\n\n\n130.000000\n\n\n130.000000\n\n\n194.999999\n\n\n177.000003\n\n\n1.270000e+02\n\n\n-3.174695e-07\n\n\n-0.000002\n\n\n-5.629809e-07\n\n\n-3.012789e-07\n\n\n-1.447285e-07\n\n\n-6.046403e-07\n\n\n1.118729e-07\n\n\n-9.513843e-07\n\n\n-2.911069e-07\n\n\n8.100000e+01\n\n\n186.000001\n\n\n176.000003\n\n\n1.970000e+02\n\n\n148.000000\n\n\n174.000000\n\n\n162.999999\n\n\n164.000000\n\n\n156.000000\n\n\n194.999999\n\n\n1.530000e+02\n\n\n164.000000\n\n\n161.000000\n\n\n161.000000\n\n\n155.000001\n\n\n87.999999\n\n\n192.999999\n\n\n176.000001\n\n\n139.000002\n\n\n-0.000002\n\n\n9.265024e-07\n\n\n0.000002\n\n\n4.011770e-07\n\n\n1.363091e-07\n\n\n3.289930e-07\n\n\n-2.624320e-07\n\n\n-0.000002\n\n\n-0.000002\n\n\n90.000000\n\n\n1.830000e+02\n\n\n198.999996\n\n\n1.550000e+02\n\n\n1.290000e+02\n\n\n183.000000\n\n\n161.000000\n\n\n165.000000\n\n\n156.000000\n\n\n195.000000\n\n\n155.000000\n\n\n159.000000\n\n\n164.000001\n\n\n158.999999\n\n\n166.999998\n\n\n100.000000\n\n\n1.940000e+02\n\n\n175.000002\n\n\n149.999999\n\n\n-0.000002\n\n\n-0.000002\n\n\n7.528400e-07\n\n\n-1.202886e-08\n\n\n-1.592361e-07\n\n\n5.095788e-07\n\n\n-4.052591e-08\n\n\n-2.873870e-07\n\n\n5.500636e-07\n\n\n103.000001\n\n\n181.000005\n\n\n2.090000e+02\n\n\n1.010000e+02\n\n\n147.000000\n\n\n1.780000e+02\n\n\n1.630000e+02\n\n\n165.000000\n\n\n155.000000\n\n\n197.000001\n\n\n162.000000\n\n\n1.570000e+02\n\n\n165.000001\n\n\n161.000001\n\n\n161.000002\n\n\n1.520000e+02\n\n\n191.000005\n\n\n1.700000e+02\n\n\n1.600000e+02\n\n\n-1.849482e-07\n\n\n0.000002\n\n\n2.427885e-08\n\n\n-2.188508e-07\n\n\n1.379810e-07\n\n\n-1.391677e-07\n\n\n1.906360e-07\n\n\n4.654391e-07\n\n\n0.000001\n\n\n113.000001\n\n\n1.810000e+02\n\n\n2.100000e+02\n\n\n79.000001\n\n\n1.720000e+02\n\n\n173.000000\n\n\n166.000000\n\n\n169.000001\n\n\n1.570000e+02\n\n\n197.999999\n\n\n167.000000\n\n\n1.570000e+02\n\n\n161.999999\n\n\n161.999998\n\n\n157.000002\n\n\n1.520000e+02\n\n\n184.000001\n\n\n173.000003\n\n\n156.000003\n\n\n7.859119e-07\n\n\n2.466921e-07\n\n\n1.444405e-07\n\n\n5.295760e-08\n\n\n-1.867194e-08\n\n\n-2.868018e-07\n\n\n-7.280360e-07\n\n\n-6.257008e-07\n\n\n0.000001\n\n\n1.320000e+02\n\n\n179.000002\n\n\n2.110000e+02\n\n\n101.000000\n\n\n180.000001\n\n\n170.000001\n\n\n167.000000\n\n\n171.000001\n\n\n158.000001\n\n\n193.000000\n\n\n1.720000e+02\n\n\n163.000001\n\n\n155.000000\n\n\n165.000000\n\n\n159.999998\n\n\n150.000003\n\n\n165.000004\n\n\n181.000002\n\n\n1.570000e+02\n\n\n7.765040e-07\n\n\n-1.484936e-08\n\n\n5.969346e-07\n\n\n4.756745e-08\n\n\n-9.525822e-08\n\n\n-1.043257e-07\n\n\n-3.057516e-08\n\n\n-7.851076e-07\n\n\n-8.448512e-07\n\n\n148.000005\n\n\n180.000002\n\n\n2.020000e+02\n\n\n1.130000e+02\n\n\n179.000001\n\n\n1.660000e+02\n\n\n164.999998\n\n\n166.000001\n\n\n161.999999\n\n\n180.999998\n\n\n1.640000e+02\n\n\n162.999997\n\n\n160.000002\n\n\n157.000003\n\n\n1.560000e+02\n\n\n1.540000e+02\n\n\n162.000005\n\n\n169.999998\n\n\n156.000000\n\n\n0.000002\n\n\n5.335659e-07\n\n\n-5.266864e-07\n\n\n8.280237e-08\n\n\n3.033599e-08\n\n\n5.283691e-08\n\n\n1.380712e-07\n\n\n1.159922e-07\n\n\n-0.000001\n\n\n150.999996\n\n\n182.000001\n\n\n1.810000e+02\n\n\n136.000000\n\n\n2.090000e+02\n\n\n177.999998\n\n\n1.770000e+02\n\n\n177.999999\n\n\n171.999998\n\n\n193.999998\n\n\n176.999999\n\n\n168.000000\n\n\n171.000002\n\n\n1.690000e+02\n\n\n174.000004\n\n\n167.000003\n\n\n160.000002\n\n\n1.870000e+02\n\n\n166.999997\n\n\n-5.421757e-07\n\n\n6.163737e-07\n\n\n-5.578727e-08\n\n\n4.793912e-09\n\n\n-4.510589e-09\n\n\n-9.451520e-09\n\n\n-1.198971e-07\n\n\n-2.364927e-08\n\n\n-9.384266e-07\n\n\n1.520000e+02\n\n\n186.999998\n\n\n1.790000e+02\n\n\n6.000001\n\n\n92.000001\n\n\n108.000001\n\n\n119.000000\n\n\n119.000001\n\n\n117.000001\n\n\n154.999999\n\n\n147.999997\n\n\n140.000001\n\n\n144.000000\n\n\n130.000002\n\n\n77.999999\n\n\n-0.000001\n\n\n38.000000\n\n\n209.999999\n\n\n146.000004\n\n\n4.965935e-07\n\n\n-2.344399e-07\n\n\n-1.488622e-08\n\n\n1.608706e-08\n\n\n-3.893886e-11\n\n\n-3.305654e-09\n\n\n-5.438611e-09\n\n\n-4.320514e-08\n\n\n-2.151095e-07\n\n\n1.660000e+02\n\n\n2.090000e+02\n\n\n1.280000e+02\n\n\n2.924545e-07\n\n\n-2.304249e-08\n\n\n9.522988e-07\n\n\n-3.426212e-07\n\n\n-6.002562e-07\n\n\n-3.102456e-07\n\n\n8.776177e-07\n\n\n-0.000002\n\n\n6.155732e-07\n\n\n-3.593068e-07\n\n\n0.000002\n\n\n3.771737e-07\n\n\n5.207580e-07\n\n\n-1.732680e-07\n\n\n-6.984834e-07\n\n\n-2.398092e-07\n\n\n2.374313e-07\n\n\n-4.985816e-08\n\n\n-2.682765e-10\n\n\n-2.551856e-09\n\n\n4\n\n\n\n\n\nBehind the scenes this also splits our data into a training dataset that we use to update the model parameters, and a validation dataset that we use to evaluate the model. This is really important because neural networks are incredibly flexible and can often memorise the training data; the validation dataset is an exam with questions the model has never seen before.\nWe can access the individual dataloaders with .train and .valid respectively.\ndls.train, dls.valid\n(<fastai.tabular.core.TabDataLoader at 0x7f4c0e502d10>,\n <fastai.tabular.core.TabDataLoader at 0x7f4c0e502e90>)\nThese can be iterated on to get batches of examples to train or evaluate the model on.\nThis particular dataloader returns a tuple containing 3 items\nbatch = next(iter(dls.train))\ntype(batch), len(batch)\n(tuple, 3)\nThe first is an empty array. This would contain any categorical variables in our model, but since we are only using the continuous pixel values it’s empty.\nbatch[0]\ntensor([], size=(4096, 0), dtype=torch.int64)\nThe second is a 4096x784 array of numbers. These correspond to 4096 of the rows from the initial training data.\nprint(batch[1].shape)\nbatch[1]\ntorch.Size([4096, 784])\n\n\n\n\n\ntensor([[-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        ...,\n        [-0.0104, -0.0225, -0.0271,  ...,  3.5075,  8.2715, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321]])\nWe can see the image has been slightly whitened by the normalization. This is because we normalized each pixel column independently; we may get better results if the normalize them all together. But you can still tell it’s some kind of top.\nplt.imshow(batch[1][0].reshape(28, 28), cmap='Greys')\n<matplotlib.image.AxesImage at 0x7f4c0e45fbd0>\n\n\n\npng\n\n\nThe final part of the batch is the labels from 0-9 corresponding to the each row; what we are trying to predict.\nprint(batch[2].shape)\nbatch[2]\ntorch.Size([4096, 1])\n\n\n\n\n\ntensor([[2],\n        [9],\n        [5],\n        ...,\n        [7],\n        [1],\n        [9]], dtype=torch.int8)\nApparently the image above is a shirt (and not a pullover or t-shirt/top).\nlabels[str(batch[2][0][0].item())]\n'Pullover'\nWe can iterate through the batches to see we have about 4500 labels from each category in the training dataloader\nfrom collections import Counter\n\ntrain_label_count = Counter()\nfor batch in dls.train:\n    train_label_count.update(batch[2].squeeze().numpy())\n\ntrain_label_count\nCounter({1: 4507,\n         9: 4567,\n         3: 4449,\n         7: 4554,\n         6: 4517,\n         0: 4447,\n         4: 4486,\n         5: 4540,\n         2: 4496,\n         8: 4493})\nSimilarly the validation data contains around 1200 rows each.\nvalid_label_count = Counter()\nfor batch in dls.valid:\n    valid_label_count.update(batch[2].squeeze().numpy())\n\nvalid_label_count\nCounter({9: 1137,\n         5: 1171,\n         0: 1271,\n         4: 1205,\n         3: 1231,\n         1: 1213,\n         7: 1157,\n         2: 1217,\n         8: 1210,\n         6: 1188})\n20% of the data has gone into the validation set, but only a little over 75% is in the validation set, we’ve dropped around 5% of the data.\nn_valid = sum(valid_label_count.values())\nn_train = sum(train_label_count.values())\n\n{'n_train': n_train,\n 'pct_train': '{:.2%}'.format(n_train / len(df)),\n 'n_valid': n_valid,\n 'pct_valid': '{:.2%}'.format(n_valid / len(df))}\n{'n_train': 45056,\n 'pct_train': '75.09%',\n 'n_valid': 12000,\n 'pct_valid': '20.00%'}\nThe reason for this is fastai has made all the batches equal length by dropping the extra examples.\nn_train / 4096\n11.0\n\n\n5. Learner\nNow we have our dataloaders to load the data for training and validation we need a way to learn from the data. The fastai learner contains all the things we need to do that:\n\nthe dataloaders\na model consisting of an architecture and parameters, which can make output predictions from inputs\nany metrics for quantitatively evaluating the system\na loss function for automatically evaluating the quality of output predictions against labels\nan optimiser for updating the parameters to minimise the loss function\n\nWe do all this with a tabular_learner, we specify:\n\ndls: the dataloaders\nlayers: The hidden layers that define the architecutre of the model, we use a single layer of dimension 100\nopt_fun: The optimiser to use for updating parameters, here Stochastic Gradient Descent\nmetrics: human interpretable metrics; accuracy is the proportion of labels the model correctly guesses\nconfig: model configuration; here we are turning off BatchNorm which is a technique to help train Deep Neural Networks. As we’re trying to keep the model simple we leave them off.\n\nlearn = tabular_learner(dls, layers=[100], opt_func=SGD, metrics=accuracy, config=dict(use_bn=False, bn_cont=False))\nLet’s step through the parts of the learner"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#dataloaders",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#dataloaders",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "5.1 Dataloaders",
    "text": "5.1 Dataloaders\nWe access the dataloaders using .dls, and can use them just as before\nlearn.dls\n<fastai.tabular.data.TabularDataLoaders at 0x7f4c0e5766d0>\nbatch = next(iter(learn.dls.valid))\ntuple(x.shape for x in batch)\n(torch.Size([4096, 0]), torch.Size([4096, 784]), torch.Size([4096, 1]))"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#model",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#model",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "5.2 Model",
    "text": "5.2 Model\nThe model can take our input data and make predictions.\nThe get_preds function returns the model predictions and input labels from a dataloader (the validation dataloader by default).\nprobs, actual = learn.get_preds()\n\nThe probs is a bunch of numbers corresponding to the probability the image of the corresponding class\nprint(probs.shape)\nprobs\ntorch.Size([12000, 10])\n\n\n\n\n\ntensor([[0.1305, 0.0419, 0.0984,  ..., 0.1273, 0.0680, 0.0738],\n        [0.1179, 0.0815, 0.1189,  ..., 0.0941, 0.0969, 0.1004],\n        [0.1168, 0.0627, 0.1048,  ..., 0.0942, 0.1132, 0.0914],\n        ...,\n        [0.1110, 0.0944, 0.0760,  ..., 0.1031, 0.1014, 0.0918],\n        [0.1116, 0.0645, 0.0845,  ..., 0.0651, 0.1352, 0.0923],\n        [0.0975, 0.0764, 0.1297,  ..., 0.0754, 0.1255, 0.0889]])\nThe probabilities sum to 1\nprobs.sum(axis=1)\ntensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\nThe actual categories from the validation data is the second argument.\nprint(actual.shape)\nactual\ntorch.Size([12000, 1])\n\n\n\n\n\ntensor([[9],\n        [5],\n        [0],\n        ...,\n        [8],\n        [0],\n        [2]], dtype=torch.int8)\nWe can check that the actuals match the labels from the first validation batch\nassert (actual[:len(batch[2])] == batch[2]).all().item()\nThe predictions come from the underlying model running a batch at a time\nbatch_pred = learn.model(batch[0], batch[1])\nbatch_pred\ntensor([[ 0.3774, -0.7596,  0.0953,  ...,  0.3521, -0.2752, -0.1932],\n        [ 0.1231, -0.2459,  0.1313,  ..., -0.1020, -0.0729, -0.0379],\n        [ 0.1825, -0.4396,  0.0746,  ..., -0.0320,  0.1516, -0.0628],\n        ...,\n        [ 0.1052, -0.6275,  0.0955,  ..., -0.0851,  0.1674, -0.1369],\n        [ 0.1469, -0.3495, -0.2195,  ..., -0.2075,  0.0509, -0.0478],\n        [-0.2378, -0.6591, -0.1431,  ..., -0.0598,  0.1025,  0.2491]],\n       grad_fn=<AddmmBackward0>)\nYou might notice these aren’t probabilities; some of them are negative.\nThere’s a trick to make numbers into probabilities, called the softmax function.\nbatch_probs = F.softmax(batch_pred, dim=1)\nbatch_probs\ntensor([[0.1305, 0.0419, 0.0984,  ..., 0.1273, 0.0680, 0.0738],\n        [0.1179, 0.0815, 0.1189,  ..., 0.0941, 0.0969, 0.1004],\n        [0.1168, 0.0627, 0.1048,  ..., 0.0942, 0.1132, 0.0914],\n        ...,\n        [0.1077, 0.0518, 0.1067,  ..., 0.0891, 0.1146, 0.0846],\n        [0.1224, 0.0745, 0.0849,  ..., 0.0859, 0.1112, 0.1007],\n        [0.0739, 0.0485, 0.0813,  ..., 0.0883, 0.1039, 0.1203]],\n       grad_fn=<SoftmaxBackward0>)\nThese give exactly the same predictions for the batch as before\nassert (probs[:len(batch_pred)] == batch_probs).all().item()\nWe can look at the underlying model architecture.\nIgnore (embeds) and (emb_drop); the main part of the model is the (layers) consisting of a Sequential containing:\n\nLinear model that takes 28x28=784 features in, and output 100 features\nReLU (which just means “set all negative values to 0”)\nLinear model that takes in 100 features and outputs 10 features\n\nThat is it’s just two linear functions with a “set negative values to 0” in between!\nlearn.model\nTabularModel(\n  (embeds): ModuleList()\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=784, out_features=100, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=10, bias=True)\n    )\n  )\n)\nWe can also look at the underlying parameters from the model:\n\n100 x 784 parameters for the first linear function\n100 parameters for the first bias\n10 x 100 parameters for the second linear function\n10 parameters for the second bias\n\n[x.shape for x in learn.parameters()]\n[torch.Size([100, 784]),\n torch.Size([100]),\n torch.Size([10, 100]),\n torch.Size([10])]\n\n5.3 Metrics\nThe metrics are the human interpretable quantitative measures of the model; in this case we just used the accuracy.\nWe can get the loss and any metrics we passed in by calling learn.validate().\nThe accuracy should be close to 10% because we have a randomly initialised model with 10 equally likely categories.\nlearn.validate()\n\n(#2) [2.344938278198242,0.08091666549444199]\nWe can list out all the metrics\nlearn.metrics\n(#1) [<fastai.learner.AvgMetric object at 0x7f4c0e492490>]\nWe can get the name of each metric\nlearn.metrics[0].name\n'accuracy'\nand call it on our predictions to get the accuracy\nlearn.metrics[0].func(probs, actual)\nTensorBase(0.0809)\nOur actual predictions are the categories with the highest probability\npreds = probs.argmax(axis=1)\npreds\ntensor([4, 2, 0,  ..., 5, 8, 2])\nThen the accuracy is just the proportion of predictions that are the same as the actuals\nsum(preds == actual.flatten()) / len(preds)\ntensor(0.0809)\n\n\n5.4 Loss\nAccuracy is a good easy to understand metric, but it’s hard to optimise. The accuracy only changes when the order of the probabilities change. A small change in probabilities won’t change accuracy most of the time so it’s hard to tell which direction to move the parameters to make it better.\nInstead for multicategory classification we use something called CrossEntropyLoss\nlearn.loss_func\nFlattenedLoss of CrossEntropyLoss()\nWe can evaluate it on a single batch by passing the model predictions (not the probabilities) and the labels\nlearn.loss_func(batch_pred, batch[2])\nTensorBase(2.3452, grad_fn=<AliasBackward0>)\nWhat is CrossEntropyLoss?\n\nFind the probability of each actual category\nTake the negative logarithm of each\nAverage over all entries\n\nSince the logarithm is bigger the bigger the input (in mathematical jargon it’s strictly monotonic) the higher the probability for the correct class the lower the CrossEntropyLoss. If we bump up the probability for the correct class by x for all predictions, then the loss decreases by -log(x).\nactual_probs = torch.tensor([prob[idx] for prob, idx in zip(batch_probs, batch[2].flatten())])\n-actual_probs.log().mean()\ntensor(2.3452)\nHere’s a way to do this with just indexing:\n\npass torch.arange(len(batch_probs)), this generates the list [0, 1, 2, ..., N]\npass the label index as a long [0, 0, ... 9]\n\nThis will extract the pairs of row 0 to N, and the corresponding label column.\nThis is faster and PyTorch knows how to differentiate it.\nactual_probs = batch_probs[torch.arange(len(batch_probs)), batch[2].flatten().long()]\nloss = -actual_probs.log().mean()\nloss\ntensor(2.3452, grad_fn=<NegBackward0>)\n\n\n5.5 Optimizer\nOnce we have a loss we need a way to update the model parameters in a way that decreases the loss; we call this component an optimizer.\nThis isn’t automatically created so we create it using create_opt\nlearn.opt_func\n<function fastai.optimizer.SGD(params, lr, mom=0.0, wd=0.0, decouple_wd=True)>\nlearn.opt = learn.opt_func(learn.parameters(), lr=0.1)\nLet’s create a copy of the old parameters for reference\nold_params = [p.detach().numpy().copy() for p in learn.parameters()]\nWe want to move the parameters in the direction that decreases the loss. To do this we call backward to fill in all the derivatives with respect to the parameters\n*x, y = next(iter(dls.valid))\npreds = learn.model(*x)\nloss = learn.loss_func(preds, y)\nloss\nTensorBase(2.3452, grad_fn=<AliasBackward0>)\nwith torch.no_grad():\n    loss.backward()\n    learn.opt.step()\n    learn.zero_grad()\nnew_params = [p.detach().numpy() for p in learn.parameters()]\nAnd the weights have moved!\nold_params[-1] - new_params[-1]\narray([-0.00016104, -0.00301814,  0.00073723,  0.0009996 ,  0.00075129,\n        0.00130542, -0.00203079, -0.00050665,  0.00146017,  0.00046292],\n      dtype=float32)\nAnd the loss on the batch has decreased\npreds = learn.model(*x)\nloss = learn.loss_func(preds, y)\nloss\nTensorBase(2.0883, grad_fn=<AliasBackward0>)\n\n\n6. Fit\nThe fit function just runs the training loop above. In each epoch for each batch in the training dataloader it:\n\nevaluates the model on the inputs\ncalculates the loss against the outputs\nupdates the parameters with the optimizer to reduce the loss\n\nThen at the end of each epoch it reports the metrics on the validation set (as well as the losses).\nThe fit argument takes two parameters:\n\nn_epoch: Number of times to run throgh the training data\nlr: The learning rate to use in the optimizer\n\nlearn.fit(n_epoch=4, lr=0.1)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.391490\n\n\n0.972408\n\n\n0.700417\n\n\n00:01\n\n\n\n\n1\n\n\n1.085862\n\n\n0.760498\n\n\n0.741750\n\n\n00:01\n\n\n\n\n2\n\n\n0.928742\n\n\n0.673375\n\n\n0.765333\n\n\n00:01\n\n\n\n\n3\n\n\n0.828868\n\n\n0.620714\n\n\n0.781417\n\n\n00:01"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#imports",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#imports",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "1. Imports",
    "text": "1. Imports\nThis time we’ll only use three fundamental things from fastai: the Learner, the SGD optimizer and the DataLoaders object\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch import tensor\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom fastai.data.core import DataLoaders\nfrom fastai.learner import Learner\nfrom fastai.optimizer import SGD"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#load-data",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#load-data",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "2. Load Data",
    "text": "2. Load Data\nWe’ll do this with Pandas as before, but this time we won’t worry about converting the label into a categorical datatype.\ndf = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\ndf_test = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')\ndf\n\n\n\n\n\n\n\n\n\nlabel\n\n\npixel1\n\n\npixel2\n\n\npixel3\n\n\npixel4\n\n\npixel5\n\n\npixel6\n\n\npixel7\n\n\npixel8\n\n\npixel9\n\n\n…\n\n\npixel775\n\n\npixel776\n\n\npixel777\n\n\npixel778\n\n\npixel779\n\n\npixel780\n\n\npixel781\n\n\npixel782\n\n\npixel783\n\n\npixel784\n\n\n\n\n\n\n0\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n1\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n6\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n5\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n30\n\n\n43\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n2\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n59995\n\n\n9\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59996\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n73\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59997\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n160\n\n\n162\n\n\n163\n\n\n135\n\n\n94\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59998\n\n\n8\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n59999\n\n\n7\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n…\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n60000 rows × 785 columns\n\n\n\n3. Data Loaders\nPreviously we ran\ndls = TabularDataLoaders.from_df(df, y_names='label', bs=4096, procs=[Normalize])\nWe will do the steps manually:\n\ncreate a random validation split\ncreate training and validation datasets\nwrap these datasets in dataloaders with batchsize of 4096\nnormalize the data\n\nvalid_pct = 0.2\n\nvalid_mask = np.random.choice([True, False], len(df), p=(valid_pct, 1-valid_pct))\nvalid_mask\narray([False, False, False, ..., False, False, False])\nnp.mean(valid_mask)\n0.20161666666666667\nWe can create Datasets containing the pairs of (image, label) for each of the train, validation and test splits.\nWe normalize the pixels to be between 0 and 1. (This is slightly different to Normalize which performs a linear transform on each column so that it has mean 0 and standard deviation 1).\nds_train = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df[~valid_mask].iterrows()]\nds_valid = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df[ valid_mask].iterrows()]\nds_test  = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df_test.iterrows()]\nWe can pick out an example\nx, y = ds_train[0]\n\n\nplt.imshow(x.reshape(28,28), cmap='Greys')\ny\n2\n\n\n\npng\n\n\nWe then put these into a PyTorch DataLoaders to shuffle them and collate them into batches\nbatch_size = 4096\n\ndl_train = DataLoader(ds_train, batch_size, shuffle=True)\ndl_valid = DataLoader(ds_valid, batch_size)\ndl_test = DataLoader(ds_test, batch_size)\nx, y = next(iter(dl_train))\nx, y\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 8,  ..., 8, 3, 9]))\nWe can then wrap these in a DataLoaders object\ndls = DataLoaders(dl_train, dl_valid, dl_test)\ndls.train, dls[0]\n(<torch.utils.data.dataloader.DataLoader at 0x7f4c0e6e0450>,\n <torch.utils.data.dataloader.DataLoader at 0x7f4c0e6e0450>)\n\n\n4. Learner\nUsing the high level API did a lot of things:\nlearn = tabular_learner(dls, layers=[100], opt_func=SGD, metrics=accuracy, config=dict(use_bn=False, bn_cont=False))\n\nbuild and initialise the model\nset the metrics\nset an appropriate loss function\nregister the optimizer\n\nWe’ll do these parts manually and put them into a Learner.\n\n4.1 Model\nUsing PyTorch’s Sequential we can easily rewrite the model manually\nmodel = nn.Sequential(\n    nn.Linear(784, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n)\nAnd run it over an example batch of data\nx.shape\ntorch.Size([4096, 784])\nWe get 10 outputs for each item in the batch, as expected.\npred = model(x)\nprint(pred.shape)\npred\ntorch.Size([4096, 10])\n\n\n\n\n\ntensor([[ 0.0838,  0.1000,  0.0104,  ..., -0.0319, -0.1107,  0.0448],\n        [ 0.0488,  0.0577,  0.0980,  ...,  0.0814, -0.1584,  0.1305],\n        [ 0.1289,  0.0631,  0.0885,  ...,  0.0194, -0.0895,  0.0239],\n        ...,\n        [ 0.1703,  0.0883,  0.0789,  ..., -0.0812, -0.1138,  0.1045],\n        [ 0.0145,  0.0626,  0.1440,  ..., -0.0434, -0.1266,  0.2100],\n        [ 0.0092,  0.2286,  0.2602,  ...,  0.0141, -0.0817,  0.0991]],\n       grad_fn=<AddmmBackward0>)\n\n\n4.2 Metrics\nWe can calculate accuracy as the number of predictions that are the same as the labels. Since we have 10 equally likely classes for our randomly initialised model it should be about 10%.\ndef accuracy(prob, actual):\n    preds = prob.argmax(axis=-1)\n    return sum(preds == actual.flatten()) / len(actual)\naccuracy(pred, y)\ntensor(0.0835)\n\n\n\n4.3 Loss\nThe appropriate loss function for multiclass classification is CrossEntropy loss.\nloss_function = nn.CrossEntropyLoss()\nloss_function(pred, y)\ntensor(2.3009, grad_fn=<NllLossBackward0>)\n\n\nOptimizer\nPyTorch provides torch.optim.SGD optimizer but we can’t use it directly with a Learner; from the docs\n\nThe most important is opt_func. If you are not using a fastai optimizer, you will need to write a function that wraps your PyTorch optimizer in an OptimWrapper. See the optimizer module for more details. This is to ensure the library’s schedulers/freeze API work with your code.\n\nWe’ll use fastai’s SGD instead for now.\n\n\nPutting it into Learner\nlearn = Learner(dls=dls, model=model, loss_func=nn.CrossEntropyLoss(), opt_func=SGD, metrics=[accuracy])\nNote that this performs slightly worse than our original model which got to 82% accuracy and 0.5 validation loss in 5 epochs. It would be interesting to know what’s changed!\nWith this kind of machine learning code a small change can make a big difference in how fast a model trains and how accurate it gets; this is why it’s good to be able to dig into the detail!\nlearn.fit(5, lr=0.2)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\naccuracy\n\n\ntime\n\n\n\n\n\n\n0\n\n\n1.773703\n\n\n1.286911\n\n\n0.647599\n\n\n00:00\n\n\n\n\n1\n\n\n1.426255\n\n\n1.041697\n\n\n0.638919\n\n\n00:00\n\n\n\n\n2\n\n\n1.228973\n\n\n0.891194\n\n\n0.687113\n\n\n00:00\n\n\n\n\n3\n\n\n1.102247\n\n\n0.816285\n\n\n0.696784\n\n\n00:01\n\n\n\n\n4\n\n\n1.006129\n\n\n0.768820\n\n\n0.736877\n\n\n00:00"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#import-1",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#import-1",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "1. Import",
    "text": "1. Import\nWe’ll import as few utilities as we can\nimport numpy as np\nfrom torch import tensor, randn, arange, no_grad, stack"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#load-data-1",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#load-data-1",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "2. Load data",
    "text": "2. Load data\nThis time we’ll load the data in using pure Numpy; because the data is just numbers it’s easy to do this.\ndata = np.loadtxt('../input/fashionmnist/fashion-mnist_train.csv', skiprows=1, delimiter=',')"
  },
  {
    "objectID": "peeling-fastai-layered-api-with-fashion-mnist/index.html#dataloaders-1",
    "href": "peeling-fastai-layered-api-with-fashion-mnist/index.html#dataloaders-1",
    "title": "Peeling back the fastai layered AI with Fashion MNIST",
    "section": "3. Dataloaders",
    "text": "3. Dataloaders\nvalid_mask = np.random.choice([True, False], len(data), p=(0.2, 0.8))\nX_train, y_train = tensor(data[~valid_mask, 1:].astype(np.float32) / 255.), tensor(data[~valid_mask,0].astype(np.int64))\nX_valid, y_valid = tensor(data[ valid_mask, 1:].astype(np.float32) / 255.), tensor(data[ valid_mask,0].astype(np.int64))\n\nLearner\n\n\n4.1 Model\nOur model consists an architecture and parameters; we’ll need a way to initialise those parameters.\ndef init_params(size, std=1.0): return (randn(size)*std).requires_grad_()\nNow, as before, we can set up 2 linear models with a ReLU between them. In torch nn code this looks like:\nmodel = nn.Sequential(\n    nn.Linear(784, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n)\nThe first linear layer consists of 784 * 100 weights plus 100 biases. The ReLU layer has no parameters; it’s just a nonlinear activation. The final layer consists of 100 * 10 weights to 10 biases.\nw1, b1 = init_params((784, 100)), init_params((100,))\nw2, b2 = init_params((100, 10)), init_params((10,))\n\nparams = [w1, w2, b1, b2]\nOur model then takes in the 784 pixels and performs:\n\naffine projection onto 100 dimensional space\nReLU: Replace all the negative values by zero\naffine transformation onto 10 dimensional space\n\nThat looks like this:\ndef model(x):\n    act1 = x@w1 + b1\n    act2 = act1 * (act1 > 0)\n    act3 = act2@w2 + b2\n    return act3\nThis can take the predictor from our dataloader\nx, y = X_train[:1024], y_train[:1024]\n\npred = model(x)\n\npred.shape\ntorch.Size([1024, 10])\n\n\nMetrics\ndef accuracy(pred, y): return sum(y.flatten() == pred.argmax(axis=1)) / len(y)\n\naccuracy(pred, y)\ntensor(0.0869)\naccuracy(model(X_valid), y_valid)\ntensor(0.0910)\n\n\n3. Loss function\nOur loss function is the negative log likelihood; the likelihood is how probable the data is given the model, that is we average the probabilities for the correct label, and then take the negative log.\nThe first step in calculating this is getting the model probabilities. We normalise th predictions with a softmax; expoentiate to make positive, and then divide by the sum to normalise to 1.\nUnfortunately if we do this naively we end up getting infinity because of the limits of floating point arithmetic.\npred.exp().sum(axis=1)\ntensor([inf, inf, inf,  ..., inf, inf, inf], grad_fn=<SumBackward1>)\nInstead we use the log probabilities, which have a better range in floating point space, and use the log-sum-exp trick to make it stable (PyTorch has a logsumexp function, but it’s easy to write.\ndef logsumexp(x):\n    c = x.max(axis=1).values\n    x_shift = x - c[:, None]\n    return c + x_shift.exp().sum(axis=1).log()\nCheck they are the same\na = tensor([[1,2,3], [4,5,7]])\na.exp().sum(axis=1).log(), logsumexp(a)\n(tensor([3.4076, 7.1698]), tensor([3.4076, 7.1698]))\nWe can then calculate the log probabilities using the softmax\nlogprob = a - logsumexp(a)[:, None]\nlogprob\ntensor([[-2.4076, -1.4076, -0.4076],\n        [-3.1698, -2.1698, -0.1698]])\nAnd if we exponentiate them they sum to 1.\nlogprob.exp().sum(axis=1)\ntensor([1., 1.])\ndef pred_to_logprob(pred):\n    return pred - logsumexp(pred)[:, None]\npred_to_logprob(pred)[range(len(y)), y.long()]\ntensor([-2.9484e+02, -4.0163e+02, -8.3874e+01,  ..., -1.9576e+02,\n        -3.4655e+02, -4.0436e-03], grad_fn=<IndexBackward0>)\ndef loss_func(pred, y):\n    logprob = pred_to_logprob(pred)\n\n    true_prob = logprob[range(len(y)), y]\n\n    return -true_prob.mean()\nOur randomly initialised weights should on average give a ~1/10 probability to each class, and so the loss should be around -log(1/10) = 2.3.\nloss = loss_func(pred, y)\nloss\ntensor(126.1162, grad_fn=<NegBackward0>)\n\n\nOptimizer\nThe SGD optimizer just moves each paramater a small step down the gradient to reduce the overall loss (and then we need to reset the gradients to zero).\nWe can easily run the whole training loop as follows (though note we get slightly worse accuracy than last time).\nbatch_size = 2048\nlr = 0.2\n\nfor epoch in range(5):\n    for _batch in range(len(X_train) // batch_size):\n        # Data loader\n        idx = np.random.choice(len(X_train), batch_size, replace=False)\n        X, y = X_train[idx], y_train[idx]\n\n        pred = model(X)\n        loss = loss_func(pred, y)\n        loss.backward()\n        with no_grad():\n            for p in params:\n                p -= lr * p.grad\n                p.grad.zero_()\n\n\n    print(epoch, accuracy(model(X_valid), y_valid))\n0 tensor(0.5943)\n1 tensor(0.6033)\n2 tensor(0.6119)\n3 tensor(0.6306)\n4 tensor(0.6387)"
  },
  {
    "objectID": "endurance-counting/index.html",
    "href": "endurance-counting/index.html",
    "title": "Endurance Counting",
    "section": "",
    "text": "A good counting based technique for endurance is box breathing. It involves repeatedly inhaling to a count of 4, holding to a count of 4, exhaling to a count of 4 and holding to a count of 4. This is a technique used by Navy SEALs to induce calm and focus. I’ve personally found it effective for lengthening a cold shower."
  },
  {
    "objectID": "sentencing-remarks/index.html",
    "href": "sentencing-remarks/index.html",
    "title": "Getting Sentencing Data",
    "section": "",
    "text": "The Courts Administration Authority of South Australia published Sentencing Remarks for judgements, but it only keeps data back one month. Sentencing remarks are very informative because they explain the rationale behind the sentencing; the factors the judge consciously weighted (in Noise they talk about unconscious factors citing that Judges give more favourable rulings after lunch, but this could just be correlation with whether the defendants have legal representation). The Australasian Legal Information Institute has a great public database of case law, but it doesn’t have all the sentencing remarks for South Australia (there were cases where I could only find the judgements, not the sentencing remarks).\nIt’s fantastic that the South Australian Sentencing Remarks are publicly available online; they inform the public on how these decisions are made which should be in line with community expectations. Without that feedback loop it would be harder for the public to find out why a sentence was given. However it’s a pity it only goes back one month, the only way I could find to get sentences beyond that was by going through news media websites. It would be a great service if someone archived these and made them publicly available.\nFor a very short time in 2013 the Wayback Machine captured some of these sentencing remarks. The remarks are listed on http://www.courts.sa.gov.au/SentencingRemarks/Pages/default.aspx and link to URLs like http://www.courts.sa.gov.au/SentencingRemarks/Pages/lightbox.aspx?IsDlg=1&Filter=NNNN where the last characters are some number unique to the case.\nWhile these are easy for a human to read, I thought it could be useful to try to extract relevant sections of the remarks programatically using Python. Here’s a brief snippet to read an archived sentencing remark into Python using CDX Toolkit (which can also access Common Crawl):\nimport cdx_toolkit\n\n# Fetcher for Internet Archive's Wayback Machine\ncdx = cdx_toolkit.CDXFetcher(source='ia')\n\n# Search for results\nurl = 'http://www.courts.sa.gov.au/SentencingRemarks/Pages/lightbox.aspx*'\n# At running gets 784 results, 759 of them unique\nresults = [result for result in cdx.get(url) if 'Filter=' in result.data['url']]\n\n# Get the plain text\ntext = extract_text(results[0].text)\nThe extract_text uses a very simple approach of converting the HTML to text (there are more robust ways to do this):\nimport parsel\nimport re\n\ndef extract_text(html: str) -> str:\n    sel = parsel.Selector(r.text)\n    selector = sel.css('div.item *::text')\n    text = ''.join([remove_multiple_newlines(fragment) for fragment in selector.getall()])\n    return remove_multiple_spaces(text)\n\ndef remove_multiple_spaces(text: str) -> str:\n    return re.sub('[ \\xa0]+', ' ', text)\n\ndef remove_multiple_newlines(text: str) -> str:\n    # Insert a space because these can happen across markup\n    return re.sub(r'\\r\\n(\\r\\n)?', r' \\1', text)\nFrom here you could use text matching or NLP to find relevant parts of the remarks."
  },
  {
    "objectID": "tower-of-hacks/index.html",
    "href": "tower-of-hacks/index.html",
    "title": "Operating a Tower of Hacks",
    "section": "",
    "text": "This may sound ridiculous but sometimes I’ve had situations that feel like this. Things get built under operational pressure and you just have to get it working. But when you wake up in a cold sweat at 5am because your pager didn’t go off overnight, and so you assume the alerting system must be broken then its gone way too far.\nIt can be hard to get investment to stabilise these hacky systems, but leaving them as they are is extremely stressful. I’ve found it useful to call out the operational risks, go through scenarios of the potential impact of near misses, and push back on other priorities because I’m busy trying to keep this system running.\nOne useful thing step can be to take the time to record each step of the process, including each hacky fix. Once you’ve done this enough times to hit the edge cases you can hand this off to anyone else. It becomes clearer where the really bad parts are, and helps prioritise the fixes."
  },
  {
    "objectID": "pipetable-csv/index.html",
    "href": "pipetable-csv/index.html",
    "title": "Pipetable to CSV",
    "section": "",
    "text": "I often get data output from an SQL query like this\n text         | num  | value\n--------------+------+-------------\n   Some text  |  0.3 | 0.2\n   Rah rah    |  7   | 0.00123(2 rows)\nRunning sed 's/\\(^  *\\| *|\\|(.*\\) */,/g' gives:\n,text,num,value\n--------------+------+-------------\n,Some text,0.3,0.2\n,Rah rah,7,0.00123,\nI can delete the divider and then use as a CSV. Even better I can run this same regular expression in Vim or Emacs Evil mode as an Ex command. This won’t work if the data contains parentheses or pipes, but is useful for quick extracts."
  },
  {
    "objectID": "nquad-grep/index.html",
    "href": "nquad-grep/index.html",
    "title": "Processing RDF nquads with grep",
    "section": "",
    "text": "With a short grep script we can get twenty thousand Australian Job Postings with metadata from 16 million lines of compressed nquad in 30 seconds on my laptop. This can be run against any Web Data Commons Extract of Job Postings.\nzgrep -aE \\\n'(<https?://schema.org/([^ >]+/)?(addressCountry|name|salaryCurrency|currency)> \"(Australia|AU|AUS|AUD)\")|( <https?://[^ >/]+\\.au/[^ >]*> \\.$)' \\\n *.gz | \\\n grep -Eo '<https?://[^ >]+> .$' |\n uniq | \\\n sed -E 's/<([^ >]+)> .$/\\1/' | \\\n  sort -u > \\\n au_2019_urls.txt\nThe top 10 domains (of 450) from this extract look reasonable; recruitment agencies operating in Australia.\n\n\n\n# URLs\nDomain\n\n\n\n\n1631\nwww.davidsonwp.com\n\n\n952\nwww.cgcrecruitment.com\n\n\n809\nwww.peoplebank.com.au\n\n\n634\nwww.people2people.com.au\n\n\n610\nwww.designandbuild.com.au\n\n\n590\nwww.perigongroup.com.au\n\n\n554\nwww.ambition.com.au\n\n\n532\nwww.medicalrecruitment.com.au\n\n\n528\nwww.talentinternational.com.au\n\n\n456\nwww.accountability.com.au\n\n\n\n\nUsing SPARQL in rdflib\nTo stream the nquads into rdflib one graph at a time I needed to use a regular expression to get the URL. This is safe to do because in Web Data Commons the last quad is always the URI that the data was obtained from, and URIs can not contain spaces or tabs.\nimport re\nRDF_QUAD_LABEL_RE = re.compile(\"[ \\t]+<([^ \\t]*)>[ \\t]+.\\n$\")\ndef get_quad_label(s):\n    return RDF_QUAD_LABEL_RE.search(line).group(1)\nIf we want to check that the domain is .au then we can parse it with urllib.\nimport urllib\ndef get_domain(url):\n    return urllib.parse.urlparse(url).netloc\nWe can then filter out the Australian domains in about 4 minutes, giving 17,000 distinct URLs. Because this extract only contains URLs with a JobPosting these will all have structured job ads.\nfrom collections import groupby\nimport gzip\nfrom tqdm.notebook import tqdm\nf = iter(tqdm(gzip.open(JOBS_JSON_2019, 'rt'), total=16_925_915))\nau_urls = []\nfor url, _ in  groupby(f, get_quad_label):\n    if get_domain(url).endswith('au'):\n        au_urls.append(url)\nWe can extend this with a SPARQL query to search for Australian country or currency.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_pl: <http://schema.org/Place/>\nPREFIX sdo_pa: <http://schema.org/PostalAddress/>\nPREFIX sdo_co: <http://schema.org/Country/>\nPREFIX sdo_mv: <http://schema.org/MonetaryValue/>\nPREFIX sdos_mv: <https://schema.org/MonetaryValue/>\n\n\nASK WHERE {\n  {\n    {[] a sdo:JobPosting ;\n         (sdo:jobLocation|sdo_jp:jobLocation)/\n         (sdo:address|sdo_pl:address)/\n         (sdo:addressCountry|sdo_pa:addressCountry)/\n         ((sdo:name|sdo_co:name)?) ?country .\n         FILTER (isliteral(?country) && \n                 lcase(replace(str(?country),\n                               '[ \\n\\t]*(.*)[ \\n\\t]*',\n                               '\\\\1')) in ('au', 'australia'))\n    }\n    UNION\n    {[] a sdo:JobPosting ;\n        ((sdo:salaryCurrency|sdo_jp:salaryCurrency)|\n         (sdo:baseSalary|sdo_jp:baseSalary)/\n         (sdo:currency|sdo_mv:currency|sdos_mv:currency)) ?currency .\n    BIND (replace(str(?currency), '[ \\n\\t]+', '') as ?curr)\n    FILTER (lcase(?curr) = 'aud')}\n  }\n }\nThen we can build a filter with this on the graph, and add a function to get the domain for the graph to search for .au domains.\ndef query_au(g):\n    result = list(g.query(aus_sparql))[0]\n    return result\n\ndef graph_domain(g):\n  url = graph.identifier.toPython()\n  return get_domain(url)\nWe can then apply this filter but it takes 40 minutes; most of this time is parsing the graph with rdflib.\nf = iter(tqdm(gzip.open(JOBS_JSON_2019, 'rt'), total=16_925_915))\nau_urls = []\nfor graph in parse_nquads(f):\n    if graph_domain(graph).endswith('au') or query_au(graph):\n        au_urls.append(graph_url(graph))\n\n\nTranslating to shell\nThe first script searching for .au domains is essentially a regular expression, and can be translated directly into grep and sed. This takes 30s, so is about 8 times faster than Python.\nzgrep -aEo ' <https?://[^/ >]+\\.au/[^ >]*> .$' \\\n      2019-12_json_JobPosting.gz | \\\n  uniq | \\\n  sed -E 's/<([^ >]+)> .$/\\1/' | \\\n  sort -u > \\\n au_urls.txt\nFor finding the country or currency we make a simplifying assumption; that if a name/country/currency is “Australia”, “AU”, “AUS” or “AUD” then it is a job ad located in Australia. This is a pretty safe assumption; it’s unlikely that e.g. a company name would by “Australia” or “AUS”. At worst we can filter out false negatives later with more processing. We can then extract the URL with sed as before, and it still takes 30s (rather than 40 minutes using rdflib).\nzgrep -E '<https?://schema.org/([^ >]+/)?(addressCountry|name|salaryCurrency|currency)> \"(Australia|AU|AUS|AUD)\"'\n 2019-12_json_JobPosting.gz | \\\n grep -Eo '<https?://[^ >]+> .$' |\n uniq | \\\n sed -E 's/<([^ >]+)> .$/\\1/' | \\\n  sort -u > \\\n au_extra_urls.txt\nFinally we can combine the two expressions to get the process at the start of the article. We can then analyse the output with some shell processing to get the top domains at the start of the article.\nsed -E 's|https?://([^ /]+)/.*|\\1|'\\\n    au_urls.txt |\\\n    sort |\\\n    uniq -c |\\\n    sort -nr |\\\n    head"
  },
  {
    "objectID": "jupyter-init/index.html",
    "href": "jupyter-init/index.html",
    "title": "Jupyter Notebook Preamble",
    "section": "",
    "text": "Whenever I use Jupyter Notebooks for analysis I tend to set a bunch of options at the top of every file to make them more pleasant to use. Here they are for Python and R with IRKernel"
  },
  {
    "objectID": "jupyter-init/index.html#python",
    "href": "jupyter-init/index.html#python",
    "title": "Jupyter Notebook Preamble",
    "section": "Python",
    "text": "Python\n# Automatically reload code from dependencies when running cells\n# This is indispensible when importing code you are actively modifying.\n%load_ext autoreload\n%autoreload 2\n\n# I almost always use pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n# Set the maximum rows to display in a dataframe\npd.options.display.max_rows = 100\n# Set the maximum columns to display in a dataframe\npd.options.display.max_columns = 200\n# Set the maximum width of columns to display in a dataframe\npd.options.display.max_colwidth = 80\n# Don't render $..$ as TeX in a dataframe\npd.options.display.html.use_mathjax = False"
  },
  {
    "objectID": "jupyter-init/index.html#r",
    "href": "jupyter-init/index.html#r",
    "title": "Jupyter Notebook Preamble",
    "section": "R",
    "text": "R\nFor R I configure similar display options to Python through repr:\n# Set the maximum number of columns and rows to display\noptions(repr.matrix.max.cols=150, repr.matrix.max.rows=200)\n# Set the default plot size\noptions(repr.plot.width=18, repr.plot.height=12)\n\n# Usual analysis libraries\nsuppressPackageStartupMessages({\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(glue)\nlibrary(lubridate)\n}\n\n# Database Libraries\nlibrary(DBI)\nlibrary(dbplyr)"
  },
  {
    "objectID": "prior-regularise/index.html",
    "href": "prior-regularise/index.html",
    "title": "Priors as Regularisation",
    "section": "",
    "text": "A typical machine learning approach to regression is to minimise the root mean squared error. A probabilistic perspective for this is to consider the regression \\(y = f_\\theta(X) + \\epsilon\\), where y is the outcome, X are the predictors, \\(f_\\theta\\) is a function parameterised by \\(\\theta\\), and \\(\\epsilon\\) is the error term. If we assume that \\(\\epsilon \\in N(0, \\sigma^2)\\) is normally distributed, this is equivalent to saying that \\(y \\in N(f_\\theta^2(X), \\sigma^2)\\). We then need to pick the most likely parameters \\(\\theta\\) given the data.\nThe Bayesian perspective on this is if we have a prior on the parameters \\(p(\\theta)\\), and data \\(X_i, y_i\\) then the posterior estimate is \\(p(\\theta \\vert X_i, y_i) = \\frac{p(X_i, y_i \\vert \\theta) p(\\theta)}{p(\\{X_i,y_i\\}_i)}\\). In Bayesian statistics we estimate the whole distribution, but we can focus on the maximum likelihood estimator, the value of \\(\\theta\\) that maximises the posterior probability. Since the logarithm is a monotonic function, the maximum likelihood occurs as the same point as the maximum log likelihood. Taking the logarithm and plugging in the normal distribution for \\(p(X,y \\vert \\theta)\\) gives \\(l(\\theta, \\sigma) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} (f_{\\theta}(X_i) - y_i)^2 + \\log(p(\\theta)) - N \\log(\\sigma) + c\\) for some constant c. In the case of a flat prior, \\(p(\\theta) = 1\\) then the maximum likelihood estimator is equivalent to minimising the (root) mean squared error. However in general the prior acts as a regularisation; for example if we take a prior that the parameters are normally distributed it reduces to Tikhonov Regularisation. However we could pick other prior distributions to recover an Lᵖ regularisation, and in particular a Laplace distribution recovers the LASSO.\nThere’s more here, in Bayesian statistics people tend to use a Horseshoe Prior instead of a Laplace Distribution, and Michael Betancourt has an article on my reading list on Bayes Sparse Regression that goes through the trade-offs with different regularising priors.\n\nBinary classification\nSimilar ideas can be applied in Binary Classification, here the metric is typically Binary Cross Entropy. From a probabilistic perspective we can assume the data comes from a Binomial distribution \\(y \\in B(f_\\theta(X))\\). Here \\(p(X_i, y_i \\vert \\theta) = f_\\theta(X_i)^{y_i} (1 - f_\\theta(X_i))^{1-y_i}\\) (keeping in mind that \\(y_i\\) can only take the values 0 or 1). Then, as in the normal regression case, we can find the maximum likelihood estimator by minimising the log likelihood \\(l(\\theta) = \\sum_{y_i = 1} \\log(f_\\theta (X_i)) + \\sum_{y_i=0} \\log(1 - f_\\theta(X_i) + \\log(p(\\theta)) + c\\). With a flat prior this maximising the log likelihood is equivalent to minimising the Binary Cross Entropy.\nConsider in particular the constant model \\(f_\\theta(X_i) = \\theta\\), where this reduces to \\(l(\\theta) = s \\log(\\theta) + (N-s) \\log(1-\\theta) + \\log(p(\\theta))\\), where s is the number of successes and N is the total number of trials. A bit of calculus and algebra shows that with a flat prior this is maximised when \\(\\hat{\\theta} = \\frac{s}{N}\\).\nOne problem with this is the variance of the binomial is \\(\\sqrt{\\frac{\\theta(1-\\theta)}{N}}\\), and so if we have 0 or N successes the maximum likelihood estimate for the variance is 0, which in most cases isn’t right - we’re not going to be exactly zero. A method for handling this, which I learned in the book Regression and Other Stories, is to set a prior of \\({\\rm Beta}(3,3)\\) which is equivalent to adding 4 extra trials with 2 successes. Then the maximum likely estimate for the parameter is \\(\\hat{\\theta} = \\frac{s+2}{N+4}\\) and the variance will always be non-zero.\nIn the log likelihood this adds a penalty of \\(\\log(\\theta^2 (1-\\theta)^2) + c'\\), for some constant \\(c'\\). Rewriting \\(\\psi = \\theta - \\frac{1}{2}\\) and rearranging gives the penalty, up to a constant, as \\(2 \\log(\\frac{1}{4} - \\psi^2)\\). For small \\(\\psi\\) we can do a Taylor expansion to get \\(-8 \\psi^2 = -8 (\\theta - \\frac{1}{2})^2\\). So this transformation is similar to a \\(l^2\\) penalty (I suspect this is for the same reason a Binomial converges to a Gaussian for large samples and moderate probabilities).\nWhat’s interesting here is the Beta prior gives a more reasonable and understandable regularisation than \\(l^2\\) regularisation, especially for probabilities close to 0 or 1. I would never have thought of a log Beta penalty, but thinking of it as a prior it makes really good sense. On the other hand being able to switch to a maximum likelihood, and thinking of the prior as a penalty, makes things much quicker to calculate than trying to estimate the whole posterior. There’s a Wikipedia article on Bayesian interpretation of Kernel Regularisation It’s useful being able to switch between the two viewpoints."
  },
  {
    "objectID": "comment-to-function/index.html",
    "href": "comment-to-function/index.html",
    "title": "Comment to Function",
    "section": "",
    "text": "In Martin Fowler’s Refactoring he mentions that whenever there’s a block of code that has (or requires) a comment to describe what it does, that’s a good opportunity to package that code into a function. I’ve found that a very good rule of thumb to follow; if the functions are well named it makes the code much clearer. Here’s a typical sort of simple example:\n# Read in the dataframe\ndf = pd.read_csv('data.csv', keep_default_na=False, low_memory=False)\n# Convert temperatures to C\ndf['celcius'] = (df['temp']  - 32) * 5/9\n# Lookup the temperature ranges\ndf = df.merge(temperature_range, how='left', left_on=['celcius'], right_index=True)\nIn a real example these blocks could be much longer and more obscure. However if we package these up as functions the flow becomes a bit clearer:\ndf = read_data('data.csv')\ndf['celcius'] = fahrenheit_to_celcius(df['temp'])\n# Enrich modifies df\nenrich_temperature_range(df, temperature_range)\nBy talking about what we’re doing instead of how we’re doing it the logic and dataflow becomes much clearer. It also means these individual functions can be separately tested to ensure they’re functioning correctly. I’ve found lots of bugs and unreachable code blocks from this process of creating functions and breaking the code into small pieces."
  },
  {
    "objectID": "key-web-captures/index.html",
    "href": "key-web-captures/index.html",
    "title": "Unique Key for Web Captures",
    "section": "",
    "text": "Each capture represents a snapshot of a URL at a certain point in time; in this way we could represent a capture with a timestamp and URL. Another view is each capture is stored somewhere where we retrieve it, and so could be represented by a pointer to its location. Finally we could just think of the capture by its contents (the HTML page) and represent it by some hash. These different choices all have different trade-offs summarised in the table below:\n\n\n\n\nTimestamp/URL\nPointer\nContent Hash\n\n\n\n\nHuman Interpretable\nYes\nNo\nNo\n\n\nRepresentation Size\nVery Long\nLong\nShort\n\n\nAlways Exist\nNo\nYes\nYes\n\n\nPermenant\nYes\nTypically\nYes\n\n\nUnique\nTypically\nYes\nVulnerable\n\n\n\nA timestamp/URL is easy to interpret as a person, whereas the pointer and the hash often are opaque as to what the resource is. However a URL can be very long (especially when it contains a query string with lots of parameters), whereas a pointer has a typical fixed length and a content hash is guaranteed to have a fixed length. We are guaranteed to have a pointer to the data (otherwise we can’t obtain it), and we can always hash the content (although it may be expensive to do so), but there may be cases where we have data but not the URL or time of capture. The pointer can change if the assets are moved; this can be incorporated by some sort of mapping from old to new locations (assuming it doesn’t happen vary often), wheras the Timestamp/URL and Content Hash should never change (assuming the hash algorithm stays the same).\nUniqueness is a bit more subtle; are two captures of the same URL at different times with the same content different? For the purposes of extraction they are the same - they contain the same data so we will extract the same results. However we may want to distinguish them if we are monitoring the changes to a web page, so in that sense the timestamp/URL and pointer contain additional information. We may also potentially capture the same URL twice at the same timestamp, but that’s unlikely to happen much and when it does the content is likely to be the same (but not guaranteed, especially if the requests have different parameters and come from different IP addresses). Finally for a hash of the content there’s always the chance of a hash-collision; for example both Common Crawl and the Wayback Machine contain a SHA-1 for which there are practical algorithms to generate collisions. However a random collision is very unlikely; from the Birthday Problem the probability of a random collision between n items in a space with m values is approximately \\(\\frac{n^2}{2m}\\), given that for SHA-1 the space is \\(m = 2^{160}\\) we only need to start really worrying about collisions around \\(2^{80} \\approx 10^{24}\\) distinct documents.\nThe content hash seems like the best option for a cache key; attacks and collisions are possible but unlikely (and can be mitigated by a change of hash function), but it always exists and has a short representation (which can even be used for a filename on modern filesystems). Given that WARC currently captures SHA-1 digest, and CDX servers provide it, makes it practical to use (although if they change the algorithm it may lead to some difficulties). It also makes sense from a functional perspective; if the content is equivalent then a pure function should give always give the same result. If we want to track other kinds of identity, such as the timestamp and URL of capture, we can store that separately in a database (where the hash itself is not a key)."
  },
  {
    "objectID": "adding-open-library/index.html",
    "href": "adding-open-library/index.html",
    "title": "Adding a Book to Open Library",
    "section": "",
    "text": "Overall Open Library prioritises making it easy for people to add data, and then has facilities to edit it. There are few required fields, most fields are free text, and there is minimal validation. This is really helpful in getting lots of books in the system, at the cost of duplication and sparse and erroneous data. For their goal of getting a web page for every book this is the right approach, but it has implications for the resulting data.\nTo add a book requires minimal data; the title, author, publisher and optionally an identifier like an ISBN. After adding a book there’s a prompt to encourage for additional details like a description and subject keywords. Then the work can be edited to add additional details such as a cover, other identifiers, and language. The author can also be edited to add a biography, birth and death date, and other identifiers.\n\nAdding a Work\nOpen Library is publicly editable, you just need to create an account linked to an email address and log in. Adding a book easy; you just need to supply the title, author, publisher, when published and optionally an ID (an ISBN10, ISBN13, LCCN). The title is free text, the author field provides an autocomplete drop down of existing authors, and the publisher and published date is free text. There seems to be minimal validation, which helps get data into the system but allows incorrect data and duplicates. There doesn’t seem to be any indication of whether the title is already in the database, which explains why there are often duplicates. The title is parsed as Title: Subtitle, which is convenient but also can lead to certain kinds of errors. The author field by default creates a new author, which explains all the duplicate authors, and doesn’t prompt for any information about the author. The publisher and published date are free text, and I’ve seen many small variations in actual usage (there is a nudge “Are you sure that’s the published date?” if there’s nothing that looks like a year in the date). The ID field is an optional drop down; I’m not sure whether this is validated but encourages adding just one ID (which is why there’s often an ISBN10 or ISBN13 but not both).\n\n\nAdding Work details\nAfter you submit this page (and the CAPTCHA) the book is added, and you’re sent to a new screen and encouraged to add more details.\n\n\n\nAdd book Detail Page\n\n\nA lot of the fields here are very open ended. For the “How would you describe the book?” field there is explanatory text saying “There’s no wrong answer here.”. I found it really hard to describe the book, so I put the blurb.\nThen there were fields for subject keywords, people, places, and times. In this case only keywords seemed appropriate, and it’s hard to think of some (the example text suggests: cheese, Roman Empire, psychology). I picked pricing,business,microeconomics but I could think of many variations; these are unlikely to be consistent.\nThere’s also an option to add excerpts or the first sentence, but I couldn’t find one I thought was worth adding. Finally there’s a link section, giving an example of “New York Times review”; I added the publisher’s website.\nHowever after doing this the book data is still very sparse. If you view the work and click “Edit” you get many more options.\n\n\nEditing Edition Details\n\n\n\nEdit Open Library Book\n\n\nThe first thing to note is that in the add book page it said to use “Title: Subtitle”, but in the edit page they are separate fields! Most of the fields are free text, but the “Languages” field has an auto-suggest dropdown (where I could select “English”). The ID field has a large dropdown list of catalogues you can link with, but above the --- are ISBN 10, ISBN 13, LC Control Number, OCLC/World Cat, Goodreads, and Library Thing. There is some validation around fields like dimensions need to be numbers. I didn’t immediately realise I could change the cover here until I accidentally moused over the grey cover and saw a tooltip saying “Add cover image”. Clicking here opens a popup to add a book cover.\n\n\n\nAdd Book Cover\n\n\nIt’s nice that you can add either a file or a URL; I added a cover URL and it worked well. This automatically populated the cover image for the work.\nI couldn’t see how to edit things like the subject keywords that were added after adding the work.\n\n\nAdding Edition\nI also tried adding a new edition for the digital version. The add book page is largely similar, with the title and author pre-filled but modifiable (and authors can be added). The publisher and publish date need to be added, along with IDs.\nAfter adding the edition there is a detail page similar to when we added the book originally. All the fields are emptied, including things such as subtitle which are likely to be stable across editions. This means the data is often going to be missing or inconsistent across editions.\n\n\nEditing Author\n\n\n\nEdit author\n\n\nI also looked at editing the author, by clicking the author’s name and then edit. Biographies are hard to write; I just copied the biography from the back cover - it suggests to attribute it but with no guide as to how. I didn’t know his date of birth, so left it blank (but note they are free text). I didn’t know other names, or identifiers, so I added an image from the internet and submitted it.\n\n\nInterfaces for Data Collection\nAdding two editions of Overcoming Floccinaucinihilipilification by Jon Manning to Open Library was easy. However I had to carefully explore the interface to fill in a lot of detailed data, many works will be missing data. The lack of checks and prompts mean that there will be duplication and inconsistencies, especially around subtitle which was inconsistently handled throughout the interface. However the most important data, the title and author and linking id are often clearly captured.\nFrom Open Library’s perspective a lot of these design decisions are good; it should be as easy as possible to add a book. There are opportunities to make certain fields more accessible, improve the consistency of capturing subtitle, and pre-fill. But from the perspective of a data consumer it means we expect very high coverage, but need to be tolerant to duplicates, inconsistencies, and errors."
  },
  {
    "objectID": "html2text-doubleemph/index.html",
    "href": "html2text-doubleemph/index.html",
    "title": "Double emphasis error in html2text",
    "section": "",
    "text": "In this case I found a term that was emphasised twice: <strong><strong>word</strong></strong>. I’m pretty sure for a browser this is just the same as doing it once; <strong>word</strong>. This is likely the result of some strange processing that no one noticed because it makes no visual difference.\nUnfortunately html2text doesn’t handle this; for each <strong> tag it just surrounds the word with two asterisks. So we get ****word****, which isn’t the correct markdown; it should just be **word**. If I pass this back through a Markdown parser I’ll get back something like <p><strong><em>*word</em></strong>*</p>.\nThe annoying thing is I didn’t look very hard for this example. It, and the previous bug, were in the first twenty job ads I found in Common Crawl. While I could patch it I feel like these kinds of examples will keep coming up.\nI was hoping that html2text could do the heavy lifting of dealing with all the messy HTML, but it seems like I’m going to keep hitting edge cases. I think the best solution would be to customise the html2text parser to output what I want. However I’m not entirely sure what I want, and that would be a lot of effort.\nI’m going to use it as the basis of a good enough solution for now, and wait until there’s a practical need to invest more in it."
  },
  {
    "objectID": "ask-hn-book-recommendations/index.html",
    "href": "ask-hn-book-recommendations/index.html",
    "title": "Ask HN Book Recommendations",
    "section": "",
    "text": "Ask HN is a kind of post on Hacker News that allows asking questions to the community. We can’t identify them in the dataset but they typically start with “Ask HN” in the title. Searching for titles matching the regex ^Ask HN.*\\b(?:text)?books?\\b gives some of these threads but also threads about discussing books (“How much do you love discussing books with the people who read them?”), writing books (“Is it worth it for a tech startup founder to write a chapter in a book?”), and reading books (“Ask HN: Have you stopped reading books?”). By further refinining to books containing recommendation words '\\b(?:recommend(?:ed)|best|favou?rite|top)\\b' gives almost entirely book recommendation threads (with the ocassional exception such as “Best place to purchase (used) technical books that’s not Amazon?”).\nThe top level comments to these threads are almost always book recommendations, and I saved these.\nI find it difficult to understand how to make the tradeoff between recall and precision. These rules could be used as weak labelers, as a seed model for binary classification, or to filter examples to annotate for NER. For the latter case these high precision methods make sense (to save time skipping texts with nothing to annotate); for other cases maybe lower precision methods make sense. I need to think more about my strategy before making more of these rules.\nIf you want to see the detail I’ve comitted the exploratory notebook."
  },
  {
    "objectID": "life-optimising/index.html",
    "href": "life-optimising/index.html",
    "title": "Life Optimisation",
    "section": "",
    "text": "A study from Daniel Kahneman and the economist Angus Deaton says High income improves evaluation of life but not emotional well being. Optimising income doesn’t optimise ones own happiness, but it does increase one’s satisfaction with their own life. Kahneman observes that people tend to maximise their own satisfaction, which is based on comparisons with people they consider peers. We don’t really optimise for happiness, which is often fleeting and forgotten.\nIt shows the kind of creature we are where our own success is doing better than our peers, even in situations where there’s enough resources to go around. Success is often defined by our peers, parents or upbringing as well; what our society values. I’m not aware of a way to develop your own goals, independent of society.\nAnother argument is reproduction is what we should be optimising. From an evolutionary standpoint people who don’t reproduce don’t have children, and so their genes aren’t going to make it to the next generation. This doesn’t mean we should optimise it though, it’s almost a diagnosis.\nIn a way it’s sad to me in this freedom and wealth many people live in, they’re still focused on satisfaction and proving their worth to other people."
  },
  {
    "objectID": "bayes_toy_coin/index.html",
    "href": "bayes_toy_coin/index.html",
    "title": "Estimating Bias in a Coin with Bayes Rule",
    "section": "",
    "text": "I wanted to work through an example of applying Bayes rule to update model paremeters based on toy data This example comes from Kruschke’s Doing Bayesian Data Analysis, Section 5.3.\nThe model is that we have a coin and we’re trying to estimate the bias in the coin, that is the probability that it will come up heads when flipped. For simplicity we assume the bias, theta is a multiple of 0.1. We take a triangular prior centred at 0.5."
  },
  {
    "objectID": "bayes_toy_coin/index.html#impact-of-seeing-a-head",
    "href": "bayes_toy_coin/index.html#impact-of-seeing-a-head",
    "title": "Estimating Bias in a Coin with Bayes Rule",
    "section": "Impact of seeing a head",
    "text": "Impact of seeing a head\nLet’s consider how the model distribution changes by Bayes’ rule in the case we flip the coin once and see a head.\nThe likelihood of getting a head given theta, is just theta (because it’s the head bias by definition)\n\nlikelihood <- theta\n\n\n\n\n\n\nThen Bayes’ rule says the posterior is proportional to the prior and the likelihood. We can ignore the constant of proportionality by normalising it to 1.\n\n# Posterior given the head\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n\n\n\n\n\n\nThe posterior has moved markedly to the right, it’s more likely the coin is head biased now. The expected of bias has changed from 0.5 with the prior distribution, to 0.58 after incorporating the data of having seen one heads. The most likely value for theta is still 0.5."
  },
  {
    "objectID": "bayes_toy_coin/index.html#what-if-we-had-head-then-tails",
    "href": "bayes_toy_coin/index.html#what-if-we-had-head-then-tails",
    "title": "Estimating Bias in a Coin with Bayes Rule",
    "section": "What if we had head then tails?",
    "text": "What if we had head then tails?\nSuppose the second flip gave tails instead of heads. Then our likelihood of tails is the likelihood of not flipping heads, 1 - theta. Then we can get the posterior by applying Bayes’ rule as before\n\nlikelihood <- 1 - theta\n\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n\n\n\n\n\n\nExpectation value is 0.5, as you would expect by symmetry. Notice that the distribution is less spread around 0.5 than our original prior."
  },
  {
    "objectID": "python-offline-translation/index.html",
    "href": "python-offline-translation/index.html",
    "title": "Offline Translation in Python",
    "section": "",
    "text": "Suppose you want to translate text from one language to another. Most people’s first point of call is an online translation service from one of the big cloud providers, and most translation libraries in Python wrap Google translate. However the free services have rate limits, the paid services can quickly get expensive, and sometimes you have private data you don’t want to upload online. An alternative is to run a machine translation model locally, and thanks to Hugging Face it’s pretty simple to do.\nThere are some downsides to running these models locally. The quality of the translations will be lower than the cloud providers, and even they produce very strange results like translating gibberish into religious prophecies and sometimes amusing, like in a Google translation of Final Fantasy IV. The quality is often quite good, but sometimes the output is bizarre or wrong. It can also be quite slow to run, especially if you don’t run it on a GPU.\nThis article will compare two options; Argos Translate (a wrapper for OpenNMT) with Marian Machine Translation. Argos Translate is a more complete solution, is easier to get set up, and is substantially faster. However Marian Machine Translation gives better translations, supports more languages, and better supports batch translations."
  },
  {
    "objectID": "python-offline-translation/index.html#handling-long-text",
    "href": "python-offline-translation/index.html#handling-long-text",
    "title": "Offline Translation in Python",
    "section": "Handling Long Text",
    "text": "Handling Long Text\nIf you run it on a long text you will get IndexError: index out of range in self; this is a limitation of Transformer models where they have a maximum size input. The model only supports up to 512 tokens (where a token is dependent on the SentencePiece encoding, most words are made up of at most a few tokens), and if you pass any more it fails. The tokens returned by the tokenizer is a dictionary containing two tensors, the input_ids (which are the actual sentencepiece token ids) and attention_mask (which seems to be all 1s). The tensors are rank 2 with shape the number of texts by the maximum number of tokens in any of the texts. For long texts the best thing to do is to break the text apart at sentence boundaries and then paste it back together again.\nA robust way to break a long text into sentences is with Stanza:\nimport stanza\n \n# First you will need to download the model\n# stanza.download('ru')\nnlp = stanza.Pipeline('ru', processors='tokenize')\n\nfor sentence in nlp.process('Сдается однокомнатная мебелированная квартира квартира. Ежемесячная плата 18 тыс.р. + свет.').sentences:\n    print(sentence.text)\n    \n# Сдается однокомнатная мебелированная квартира квартира.\n# Ежемесячная плата 18 тыс.р. + свет.\nHowever we lose the space between sentences. To be able to capture this we need to be able to get both the sentence and the boundary preceeding it. Start with a generic container for the text and a prefix:\nfrom dataclassess import dataclass\n\n@dataclass(frozen=True)\nclass SentenceBoundary:\n    text: str\n    prefix: str\n        \n    def __str__(self):\n        return self.prefix + self.text\nAnd then create a SentenceBoundary object that can extract these from a Stanza Document, appending an empty text to get the trailing characters of the document. We also add methods for getting the non-empty sentences for translation, and for mapping the text through a dictionary of translations.\nfrom __future__ import annotations # For Python 3.7\nfrom typing import List\n\n@dataclass(frozen=True)\nclass SentenceBoundaries:\n    sentence_boundaries: List[SentenceBoundary]\n        \n    @classmethod\n    def from_doc(cls, doc: stanza.Document) -> SentenceBoundaries:\n        sentence_boundaries = []\n        start_idx = 0\n        for sent in doc.sentences:\n            sentence_boundaries.append(SentenceBoundary(text=sent.text, prefix=doc.text[start_idx:sent.tokens[0].start_char]))\n            start_idx = sent.tokens[-1].end_char\n        sentence_boundaries.append(SentenceBoundary(text='', prefix=doc.text[start_idx:]))\n        return cls(sentence_boundaries)\n    \n    @property\n    def nonempty_sentences(self) -> List[str]:\n        return [item.text for item in self.sentence_boundaries if item.text]\n    \n    def map(self, d: Dict[str, str]) -> SentenceBoundaries:\n        return SentenceBoundaries([SentenceBoundary(text=d.get(sb.text, sb.text),\n                                                    prefix=sb.prefix) for sb in self.sentence_boundaries])\n    \n    def __str__(self) -> str:\n        return ''.join(map(str, self.sentence_boundaries))\nBecause the all the texts are put into a single tensor there needs to be enough memory (CPU or GPU) available to store it all. So we need to minibatch the sentences into smaller groups. In fact since it needs to be processed in a rectangular block you should try to process all the shortest texts together and all the longest texts together for best efficiency. Moreover it’s worth caching any repeated texts to stop retranslating.\nPutting this all together we get a more robust translator:\nclass Translator:\n    def __init__(self, source_lang: str, dest_lang: str, use_gpu: bool=False) -> None:\n        self.use_gpu = use_gpu\n        self.model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{dest_lang}'\n        self.model = MarianMTModel.from_pretrained(self.model_name)\n        if use_gpu:\n            self.model = self.model.cuda()\n        self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)\n        self.sentencizer = stanza.Pipeline(source_lang, processors='tokenize', verbose=False, use_gpu=use_gpu)\n        \n    def sentencize(self, texts: Sequence[str]) -> List[SentenceBoundaries]:\n        return [SentenceBoundaries.from_doc(self.sentencizer.process(text)) for text in texts]\n                \n    def translate(self, texts: Sequence[str], batch_size:int=10, truncation=True) -> Sequence[str]:\n        if isinstance(texts, str):\n            raise ValueError('Expected a sequence of texts')\n        text_sentences = self.sentencize(texts)\n        translations = {sent: None for text in text_sentences for sent in text.nonempty_sentences}\n    \n        for text_batch in minibatch(sorted(translations, key=len, reverse=True), batch_size):\n            tokens = self.tokenizer(text_batch, return_tensors=\"pt\", padding=True, truncation=truncation)\n            if self.use_gpu:\n                tokens = {k:v.cuda() for k, v in tokens.items()}\n            translate_tokens = self.model.generate(**tokens)\n            translate_batch = [self.tokenizer.decode(t, skip_special_tokens=True) for t in translate_tokens]\n            for (text, translated) in zip(text_batch, translate_batch):\n                translations[text] = translated\n            \n        return [str(text.map(translations)) for text in text_sentences]\nNote that we set truncation=True in the tokenizer, so if a text is too long after breaking it into sentences we just drop the rest of the text rather than failing."
  },
  {
    "objectID": "python-offline-translation/index.html#effect-of-punctuation",
    "href": "python-offline-translation/index.html#effect-of-punctuation",
    "title": "Offline Translation in Python",
    "section": "Effect of punctuation",
    "text": "Effect of punctuation\nThe MarianMT model is very sensitive to punctuation and capitalisation, the OpenNMT model used by Argos is less sensitive to punctuation but is sensitive to tokenisation. This is likely because the underlying sentencepiece tokenizer doesn’t treat these specially; this makes it incredibly flexible for languages with non-European punctuation and tokenisation (for example Thai, Arabic and Mandarin Chinese). However it means it performs less well with strange punctuation and capitalisation. Consider the following description fragment from the Avito competition:\n\nЧтобы посмотреть весь ассортимент нашего магазина перейдите по ссылке в блоке справа ⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒/\nНАЛИЧИЕ ТОВАРА УТОЧНЯЙТЕ ПО КОТАКТНОМУ ТЕЛЕФОНУ./\nПродам Кулер для компьютера COOLER MASTER/\n/\nВ НАШЕМ МАГАЗИНЕ НА ТЕХНИКУ ДАЕТСЯ ГАРАНТИЯ!!!!!!/ ========================================/\n\nGoogle Translate gives a plausible translation:\n\nTo view the entire range of our store, follow the link in the block on the right ⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒ /\nCHECK OUT THE AVAILABILITY OF THE GOODS BY CONTACT PHONE. /\nSelling Cooler for computer COOLER MASTER /\n/\nIN OUR STORE IT IS GIVEN A WARRANTY !!!!!! /\n======================================== /\n\n\nMarianMT\nMarianMT (without breaking into sentences) loses a lot of the punctuation information, it drops the middle sentence about contacting by telephone, and misses some translations (МАГАЗИНЕ should be shop or store like in the Google translate, but it’s been just transliterated to magasine)\n\nTo look at the entire range of our store, you can cross-reference in the right-hand-hand box to sell the Cooler Master/// in our magasine to technic to produce garantium.\n\nIt’s very sensitive to the punctuation, if we just remove the last forward slash we get a bunch of hallucinated punctuation:\n\nTo look at the entire range of our store, I want you to cross-reference to the right-hand box in our computer, COLER MASTER// IN TECHNOLOGY, GARANTIA!!!!!!!!!!!!/=======================================================================================)============)========================================)=========================================== )))))))))))))))))) )))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n\nThis could be due to sentencepiece’s encoding being impacted by the punctuation (you can see Google translated broke apart the forward slashes from the spaces). What if we remove the extra punctuation?\n\nЧтобы посмотреть весь ассортимент нашего магазина перейдите по ссылке в блоке справа.\nНАЛИЧИЕ ТОВАРА УТОЧНЯЙТЕ ПО КОТАКТНОМУ ТЕЛЕФОНУ.\nПродам Кулер для компьютера COOLER MASTER.\nВ НАШЕМ МАГАЗИНЕ НА ТЕХНИКУ ДАЕТСЯ ГАРАНТИЯ!!!!!!\n\nStrangely MarianMT drops all but the first line (and it’s not because of the newlines):\n\nTo look at the entire range of our store, you can cross the link in the block on the right.\n\nTranslating it line by line gives a better result, but it’s more literal:\n\nTo see the full range of our store, cross the link in the block on the right.\nCash the product, get it off the phone.\nI’ll sell Cooler for COOLER MASTER.\nTHE TECHNOLOGY GUARANTIES IN OUR MAGAZINE!!!!!!!\n\nIf we first lowercase it all (using .lower() in Python) the line-by-line translation gets a little better:\n\nTo see the entire range of our store, cross the link in the block on the right.\nCheck whether the product is available on the kitty phone.\nI’m gonna sell a cooler master cooler.\nin our hardware store there’s a guarantee!!!!!!!!!!!\n\n\n\nArgos Translate\nArgos Translate gives a readable translation that drops some punctuation, and after the first sentence is mostly nonsense, and hallucinates a final line. Curiously translating ======== with MarianMT also gives a similar translation == sync, corrected by elderman == @elder_man'.\n\nIn order to look at the entire range of our stores, go through the reference to the right of the right, the State states that it states:\nI’d like you to take a look at the TEST. /\nI’ll sell Couler for COOLER MASTER/\n/\nI’ve got a lot of guitar! /\n== sync, corrected by elderman ==\n\nRemoving the extra punctuation doesn’t change the translation content much, except removing the hallucination.\n\nTo see our store’s entire range, you’re gonna have to go to the right block. I’d like you to take a look at the TEST.\nI’ll sell Couler for the COOLER MASTER computer.\nI’m in the middle of a guitar!\n\nPassing the sentences separately gives the same result. First lower-casing all the words gives a very slightly better translation:\n\nTo’ve seen the store’s all sorts of stuff, you’re gonna have to go to the right block.\nIf there’s a merchandise, please click on the cable phone.\nI’ll sell the cooler master computer.\nWe’ve got security at our hardware store!"
  },
  {
    "objectID": "energy-9v-battery/index.html",
    "href": "energy-9v-battery/index.html",
    "title": "How Much Energy is there in a 9V Battery",
    "section": "",
    "text": "Estimate the energy in a 9-volt battery. Is it enough to launch the battery into orbit?\n\nWe’re just going to estimate the first part.\n\nBattery Energy\nA volt is energy per unit charge \\(V = \\frac{E}{q}\\). To get towards an energy we need an amount of charge; the current in Ampere is the charge per unit time \\(I = frac{q}{t}\\). So the product \\(V I = \\frac{E}{t}\\) is energy per unit time, or power.\nMy smoke detector needs a 9V battery, and should be replaced every year. If I can estimate the current the smoke detector draws I can estimate the energy.\nThe current drawn can be guessed with gut estimates based on my experience with current. 1 A is a lot of current, it’s probably less than that, but it could be 100 mA. 1 mA is not much current it’s probably more than that, but it could be 10 mA. Guessing the geometric mean of 1 A and 1 mA gives 30 mA, or 0.03 A.\nThen the Energy in a 9V battery is given by\n\\[E = V I t = 9 \\rm{V} \\times 0.03 \\rm{A} \\times 3 \\times 10^{7} s = 10^{7} \\rm{J}\\].\nA common unit for energy in batteries is Watt Hours, which is 1/3600 J. So the energy is roughly 2500 Wh, or 2.5 kWh.\n\n\nChecking\nWikipedia lists the typical capacity of 9V batteries as around 500 mAh. This then corresponds to around 5 Wh of energy. I’ve overestimated by a factor of 500.\nIt’s easy to trace back where I went wrong; the most uncertain estimate was the current drawn by a smoke detector. According to energyrating.gov.au profile on Smoke Alarms the power drawn by a smoke alarm is less than 0.1 mW. So dividing the power by the 9 V gives a current drawn of around 0.01 mA.\nIt turns out my gut was completely wrong; I didn’t really know enough about currents to make a gut estimate. I would have to think about another way to estimate the energy that doesn’t rely on knowledge about currents that I don’t have."
  },
  {
    "objectID": "choosing-static-site-generator/index.html",
    "href": "choosing-static-site-generator/index.html",
    "title": "Choosing a Static Site Generator",
    "section": "",
    "text": "Static website generators compile input assets into a set of static HTML, CSS and Javascript files that can be deployed almost anywhere. They can do useful things like render formatting in pages, create indexes, RSS feeds, optimise images and minify assets. However different generators are built in different programming languages, have different features and conventions for assets, and different plugins.\nI’m going to say if you don’t know what to choose for a blogpost go with Jekyll. It’s one of the most popular generators, and one of the oldest (older than the Javascript frameworks used for many popular generators). This means that it’s featureful, stable, has a huge community and consequently tons of themes and plugins. It’s written in Ruby which is a dynamic language making it relatively easy to write plugins.\nThis site is currently using Hugo through a Github Action. I’ve also used it to publish Jupyter notebooks and R Blogdown posts. However Hugo makes breaking changes, which forced me to change my theme and doesn’t have an easy way to render diagrams.\nIt would be great to be able to put declarative diagrams inline with the article. Unfortunately it looks very unlikely Hugo will support PlantUML; someone would have to port it to Go. I ended up resorting to using Mermaid for diagrams but client side rendering makes it much slower to load and paint the page (it’s a major factor according to Google site tools). This makes it a less pleasant experience and hurts SEO. Jekyll will be much slower to generate the website than Hugo, but with something like the Jekyll PlantUML it could generate a website that’s much faster to load. Another way to do this would be with Blogdown, which renders the pages using RMarkdown which executes code in R then uses Pandoc to convert the output, and hands off to Hugo (or other generators) to make related assets from the HTML output.\nI’m not sure if it’s worth me switching to Jekyll now with it’s wide array of plugins and themes. I’ll have to investigate the tradeoffs more, and compare it to filling the gaps with RMarkdown. But I regret picking Hugo over the more stable, popular and featureful Jekyll. I’m not the only person who has had issues with Hugo, as can be seen on this Hacker news thread."
  },
  {
    "objectID": "recipe-ner-transformers/index.html",
    "href": "recipe-ner-transformers/index.html",
    "title": "Training Recipe Ingredient NER with Transformers",
    "section": "",
    "text": "The underlying training and test data is from A Named Entity Based Approach to Model Recipes, by Diwan, Batra, and Bagler. They manually annotated a large number of ingredients from AllRecipes.com and FOOD.com with the tags below.\n\n\n\nTag\nSignificance\nExample\n\n\n\n\nNAME\nName of Ingredient\nsalt, pepper\n\n\nSTATE\nProcessing State of Ingredient.\nground, thawed\n\n\nUNIT\nMeasuring unit(s).\ngram, cup\n\n\nQUANTITY\nQuantity associated with the unit(s).\n1, 1 1/2 , 2-4\n\n\nSIZE\nPortion sizes mentioned.\nsmall, large\n\n\nTEMP\nTemperature applied prior to cooking.\nhot, frozen\n\n\nDRY/FRESH\nFresh otherwise as mentioned.\ndry, fresh\n\n\n\nI have previously replicated their benchmark using Stanford NER, a Conditional Random Fields model. Here are the f1-scores reported in the paper (columns are training set, rows are testing set).\n\n\n\nBenchmark - Paper\nAllRecipes\nFOOD.com\nBOTH\n\n\n\n\nTesting Set\n\n\n\n\n\nAllRecipes\n96.82%\n93.17%\n97.09%\n\n\nFOOD.com\n86.72%\n95.19%\n98.48%\n\n\nBOTH\n89.72%\n94.98%\n96.11%\n\n\n\nWhile these may look impressive, using the model of predicting the most common label for each token, and O for out of vocabulary tokens, gets an f1-score over 92%. This is actually quite a simple problem because most tokens have a label and ambiguity is rare.\nI followed the process of training an NER with transformers from Chapter 4 of Natural Language Processing with Transformers by Tunstall, von Werra, and Wolf (using their public notebooks as a guide). There was marked improvement on using the smaller AllRecipes dataset (1470 training samples). However on the larger FOOD.com dataset (5142 training samples) the increase in performance was smaller, and on the combined dataset it was very marginal. Note this is keeping a validation set, and using the default hyperparameters from the text; I haven’t tried to optimise it at all or use every data point.\n\n\n\nTransformer (XLM Roberta)\nAllRecipes\nFOOD.com\nBOTH\n\n\n\n\nTesting Set\n\n\n\n\n\nAllRecipes\n96.94%\n95.73%\n97.34%\n\n\nFOOD.com\n91.64%\n96.04%\n95.77%\n\n\nBOTH\n92.9%\n95.96%\n96.15%\n\n\n\nI suspect the reasons it doesn’t do much better are because of inconsistencies in the annotation and hitting the limits. Running an error analysis, as per the NLP with transformers text, showed some issues. Often only the first of multiple ingredients is annotated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntoken\n1\nteaspoon\norange\nzest\nor\n1\nteaspoon\nlemon\nzest\n\n\n\n\nlabel\nQUANTITY\nUNIT\nNAME\nNAME\nO\nO\nO\nO\nO\n\n\n\nIn this case all but the last ingredient name is annotated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntoken\n1/4\ncup\nsugar\n,\nto\ntaste\n(\ncan\nuse\nhoney\n,\nagave\nsyrup\n,\nor\nstevia\n)\n\n\n\n\nlabels\nQUANTITY\nUNIT\nNAME\nO\nO\nO\nO\nUNIT\nO\nNAME\nO\nNAME\nNAME\nO\nO\nO\nO\n\n\n\nThe inconsistencies confused both me and the model. There are instances in both “firm tofu” and “firm tomatoes” where firm is considered part of the name, and others where it is part of the state. Similarly in “stewing beef”, stewing is sometimes a state and sometimes part of the name. Though there were real issues in the model; it couldn’t distinguish “clove” in “garlic clove” (a unit), from “ground cloves” (a name).\nAn amazing thing about using a multilingual transformer model like XLM Roberta is it has some zero-shot cross-language generalisation. Even though all the examples are English it does better than random on other languages. Admittedly the pattern of ingredients makes it easier (e.g. a numerical quantity, followed by a unit, followed by a name), but it picked up some other things. I didn’t have a dataset to test on, but tried it on a few examples I could find. If you want to try more you can try it in the Huggingface model hub and share what you find.\nAs you might expect it does well on a French example, where there’s a lot of similar vocabulary. However any model relying on token lookups would not be able to learn this from the training data.\n\n\n\ntoken\n1\npetit\noignon\nrouge\n\n\n\n\ntranslation\n1\nsmall\nonion\nred\n\n\nactual\nI-QUANTITY\nI-SIZE\nI-NAME\nI-NAME\n\n\nprediction\nI-QUANTITY\nI-SIZE\nI-NAME\nI-NAME\n\n\n\nGoing a bit further afield to Hungarian it certainly does better than random. Here’s an example where it only makes a mistake on one entity; but picks up that fagyasztott is not part of the name.\n\n\n\ntoken\n1\ncsomag\nfagyasztott\nkukorica\n\n\n\n\ntranslation\n1\npacket\nfrozen\ncorn\n\n\nactual\nQUANTITY\nUNIT\nTEMP\nNAME\n\n\nprediction\nQUANTITY\nUNIT\nSTATE\nNAME\n\n\n\nHere’s another Hungarian example where it gets the name wrong because it missed the unit (konzerv).\n\n\n\ntoken\n50\ndkg\nkukorica\nkonzerv\n\n\n\n\ntranslation\n50\ndkg (10g)\ncorn\ncanned\n\n\nactual\nQUANTITY\nUNIT\nNAME\nUNIT\n\n\nprediction\nQUANTITY\nUNIT\nNAME\nNAME\n\n\n\nHowever here’s a harder example that it gets precisely right.\n\n\n\ntoken\nőrölt\nfehér\nbor\n\n\n\n\ntranslation\nground\nwhite\npepper\n\n\nactual\nSTATE\nNAME\nNAME\n\n\nprediction\nSTATE\nNAME\nNAME\n\n\n\nRussian should be even harder since it’s a different script, although is straightforward to transliterate. However here’s an example that it gets exactly right:\n\n\n\ntoken\nСало\nсвиное\nсвежее\n-\n50\nг\n\n\n\n\ntranslation\nfat\nport\nfresh\n-\n50\ng\n\n\nactual\nNAME\nNAME\nDF\nO\nO\nI-UNIT\n\n\nprediction\nNAME\nNAME\nDF\nO\nO\nI-UNIT\n\n\n\nIf one wanted to extend this model to one of these other languages the existing predictions would be a good way to start. Then annotators could correct the mistakes, especially where the model is unsure, which is much faster than manually labelling every token. In this way a good training set could be constructed relatively quickly by bootstrapping from another language. For more ideas on dealing with few to no labels, see Chapter 9 of the Natural Language Processing with Transformers book.\nTo take the model further we could fix the annotation errors, in particular multiple annotations within an ingredient, and retrain the model. We could also annotate more diverse ingredient sets; the NY Times released a similar ingredient phrase tagger along with training data (and the corresponding blog post is informative). However the tagger is already really very good.\nThough really the model is really good and a better thing to do would be to run it over a large number of recipe ingredients to extract information. There are many recipes that can be extracted from the internet; for example using Web Data Commons extracts of Recipes, recipes subreddit (via pushshift), or exporting the Cookbook wikibook, or using OpenRecipes (or their latest export). Practically the CRF model is likely a better choice since it works roughly as well and would run much more efficiently. Then you could look at which ingredient names occur together, estimate nutritional content, convert quantities by region or do more complex tasks like suggest ingredient substitutes or generate recipes."
  },
  {
    "objectID": "really-mean-rn/index.html",
    "href": "really-mean-rn/index.html",
    "title": "Do you really mean ℝⁿ?",
    "section": "",
    "text": "Consider mechanics or geometry, these are often represented as theories in \\(\\mathbb{R}^n\\) , but really they don’t occur in a vector space at all! Look around you, a three-dimensional description of space probably seems reasonable, but where’s the origin? [Perhaps the centre of your eyes could be an origin, but someone else would disagree with you]. Classical mechanics, special relativity and geometry are much better described as an affine space – which is a vector space without an origin.\n\nOf course to do any calculations we need to choose an origin in the affine space and choose some axes, and this allows us to represent our space as \\(\\mathbb{R}^n\\) . But if we calculate the volume of a box (relative to another box) using two different origins we expect to get the same answer. This is precisely what the affine structure says: our results are independent of our choice of origin (that is they are invariant/covariant with respect to translations).\nNow suppose you have a genuine \\(n\\)-dimensional vector space (perhaps you’ve chosen an origin in your geometry), we can now call this \\(\\mathbb{R}^n\\) right? Wrong; \\(\\mathbb{R}^n\\) has a natural ordered basis \\(e_1=(1,0,\\ldots,0), \\ldots, e_n=(0,\\ldots,0,1)\\) . Again, taking you as the origin in the space around you – what are the three basis vectors? There are lots of choices!\nAlright so let’s choose \\(n\\) (ordered) linearly independent basis vectors, by identifying the the \\(i^\\mathrm{th}\\) one with \\(e_i\\) (for the space around us we could choose (magnetic) North, (magnetic) East and (gravitational) up), and this gives us axes. But there’s nothing super-special about these choices – again our box-volume should be independent of our choice of axes. This is encoded in the vector space structure: there’s no canonical choice of basis, and all physical quantities should be properly covariant with an appropriate change of basis.\nFine — we’ve taken an \\(n\\)-dimensional vector space and chosen an ordered basis — surely we can now call it \\(\\mathbb{R}^n\\) ? Well it depends on how much extra structure you have on your vector space. The ordered basis for \\(\\mathbb{R}^n\\) gives it a canonical orientation. There is a canonical inner product on \\(\\mathbb{R}^n\\) , the dot product \\(\\langle e_i, e_j \\rangle = \\delta_{ij}\\) . (It is exceedingly rare to see the distinction made between \\(\\mathbb{R}^n\\) as a vector space, an oriented vector space and an inner product space.) So your vector space must at least have an orientation and an inner product (and your choice of ordered basis must be oriented and orthonormal) – but this is a lot of extra structure to throw in.\nA lot can be said for the geometry of affine spaces over a vector space (that is without a metric or orientation). There is no concept of absolute volume (this is given by a metric), but there is a relative notion of oriented volume for \\(n\\)-dimensional parallelograms (which is essentially “how many copies of parallelogram 1 fit into parallelogram 2″ with the conditions that volume is translation invariant and scaling a side by some factor scales the volume by the same factor). In fact we can talk about the relative volume for any hyper-polygon (by cutting it into triangular pieces – a parallelogram is just two triangles stuck together) and approximate the relative volume of other objects such as spheres (this is how a lot of Greek geometry was done). There is also the notion of an affine transformation, one that preserves the affine structure (essentially a shift followed by a linear transformation), and a determinant which is the amount by which an affine transformation scales the volume of \\(n\\)-dimensional parallelograms.\nIf we push even further to locally affine transformations the determinant will naturally generalise to the Jacobian of the transformation, and one can define the exterior derivative. Or thinking of spaces that are locally affine leads to manifolds. But this is a whole essay in itself.\nOf course we do often have more structure – Euclidean geometry, classical mechanics, Electrodynamics and special relativity all have metrics – a notion of absolute volume. But it is interesting to separate the metric independent behaviour (for instance in Electrodynamics the Maxwell equations \\(\\mathrm{d} \\mathbf{F}=0\\) ) from the metric behaviour ( \\(\\mathrm{d} \\star \\mathbf{F} = \\mathbf{J}\\) ). Certainly it is useful to write equations in a form where they are explicitly independent of the choice of origin and basis (as opposed to writing them in \\(\\mathbb{R}^n\\) ) – this is the advantage of Lagrange’s method of mechanics over Newton’s.\nWe should explicitly distinguish these structures from \\(\\mathbb{R}^n\\). A vector space should just be denoted by a letter like V, with possibly a superscript to denote its dimension like \\(V^{(n)}\\) . An affine space could be denoted \\(A^{(n)}\\) , an affine space with a positive definite inner product is called Euclidean space \\(E^{(n)}\\) and an affine space with an inner product signature \\((n-1,1)\\) is called Minkowski \\(n\\)-space \\(M^{(n)}\\) . I would advocate using \\(E^{(m,n)}\\) for an affine space with an inner product signature \\((m,n)\\).\nAs I’ve said at the end of the day to do calculations we will go into a specific representation which may look like \\(\\mathbb{R}^n\\) and this is fine – but it is important to state and develop the theory in ways that are independent of these choices.\nEnd Note: Given a basis for a vector space V \\(\\{e_1,\\ldots,e_n\\}\\) we can define a dual basis for \\(V^*\\), \\(\\{e^1,\\ldots,e^n\\}\\) by \\(e^i(e_j) = \\delta^i_j\\) . The mapping of a basis onto its dual gives an isomorphism from \\(V\\) to \\(V^*\\), but this isomorphism isn’t natural. This has a specific categorical meaning but roughly its because if we have a linear transformation \\(L\\) on \\(V\\), \\(L\\) has a natural action on \\(V^*\\). In terms of the matrix of the transformation the action on \\(V^*\\) is the adjoint matrix (that is the transpose or Hermitian conjugate depending if the matrix is real or complex). But the dual basis transforms by the inverse to maintain \\(e^i(e_j) = \\delta^i_j\\) . So the adjoint action preserves the identification of bases (and so the isomorphism is independent of our choice of basis) if and only if \\(T^* = T^{-1}\\) , which means the matrix is orthogonal or unitary. However apriori there is no reason to only use this type of linear transformation so the identification is not canonical. If, however, there is a metric on our vector space we should only use linear transformations that preserve this metric, and in terms of an orthonormal basis, this means we only use orthogonal/unitary transformations. So a metric induces a canonical isomorphism between a vector space and its dual! It is not too hard to see conversely that an isomorphism between a vector space and its dual gives a metric.\nThis is another reason (related to the existence of a natural inner product) a vector space should not be represented by \\(\\mathbb{R}^n\\) – it is canonically identified with its dual."
  },
  {
    "objectID": "jarvislab-ssh-config/index.html",
    "href": "jarvislab-ssh-config/index.html",
    "title": "Automatically updating SSH Config for Jarvislabs",
    "section": "",
    "text": "Jarvislabs is a very cost efficient cloud GPU provider for deep learning. One slight issue I ran into is that every time you resume an instance it gets a different SSH host and port. If you’re using their Jupyter notebook interface this is fine, but I wanted to control my own environment more carefully and would SSH into the instance.\nI wanted to hide this by hiding it behind an SSH configuration hostname, with the credentials updating automatically when I resume the instance. Luckily they provide an API and so I was able to hack a script to do this in a couple of hours. This means I can just start the instance from the command line and then run ssh myinstance without having to visit the web UI.\nWe will walk through how we do this, first by getting the instance from the Jarvislabs API, then parsing and updating the SSH config."
  },
  {
    "objectID": "jarvislab-ssh-config/index.html#authenticate-to-jarvisclient",
    "href": "jarvislab-ssh-config/index.html#authenticate-to-jarvisclient",
    "title": "Automatically updating SSH Config for Jarvislabs",
    "section": "Authenticate to Jarvisclient",
    "text": "Authenticate to Jarvisclient\nThe jarvisclient library requires a global configuration. After generating an API token I set them as environment variables that can be read in (these could be stored in other ways too).\nimport os\nfrom jlclient import jarvisclient\n\njarvisclient.token = os.getenv('JARVISLABS_TOKEN')\njarvisclient.user_id = os.getenv('JARVISLABS_USER_ID')\n\nassert jarvisclient.token is not None\nassert jarvisclient.user_id is not None"
  },
  {
    "objectID": "jarvislab-ssh-config/index.html#getting-jarvislabs-instance-by-name",
    "href": "jarvislab-ssh-config/index.html#getting-jarvislabs-instance-by-name",
    "title": "Automatically updating SSH Config for Jarvislabs",
    "section": "Getting Jarvislabs instance by name",
    "text": "Getting Jarvislabs instance by name\nNow we need to get our set up instance to resume and get the SSH details. The library doesn’t provide a way to do get an individual instance, so we search through all the instance and find the one with a matching name. This particular function assumes that a single instance by that name exists; if it doesn’t exist it will return an error.\nfrom jlclient.jarvisclient import Instance, User\n\ndef get_instance_by_name(name: str) -> Instance:\n    instance = [i for i in User.get_instances() if i.name==name][0]\n    return instance\nNow we have our Instance we can .resume() it and then update our SSH configuration."
  },
  {
    "objectID": "jarvislab-ssh-config/index.html#parsing-ssh-configuration",
    "href": "jarvislab-ssh-config/index.html#parsing-ssh-configuration",
    "title": "Automatically updating SSH Config for Jarvislabs",
    "section": "Parsing ssh configuration",
    "text": "Parsing ssh configuration\nWe can parse this string with some simple regex. We will wrap it in a simple SSHConfig dataclass to make it easy to update the configuration, as explained in the next section.\nimport re\n\ndef parse_ssh_config(s: str) -> SSHConfig:\n    match = re.match(r'^ssh -p (?P<Port>[0-9]+) (?P<User>[a-z]+)@(?P<Hostname>[a-z\\.]+)$', instance.ssh_str)\n    return SSHConfig(**match.groupdict())"
  },
  {
    "objectID": "jarvislab-ssh-config/index.html#updating-ssh-configuration",
    "href": "jarvislab-ssh-config/index.html#updating-ssh-configuration",
    "title": "Automatically updating SSH Config for Jarvislabs",
    "section": "Updating SSH Configuration",
    "text": "Updating SSH Configuration\nSSH configuration is documented in the ssh_config(5) man page, and is fairly simple to parse. However it’s complicated enough I used the sshconf library to update it for me. It provides a handy .set(section, **kwargs) method to update arguments in a given Host section of the SSH file.\nimport dataclasses\nfrom dataclasses import dataclass\nfrom sshconf import SshConfig\n\n@dataclass\nclass SSHConfig:\n    Port: int\n    User: str\n    Hostname: str\n\n    def update(self, config: SshConfig, section: str) -> None:\n        config.set(section, **dataclasses.asdict(self))"
  },
  {
    "objectID": "spellcheck-article/index.html",
    "href": "spellcheck-article/index.html",
    "title": "Spellchecking Articles with Aspell",
    "section": "",
    "text": "The Emacs manual on spelling says it supports Hunspell, Aspell, Ispell and Enchant. For English the choice is between Hunspell and Aspell; Ispell is a predecessor or Aspell, and Enchant wraps other spelling libraries (most notably specific ones for Turkish and Finnish). Running some examples I found Aspell gave much better suggestions so I decided to go with that.\nFor spell checking existing posts I found a guide from ThorneLabs to output all the words Aspell marks as misspelled, which I adapted a little.\nfor POST in content/post/*\ndo\n    cat $POST | aspell list --ignore 2 --mode markdown --camel-case -l en_AU\ndone | sort | uniq -c | sort -nr | sed -E 's/^ *[0-9]+ *//' > words.txt\nThe flags are:\n\n--ignore 2 ignore any words of 2 or less characters\n--camel-case accept CamelCased words as valid spellings (occurs a lot in some articles like job posting schemata)\n-l en_AU set the language to Australian English\n--mode markdown treat the files as markdown\n\nThe pipes at the end are a shell transformation to sort the words Aspell by frequency, and output it to a file called words.txt. I could then go through this file and sort out valid words not in Aspell’s dictionary from actually misspelled words. Sometimes I would need to check the context using grep -r '\\bmodularity\\b' content/post, or check Aspell’s suggestions with echo modularity | aspell -a.\nI ended up with a list of words that I use frequently that needed to be added to my personal dictionary at $HOME/.aspell.en.pws. These were a mixture of technical words (integrable, geocoder, quantile, submodule), technologies (Jupyter, spaCy, Javascript), acronyms (WSL, NLP, RDF), contractions (codebase, whitespace, substring, postamble) and TeX commands (frac, ldots, infty). Unfortunately I had to remove any phrases (such as ad hoc), words with hyphens (such as Gell-Mann), or unicode characters (such as Dieudonné or Möbius) from the dictionary or Aspell would complain.\nI ended up with a second list of words that I’d misspelled frequently, and could run back through aspell to correct. Apparently I commonly misspelled words like hierarchy, manageable, metres, occurred, focusing, ridiculous, and inheritance. Another thing I get wrong is capitalisation of name like GitHub, TensorFlow, or Emacs. I fixed the most common ones and ones that were obviously wrong with sed, then viewed the diff in Magit (using magit-diff-refine-hunk to highlight the changed words as in the cover image). There were a couple cases I needed to remove, like changes in URLs or quotes, but the majority changes were correct.\nI then could add the dictionary to my dotfiles, and use it in Emacs through the ispell commands or flyspell commands (which I still need to configure). I’m not the only person to have this idea and it’s interesting to see other people’s personal dictionaries."
  },
  {
    "objectID": "efficiency-747/index.html",
    "href": "efficiency-747/index.html",
    "title": "Fuel Efficiency of a 747",
    "section": "",
    "text": "Based on the cost of a long-distance plane ticket, estimate the following quantities: (a) the fuel efficiency of a 747, in passenger miles per gallon or passenger kilometres per liter; and (b) the volume of its fuel tank. Check your estimates against the technical data for a 747.\n\nA flight from Melbourne to Las Angeles direct costs about $1,000.\nThere are a number of costs of a long distance flight; such as crew, food, and airport fees. It’s reasonable to assume the majority of the cost is fuel. There’s also some profit margin for the airline, and allowance for a partially filled plane. Assume fuel is around 80% of the costs, or $800 per passenger.\nJet fuel is more expensive than car fuel (being higher octane), but airlines would get discounts buying the fuel in bulk. Assume that the cost of jet fuel is about the same as petrol $1.50 per litre.\nThen the fuel usage is $800 per passenger divided by $1.50 per litre, or 500 litres per passenger.\nMelbourne and Las Angeles are roughly on opposite sides of the world. The world is about 6,000 km radius. So halfway around the world is about 15,000 kilometres.\nThen the fuel efficiency of a 747 is about 15,000 kilometres divided by 500 litres per passenger, which is about 30 passenger kilometres per litre.\nMelbourne to Las Angeles is around the limit of distance for a 747, so it would almost use a full fuel tank. A 747 has about 50 rows of 10 seats, which allows around 500 passengers. So the total volume is about 500 passengers by 500 litres per passenger which is 250,000 litres.\n\nChecking the results\nI can get the specifications of 747s from Wikipedia which references the actual technical sheets.\nA 747 can have about 50,000-60,000 US Gallons of fuel which is about 190,000 to 230,000 litres. So my estimate is of 250,000 litres is quite reasonable. This is quite impressive, the plane is about 200 tonnes heavier at takeoff than at landing.\nThe range of a 747 is about 12,000 kilometres, and typically has about 400 seats. So the fuel efficiency is about 12,000 kilometres x 400 passengers / 250,000 litres, or 20 passenger kilometres per litre. This is not too far from my estimate of 30 passenger kilometres per litre, and is comparable with car travel."
  },
  {
    "objectID": "minhash/index.html",
    "href": "minhash/index.html",
    "title": "Detecting Near Duplicates with Minhash",
    "section": "",
    "text": "MinHash is a very clever probabilistic algorithm that trades off time and memory for accuracy, and was developed at Alta Vista for clustering similar web pages. The algorithm finds elements that have a large approximate Jaccard index; which we’ve already seen is effective for finding similar texts. The underlying idea is if you randomly order the items from the sets the chance that the smallest item is in both sets is equal to the number of elements in both sets divided by the number of elements in either set - which is exactly the Jaccard index. We can create a number of these orderings efficiently with Universal Hashing and only need to store the minimum element of each for each hash function. Then the Jaccard index between two items is approximately the number of equal minimum elements from the different hash functions. There is some chance of false collisions with the hash functions, but with several hash functions the impact is smaller.\nThere’s actually a whole family of these sketch algorithms that tradeoff time and memory for precision in the same way Jaccard does like the Bloom Filter for set membership, HyperLogLog for counting distinct items, and the Count-min sketch for frequency tables.\nThe MinHash can be used to efficiently search through near-duplicates using Locality Sensitive Hashing as explored in the next article. In this article we’ll look at calculating the minhash and how well it works as an approximation to Jaccard.\n\nImplementation\nI use the excellent datasketch library for calculating the MinHash and finding near duplicates. We can calculate the minhash of an arbitrary sequence of strings with this function.\nfrom datasketch import MinHash, LeanMinHash\nimport xxhash\ndef minhash(seq:List[str], num_perm=num_perm):\n    m = MinHash(num_perm=num_perm, hashfunc=xxhash.xxh64_intdigest)\n    for s in seq:\n        m.update(s.encode('utf8'))\n    return LeanMinHash(m)\nNote that we’re using xxhash to hash the items because it’s much faster than the default SHA1 from Python’s hashlib. The parameter num_perm corresponds is the number of different hash functions to use; setting it higher improves accuracy but runs slower; I used 128. It returns a LeanMinHash to save some memory.\n\n\nHow effective is the MinHash approximation?\nAs a starting point I tested it on all pairs of the first 100 job ads:\nminhashes = []\nfor i in range(100):\n    minhashes.append(minhash(tokenize(ads[i])))\n\nmh_distances = {}\nfor i in range(100):\n    for j in range(100):\n        if i < j:\n            mh_distances[(i, j)] = minhashes[i].jaccard(minhashes[j])\nand compared the result with the exact Jaccard distance:\njaccard_distances = {}\nfor i in range(100):\n    for j in range(100):\n        if i < j:\n            jaccard_distances[(i,j)] = jaccard(tokenize(ads[i]), tokenize(ads[j]))\nThe minhash values tended to be slightly higher, and look normalish with a standard deviation of 0.025:\nimport matplotlib.pyplot as plt\ndiffs = [jaccard_distances[k] - v for k,v in list(mh_distances.items())]\np = plt.hist(diffs, bins=30)\n\n\n\nMinhash Error distribution\n\n\nHowever it doesn’t matter so much how much error there is in general because we’re going to use it to try to detect pairs with a high overlap. So what we really want to know is what’s the relative error for pairs. Doing a scatter plot of them makes it clear the approximation works quite well (as we’d expect on theoretical grounds), and we don’t need to increase the number of permutations:\nplt.hexbin([jaccard_distances[k] for k in mh_distances], list(mh_distances.values()), bins='log')\nplt.xlabel('Jaccard Distance')\nplt.ylabel('MinHash approximation')\nplt.title('MinHash correlates very well with Jaccard')\n\n\n\nMinHash vs Jaccard\n\n\n\n\nMinHashing a Corpus\nI created a convenience function to calculate MinHashes for a corpus. This is the time consuming step; these can be stored (e.g. with pickle) for later search.\ndef get_minhashes(corpus, n_shingle=1, preprocess=tokenize):\n    return [minhash(shingle(preprocess(text), n_shingle)) for text in corpus]\nThis takes about 30 minutes on a single core for the 400,000 job ads (it is trivially paralellisable). The output pickle file is 200MB, so it’s about 0.5kB per job ad (where we’re essentially representing the ad as a bag of tokens).\ni = 3\nminhashes = get_minhashes(ads, n_shingle=i)\nwith open(f'minhash_{i}.pkl', 'wb') as f:\n    pickle.dump(minhashes, f)"
  },
  {
    "objectID": "making-words-singular/index.html",
    "href": "making-words-singular/index.html",
    "title": "Making Words Singular",
    "section": "",
    "text": "Normally I would use a library for something like this but I couldn’t find anything that would work. The inflect library has a singularize_noun function that works on plural words, but it has no way of detecting whether a noun is plural and changes singular words. Similarly textblob has a singularize function, but it is largely based on inflect and has the same issues. Some examples are “gas” becomes “ga”, “bus” becomes “bu” and “analysis” becomes “analysi”.\nOne way to deal with this is to use a part of speech tagger, like SpaCy’s, to identify the plural nouns. If the part of speech is NNS (plural noun) or NNPS (plural proper noun) then we run singularize_noun. However bringing in a POS tagger seems like a big thing for a simple task, and it’s going to be hard to fix where the part of speech is misclassified. I wanted to see if I could build some simple rules to do it myself.\nBetween the infect library and this article on plurals there is a lot of material on building a robust algorithm to make words plural or singular. But I needed something simple for my use case, so I started with the simplest rule, drop the trailing “s”. Then I looked through the most common words in my dataset to find exceptions and infer rules; like bus, plus, fabulous and sous I wonder if ending in “us” is a general exception. Then I would look through the most frequent words ending in “us” and find the rule applies except for in “menus”.\nThrough this I built up a list of rules and test cases:\n\n\n\n\n\n\n\n\n\n\nOriginal Form\nSingular Form\nOriginal Suffix\nSingular Suffix\nOriginal Plural\n\n\n\n\naccounts\naccount\ns\n(None)\nYes\n\n\nexecutives\nexecutive\nes\ne\nYes\n\n\nnannies\nnanny\nies\ny\nYes\n\n\nmidwives\nmidwife\nwives\nwife\nYes\n\n\nsous\nsous\nus\nus\nNo\n\n\nmenus\nmenu\nus\nu\nYes\n\n\nbusiness\nbusiness\nss\nss\nNo\n\n\nanalysis\nanalysis\nis\nis\nNo\n\n\nworkmen\nworkman\nmen\nman\nYes\n\n\nsalespeople\nsalesperson\npeople\nperson\nYes\n\n\nsales\nsales\nsales\nsales\nNo\n\n\nchildren’s\nchildren’s\n’s\n’s\nNo\n\n\ngas\ngas\ngas\ngas\nNo\n\n\ngeophysics\ngeophysics\nphysics\nphysics\nNo\n\n\nasbestos\nasbestos\nasbestos\nasbestos\nNo\n\n\n\nNote these rules are applied with the most specific rule first; for example nannies ends in “ies” and “es”; but “ies” is more specific so we get nanny and not nanne. These are nowhere near comprehensive but are a good start for normalising role titles.\nI then build these rules into a function:\nSINGULAR_UNINFLECTED = ['gas', 'asbestos', 'womens', 'childrens', 'sales', 'physics']\n\nSINGULAR_SUFFIX = [\n    ('people', 'person'),\n    ('men', 'man'),\n    ('wives', 'wife'),\n    ('menus', 'menu'),\n    ('us', 'us'),\n    ('ss', 'ss'),\n    ('is', 'is'),\n    (\"'s\", \"'s\"),\n    ('ies', 'y'),\n    ('ies', 'y'),\n    ('es', 'e'),\n    ('s', '')\n]\ndef singularize_word(word):\n    for ending in SINGULAR_UNINFLECTED:\n        if word.lower().endswith(ending):\n            return word\n    for suffix, singular_suffix in SINGULAR_SUFFIX:\n        if word.endswith(suffix):\n            return word[:-len(suffix)] + singular_suffix\n    return word\nThere are numerous places this will fail; wolves, wives, oxen, crises and many others. These rules could be further expanded using the rules from inflect, but you would have to be careful not to break singular words. For example it has a rule “lves” -> “ves” which is fine for wolves, calves and shelves, but will break evolves and involves.\nThere are also some proper nouns in my data, like Leeds and Wales, that neither algorithm can handle. This may be where a part of speech approach may be more powerful.\nHowever this simple set of rules is useful enough for normalising role titles in the Adzuna job ads."
  },
  {
    "objectID": "analytics-melbourne/index.html",
    "href": "analytics-melbourne/index.html",
    "title": "Finding Analytics in Melbourne",
    "section": "",
    "text": "At the time I had never heard of Hitwise and had no idea of the kind of data processing that they were doing. Now I’ve got a few more years of experience in the field and know quite a few more people I’m aware of quite a few companies that do analytics in Melbourne. But there must be dozens more that I’ve never heard of doing interesting work.\nHow can we find companies doing analytics in Melbourne? Silverpond have a (incomplete) list of organisations working on AI in Victoria in their 2019 MLAI Survey on pages 19-21. Searching the organisations many of them have had representatives present at Melbourne Analytics (for example here’s a past agenda). A few of these have presented at Data Science Melbourne, for example on how to be a Data Science Consultant, Startup Data Science and other talks.\nThis gives a reasonable starting point for a walk through analytics companies. Look for appropriate conferences, courses and events in the analytics space. Follow up speakers, sponsors and attendees and the companies they work for. Look at who works at those companies and where else they have worked. To get real depth start talking to those people and finding out more about them.\nAnother approach for finding medium to large companies is to monitor job postings. Keep saved searches of relevant analytics terms and start collating the companies that appear.\nI think it would be really interesting to map this out at a high level; but it’s going to take a lot of work."
  },
  {
    "objectID": "specific-brand/index.html",
    "href": "specific-brand/index.html",
    "title": "Targeting my brand",
    "section": "",
    "text": "My friend has four different magnets for plumbers on his fridge. Three of them are generic rectangular magnets that have generic information and contact details. One of them was in the shape of a dripping tap, mentioning they were experts in leaks and drips. If they had a leaking faucet it’s pretty easy to guess which plumber they would call; the specialists in dripping taps. On the other hand if they had a clogged toilet it’s down to chance which of the plumbers they would call, although they’re less likely to call the dripping tap specialist they’re also more likely to forget to look at the fridge and just search for a plumber online.\nPeople will generally assign a higher value to a specialist than a generalist. Would you rather have heart surgery performed by an experienced cardiac surgeon who has done that specific operation hundreds of times, or by a generalist who has performed hundreds of surgeries, but only a couple of heart surgeries? If you were moving a valuable antique piano, would you hire a general furniture removalist who says they will move pianos, or pay a premium for an antique piano removalist? If you found a rat infestation in your house would you be more likely to start searching for “pest control” or “rat catcher”? When you have a specific problem you first look to people who are experts in solving that exact problem; which is why it can be effective for a service provider to have a landing page for different usecases.\nThis can be counterintuitive from the perspective of the service provider. A trained plumber can fix a leaking tap, a blocked toilet, a burst pipe and many other issues. Why limit their potential market by narrowing their focus to a specific type of problem? But by advertising themselves to fix a particular problem they stand out for people with that problem, and may actually get more leads.\nIt’s easy to apply these in the abstract, but it’s much harder to do in practice. I’ll explore some different ways I could position myself."
  },
  {
    "objectID": "specific-brand/index.html#data-analyst",
    "href": "specific-brand/index.html#data-analyst",
    "title": "Targeting my brand",
    "section": "Data Analyst",
    "text": "Data Analyst\nI currently refer to myself as a “data analyst”, which is a very generic term. And I can do all the standard tasks of an analyst:\n\nWork with a decision maker to understand their objective and the levers they can pull\nWork with business stakeholders, database administrators and developers to identify the relevant data to capture\nWrangle, extract and clean data from databases and third party sources\nApply appropriate analytical, statistical and machine learning techniques to solve the problem\nCommunicate insights and recommendations to stakeholders to help them make better decisions\nEffectively break down and coordinate the problem across a team, managing stakeholder expectations\nBuild and serve production data pipelines, dashboards, and machine learning products\n\nIt’s useful to use the term data analyst (or data scientist), because it’s a commonly known position that people advertise for. However I don’t stand out from the crowd; why would a hirer employ me over any other data analyst?\nI could perhaps refine this by focusing on the areas I’m better at and want to be known for. I genuinely enjoy working with a decision maker to understand their objective, current perspective and framing the problem. Doing this right is immensely important to make sure we’re working on the right problem. My real passion is diving deep into the problem domain and data to understand the problem and finding the insights that will improve outcomes. I’m always happy using whatever techniques will help shed light on the problem, and I really relish new challenged. Finally I appreciate the importance of communication in enacting change, and will work hard on the communication piece to make sure the insights are understood by the stakeholders.\nI could position myself as working in Advanced Data Analytics, or Applied Data Science. I’ve also heard specific alternatives to Data Science such as Product Science and Decision Science. I know I’m not a reporting analyst or dashboard designer because I really enjoy solving new problems more than building consistent reports and dashboards."
  },
  {
    "objectID": "specific-brand/index.html#differential-geometer",
    "href": "specific-brand/index.html#differential-geometer",
    "title": "Targeting my brand",
    "section": "Differential Geometer",
    "text": "Differential Geometer\nIt would be very unwise for me to position myself as a differential geometer, because no one is looking for them. However it’s quite relevant to a lot of my work. My thesis was on the differential geometry of 4-manifolds and I have a very solid grounding in multivariable calculus. And it turns out that most machine learning algorithms are a combination of linear algebra and differential calculus. Machine learning provides a toolkit for building models for predicting how changes to inputs will yield changes to outcomes under uncertainty, and so is incredibly useful in optimal decision making.\nLikewise I have an academic grounding in measure theory and experimental physics which puts me in a good place for understanding statistics. Statistics is incredibly useful in extracting the signal from the noise, measuring which changes actually lead to improvement and for building the models mentioned above. Even better statistician is a recognised occupation in industry, unlike differential geometer.\nThese are both poor positioning because they talk about the tools to solve the problem rather than the problem to be solved. Very few people would connect trying to work out how to increase revenue with differential geometry, calculus or linear algebra. Similarly if I had a plumbing problem I wouldn’t search for a “plunger operator” or “drain snaker”, let alone the more specific versions of these."
  },
  {
    "objectID": "specific-brand/index.html#python-data-analyst",
    "href": "specific-brand/index.html#python-data-analyst",
    "title": "Targeting my brand",
    "section": "Python Data Analyst",
    "text": "Python Data Analyst\nFocussing on a solution can work when there’s a known need. This is why tool-focused jobs like Tableau Analyst, React Developer and AWS Architect work. A large company has already decided to invest heavily in some tool will look to hire people who specifically have experience with that tool (sometimes overlooking the other non-technology requirements of the role).\nPositioning myself as a Python Data Analyst communicates my technical skills, taking me away from pure reporting roles, and is associated with machine learning and advanced analytics. However being more technical it ignores the valuable skills of being able to frame a problem, understand the context, and communicate the outcomes. Again it’s not close enough to an actual problem domain."
  },
  {
    "objectID": "specific-brand/index.html#digital-behaviour-analyst",
    "href": "specific-brand/index.html#digital-behaviour-analyst",
    "title": "Targeting my brand",
    "section": "Digital Behaviour Analyst",
    "text": "Digital Behaviour Analyst\nMost of my professional career has centred around analysing user behaviour. Concepts like sessions, conversion funnels, overlaps (how many people view both of these pages), and behavioural segments are very familiar to me. While I can use typical digital analytics tools like Google Analytics and Adobe Analytics, I’m much more comfortable using databases, even if they’re an extract from these products. I like connecting my understanding of flows through a website with what I see in the data, and connecting it to the products. An extreme example of this is when I reverse engineered a database of a customised Yellowfin instance to understand how customers were using the site.\nThis is better than any of the previous definitions because it selects a type of customer. I can help companies that have a digital product such as a website or an app, and store or export the behavioural data to a database for analysis. This also sets a minimum scale; I probably can’t offer much to companies with under 100 customers."
  },
  {
    "objectID": "specific-brand/index.html#moving-towards-a-problem",
    "href": "specific-brand/index.html#moving-towards-a-problem",
    "title": "Targeting my brand",
    "section": "Moving towards a problem",
    "text": "Moving towards a problem\nNone of my iterations are hitting the critical point of a problem someone is looking to solve. I know that I enjoy framing problems, using analytical techniques to solve them, and communicating and implementing solutions that leads to better outcomes. I prefer using statistical techniques and technical tools to enable new perspectives for hard problems. I have experience understanding user behaviour on digital products to help optimise the experience, influence strategic decisions, and monetise.\nThe next steps towards my branding are understanding potential customers more, and the specific problems I can help them solve in their own language."
  },
  {
    "objectID": "implicit-bias/index.html",
    "href": "implicit-bias/index.html",
    "title": "Implicit Bias",
    "section": "",
    "text": "The key question is given that we have implicit biases how do we act against them? If men tend to be more associated with career and women with family how do we make sure women get equal career opportunities? Hiding the gender where possible (e.g. on a resume) is one way, but it’s only possible to do so far?\nPossibly the best way is to try to draw away from intuitive judgements that tend to be more driven by biases, and spend longer assessing criteria that appeal to the rational sense. Another is to perhaps try to make a conscious adjustment after. None of these will be perfect, but it’s at least worth considering that we have these biases and how we should correct them when making decisions."
  },
  {
    "objectID": "building-layered-api-with-fashion-mnist/index.html",
    "href": "building-layered-api-with-fashion-mnist/index.html",
    "title": "Building a layered API with Fashion MNIST",
    "section": "",
    "text": "We’re going to build up a simple Deep Learning API inspired by fastai on Fashion MNIST from scratch. Humans can only fit so many things in their head at once (somewhere between 3 and 7); trying to grasp all the details of the training loop at once is difficult, especially as we add more features to it. The right abstractions can make this much easier by only having to think about what we’re changing in the interface. Coming up with a good abstraction that generalises across many usecases is hard, so we’re going to use the fast.ai interface.\nThen to show how it’s useful once we have our training loop we’ll see how we can change the model and retrain.\nThis post was generated with a Jupyter notebook. You can also view this notebook on Kaggle or download the Jupyter notebook."
  },
  {
    "objectID": "building-layered-api-with-fashion-mnist/index.html#running-metrics",
    "href": "building-layered-api-with-fashion-mnist/index.html#running-metrics",
    "title": "Building a layered API with Fashion MNIST",
    "section": "Running metrics",
    "text": "Running metrics\nIf we want to evaluate on large datasets we need a way to accumulate the metric over minibatches.\nHow this is framed is surprisingly non-standard; we’ll keep to the spirit of fastai (but not the implementation which uses callbacks), but there’s also an external library torchmetrics, and huggingface have a different concept of Metric.\nds_valid = list(zip(X_valid, y_valid))\ndl_valid = DataLoader(ds_valid, batch_size=2048, shuffle=False)\naccuracy(model(X_valid), y_valid)\ntensor(0.6303)\nTo calculate a running metric we’ll just add the accuracy, weighted by the size of the sample, divided by the length of the dataset.\ndef running(metrics, dl, model):\n    values = [0.] * len(metrics)\n    N = len(dl)\n    \n    for X, y in dl:\n        pred = model(X)\n        for idx, metric in enumerate(metrics):\n            values[idx] += metric(pred, y) * len(X) / N\n    return [v.item() if hasattr(v, 'item') else v for v in values]\nThis gives a similar result to before.\nHere we track:\n\nloss on the training set\nloss on validation set\naccuracy\n\nIdeally we’d calculate a running total of loss on the training set, but this is good enough for now.\nmodel.reset_parameters()\n\n# Note: reset creates new parameters. Could we update them instead?\noptim = SGD(model.parameters(), lr=0.2)\n\nds_train = list(zip(X_train, y_train))\ndl_train = DataLoader(ds_train, batch_size=2048, shuffle=True)\n\nmetrics = [cross_entropy, accuracy]\n\nfor epoch in range(5):\n    for X, y in dl_train:\n        pred = model(X)\n        loss = cross_entropy(pred, y)\n        loss.backward()\n        with no_grad():\n            optim.step()\n            optim.zero_grad()\n    \n    print(epoch, *running([cross_entropy], dl_train, model), *running(metrics, dl_valid, model))\n0 2.9654173851013184 2.9218735694885254 0.5206032991409302\n1 2.110874652862549 2.095884323120117 0.5787869095802307\n2 1.7752487659454346 1.7723487615585327 0.6119169592857361\n3 1.5830714702606201 1.5869991779327393 0.6269984841346741\n4 1.4491684436798096 1.4574403762817383 0.6405142545700073"
  },
  {
    "objectID": "building-layered-api-with-fashion-mnist/index.html#learner",
    "href": "building-layered-api-with-fashion-mnist/index.html#learner",
    "title": "Building a layered API with Fashion MNIST",
    "section": "Learner",
    "text": "Learner\nWe can now package up all our objects into a single class, our Learner.\nfrom dataclasses import dataclass\nfrom typing import Callable, List\n\nclass Learner:\n    def __init__(self,\n                 dl_train: DataLoader,\n                 dl_valid: DataLoader,\n                 loss_func: Callable,\n                 model: Callable,\n                 lr: float,\n                 opt_func: Callable = SGD,\n                 metrics: List[Callable] = None):\n        self.dl_train = dl_train\n        self.dl_valid = dl_valid\n        self.loss_func = loss_func\n        self.model = model\n        self.lr = lr\n        self.optim = opt_func(model.parameters(), lr)\n        self.metrics = metrics if metrics is not None else [loss_func]\n        \n    def reset(self):\n        self.model.reset_parameters()\n    \n    \n    def fit(self, n_epoch):\n        for epoch in range(n_epoch):\n            for X, y in self.dl_train:\n                pred = self.model(X)\n                loss = self.loss_func(pred, y)\n                loss.backward()\n                with no_grad():\n                    self.optim.step()\n                    self.optim.zero_grad()\n                    \n            print(epoch, *running([self.loss_func], self.dl_train, self.model), *running(self.metrics, self.dl_valid, self.model))\nmetrics = [cross_entropy, accuracy]\n\nlearn = Learner(dl_train, dl_valid, cross_entropy, model, 0.2, SGD, metrics)\nWe can now train this for a bunch of epochs, getting around 74% accuracy.\nmodel.reset_parameters()\n\nlearn.fit(40)\n0 3.2126216888427734 3.283143997192383 0.5618098378181458\n1 2.278181314468384 2.346566915512085 0.5942805409431458\n2 1.8557775020599365 1.910788655281067 0.6112576127052307\n3 1.6559522151947021 1.710391640663147 0.6208175420761108\n4 1.5421751737594604 1.5935988426208496 0.634827733039856\n5 1.3805128335952759 1.4289369583129883 0.6319432854652405\n6 1.3306803703308105 1.3788357973098755 0.6361463665962219\n7 1.2377548217773438 1.2862215042114258 0.6534531116485596\n8 1.2100512981414795 1.2579960823059082 0.6574913263320923\n9 1.152338981628418 1.19868803024292 0.6574089527130127\n10 1.1125177145004272 1.159442663192749 0.6635899543762207\n11 1.083337426185608 1.1281803846359253 0.6678754091262817\n12 1.0920311212539673 1.137237787246704 0.6494148373603821\n13 1.0610631704330444 1.1104097366333008 0.6719960570335388\n14 1.0184332132339478 1.0690116882324219 0.6782594323158264\n15 1.0005489587783813 1.0493626594543457 0.668452262878418\n16 1.0299843549728394 1.076596736907959 0.6588099598884583\n17 0.9529225826263428 1.0024800300598145 0.6869127750396729\n18 0.9454663991928101 0.9934449195861816 0.6875721216201782\n19 0.9428024291992188 0.9916077852249146 0.6855117678642273\n20 1.0193240642547607 1.0734479427337646 0.6656501889228821\n21 0.9028168320655823 0.9518789052963257 0.6953189373016357\n22 0.8912039399147034 0.9412692785263062 0.6947420239448547\n23 0.8915138244628906 0.9431529641151428 0.6955661773681641\n24 0.9840039014816284 1.0355738401412964 0.6888906955718994\n25 0.8699730634689331 0.9195016622543335 0.6972144246101379\n26 0.8723984956741333 0.9201793670654297 0.7049612998962402\n27 0.8572273254394531 0.9069074392318726 0.7002636790275574\n28 0.8710482716560364 0.9210841655731201 0.6866655945777893\n29 0.9071570634841919 0.956194281578064 0.6764463186264038\n30 0.8600081205368042 0.9094617366790771 0.6917752027511597\n31 0.825920045375824 0.8766304850578308 0.7071864008903503\n32 0.8200230598449707 0.8686937093734741 0.7075160145759583\n33 0.8196104168891907 0.8700741529464722 0.7092467546463013\n34 0.8026244044303894 0.8533946871757507 0.7150980234146118\n35 0.8220050930976868 0.8715838193893433 0.7010878324508667\n36 0.7956870794296265 0.8447741270065308 0.7160870432853699\n37 0.7948266863822937 0.8461573123931885 0.7127904891967773\n38 0.8521597981452942 0.899774432182312 0.7057029604911804\n39 0.7868410348892212 0.8378427624702454 0.7184770107269287"
  },
  {
    "objectID": "persistent-dictionaries/index.html",
    "href": "persistent-dictionaries/index.html",
    "title": "Persistent Dictionaries in Python",
    "section": "",
    "text": "Python has an inbuilt solution called shelve which does this using dbm. There’s also the sqlitedict library (from the makers of Gensim), which builds on sqlite. They both allow strings as keys and arbitrary pickleable objects as values. The best thing is they look and feel like ordinary dictionaries (with the caveat that if you mutate the value it’s only updated on disk with an explicit assignment). Sqlitedict has some advantages coming from sqlite, including handling concurrent access and multiple threads (although internally writes are serialised).\nIf you’re running out of memory in a dictionary putting it into one of these offloads it to disk, which means you can free up a lot of memory at the cost of slower access and modification. When running a periodic batch job, or a process that may fail, this gives a way to save state between runs. You could even use it as a simple task queue between processes. You can then back up the state simply by backing up the files (as long as they’re not being written to).\nThere are many cases when you’ll want a more robust solution, like persisting to a more versatile database like Postgres (or even using SQLite directly), or maintaining a proper task queue like Celery on Redis or RabbitMQ. But all this infrastructure has a lot of overhead; for many cases it’s just easier to use file-base databases through a dictionary interface."
  },
  {
    "objectID": "sessionisation-experiment/index.html",
    "href": "sessionisation-experiment/index.html",
    "title": "Sessionisation Experiments",
    "section": "",
    "text": "I’ve been working on some problems around web sessionisation. The underlying model is that when someone visits your website they may come at different times for different reasons. A session (sometimes called a “visit”) tries to capture this intent. The standard implementation in web analytics tools like Google Analytics and Adobe Analytics is a sequence of page views with no more than a 30 minute gap between them (with some differences in edge cases).\nI was debating with some members of my team around what the best period of sessionisation was in our usecase, and how good it needed to be. There were different viewpoints based on people’s intuitions around how people behave. In the end a few simple experiments resolved the issues.\nThe easiest test is invariance; if we change our definitions and it has negligible impact on the output, then it doesn’t matter which we use. An example of this is Google Analytics “end of day” rule; sessions will break at the end of the day in a specified timezone (which stops infinitely long sessions, and makes some forms of reporting easier). Running an alternative model without the “end of day” rule had a similar number of sessions, which showed this had minor impact on outcomes.\nIt’s harder when there’s a real difference because you need a ground truth. However you don’t need a lot of data. In this case there were fairly clear types of intent we were trying to distinguish, which could be annotated by hand from the weblogs. While not a great job it would be under an hour’s work for a user week in most cases.\nWe picked 5 random user-weeks and annotated them in around a day. We could measure the accuracy in separating intent in two scenarios. By the rule of 5 a 95% confidence interval for the median user-week accuracy is between the lowest and the highest. In this case one scenario was clearly better in these cases which gave confidence it would work better for typical users.\nHowever there was still concern on how it would treat edge cases. In this case we used the opposite of invariance: we isolated the cases where there was the most difference between the two models. We could then annotate them and again found the same rule worked better in most cases.\nThis kind of approach won’t detect small differences, but we didn’t need it to. All we needed was enough information to make a decision, and to get an idea of how much was left to gain from further improvements. This was an effective way of ending weeks of debate in a few days.\nIt’s surprising how often these kinds of simple data collection methods for decision making are overlooked."
  },
  {
    "objectID": "display-r-jupyter/index.html",
    "href": "display-r-jupyter/index.html",
    "title": "How to Display All Columns in R Jupyter",
    "section": "",
    "text": "This is possible with IRKernel, but if you have lots of rows and columns it will only show a sample.\n\n\n\nDataframe with truncated rows and columns\n\n\nTo show more columns in R you usually adjust width (up to 10000) or tibble.width (up to Inf) with options. However IRKernel has it’s own display method repr and out have to set the options manually on it:\noptions(repr.matrix.max.cols=150, repr.matrix.max.rows=200)\nObviously you adjust rows and columns to how many you want to display.\nAnother problem is that it will interpret text between $ characters as LaTeX, which is really annoying if you’ve got text with currency.\n\n\n\nThe term $a$ is displayed as a mathematical variable a\n\n\nThe best thing to do is to escape these characters before display using function(x) gsub('\\\\$', '\\\\\\\\$', x). When using dplyr this can be automated for all character strings by appending to the pipeline\n%>% mutate_if(is.character, function(x) gsub('\\\\$', '\\\\\\\\$', x))\nOne final trick is you may want to actually want to display some HTML you’ve marked up in Jupyter; to do this you use IRdisplay::display_html (analogous to Python’s display and HTML in IPython.core.display). For example to display a link to this website you could use:\nIRdisplay::display_html('<a href=\"https://Skeptric.com\">Skeptric</a>')\nUsing R in Jupyter isn’t quite as smooth or well documented as Python, but with these tricks you can get around some of the rough edges."
  },
  {
    "objectID": "open-source-hurdles/index.html",
    "href": "open-source-hurdles/index.html",
    "title": "Hurdles in Contributing to Open Source",
    "section": "",
    "text": "Today I was working on some code and using the excellent data-science-types to type check some Pandas code with mypy. But for some reason I was getting a weird error when reading with read_feather some data I just wrote with to_feather, and so I switched my to_feather to be to_pickle which doesn’t do as much conversion. This worked fine but then mypy had an error:\nerror: \"Series[Any]\" not callable\nIt must have thought that df.to_pickle was the name of a column, because it wasn’t in the type stub. Well through the wonders of open source I can easily fix that, I opened an issue, cloned the repository and as per the instructions installed it in a virtualenv with pip install -e \".[dev]\" and ran the tests with ./check_all.sh (it’s great that they are clear and make it easy to get set up and run the tests). But then I ran into an issue; one of the Pandas tests fails before I’ve even changed the code.\nI see that it’s using Pandas 1.2 which just came out in the last 2 weeks, so I install the previous version of Python using pip install \"pandas<1.2\" (note the quote; I keep forgetting it and my shell tries to do input redirection), and run the tests and sure enough it passes. I open an issue about the failing tests. I spent some time trying to work out what the test was and why it was failing, but I couldn’t get to the bottom of it after half an hour or so, and so move on to the changes.\nI make the changes and they’re relatively straightforward, and I set up a pull request. However the CI tests fail (but only in the Python 3.9 environment, not in Python 3.6 which must have got an earlier version of Pandas) because of the issue I had. The maintainer agrees we can workaround by limiting Pandas to <1.2 for now until the tests are fixed. Luckily I’m familiar with Github actions from publishing this website and so I quickly see how to make this change and get the tests passing.\nAt the end of the day a straightforward change ends up taking a couple of hours because I had to work around an unrelated test was failing due to a change in another package. This could have been caught earlier, at the time of the upstream change, if the tests ran regularly (say weekly)."
  },
  {
    "objectID": "common-crawl-index-athena/index.html",
    "href": "common-crawl-index-athena/index.html",
    "title": "Common Crawl Index Athena",
    "section": "",
    "text": "Common Crawl builds an open dataset containing over 100 billion unique items downloaded from the internet. There are petabytes of data archived so directly searching through them is very expensive and slow. To search for pages that have been archived within a domain (for example all pages from wikipedia.com) you can search the Capture Index. But this doesn’t help if you want to search for paths archived across domains.\nFor example you might want to find how many domains been archived, or the distribution of languages of archived pages, or find pages offered in multiple languages to build a corpus of parallel texts for a machine translation model. For these usecases Common Crawl provide a columnar index to the WARC files, which are parquet files available on S3. Even the index parquet files are 300 GB per crawl so you may want to process them with Spark or AWS Athena (which is a managed version of Apache Presto).\nCommon Crawl have a guide to setting up access to the index in Athena, and a repository containing examples of Athena queries and Spark jobs to extract information from the index. This article will explore some examples of querying this data with Athena, assuming you have created the table ccindex as per the Common Crawl setup instructions. You can run them through the AWS web console, through an Athena CLI or in Python with pyathena or R with RAthena"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#whats-in-the-columnar-index",
    "href": "common-crawl-index-athena/index.html#whats-in-the-columnar-index",
    "title": "Common Crawl Index Athena",
    "section": "What’s in the columnar index",
    "text": "What’s in the columnar index\nTo see what’s in the index let’s look at a few example rows; we’ll limit 10 to reduce the amount of data scanned (under 10MB).\nThe columns are\n\nurl_surtkey: Canonical form of URL with host name reversed\nurl: URL that was archived\nurl_host_name: The host name\nurl_host_tld: The TLD (e.g. au)\nurl_host_2nd_last_part, … url_host_5th_last_part: The parts of the host name separated by .\nurl_host_registry_suffix: e.g. .com.au\nurl_host_private_domain\nurl_protocol: e.g. https\nurl_port: The port accesed, it seems to be blank for default ports (80 for http, 443 for https).\nurl_path: The path of the URL (everything from the first / to the query parameter starting at ?)\nurl_query: Query parameter; everything after the ?\nfetch_time: When the page was retrieved\nfetch_status: The HTTP status of the request (e.g. 200 is OK)\ncontent_digest: A digest to uniquely identify the content\ncontent_mime_type: The type of content in the header\ncontent_mime_detected: The type of content detected\ncontent_charset: The characterset of the data (e.g. UTF-8)\ncontent_languages: Languages declared of the content\nwarc_filename: The filename the archived data is in\nwarc_record_offset: The offset in bytes in the archived file where the corresponding data starts\nwarc_record_length: The length of the archived data in bytes\nwarc_segment: The segment the data is archived in; this is part of the filename\ncrawl: The id of the crawl (e.g. CC-MAIN-YYYY-WW where YYYY is the year and WW is the ISO week of the year).\nsubset: Is this the ‘warc’, or ‘robotstxt’, or ‘crawldiagnostics’\n\nSELECT *\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-24'\n  AND subset = 'warc'\n  AND url_host_tld = 'au'\n  AND url_host_registered_domain = 'realestate.com.au'\nlimit 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurl_surtkey\nurl\nurl_host_name\nurl_host_tld\nurl_host_2nd_last_part\nurl_host_3rd_last_part\nurl_host_4th_last_part\nurl_host_5th_last_part\nurl_host_registry_suffix\nurl_host_registered_domain\nurl_host_private_suffix\nurl_host_private_domain\nurl_protocol\nurl_port\nurl_path\nurl_query\nfetch_time\nfetch_status\ncontent_digest\ncontent_mime_type\ncontent_mime_detected\ncontent_charset\ncontent_languages\nwarc_filename\nwarc_record_offset\nwarc_record_length\nwarc_segment\ncrawl\nsubset\n\n\n\n\nau,com,realestate)/advice\nhttps://www.realestate.com.au/advice/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/\n\n2020-05-28 22:09:01.000\n200\nBIVR34XTK7HJGQJ5H47GO65UBODZA6XY\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347400101.39/warc/CC-MAIN-20200528201823-20200528231823-00270.warc.gz\n898091083\n17598\n1590347400101.39\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-clever-storage-solutions-rental-properties\nhttps://www.realestate.com.au/advice/10-clever-storage-solutions-rental-properties/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-clever-storage-solutions-rental-properties/\n\n2020-06-06 05:39:55.000\n200\n2YJSRGZHQVSE5WWKHEHPJ26RBIIQOIMP\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590348509972.80/warc/CC-MAIN-20200606031557-20200606061557-00502.warc.gz\n855590228\n36195\n1590348509972.80\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-features-to-look-for-when-buying-a-period-home\nhttps://www.realestate.com.au/advice/10-features-to-look-for-when-buying-a-period-home/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-features-to-look-for-when-buying-a-period-home/\n\n2020-05-26 08:40:48.000\n200\nV75WAUJFWLXT5VPZFCPHUEPFHAZJY6MX\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347390755.1/warc/CC-MAIN-20200526081547-20200526111547-00430.warc.gz\n809064278\n35882\n1590347390755.1\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-foodie-towns-worth-going-rural\nhttps://www.realestate.com.au/advice/10-foodie-towns-worth-going-rural/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-foodie-towns-worth-going-rural/\n\n2020-05-27 16:31:24.000\n200\nXP4RGQCIWL5GL4O3KJG53AGRYVRXEGGV\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347394756.31/warc/CC-MAIN-20200527141855-20200527171855-00398.warc.gz\n909969420\n35652\n1590347394756.31\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-things-consider-buying-fixer-upper\nhttps://www.realestate.com.au/advice/10-things-consider-buying-fixer-upper/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-things-consider-buying-fixer-upper/\n\n2020-05-27 13:38:45.000\n200\nEZSDPRBNJIJEY4Y47C6A42T7YCDNDOSH\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347394074.44/warc/CC-MAIN-20200527110649-20200527140649-00029.warc.gz\n891295141\n34746\n1590347394074.44\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-things-moving-new-house\nhttps://www.realestate.com.au/advice/10-things-moving-new-house/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-things-moving-new-house/\n\n2020-06-02 18:28:54.000\n200\nEX24KJUKOHX5GOX36QFTNVZGACZPJO2J\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347425481.58/warc/CC-MAIN-20200602162157-20200602192157-00363.warc.gz\n879272045\n35475\n1590347425481.58\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-tips-coping-share-house-kitchen\nhttps://www.realestate.com.au/advice/10-tips-coping-share-house-kitchen/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-tips-coping-share-house-kitchen/\n\n2020-06-02 09:15:24.000\n200\nGWGMC36W4MHHSZ4GOQJA5AHGPY3LUUID\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347423915.42/warc/CC-MAIN-20200602064854-20200602094854-00528.warc.gz\n840946829\n33050\n1590347423915.42\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/10-tips-to-sharing-a-home-with-your-adult-children\nhttps://www.realestate.com.au/advice/10-tips-to-sharing-a-home-with-your-adult-children/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/10-tips-to-sharing-a-home-with-your-adult-children/\n\n2020-05-28 02:26:38.000\n200\nFMVSRNL32GKBRPLTV755XBL43G27P7YX\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347396300.22/warc/CC-MAIN-20200527235451-20200528025451-00478.warc.gz\n873617107\n31724\n1590347396300.22\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/101-guide-contents-insurance\nhttps://www.realestate.com.au/advice/101-guide-contents-insurance/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/101-guide-contents-insurance/\n\n2020-06-05 11:00:50.000\n200\nPTLEDOTFYPBVZO2AC26KUHB6SBJ6FZKG\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590348496026.74/warc/CC-MAIN-20200605080742-20200605110742-00542.warc.gz\n837847660\n31792\n1590348496026.74\nCC-MAIN-2020-24\nwarc\n\n\nau,com,realestate)/advice/11-things-you-really-need-to-tell-your-landlord\nhttps://www.realestate.com.au/advice/11-things-you-really-need-to-tell-your-landlord/\nwww.realestate.com.au\nau\ncom\nrealestate\nwww\n\ncom.au\nrealestate.com.au\ncom.au\nrealestate.com.au\nhttps\n\n/advice/11-things-you-really-need-to-tell-your-landlord/\n\n2020-05-31 04:37:28.000\n200\nMWL7BHWJEGV7TU7ORULZYGBRDNIY3GQI\ntext/html\ntext/html\nUTF-8\neng\ncrawl-data/CC-MAIN-2020-24/segments/1590347410745.37/warc/CC-MAIN-20200531023023-20200531053023-00083.warc.gz\n867625659\n33829\n1590347410745.37\nCC-MAIN-2020-24\nwarc"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#most-crawled-tlds",
    "href": "common-crawl-index-athena/index.html#most-crawled-tlds",
    "title": "Common Crawl Index Athena",
    "section": "Most crawled TLDs",
    "text": "Most crawled TLDs\nTo get an idea of the coverage of Common Crawl we can look at the most crawled TLDs, the number of captured domains and the average number of captures per domain for a snapshot.\n# Scans 150GB (~ 75c)\nSELECT url_host_tld,\n       approx_distinct(url_host_registered_domain) as n_domains,\n       count(*) as n_captures,\n       sum(1e0) / approx_distinct(url_host_registered_domain) as avg_captures_per_domain,\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-16'\n  AND subset = 'warc'\ngroup by url_host_tld\norder by n_captures desc\nThe number of domains is staggering; 15 million from .com alone, 400k for Australia. The typical average number of pages per domain is 80, but for .edu it’s nearly 5,000 and for .gov it’s nearly 2,500. Much more content is archived from a university of government pages than general domains.\n\n\n\nCounts of TLD"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#australian-domains-with-most-pages-archived",
    "href": "common-crawl-index-athena/index.html#australian-domains-with-most-pages-archived",
    "title": "Common Crawl Index Athena",
    "section": "Australian domains with most pages archived",
    "text": "Australian domains with most pages archived\nThis query finds the au domains with most pages archived from 2020-16 crawl. It takes about 5s and scans under 10MB (so it’s practically free).\nSELECT COUNT(*) AS count,\n       url_host_registered_domain\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-16'\n  AND subset = 'warc'\n  AND url_host_tld = 'au'\nGROUP BY  url_host_registered_domain\nHAVING (COUNT(*) >= 100)\nORDER BY  count DESC\nlimit 500\nThe top websites look to be government and university websites. Note this isn’t about popularity of the site, but is related to the number of pages on the site, how many links there are to each page, and the how permissive its robots.txt file is. To find the most popular sites you would need panel web traffic panel like Alexa.\n\n\n\nLargest domains in AU"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#domains-with-the-most-subdomains",
    "href": "common-crawl-index-athena/index.html#domains-with-the-most-subdomains",
    "title": "Common Crawl Index Athena",
    "section": "Domains with the most subdomains",
    "text": "Domains with the most subdomains\nSome domains have a lot of different subdomains which they provide to users as namespaces. Wordpress is a common example where you can get a free personal website with a Wordpress domain. These could be good places to look for user generated content, each subdomain belonging to a user.\nSELECT url_host_registered_domain,\n       approx_distinct(url_host_name) AS num_subdomains\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-16'\n  AND subset = 'warc'\nGROUP BY url_host_registered_domain\nORDER BY num_subdomains DESC\nLIMIT 100\n# Scans 1GB data\nThe results make sense, the top 5 sites; blogspot, wordpress, wixsite, weebly and fc2 are all sites for hosting personal content.\n\n\n\nLargest number subdomains in Common Crawl"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#public-dropbox-content-types",
    "href": "common-crawl-index-athena/index.html#public-dropbox-content-types",
    "title": "Common Crawl Index Athena",
    "section": "Public Dropbox Content Types",
    "text": "Public Dropbox Content Types\nThere were almost five thousand subdomains of dropboxusercontent.com, it would be interesting to see what type of content is in there.\nSELECT content_mime_detected,\n       count(*) as n\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-16'\n  AND subset = 'warc'\n  AND url_host_registered_domain = 'dropboxusercontent.com'\nGROUP BY 1\nORDER BY n DESC\nLIMIT 100\nIt’s mostly PDFs, but there’s some zip files and mobile applications. I wonder how much of it is malware.\n\n\n\ncontent_mime_detected\nn\n\n\n\n\napplication/pdf\n5033\n\n\napplication/zip\n204\n\n\naudio/mpeg\n186\n\n\napplication/vnd.symbian.install\n130\n\n\ntext/plain\n127\n\n\nimage/jpeg\n63\n\n\napplication/vnd.android.package-archive\n55\n\n\nvideo/mp4\n38\n\n\napplication/epub+zip\n36"
  },
  {
    "objectID": "common-crawl-index-athena/index.html#downloading-some-content",
    "href": "common-crawl-index-athena/index.html#downloading-some-content",
    "title": "Common Crawl Index Athena",
    "section": "Downloading some content",
    "text": "Downloading some content\nLet’s find some SQL files on Github:\nSELECT url,\n       warc_filename,\n       warc_record_offset,\n       warc_record_offset + warc_record_length as warc_record_end\nFROM \"ccindex\".\"ccindex\"\nWHERE crawl = 'CC-MAIN-2020-16'\n  AND subset = 'warc'\n  AND url_host_registered_domain = 'githubusercontent.com'\n  AND content_mime_detected = 'text/x-sql'\nLIMIT 5\n\n\n\n\n\n\n\n\n\nurl\nwarc_filename\nwarc_record_offset\nwarc_record_length\n\n\n\n\nhttps://raw.githubusercontent.com/arey/spring-batch-toolkit/blog-parallelisation/src/test/resources/com/javaetmoi/core/batch/test/TestParallelAndPartitioning.sql\ncrawl-data/CC-MAIN-2020-16/segments/1585370493684.2/warc/CC-MAIN-20200329015008-20200329045008-00304.warc.gz\n689964496\n689966615\n\n\nhttps://raw.githubusercontent.com/bailangzhan/demo/master/category.sql\ncrawl-data/CC-MAIN-2020-16/segments/1585371896913.98/warc/CC-MAIN-20200410110538-20200410141038-00434.warc.gz\n616371570\n616562918\n\n\nhttps://raw.githubusercontent.com/citrix/sample-scripts/master/storefront/Create-ImportSubscriptionDataSP.sql\ncrawl-data/CC-MAIN-2020-16/segments/1585371896913.98/warc/CC-MAIN-20200410110538-20200410141038-00251.warc.gz\n619544564\n619546850\n\n\nhttps://raw.githubusercontent.com/citrix/sample-scripts/master/storefront/Create-StoreSubscriptionsDB-2016.sql\ncrawl-data/CC-MAIN-2020-16/segments/1585370506673.7/warc/CC-MAIN-20200402045741-20200402075741-00091.warc.gz\n669277710\n669280288\n\n\nhttps://raw.githubusercontent.com/cwoodruff/ChinookDatabase/master/Scripts/Chinook_SqlServer.sql\ncrawl-data/CC-MAIN-2020-16/segments/1585370506673.7/warc/CC-MAIN-20200402045741-20200402075741-00421.warc.gz\n652805559\n652898120\n\n\n\nWe can then use this to retrieve the WARC for the first record; we just prepend https://data.commoncrawl.org/ to the filename and only get the relevant bytes with the Range header.\ncurl https://data.commoncrawl.org/crawl-data/CC-MAIN-2020-16/segments/1585370493684.2/warc/CC-MAIN-20200329015008-20200329045008-00304.warc.gz \\\n    -H \"range: bytes=689964496-689966615\" > sql_sample.warc.gz\nThen you can inspect the file with zcat\n\n\n\nSQL WARC file\n\n\nIf you needed to export the WARC data at scale Common Crawl have a script for producing the WARC extract from a SparkSQL query."
  },
  {
    "objectID": "reading-parquet-metadata/index.html",
    "href": "reading-parquet-metadata/index.html",
    "title": "Read Common Crawl Parquet Metadata with Python",
    "section": "",
    "text": "The columnar indexes for Common Crawl are too large to quickly download. For example the 2022-05 crawl has a columnar index consisting of 300 parquet files with a total compressed size of 205 GB. Keep in mind this is just an index, it doesn’t contain any crawled web page data; the actual data is over 72TB spread across 72,000 files. It would take a lot of time to download all these files to look for crawls across a handful of domains, but it turns out we don’t need to. The Parquet files are approximately sorted by url_surtkey, which is a canonical form of URL with host name reversed. For example the url_surtkey for this page would be com,skeptric)/reading-parquet-metadata. By searching the Parquet files that contain the url_surtkey we are looking for we can just query one or two of the 300 files.\nIn this article we download the Parquet metadata for all 25,000 WARC indexes in under 7 minutes from Australia, using asyncio. A way to make it even faster would be to run it in an EC2 instance where the data is, us-east-1. We can then use it to query a single domain for a single crawl in about a minute. This is fairly slow but we could make many requests concurrently across crawls and domains. You can view the corresponding Jupyter notebook (or the raw ipynb).\n\nReading Metadata with PyArrow\nPyArrow, part of the Apache Arrow project, provides great support for Parquet through the dataset interface. We can pass it the base path and it will discover all the partitions and Parquet files in seconds. Note that Common Crawl requires authentication for S3 access, so this requires having an AWS account setup and configured (see the boto3 documentation for how to do that).\nimport pyarrow.dataset as ds\n\ncc_index_s3_path = 's3://commoncrawl/cc-index/table/cc-main/warc/'\ncc_index = ds.dataset(cc_index_s3_path, format='parquet', partitioning='hive')\nThe individual files can be accessed from the dataset using the get_fragments method, and we can pass a filter on the partition keys to efficiently only access certain files. Here we get the index to the WARC files for the 2022-05 crawl:\nfragments = list(\n    cc_index.get_fragments(\n        filter=(ds.field('crawl') == 'CC-MAIN-2022-05') &\n               (ds.field('subset') == 'warc')))\nlen(fragments)\n# 300\nPyArrow exposes methods for accessing metadata for these fragments (files), such as the number of rows and their sizes. Each fragment contains “row groups” which are a large block of records. In this case there are associated statistics stored in the row group metadata:\nfragments[0].row_groups[0].statistics\nWhich gives a long list of data starting with:\n{'url_surtkey': {'min': 'com,wordpress,freefall852)/2016/03/29/billy-guy',\n  'max': 'com,worldpackers)/search/skill_hospitality_entertainment/type_hotel?location_categories[]=nature&location_types[]=hotel&min_meals_count[]=3&months[]=11&skills[]=music'},\n 'url': {'min': 'http://03.worldchefsbible.com/',\n  'max': 'https://zh.worldallianceofdramatherapy.com/he-mission'},\n 'url_host_name': {'min': '03.worldchefsbible.com',\n  'max': 'zr1.worldblast.com'},\n 'url_host_tld': {'min': 'com', 'max': 'com'},\n 'url_host_2nd_last_part': {'min': 'wordpress', 'max': 'worldpackers'},\n 'url_host_3rd_last_part': {'min': '03', 'max': 'zr1'},\n 'url_host_4th_last_part': {'min': 'bbbfoundation', 'max': 'www'},\n 'url_host_5th_last_part': {'min': 'http', 'max': 'toolbox'},\n 'url_host_registry_suffix': {'min': 'com', 'max': 'com'},\n 'url_host_registered_domain': {'min': 'wordpress.com',\n  'max': 'worldpackers.com'},\n 'url_host_private_suffix': {'min': 'com', 'max': 'com'},\n 'url_host_private_domain': {'min': 'wordpress.com',\n  'max': 'worldpackers.com'},\n 'url_host_name_reversed': {'min': 'com.wordpress.freefall852',\n  'max': 'com.worldpackers.www'},\n 'url_protocol': {'min': 'http', 'max': 'https'},\n 'url_port': {'min': 443, 'max': 2000},\n ...\nOne thing to notice is the url_surtkey goes through a pretty narrow alphabetical range from com,wordpress through to com,worldpackers (as does url_host_private_domain but it’s not available for older crawls). If we wanted to find crawl data from a particular domain it’s easy to tell whether it’s from this domain. The fragments have a method to_row_groups which has a filter argument which can use these statistics to skip over blocks. Unfortunately it’s quite slow for Pyarrow to just read the Parquet metadata; from my laptop it can process about 2-3 files per second.\n\n\nUsing fastparquet\nFastparquet, part of the Dask Project, also provides a good interface for reading Parquet files. However it is quite slow; it takes 2.3 seconds to access a single file, which just reads the metadata.\nfrom fastparquet import ParquetFile\nimport s3fs\n\nfs = s3fs.S3FileSystem()\npf = ParquetFile(fn=fragments[0].path, fs=fs)\nAll the file metadata is accessible for each of the columns through the fmd attribute.\npf.fmd.row_groups[0].columns[0].meta_data._asdict()\n{'type': 6,\n 'encodings': [0, 4],\n 'path_in_schema': ['url_surtkey'],\n 'codec': 2,\n 'num_values': 1730100,\n 'total_uncompressed_size': 117917394,\n 'total_compressed_size': 23113472,\n 'key_value_metadata': None,\n 'data_page_offset': 4,\n 'index_page_offset': None,\n 'dictionary_page_offset': None,\n 'statistics': {'max': None,\n  'min': None,\n  'null_count': 0,\n  'distinct_count': None,\n  'max_value': \"b'com,worldpackers)/search/skill_hospitality_entertainment/type_hotel?location_categories[]=nature&location_types[]=hotel&min_meals_count[]=3&months[]=11&skills[]=music'\",\n  'min_value': \"b'com,wordpress,freefall852)/2016/03/29/billy-guy'\"},\n 'encoding_stats': [{'page_type': 0, 'encoding': 0, 'count': 122}],\n 'bloom_filter_offset': None}\n\n\nReading the data manually\nTo understand what’s happening here we can access the data manually. The Parquet file layout contains all the data, followed by the File Metadata, followed by a 32-bit integer describing the length of the metadata in bytes, and then the magic string “PAR1” to identify the file type.\n\n\n\nParquet File Layout\n\n\nTo read the data first we need to find the length of the file so that we can find the end of it. We can access the HTTP REST endpoint for AWS S3 using boto3 to make a HEAD request to find the Content-Length without reading any data.\nimport boto3\n\ns3 = boto3.client('s3')\n\nfile = fragments[0].path\nbucket, key = file.split('/', 1)\nmetadata = s3.head_object(Bucket=bucket, Key=key)\ncontent_length = int(metadata['ContentLength'])\nf'{content_length / (1024**3):0.1f} GB'\n# 1.3 GB\nWe then need to find the length of the file metadata by reading the last 8 bytes, using a HTTP Range Request.\nend_byte = content_length\nstart_byte = end_byte - 8\n\nresponse = s3.get_object(Bucket=bucket,\n                         Key=key,\n                         Range=f'bytes={start_byte}-{end_byte}')\nend_content = response['Body'].read()\n\nassert end_content[-4:] == b'PAR1'\nfile_meta_length = int.from_bytes(end_content[:4], byteorder='little')\nf'{file_meta_length / 1024:0.1f} kb'\n# 57.4 kb\nNow that we know how long the file metadata is we can work backwards to extract it.\nend_byte = content_length - 8\nstart_byte = content_length - 8 - file_meta_length\nresponse = s3.get_object(Bucket=bucket,\n                         Key=key,\n                         Range=f'bytes={start_byte}-{end_byte}')\nfile_meta_content = response['Body'].read()\nThe metadata format is pretty complicated, so instead of parsing it we can use fastparquet to extract a representation using Apache Thrift.\nfrom fastparquet.cencoding import from_buffer\n\nfmd = from_buffer(file_meta_content, \"FileMetaData\")\ntype(fmd)\n# fastparquet.cencoding.ThriftObject\nThe attributes of this object correspond to the metadata, and can be accessed as directly with fastparquet. The whole process takes under a second per file; faster than both PyArrow and fastparquet, but still a little slow.\n\n\nConcurrent request with asyncio\nWe want to read the metadata from many parquet files but we spend a lot of time waiting to get data from the server. This kind of I/O bound problem is perfect for asyncio. We could use aiobotocore to access S3 in an asynchronous context, but instead I decided to use the Common Crawl HTTP Interface. This is publicly accessible and does not require any authentication, but unfortunately the Parquet files can’t be automatically discovered over HTTP. We can convert our files discovered by Apache Arrow over S3 into HTTP frontend by replacing the Common Crawl bucket commoncrawl/ with the prefix https://data.commoncrawl.org/.\n# Just get the warc files\nwarc_files = [f for f in cc_index.files if '/subset=warc/' in f]\n# Convert from S3 bucket/key to data.commoncrawl.org/\nhttp_files = ['https://data.commoncrawl.org/' +\n              x.split('/', 1)[1] for x in warc_files]\nI don’t understand asyncio in as much detail as I would like, so this code may not be quite right. I modified a stackoverflow answer on using aiohttp to download files to this usecase.\nimport asyncio\nimport aiohttp\n\nasync def _async_parquet_metadata_http(url, session):\n    \"\"\"Retrieve Parquet file metadata from url using session\"\"\"\n    async with session.head(url) as response:\n        await response.read()\n        output_headers = response.headers\n        content_length = int(output_headers['Content-Length'])\n\n    headers={\"Range\": f'bytes={content_length-8}-{content_length}'}\n    async with session.get(url=url, headers=headers) as response:\n        end_content = await response.read()\n\n    if end_content[-4:] != b'PAR1':\n        error = 'File at %s does not look like a Parquet file; magic %s'\n        raise ValueError(error % (path, end_content[-4:]))\n    file_meta_length = int.from_bytes(end_content[:4], byteorder='little')\n\n    start = content_length-8-file_meta_length\n    end = content_length-8\n    headers={\"Range\": f'bytes={start}-{end}'}\n    async with session.get(url, headers=headers) as response:\n        file_meta_content = await response.read()\n    return file_meta_content\n\nasync def fetch_parquet_metadata_http(urls):\n    \"\"\"Retrieve Parquet metadata for urls asyncronously, returning exceptions\"\"\"\n    async with aiohttp.ClientSession(raise_for_status=True) as session:\n        ret = await asyncio.gather(*[_async_parquet_metadata_http(url, session)\n                                    for url in urls],\n                                   return_exceptions=True)\n    return ret\nTo scan all the indexed WARC crawls is over 25,000 files and the Parquet metadata alone is over 1 GB. When dealing with this many requests, 3 per file, at least one request is likely to fail. There are three kinds of failures I can think of:\n\nAn intermittent issue causes an individual load to fail.\nThere is an issue with an individual file and so loading will always fail.\nThere is an environmental issue (server rejecting all requests, network down) and all requests fail.\n\nNote about we passed return_exceptions=True in asyncio.gather which allows us to handle these errors. A simple approach is:\n\nRun a batch of N requests\nCapture any errors and put these jobs in a retry cue (to handle 1)\nRemove any jobs that have been retried too many times (to handle 2)\nPersist successful results so they can be reused\nIf more than x% of the N requests fail abort the process (to handle 3).\n\nRunning in small batches also avoids the problem of running too many asynchronous tasks in parallel and locking the CPU. We can store our metadata in an sqlitedict and maintain a job queue and retry counts.\nfrom collections import defaultdict\nfrom sqlitedict import SqliteDict\n\nmetadata_store = SqliteDict('common_crawl_columnar_index_metadata.sqlite')\n\nmax_retries = 3\nmax_exceptions_per_batch = 5\nbatch_size = 1000\n\nretries = defaultdict(int)\nexceptions = defaultdict(list)\nseen = set(metadata_store.keys())\n\njobs = [x for x in http_files if x not in seen]\nlen(jobs)\n# 25193\nWe can now run through all the jobs:\nstart_time = time.time()\n\nwhile len(jobs) > 0:\n    batch = jobs[:batch_size]\n\n    batch_metadata = await(async_parquet_metadata_http(batch))\n\n    num_exceptions = 0\n\n    for job, metadata in zip(batch, batch_metadata):\n        if isinstance(metadata, Exception):\n            logging.warning(f'{job}: {type(Exception)}')\n            num_exceptions += 1\n            exceptions[job].append(metadata)\n            retries[job] += 1\n            if retries[job] >= max_retries:\n                jobs.remove(job)\n        else:\n            assert isinstance(metadata, bytes)\n            metadata_store[job] = metadata\n            jobs.remove(job)\n\n    metadata_store.commit()\n    if num_exceptions > max_exceptions_per_batch:\n        print('Too many exceptions %i' % num_exceptions)\n        break\n\nprint(f'Finished in {time.time() - start_time:0.0f}s')\n# Finished in 408s\nThis is processing over 500 Parquet files per second, far better than our starting point of 2-3 Parquet files per second. We can then extract the relevant metadata into a Pandas Dataframe.\nimport pandas as pd\n\nstats_columns = ['url_surtkey']\n\ndef extract_row_group_metadata(k, v):\n    fmd = from_buffer(v, \"FileMetaData\")\n\n    for idx, row_group in enumerate(fmd.row_groups):\n        result = {\n            'path': k[len('https://data.commoncrawl.org/'):],\n            'crawl': k.split('/')[-3].split('=')[-1],\n            'subset':  k.split('/')[-2].split('=')[-1],\n            'row_group': idx,\n            'num_rows': row_group.num_rows,\n            'byte_size': row_group.total_byte_size,\n        }\n        for col in stats_columns:\n            minimum = get_column_metadata(col, row_group).statistics.min_value\n            maximum = get_column_metadata(col, row_group).statistics.max_value\n            # Convert byte strings into unicode\n            if isinstance(minimum, bytes):\n                minimum = minimum.decode('utf-8')\n            if isinstance(maximum, bytes):\n                maximum = maximum.decode('utf-8')\n\n            result[f'min_{col}'] = minimum\n            result[f'max_{col}'] = maximum\n        yield result\n\ndef extract_metadata(metadata_store):\n    for k, v in metadata_store.items():\n        for row in extract_row_group_metadata(k, v):\n            yield row\n\ndf_metadata = pd.DataFrame(extract_metadata(metadata_store))\ndf_metadata.head()\n\n\n\nFirst few rows of extracted metadata\n\n\n\n\nUsing the metadata\nNow that we have the metadata we can use it to look for crawls of commoncrawl.org in the 2020-24 crawl. First we query the metadata to find which files and row groups may contain this domain.\nresults = df_metadata.query('crawl == \"CC-MAIN-2020-24\" &\\\nmin_url_surtkey <= \"org,commoncrawl)/\" <= max_url_surtkey')\nTo iterate over these efficiently we can make a mapping from the paths to a list of row groups.\npath_to_row_groups = (\n    results\n    .groupby('path')\n    .agg(row_groups = ('row_group', list))\n    ['row_groups']\n    .to_dict()\n)\nWe can then use Pyarrow to read the required files and row groups:\nfrom fsspec.implementations.http import HTTPFileSystem\n\nhttp = HTTPFileSystem()\nsearch_ds = ds.dataset('https://data.commoncrawl.org/' + path,\n                       filesystem=http,\n                       format='parquet',\n                       partitioning='hive')\n\ncolumns = ['url', 'url_host_name',\n           'warc_filename', 'warc_record_offset',\n           'warc_record_length']\n\nfor fragment in search_ds.get_fragments():\n    path = fragment.path[len(http_prefix):]\n    row_groups = fragment.split_by_row_group()\n    for row_group_idx in path_to_row_groups[path]:\n        row_group = row_groups[row_group_idx]\n        data = row_group.to_table(\n            columns=columns,\n            filter=ds.field('url_host_name') == 'commoncrawl.org')\n        if len(data) > 0:\n            all_groups.append(data)\n\nresults = pa.concat_tables(all_groups).to_pandas()\nThis takes around 1 minute to scan through the 5 row groups; it could potentially be sped up using concurrent requests. We can then use this to download a single WARC files\nimport zlib\n\na = results.iloc[0]\nurl = 'https://data.commoncrawl.org/' + a.warc_filename\n\nstart = a.warc_record_offset\nend = a.warc_record_offset + a.warc_record_length\nheader = {\"Range\": f'bytes={start}-{end}'}\n\nr = requests.get(url, headers=header)\nwarc_data = r.content\ndata = zlib.decompress(warc_data, wbits = zlib.MAX_WBITS | 16)\nprint(data.decode('utf-8')[:2000])\n\n\nMissing Data\nLooking through the crawl data between 2017-43 and 2018-43 there are not any statistics for url_surtkey:\n(\n df_metadata\n .groupby('crawl')\n .agg(row_groups = ('path', 'count'),\n      files = ('path', 'nunique'),\n      min_url = ('min_url_surtkey', 'count'),\n      max_url = ('max_url_surtkey', 'count'),\n     )\n  .query('min_url != row_groups')\n  .sort_index()\n)\n\n\n\nData from 2017-43 to 2018-43 has no url_surtkey statistics\n\n\nOne way to construct this manually by reading through all the Parquet files and calculating the minimum and maximum url_surtkey by row group. If we’re happy to only filter at a file level we could also use AWS Athena to find them:\nSELECT \"$path\" as path,\n       min(url_surtkey) AS min_url_surtkey,\n       max(url_surtkey) AS max_url_surtkey\nFROM ccindex\nWHERE crawl BETWEEN 'CC-MAIN-2017-43' AND 'CC-MAIN-2018-43'\n  AND subset = 'warc'"
  },
  {
    "objectID": "skeptric-google-analytics/index.html",
    "href": "skeptric-google-analytics/index.html",
    "title": "Insights From Google Analytics for a Small Blog",
    "section": "",
    "text": "I installed Google Analytics a couple of weeks ago on the website to see how people are actually viewing my site. Here are my top insights based on recent usage:\n\nThree in four people come through a web search to solve a specific technical problem and then leave\nOne in four people come to this site through commoncrawl.org.\nAbout 150 people view this website each day\n\n\n3 in 4 people search for a problem then leave\nMost of my traffic comes from web search and lands on specific well named pages that solve a specific technical issue, and then leave. These articles that go deeply into solving particular problems reach a lot of people, and I find myself going back to them when I hit the issue again. Writing more of these kinds of articles is useful, but isn’t likely to build return traffic.\nThe most visited pages are all solving problems that people are likely to search for, and indeed I searched before writing them. The top visited page is Running an X Server with WSL2 which is a common problem, and I put together a few solutions to get a working answer. Similarly other top pages from search are Integer Division in Presto, converting HTML to text, exporting data from Amazon Athena, calculating moving averages in SQL, and ngrams in Python and Pandas. Each of these focused on a specific technology and solving a well-defined problem.\nThere are definitely similar articles that I find useful that don’t make it to the top. I’ve gone back to turning of LaTeX in Jupyter, displaying all columns in R Jupyter and showing side-by-side diffs in Python a number of times but they get no hits. I’m not sure if it’s because these are unusual problems, or I’ve used strange words to describe them, or if there’s better content available. I’m glad I wrote them since I find them useful but I wonder if there were better ways to describe them to make them appear in searches.\nAfter people have visited the article that addresses their problem they leave. In fact 90% of people arriving by search only view one page, which makes sense since they’re trying to solve a specific problem, and are not likely to feel like exploring. I have no visibility of whether it actually solved their problem, since even if it didn’t they would go back to search and try the next result. There could be an opportunity here to offer further expertise on the technology they are reading about to come back to another time, but I’m not sure what the uptake would be like.\n\n\n1 in 4 people come from commoncrawl.org\nIt was a huge surprise to me that a large amount of my traffic was from commoncrawl.org. This is because they’ve recently listed my articles on their example project list. The lesson for me is that writing useful content for existing projects is a great way to build referral traffic.\nThis is because they’ve put my articles on common crawl to the top of their chronological list of example projects. I’m honoured they put my articles there; I learned a lot from those projects before I built those articles. I never reached out to them so I assume they must have come across my site through a search or alert on common crawl. A similar thing happened when DAVx5 mentioned a post on Twitter.\nI have 4 articles on their list and each article further down the list gets about half the traffic of the one above it. Many people must click just the top link, fewer the one below it and so on. While this is a good source of traffic for now, the next time they update the list and bump down my articles the amount of traffic will drop off quickly. That being said the quality of the traffic will get better; the people who view articles further down are more dedicated. I may really help someone else build something useful with Common Crawl some time from now.\nNext time I post an article on an existing project I will consider reaching out to the project and its users. It may help reach more people that find it useful.\n\n\nAbout 150 people view this website each day\nI’m pretty impressed that I reach 150 people each day without doing any promotional work. They’re likely different people each day given the nature of the traffic. Even if I only help a third of the people that come that’s 50 each day. I’ve only been writing daily for 6 months, in another 6 months I could help ten thousand people. This may be small to a media outlet, but for me that’s incredible.\nEven so it could be worth promoting this website in small ways to help reach the people that could find it useful. A little research into SEO and doing things like making the site faster and tagging appropriate keywords could help more people find the right posts. Sharing posts through social media, especially when it connects to relevant projects, could also help content reach the right people. I don’t have any need to build an audience since I don’t sell anything, but it would be great to get what I write to the people who want to read it."
  },
  {
    "objectID": "community-detection/index.html",
    "href": "community-detection/index.html",
    "title": "Community detection in Graphs",
    "section": "",
    "text": "Santo Fortunato’s review article and user guide provides a really good introduction to community detection. The intuitive idea is that you want to group nodes such that a node in the group will on average have more connections to other nodes in the group than to nodes in other groups. However the devil is in the detail and there’s no clear definition of whether one community is “better” than the other.\nIn fact it’s even hard to say how similar two partitions into communities are. In the article they recommend using the variation of information: \\(V(X, Y) = H(X \\vert Y) + H (Y \\vert X)\\) where H is the Shannon entropy of the cluster assignments. But a deeper problem is there are rarely ground truths for what the communities are. There exist some examples in the literature; like the Zacchary Karate Club network about members of a Karate Club that split into two separate clubs, or another about bottlenose dolphins observed together that later migrated to different areas. But the datasets are typically small, noisy (a single missed observation could radically change the graph) and scarce and it’s hard to say how true their ground truths are.\nBecause of the ambiguity of definition and the rarity of well grounded datasets there are many different techniques to solve the problem, and it’s really hard to evaluate which ones are best. There are methods that try to cut the graph into pieces; either directly or via looking at the spectrum of the Laplacian matrix. There are optimisation methods that try to maximise some function of how good the partitions are; modularity being a popular one but that has some limitations on how small a community it can detect and has many “near maxima”. There are methods based on statistical inference; most popular are stochastic block models that assume there’s a constant probability of connection between each community and tries to infer the communities that maximise the log likelihood. For stochastic block models the number of communities is a hyperparameter, but there are techniques that address this by setting a Bayesian prior on the number of communities. Another approach is to look at dynamics on the network like random walks, synchronisation of coupled oscillators or spin glasses and define communities based on the domains (attractors, synchronised groups, final spin state). There’s also label propagation where each node is iteratively assigned to be in the same group as the majority of its neighbours, randomly picked when there is a tie.\nThere are even meta-methods like consensus clustering which generates a new algorithm from existing clustering algorithms. The original stochastic clustering algorithm is run a number of times to generate the consensus matrix which contains the probability of two vertices being in the same cluster under the clustering algorithm. This consensus matrix is then thresholded and the the process is run iteratively until the clustering algorithm run multiple times gives the same result every time.\nSo if there are lots of different ways of creating communities, and they are hard to evaluate where do we start? It makes sense to start with some domain specific measures on how good the communities are. For example you might already have some measures on your products or customers that you would expect to be reflected in their behaviour; you should check how tightly the communities segment this behaviour. You may also have some constraints on the number and size of groups to be a useful grouping. Finally you may have some internal categorisation of products, or even some intuitive knowledge of what should be similar, that you could use to check the communities against. None of these are perfect, but you can at least determine whether a community assignment is viable.\nThen unless you have an expectation a particular algorithm would work best, I would go through the algorithms that are popular, have good implementations and are efficient enough to complete on your dataset, until you find one that is viable. For example the Louvian Algorithm for maximising modularity is efficient and has implementations in Neo4J, Python and R, and you can walk down the dendrogram until you get to a number of clusters that looks viable. Or Infomap based on random walks has implementations in Python and R. The graph-tool package has a really powerful implementation of stochastic block models using MCMC that can deal with multiple types of graphs (and metadata can be seen as a type of graph). It’s worth looking through the algorithms available in NetworkX, igraph and Neo4J.\nWhile it can seem overwhelming these techniques can reduce an intractable problem of grouping tens of thousands of items into a tractable one of evaluating hundreds of groups. If you can come up with good heuristic criteria for evaluating the groups you can sample from the buffet of techniques for detecting communitites and see what works in your application."
  },
  {
    "objectID": "cloudrun/index.html",
    "href": "cloudrun/index.html",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "",
    "text": "I sometimes build hobby machine learning APIs that I want to show off, like whatcar.xyz. Ideally I want these to be cheap and low maintenance; I want them to be available most of the time but I don’t want to spend much time or money maintaining them and I can have many of them running at the same time. My current solution is Cloud Compute (e.g. Digital Ocean or Linode) which has low fixed costs (around $5 per month). Unfortunately once every couple of months it goes down; so I have to monitor it and get the server running again (which can take me a few days to get to if I’m busy). I’ve looked at switching to a hosted Application Platforms like Heroku, Digital Ocean App Platform and Google App Engine; but they’re quite a bit more expensive, and not worth it for hobby projects. There are also Hosted Kubernetes solutions but they’re also more expensive and require learning a bunch of Kubernetes. Google Cloud Run is a recent alternative which seems to be the best of all worlds - runs off of containers, has minuscule costs for low traffic applications, and handles all the operations for you. The only downside is the maximum cost is quite large, and it has to be managed.\nGoogle Cloud Run is a managed Knative which will manage scaling and running containers, down to zero (which is perfect if you don’t get much traffic). Each container must run a webserver, in Python you could use FastAPI or Flask to manage the requests, and it can serve a configurable number of concurrent requests. You can configure the memory (up to 8GB) and the CPUs (up to 4), scale from 0 to 1000 instances and only get charged for the containers that are running (to the nearest 100ms), and the pricing is reasonable. The main limitation is that the service must not have any local mutable state; state needs to be managed in an external database or object store. This is very convenient for Machine Learning Inference, which tend to have complex dependencies (which can be handled with containers), moderate memory requirements and no state.\nThere may be some other alternatives; there are machine learning specific platforms with inference like AWS Sagemaker and Google’s AI Platform, but they tend to try to force you into a certain way of doing things and it’s hard to go outside their supported tools (although it’s possible with Sagemaker to use a custom Docker container). AWS Lambda recently announced container support which makes it much easier to deploy machine learning endpoints, but it requires a bit more work to set up (I’d be interested in how the costs and performance compares).\nThis article will explore how I migrated whatcar.xyz to Google Cloud Run, while trying to manage the risk of costs."
  },
  {
    "objectID": "cloudrun/index.html#billing-alerts",
    "href": "cloudrun/index.html#billing-alerts",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Billing Alerts",
    "text": "Billing Alerts\nOne tool Google Cloud Platform provides is Budget Alerts, under Billing. You can set it to send you an email when your bill reaches a certain value. The downside is it takes some time for them to process costs, and in the event of intense traffic you may not get alerted until you’re way over your budget.\nIt makes sense to set up a billing alert with a reasonable budget for when you’re slowly using more than you intend, with the idea that you’ll get notified within a day or two."
  },
  {
    "objectID": "cloudrun/index.html#monitoring-and-alerting",
    "href": "cloudrun/index.html#monitoring-and-alerting",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Monitoring and Alerting",
    "text": "Monitoring and Alerting\nFor shorter term odd behaviour you can use Google Cloud’s Monitoring. There are metrics for Cloud Run such as container/billable_instance_time for the total billable time of containers, or request_count for the number of requests. These are updated every 180s so you can see what’s happening relatively quickly.\nYou can set alerts on these, but it’s not immediately obvious to me what levels I want to set (or even whether I want to use rates or totals). I’ve made a little dashboard showing these metrics and will set some alerts when I’ve got a better idea of what normal looks like.\n\n\n\nCloud Run Instance Monitoring"
  },
  {
    "objectID": "cloudrun/index.html#creating-the-container",
    "href": "cloudrun/index.html#creating-the-container",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Creating the Container",
    "text": "Creating the Container\nGoogle Cloud documentation has good instructions on building and deploying a Python container. I already had my project configured as a Docker container, so I just had to set the port using an environment variable as in the example. In my case I had an endpoint for collecting annotations which relied on writing to local disk. While I could change this to write files to a blob store such as Google Cloud Store (with the label in the filename), I haven’t used it for a while so I just deleted it to make the application stateless.\nInstead of using Google Cloud Build I decided to directly upload to the container registry. I had to go through a little configuration in the web UI before I could get it working.\ndocker build . -name myproject\ndocker tag myproject gcr.io/{PROJECT_ID}/myproject\ndocker push gcr.io/{PROJECT_ID}/myproject\nThe push took me several minutes to upload the image. Note that whether you use the container registry of cloud it will create artifacts in a Google Cloud Storage bucket, and the container images can be quite large. Make sure you delete these, or apply lifecycle rules, if you really want to keep costs down."
  },
  {
    "objectID": "cloudrun/index.html#deploying-in-cloud-run",
    "href": "cloudrun/index.html#deploying-in-cloud-run",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Deploying in Cloud Run",
    "text": "Deploying in Cloud Run\nIn Cloud Run I could set a new service, pick the region, and select the image I had pushed to the container registry.\nImportantly to manage cost risk, in step 2 (Configure this service’s first revision) go to Advanced Settings. In Advanced Settings under Autoscaling set the Maximum Number of Instances to something much lower than the default of 100.\n\n\n\nCloud Run Autoscaling\n\n\nOnce you’re through the wizard in a few minutes the application will start."
  },
  {
    "objectID": "cloudrun/index.html#accessing-the-endpoint",
    "href": "cloudrun/index.html#accessing-the-endpoint",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Accessing the endpoint",
    "text": "Accessing the endpoint\nOnce you’ve got the Service set up there’s a URL you can test it from, listed near the top of the dashboard (mine ended in run.app). But you can do better; with a Custom Domain Mapping you can set it to run on any domain or subdomain you own. If you click on the information button next to the URL, and click on Manage Domains. Then you can “Add Mapping”.\nYou may have to verify your domains in Webmasters Central; in my case it was a matter of setting a TXT DNS record and waiting patiently for it to update. Once that’s done you can finish adding the mapping and complete adding the list of DNS records they tell you to update. It takes some time for these changes to propagate, so I left my old server running for a couple of days. The best thing is it even comes with SSL."
  },
  {
    "objectID": "cloudrun/index.html#handling-issues",
    "href": "cloudrun/index.html#handling-issues",
    "title": "Machine Learning Serving on Google CloudRun",
    "section": "Handling issues",
    "text": "Handling issues\nI quickly hit an error where the cloud instance ran out of memory. The issue was pretty prominent in the Google Cloud Console, and the logs were very clear.\n\n\n\nCloud Run Out of Memory Error\n\n\nAll I had to do was increase the memory available to the instances, because the model takes up a large amound of memory."
  },
  {
    "objectID": "qa-zero-shot-book-ner/index.html",
    "href": "qa-zero-shot-book-ner/index.html",
    "title": "Question Answeeing as Zero Shot NER for Books",
    "section": "",
    "text": "The code is simple using Transformers high level Question Answering Pipeline. I picked the first question that came to mind; some prompt engineering may produce better results.\nfrom transformers import pipeline\n\npipe = pipeline(\"question-answering\")\n\npipe(context=books[0],\n     question=\"What book is this about?\",\n     topk=5,\n     handle_impossible_answer=True)\n     \npipe(context=books[0],\n     question=\"Who is the author?\",\n     topk=5,\n     handle_impossible_answer=True)\nThese work very well when there is just one answer, the book answer is typically reliable above 0.5. When there’s more than one answer it often only captures one, and when there’s no answers it will sometimes force one (especially for author). Often the secondary answers will overlap with the top answer; we’d need to do some more filtering to get distinct answers.\nI found contextualising the author could help; first find a book name then ask \"Who is the author of <book>?\". Subjectively this worked better than the Work of Art NER when there was a book. If you want to see some examples check out the example notebook."
  },
  {
    "objectID": "excel-annotation/index.html",
    "href": "excel-annotation/index.html",
    "title": "Spreadsheets as a Rough Annotation Tool",
    "section": "",
    "text": "There are various tools for data entry like org mode tables in Emacs, or you can use a spreadsheet interface in R with data.entry (or via edit) or in Jupyter Notebooks. But I always find them kludgy when compared to full spreadsheet programs like Excel, Google Sheets, Libreoffice Calc or Gnumeric. For specific applications you could build a custom webform in HTML and CSS (or use a specific tool like for text annotation brat or prodi.gy), but a spreadsheet will do for simple things.\nThe approach is to get all the data you need to make the decision fit on a single screen; if you can’t do this a spreadsheet might not be the right tool. It’s generally quickest if you pick a simple binary annotation scheme (like y/n or 1/0); this means you can focus on making a single simple decision. If you don’t know just leave it blank and move on, the idea is to get the best return on investment. Generally this is challenging enough and if you need more detailed annotation you can do multiple rounds.\nIn my case I wanted to draw the points on a scatterplot based on the annotation to work out how to draw the decision boundary. I used a technique from a stackoverflow answer building it into separate columns with formulas like =IF($D2=\"y\",$C2,NA()) to create a separate series for each label. This gave me a plot that showed the cases and I could get an idea of what a good decision boundary looks like and either eyeball one, or train a classifier on my newly labelled data.\n\n\n\nExample labelled data"
  },
  {
    "objectID": "wsl2-networking/index.html",
    "href": "wsl2-networking/index.html",
    "title": "Fixing suddenly unable to connect to X server in WSL2",
    "section": "",
    "text": "Annoyingly there is very little feedback as to why it can’t connect to an XServer. I went back through my previous instructions of setting up an X server in WSL2, but noticed something. When I ran ip addr | grep eth0 it was in the 192.168.0.0/16 subnet. However the WSL2 X server firewall inbound rule I wrote whitelisted the 172.16.0.0/12 subnet. Apparently it can use either private subnet. I’ve now updated the instructions to include both. Because these are private subnets as long as you’re only on networks you trust this should be fairly safe to do.\nOne upside is servers running newer builds of Windows (after 18945) then WSL2 servers can be accessed through localhost on Windows. This makes testing applications in a browser much easier."
  },
  {
    "objectID": "github-action-cron/index.html",
    "href": "github-action-cron/index.html",
    "title": "Scheduling Github Actions",
    "section": "",
    "text": "By default Hugo will not publish articles with a future date, so it’s easy to keep a backlog by setting the date in front matter to a future date. By convention I tend to set it to 8am in UTC+10 (e.g. date: \"2020-06-20T08:00:00+10:00\") for that day. I can still preview these posts locally by passing the --buildFuture flag to hugo serve.\nSo if I want these articles to automatically be published I need to run the action just after this time. The time 08:00 in UTC+10 is 22:00 in UTC (on the previous day), so I want it to run daily at, say, 22:04. Github scheduled actions use a crontab syntax, so I updated the configuration to run at 4 22 * * *.\non:\n  push:\n    branches: [ master ]\n  schedule:\n    - cron: '4 22 * * *'\nThat’s all there is to it. Of course it would be straightforward to set this up on a Raspberry Pi with cron and a git server with a post-receive hook. But it’s convenient to use managed services to not have to worry about maintaining, and especially securing, the server."
  },
  {
    "objectID": "50-daily-articles/index.html",
    "href": "50-daily-articles/index.html",
    "title": "Writing 50 Daily Articles",
    "section": "",
    "text": "Inspiration\nWhile there are many sources of inspiration for my writing, Sacha Chua’s No Excuses Guide to Blogging is the biggest one. I bought the book around 2 years ago but I’ve found it useful and kept coming back to it. If you’re considering blogging stop reading this article and go read that book.\nI found the advice “It’s okay to write about different things” very useful. It takes the pressure off of writing a particular thing and means I can write about writing email in emacs, using excel and extracting job ad titles even though the audience has very little overlap.\nAnother useful piece of advice from Sacha Chua is “Turn ideas into small questions, and then answer those.” I have a tendency to want to write long articles, research and understand everything and explain it in detail. By focusing on the smallest question I could answer in one sitting has let me write articles like getting started with the Python debugger, how to look up Australian addresses using G-NAF and how to debug Powershell scripts. It’s also been the cornerstone of my series of articles on extracting information from job ads, where I can focus on one specific approach and how it works.\n\n\nWhat’s gone well\nMaking writing a blog post a daily habit has worked really well. I’ve tried weekly or semi-weekly before, but I lose track of it. Having an achievable goal of writing a post (any post) each day has been very productive, and not breaking the streak is very motivational.\nIt’s helped me record things I always lookup like how to display all columns in R with Jupyter, how to get disk usage to depth 2 or how to turn a pipe table to a CSV. Writing helps me reflect on the types of things I’ve been working on and how I spend my energy, as I sit down each night and think about what I should write about. This is making me question how I’m spending my time, and slowly guiding me to work on things I’m proud to write about.\nIt’s helped me focus on projects like extracting job ad titles. Keeping a running write-up of my experiments has kept me working on it much longer than I would have otherwise. There are times when I’m busy with other things and too tired to work on it, but I now have a reason to keep coming back to it when I have the energy.\nWriting helps me think through things more deeply. When writing about calculating moving averages in SQL I really thought deeply about the different approaches and their tradeoffs. Writing how to use the Python debugger made me read the manual and discover useful features I wasn’t aware of (like interact). Writing about the rule of 5 for confidence intervals I implemented the calculation and understood how to generalise the rule.\nHaving a picture for every blog post was a little intimidating, but makes me much prouder of the results. At worst I just take a screen grab of a terminal but it’s great when I see a relevant image when I publish the post.\nAn unexpected benefit is my article on the awesome DAVx5 app for syncing calendars and contacts was tweeted by the team (despite no publicity on my part). It’s really great to see writing something about a good product has helped them in some small way.\n\n\nWhat’s gone less well\nMost of my articles have been technical how-tos because they’re the easiest for me to write. I’m trying to write more articles that help me think through my own position, like the four analyst competencies, but I find this takes much more time and effort. This is likely because I need to spend the time to write them.\nSometimes I have distracted myself with another piece of work to write a blogpost. The 4am rule for timeseries was meant to be a quick article but I spent a couple of hours finding an example to illustrate it and realising it was more subtle than I thought. When writing about the G-NAF for location addresses I wasn’t actively working on it and spent a couple of hours re-researching how to use it. I think this is because I’ve been too scared to write about things I haven’t figured out yet, and need to be braver about sharing while I’m learning.\n\n\nWhat’s next\nI still need to keep making writing a habit until it gets easier. I want to try to find my voice and be a bit less boring. I’ll continue sharing the variety things I learn each day. I want to focus more energy on the series of extracting information from job ads.\nI want to focus on sharing while I learn, even though I find it very uncomfortable. I think finding the smallest achievement and writing out the questions is a good way to do this.\nCurrently I’m not actively promoting my writing, nor editing it to make sure it’s readable. Eventually I want to get there, but for now I’m happy writing just to share my discoveries. Maybe I’ll come back to this in another 50 blog posts.\nI’ve had a lot of fun writing daily articles and want to continue doing it. I’ll keep coming back to No Excuses Guide to Blogging as I continue on this journey of sharing learnings."
  },
  {
    "objectID": "request-warc/index.html",
    "href": "request-warc/index.html",
    "title": "Saving Requests and Responses in WARC",
    "section": "",
    "text": "from warcio.capture_http import capture_http\nimport requests  # requests must be imported after capture_http\n\n\nurls = [f'http://quotes.toscrape.com/page/{i}/' for i in range(1, 11)]\n\nwith capture_http('quotes.warc.gz'):\n    for url in urls:\n        requests.get(url)\nThis creates (or appends to an existing) file quotes.warc.gz that contains every response and request we sent. Because it contains all the data we can experiment with processing the data without ever hitting the server again. The main drawback of keeping all the raw HTML is that it requires an order of magnitude more storage (especially in websites with large reams of javascript and templated code); but often storage is cheap enough that it’s worthwhile. Another thing to consider is data validation should be done almost immediately (or in parallel) to make sure you’re not just collecting a large number of 404s or empty pages.\nThe generated WARC file contains the responses compressed within the gzip file, so annoyingly you can’t read it with zcat. However warcio has some command line tools to access it.\nWe can find what’s in the file with warcio index quotes.warc.gz; which is each response followed by the request that generated it.\n{\"offset\": \"0\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/1/\"}\n{\"offset\": \"2756\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/1/\"}\n{\"offset\": \"3210\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/2/\"}\n{\"offset\": \"6912\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/2/\"}\n{\"offset\": \"7367\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/3/\"}\n{\"offset\": \"10108\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/3/\"}\n{\"offset\": \"10557\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/4/\"}\n{\"offset\": \"13177\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/4/\"}\n{\"offset\": \"13631\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/5/\"}\n{\"offset\": \"16221\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/5/\"}\n{\"offset\": \"16673\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/6/\"}\n{\"offset\": \"19383\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/6/\"}\n{\"offset\": \"19837\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/7/\"}\n{\"offset\": \"22894\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/7/\"}\n{\"offset\": \"23348\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/8/\"}\n{\"offset\": \"26266\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/8/\"}\n{\"offset\": \"26721\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/9/\"}\n{\"offset\": \"29719\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/9/\"}\n{\"offset\": \"30171\", \"warc-type\": \"response\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/10/\"}\n{\"offset\": \"32770\", \"warc-type\": \"request\", \"warc-target-uri\": \"http://quotes.toscrape.com/page/10/\"}\nWe can see the headers of the second response, with offset 3210 (from the above index) using warcio extract --headers quotes.warc.gz 3210; Note that this includes the target URI, the datetime and the status code.\nWARC/1.0\nWARC-IP-Address: 136.243.118.219\nWARC-Type: response\nWARC-Record-ID: <urn:uuid:5da1f262-2542-4aed-bf3a-768202f1fb7b>\nWARC-Target-URI: http://quotes.toscrape.com/page/2/\nWARC-Date: 2020-07-02T11:09:00Z\nWARC-Payload-Digest: sha1:KSQKJ4N5HKIDDZR5PV7I6PELYGSU3JPD\nWARC-Block-Digest: sha1:CVS43VR3MFTMESL74QP347IXEF7Z25YE\nContent-Type: application/http; msgtype=response\nContent-Length: 3348\n\nHTTP/1.1 200 OK\nServer: nginx/1.14.0 (Ubuntu)\nDate: Thu, 02 Jul 2020 11:09:00 GMT\nContent-Type: text/html; charset=utf-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nX-Upstream: spidyquotes-master_web\nContent-Encoding: gzip\nThe actual data can be extracted the sameway with payload: warcio extract --payload quotes.warc.gz 3210 | head -n 20\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n        <meta charset=\"UTF-8\">\n        <title>Quotes to Scrape</title>\n    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n    <link rel=\"stylesheet\" href=\"/static/main.css\">\n</head>\n<body>\n    <div class=\"container\">\n        <div class=\"row header-box\">\n            <div class=\"col-md-8\">\n                <h1>\n                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n                </h1>\n            </div>\n            <div class=\"col-md-4\">\n                <p>\n\n                    <a href=\"/login\">Login</a>\nThe library is very simple to use and contains a lot of relevant information. This seems really useful for debugging if things start going wrong later down the pipeline."
  },
  {
    "objectID": "github-local-actions/index.html",
    "href": "github-local-actions/index.html",
    "title": "Using Local Github Actions",
    "section": "",
    "text": "However I have one concern; I’m passing my rsync credentials into an external action. I’ve specified a tag in my yaml uses: wei/rclone@v1, but it would be easy for the author to move this tag to another commit that sends my private credentials to their personal server. Maybe I could specify the SHA-1 of the commit, but SHA-1 is broken as a cryptographic hash and someone may be able to forge a commit with the same SHA-1.\nThe safest thing to do is to move the action locally, which is very easy to do.\n\nCopy the relevant action code to .github/actions/rclone (or wherever you want in the repository)\nUpdate the uses clause to the new destination prefixed by ./: uses: ./.github/actions/rclone\n\nThis shows how easy it could be to make your own custom actions locally. I like that it can be built on top of a Docker file, which makes it relatively straightforward to migrate to another CI/CD system.\nNow I’m much more comfortable with how my secrets are being used. I’m still trusting Github to store and pass my secrets correctly, I’m still downloading rclone from https://rclone.org/install.sh so I’m trusting that domain hasn’t been acquired (and the CA certificates fetched by Alpine Linux are correctly validating the domain). But it seems less risky than relying on some unknown Github repository with no reputation at stake (and I was already relying on all of those).\nI’m still comfortable using the external Hugo Setup action because I don’t pass any secrets."
  },
  {
    "objectID": "mace-bearer/index.html",
    "href": "mace-bearer/index.html",
    "title": "Mace-Bearer",
    "section": "",
    "text": "The ceremony was on a typical Adelaide summer’s day, hot and dry. I was going out to lunch with my parents afterwards, so I wanted to make sure I was comfortable. So I put on some casual shorts and t-shirt; after all the robes would cover everything.\nI started to realise I may have misjudged things when I arrived outside the magnificent Bonython hall and realised I was the only graduate not wearing a formal suit or dress. I went around the back to collect my robe and quickly realised my mistake. The academic robes were open at the front and and only came down just below waist level. After donning the robes you could see the bottom of my bare legs below, and my bright blue t-shirt with the slogan I’m made of meat with a cute cut of steak.\n\n\n\nMade of Meat\n\n\nAs the mace-bearer I had to walk to another room to receive my instructions for the ceremony. There I met a few of the senior people from the university including the Vice Chancellor. He was very good humoured about my casual attire, and gave me a rundown of events. I was to walk accross to the main hall carrying a mace, worth more than my life savings at that time, and then give a two sentence introduction to a speaker, and at the end of the ceremony return the mace. He suggested that I take the piece of paper with the introduction printed on it, but I’d acted in plays and memorised long scripts; I didn’t need a prompt for two lines.\nThe precession across the campus bearing said mace was uneventful. We entered the hall and after the formalities I went up and stood in front of the couple hundred science graduates for that year. In my shorts and obscure humorous t-shirt, barely covered by the robe. I quickly reeled off the first sentence of the introduction I was giving. Then as I transitioned into the second, I suddenly forgot what I was meant to say.\nThe silence was deafening. Hundreds of faces staring up at me, me staring blankly back. It probably lasted about 30 seconds, but it felt like an eternity.\nSuddenly the words came back to my mind, and I finished the introduction and scrambled off the stage.\nThe rest of the graduation went like you’d expect a graduation ceremony to. More speeches, people getting pieces of paper and handshakes, and then everyone leaving the building in an orderly fashion.\nAfter promptly returning the mace and disrobed, I went out to a nice restaurant with my parents. I was very comfortable in my shorts and t-shirt on a hot summers day, and enjoyed my delicious lunch. The next year when I got invited to my Honours graduation ceremony, I decided I’d rather not attend."
  },
  {
    "objectID": "diverge-converge/index.html",
    "href": "diverge-converge/index.html",
    "title": "Diverge then Converge",
    "section": "",
    "text": "I find the creative process of brainstorming is more effective if I do it separately to refining ideas. Taking the time to brainstorm leads to better solutions, whether thinking about what to work on, planning out a presentation or designing a technical solution. Generally the first idea you think of isn’t the best one and it’s better to have an array of options to pick from. For a particularly difficult or unfamiliar problem it can be worth doing multiple rounds of brainstorming with breaks and additional research in between to generate enough understanding.\nPicking a solution requires a different mindset. When I try to rule out solutions at the same time as coming up with ideas I find the range of ideas that are produced tend to be more narrow and I suppress ideas too early. On the other hand when I have a bunch of ideas it’s much easier to evaluate and prioritise them.\nI find the same thing with writing; it’s easier to start with writing the piece as freely as possible, and then come back and edit it at a later time. If I try to edit too much while writing I get stuck and find it harder to finish. However if I come back to edit a week or two later it’s a relatively easy job to rearrange and polish the piece.\nThis works well in a team setting too; doing a large brainstorm on post-its, then grouping up themes and voting on the most important ones can be a very effective process for diverging and then converging."
  },
  {
    "objectID": "spacy-download-model-once/index.html",
    "href": "spacy-download-model-once/index.html",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "",
    "text": "SpaCy and Prodigy are handy tools for natural language processing in Python, but are a pain to install in a reproducible way, say with a Makefile. Downloading a SpaCy model with spacy download -m will always re-download the model, which can be very time and bandwidth consuming for large models. Prodigy is a paid product and can’t be installed from PyPI.\nMy solution for this is to generate a file containing the links to all the SpaCy models.\nThen a model can be installed with pip, for example pip install -f spacy_model_links.html 'en_core_web_trf>=3.4.0<3.5'. If you run the command twice it will check the dependencies (which takes seconds) but it won’t re-download the model (which can take minutes).\nThis article will show how to install SpaCy models and Prodigy quickly and reproducibly using make and pip-tools. It will explain how installing SpaCy model packages works, and why it downloads the model every time, and some workarounds. Then it will go into how to install Prodigy (if you have a licence) and look deeper into how -f works in pip. The next section will look into how to use these with pip-tools to create a robust build process that won’t break when you try it in a new environment. Finally it will explain the shell script above (and a pure Python version), giving a simpler process for working with SpaCy models.\nThe main assumption is that you’re using a *nix system (for Windows users I recommend wsl2), that you’re using the same version of Python everywhere (using something like pyenv or conda/mamba to manage Python versions). Also you should be using some sort of virtual environment."
  },
  {
    "objectID": "spacy-download-model-once/index.html#downloading-models-with-pip",
    "href": "spacy-download-model-once/index.html#downloading-models-with-pip",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Downloading models with pip",
    "text": "Downloading models with pip\nWhen you run spacy download it will output something like\n# python -m spacy download en_core_web_trf\nCollecting en-core-web-trf==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.1/en_core_web_trf-3.4.1-py3-none-any.whl (460.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 460.3/460.3 MB 1.1 MB/s eta 0:00:00\nYou can just copy the URL above into a requirements.txt file and install it directly using pip install. The catch is that pip install will always re-download the model each time you run it; it has no way of telling whether the package at the URL is the same as the one already installed.\nIn fact this was raised as an issue back in 2017, which was solved by appending #egg= to the URL (which somehow convinced pip this was the same package). However this was removed in pip 21.2 (July 2021) so that:\n\nMost of the fragment part, including egg=, is explicitly ignored. Only subdirectory= and hash values (e.g. sha256=) are kept."
  },
  {
    "objectID": "spacy-download-model-once/index.html#downloading-models-only-once",
    "href": "spacy-download-model-once/index.html#downloading-models-only-once",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Downloading models only once",
    "text": "Downloading models only once\nOne way to solve this would be to check if it’s already installed, and only then install it. For example the following command will return a non-zero status only if en_core_web_trf is not installed.\npython -c 'import importlib.util; assert importlib.util.find_spec(\"en_core_web_trf\") is not None'\nA more robust approach is to use pip download to download the files to a local directory, and then install from there. For example if you put all the model files in a requirements-model.txt you can download them to the build/ folder using\npip download -r requirements-model.txt --no-deps --dest build/\nThe best thing is if you run this twice it doesn’t download the files a second time. You can then pip install the files from the build/ directory."
  },
  {
    "objectID": "spacy-download-model-once/index.html#putting-it-into-a-workflow",
    "href": "spacy-download-model-once/index.html#putting-it-into-a-workflow",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Putting it into a workflow",
    "text": "Putting it into a workflow\nThe strategy to tie this all together is:\n\nCreate a requirements-model.txt file with the model requirements\nCreate a requirements.in file with all the other requirements\nEvery time either file changes, download the models and produce a requirements.txt file\n\nWe can list the directory of build/ to find all the downloaded model files to install. This can be done with a Makefile like the following:\nrequirements.txt: requirements.in requirements-model.txt\n    pip download -r requirements-model.txt --no-deps --dest build/\n    cp requirements.in requirements.txt\n    find build/ -type f >> requirements.txt\n\ninstall: requirements.txt\n    pip install -r requirements.txt"
  },
  {
    "objectID": "spacy-download-model-once/index.html#gotchas",
    "href": "spacy-download-model-once/index.html#gotchas",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Gotchas",
    "text": "Gotchas\nThe main limitation here is you have to manage the dependencies for model versions yourself. If you want to upgrade SpaCy, you’ll have to manually update all the requirements-model.txt files to the compatible models. As a result you should restrict SpaCy to a major version (e.g. >=3.4.0,<3.5.0) until you’re ready to upgrade."
  },
  {
    "objectID": "spacy-download-model-once/index.html#keeping-credentials-secret",
    "href": "spacy-download-model-once/index.html#keeping-credentials-secret",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Keeping credentials secret",
    "text": "Keeping credentials secret\nThe problem is if you put this line in a Makefile it will echo your secret license key as it runs the command, which may end up exposed in a log somewhere. You can silence the whole line, but then it’s harder to debug what’s going on.\nA better solution is to use the PIP_FIND_LINKS environment variable for configuration. As long as you include the prodigy URL above in this variable you don’t need to explicitly pass it in -f and so make won’t print it out. Pip will echo the links but is smart enough to mask the sensitive user info.\nLooking in links: https://****@download.prodi.gy\nOne way to set the variable is to have a .env file in .gitignore like this, and include .env in your Makefile. Here’s what it could look like.\nPRODIGY_KEY=XXXX-XXXX-XXXX-XXXX\nexport PIP_FIND_LINKS=https://${PRODIGY_KEY}@download.prodi.gy"
  },
  {
    "objectID": "spacy-download-model-once/index.html#keeping-credentials-secret-with-pip-tools",
    "href": "spacy-download-model-once/index.html#keeping-credentials-secret-with-pip-tools",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Keeping credentials secret with pip-tools",
    "text": "Keeping credentials secret with pip-tools\nTo make things reproducible pip-tools compile will output all the find-links in the requirements.txt file, which should be committed. This is a problem because we have our license key secret in our links, but it can be turned off with --no-emit-find-links."
  },
  {
    "objectID": "spacy-download-model-once/index.html#stop-trying-to-reinstall-things-already-installed",
    "href": "spacy-download-model-once/index.html#stop-trying-to-reinstall-things-already-installed",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Stop trying to reinstall things already installed",
    "text": "Stop trying to reinstall things already installed\nFor URL or file dependencies, pip-sync will uninstall and reinstall them each time it’s run. This makes it a bit slower to use each time.\nA workaround for this is to use --find-links with the build directory itself, and pass the package names into pip-compile.\nHere’s an example workflow, you could have a requirements-model.in.raw file that looks like this:\nhttps://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.0/en_core_web_trf-3.4.0-py3-none-any.whl\nThen in the Makefile you could download all the requirements into build and produce a requirements-model.in file with the package names:\nrequirements-model.in: requirements-model.in.url\n    mkdir -p build/\n    pip download -r requirements-model.txt --no-deps --dest build/\n    ls build/ | sed 's/-.*//' > requirements-download.in\nThen when we want to install the dependencies we can pip-compile the files and run pip-sync:\nrequirements.txt: requirements.in requirements-model.in\n    python -m piptools compile --no-emit-find-links --generate-hashes \\\n        --find-links build/ \\\n        --output-file requirements.txt \\\n        requirements.in requirements-model.in\n\ninstall: requirements.txt\n    python -m piptools sync --find-links ./build/ requirements.txt"
  },
  {
    "objectID": "spacy-download-model-once/index.html#simplified-process",
    "href": "spacy-download-model-once/index.html#simplified-process",
    "title": "Preventing SpaCy from re-downloading large models",
    "section": "Simplified process",
    "text": "Simplified process\nNow that we have the links we can treat a SpaCy model like any other dependency. We just have a single requirements.in file and can specify a version such as en_core_web_trf>=3.4.0,<3.5.0. It even checks and resolves the model’s dependencies correctly.\nOur Makefile is now relatively simple:\nrequirements.txt: requirements.in\n    python -m piptools compile -q --no-emit-find-links \\\n    --find-links spacy_model_links.html\n\ninstall: requirements.txt\n    python -m piptools sync \\\n    --find-links spacy_model_links.html\nMy question is why SpaCy doesn’t provide something like this. If this HTML links page was republished somewhere with every release, then I could just point to it with find-links (assuming there’s good reason they aren’t in PyPI). But this simple solution of generating a file works until they provide a better way."
  },
  {
    "objectID": "cosine-is-euclidean/index.html",
    "href": "cosine-is-euclidean/index.html",
    "title": "Cosine Similarity is Euclidean Distance",
    "section": "",
    "text": "Cosine distance is equivalent to Euclidean distance of normalized vectors\n\nI hadn’t realised it at all, but once the claim was made I could immediately verify it. Given two vectors u and v their distance is given by the length of the vector between them: \\(d = \\| u - v \\| = \\sqrt{(u - v) \\cdot (u - v)}\\). Expanding this out and using \\(u \\cdot v = \\|u\\|\\|v\\| \\cos \\theta\\), where \\(\\theta\\) is the angle between the two vectors, gives\n\\[d = \\sqrt{\\|u\\|^2 + \\|v\\|^2 - 2 \\|u\\|\\|v\\| \\cos \\theta }\\]\nFor unit vectors the norms are 1 and this reduces to\n\\[d = \\sqrt{2 (1 - cos \\theta)}\\]\nSo cosine similarity is closely related to Euclidean distance. Of course if we used a sphere of different positive radius we would get the same result with a different normalising constant. Thus \\(\\sqrt{1 - cos \\theta}\\) is a distance on the space of rays (that is directed lines) through the origin.\nThe centroid for cosine similarity is easy to calculate; project the points on some sphere, calculate their Euclidean centroid (that is average them) and take the ray through that point. I proved this using Lagrange multipliers, where I defined the centroid as the point that maximises average cosine similarity; this is the same as minimising the average Euclidean distance and so it really is a centroid. A plausible way to see this is to note that the Euclidean centroid is the distance minimiser in Euclidean space, and the projection to the sphere is the closest point on the sphere to the centroid, so this projection must be the centroid for cosine similarity."
  },
  {
    "objectID": "asdf-python/index.html",
    "href": "asdf-python/index.html",
    "title": "Managing Python Versions with asdf",
    "section": "",
    "text": "The asdf tool allows you to configure multiple versions of applications in common interactive shells (Bash, Zsh, and Fish). In this case it could handle the installation of any version of Python and pipenv could use it to access that version. But the tool is much more versatile than that and can work with a wide variety of languages including R, Node.js and Java. It gives an easy way to install and set the version; which seems much better than fussing about with JAVA_HOME.\nIt is very easy to install asdf. There’s a very good guide to installing that lets you pick your method. You can then install a plugin like asdf plugin add python, and you’re ready to go. This was enough for pipenv to detect the version and prompt installing it. It seemed much easier to setup than pyenv, while being much more versatile since pyenv is Python specific.\nYou will need any compile time dependencies installed before you install the plugin. In my case I got an error No module named '_ctypes' because I was missing libffi. I could resolve this by apt install libffi_dev and then reinstalling the plugin; asdf uninstall python 3.7.9 and then asdf install python 3.7.9. Ultimately this shows that it’s less reliable than using a container, where you can bundle all the dependencies of the right version, but asdf is a convenient low-overhead way of managing multiple versions for a programming lanugage."
  },
  {
    "objectID": "less-is-better/index.html",
    "href": "less-is-better/index.html",
    "title": "Less Is Better",
    "section": "",
    "text": "When I picked a big bunch of grapes with a couple of rotting overripe grapes I sorted it into the rotten pile, despite there being a dozen ripe looking grapes. If I found a couple of isolated ripe green grape I would put it in the ripe pile, without knowing whether it had grown near rotten grapes. My partner’s grandmother ignored my sorting and pulled out all the ripe grapes from both piles, washed them and put them in a big bowl. The bowl of grapes looked delicious, but after picking through so many rotten grapes I didn’t feel like eating any.\nThis seems contridictory; I would prefer two ripe grapes to twenty ripe grapes and 4 rotting grapes. This resonates with an experiment in Christopher Hsee’s Less is Better paper. University students were split into groups and each was asked to price a dinnerware set. One set had 8 dinner plates, 8 soup bowls and 8 desert places all in good condition. The other set had everything in the first set plus 8 cups, 2 of which were broken, and 8 saucers, 7 of which were broken. Students who saw the first set tended to give a higher price ($32) than the second set ($23), with the same framing of typical prices of $30-$60 for dinnerware sets. Clearly having a set with more items should theoretically be more valuable. Students who saw both sets together priced the second set slightly higher.\nIn Chapter 15 of Thinking Fast and Slow, Daniel Kahneman explains this as comparison with prototypes. We value the items by a typical item in the set; having a worse example reduces the average value and so adding worthless items decreases the value of the set. He mentions an experiment from John List where a set of ten high-value baseball cards was valued higher than the same set of ten cards with three modest-value cards (by different groups of people).\nI can see some ways to rationalise this way of thinking. Maybe being close to rotten grapes increases the chance they will be rotten. Similarly the broken cups and saucer might indicate something had happened to the set and there might be other faults we hadn’t noticed. These kinds of faults align with prototype thinking; but don’t really stand up to scrutiny here.\nAnother rationale is adding some bad items means you have to do additional work to separate the good from the bad. Disposing of the bad cups and saucers is an additional task, as is throwing out the bad grapes or reselling the mediocre baseball cards. However in each of these cases the cost is very low, and so at worst the items should be close in price.\nI think about this when creating a presentation. It’s really easy to add slides containing lots of information. Once I’ve added a slide it’s really hard to delete because of the sunk time cost; it feels like I would be removing value. But removing the least valuable slides increases the average value of the presentation, and so the final product would be considered more valuable. This is why sketching out a presentation can be so effective, you remove the least valuable slides before you even make them."
  },
  {
    "objectID": "nlp-dataset-bootstrap/index.html",
    "href": "nlp-dataset-bootstrap/index.html",
    "title": "Building NLP Datasets from Scratch",
    "section": "",
    "text": "So what is the best way to build a labelled NLP dataset to solve a problem? I believe it’s using anything you can to get an initial labels to get the labels quickly, then refine the labels, test the dataset and iterate. Only once you’ve proved a solution do you invest in large annotation (and only if you can prove it has business value).\nThere are always ways to get rough labels. Heuristics based on domain knowledge. Rule based methods from the dependency parse (which are marvellously accurate with modern neural dependency parsers). Taking an external labelled dataset and modelling the labels onto your dataset. Using complementary data, such as behavioural data (think matrix factorisation) to map the data onto a space.\nOnce you have rough labels you can then refine them. You can use data programming to merge data from multiple noise labellers to get proposed labels. You can train models using the probabilistic labels to get an initial classifier. You can then use active learning to quickly improve performance, and get to a model good enough to check viability.\nI don’t believe there are any good resources on how to do this whole process from end to end, but it’s very valuable to be able to take unlabelled text and within a few days have a process for extracting meaningful information from it."
  },
  {
    "objectID": "disaster-annotation/index.html",
    "href": "disaster-annotation/index.html",
    "title": "Human in the Loop for Disaster Annotation",
    "section": "",
    "text": "The author has made the code available; when I first tried to pip install -r requirements.txt in a fresh virtual environment I was told it couldn’t find the dependencies, so I dropped the == restrictions from requirements.txt and it seemed to work fine. Running python active_learning_basics.py dropped me into a command line interface where I could annotate examples. There are a few commands which are submitted by typing and pressing enter. No input labels the shown example as not-disaster-related; a 1 labels it as disaster-related, a 2 goes back to the previous example, s saves the annotations and d shows the detailed instructions. Aside from save I needed all of these; most examples are not-disaster-related so saving a keystroke was a blessing, after a while my attention would wander and I’d need to go back to a previous example, and when I got to a tricky example I’d reread the detailed instructions.\nThe labelling process starts with random sampling, and then once enough data is trained it switches to a blend of uncertainty sampling (80%), diversity sampling (10%), and random sampling (10%). At the start it’s a real slog; there are very few disaster related examples (I labelled 17 in the first 300) and it’s really searching for anything related to disasters. After the first training it quickly gets more interesting; there’s a lot more relevant examples (and some strange examples from the diversity sampling; many in Indonesian). It’s easier to pay attention and start to see themes. After the second iteration it gets more interesting again, and I spend a lot more time looking back at the instructions. When I first started annotating any mention of fire seemed like a disaster; but as I read more examples I started thinking about whether a house fire really is a disaster. By the third iteration I got very interesting edge cases which really made me think. At this stage I really needed to go back and really understand the outcomes we’re trying to drive (for example are fears of a disaster “disaster-related”, how big does the impact need to be for it to be disaster-related). As a result of this reflection my annotations drift over time as my understanding of the task changes.\nThis kind of labelling is a human endeavour, and it has to work well with the human. Annotating the same label over and over is error prone, as I tend to lose focus and fall into bias. Uncertainty labelling gives a good mixture of positives and negatives, and really hard examples, which raises the conversation of what are we trying to determine. It’s a rewarding process seeing the more interesting examples come up. A sprinkling of random and diversity sampled texts doesn’t detract from the experience, and helps the model explore a wider space (which can lead to new conversations).\nEach time a model is trained it’s saved to a file with the date time, F1 score, AUROC, and number of labelled examples. This gives a clear indication of how much training is improving, and is vindicating that even though I think I’m off the strict guidelines I’m still teaching the model useful things. I ended up with files like this:\n20220629_055234_0.0_0.528_400.params\n20220629_060224_0.057_0.538_500.params\n20220629_070931_0.109_0.562_600.params\n20220629_085942_0.134_0.627_700.params\nThe interface matters a lot; it doesn’t need to be complicated, but it needs to be clear. I found the new text coming at the bottom of the screen to detract from my focus. I also wish there was a way to abstain or flag examples; some were not in my language, and some were too ambiguous from the headline alone. I was worried that starting with a randomly initialised model would take too much data to learn, but for this simple problem I got clear feedback at 500 parameters, and improvements every extra 100 parameters. However I wonder how to tell when the task is too hard for the model to learn and needs to be broken down into smaller pieces (or if that’s ever a problem for modern language models)."
  },
  {
    "objectID": "topn-chaining/index.html",
    "href": "topn-chaining/index.html",
    "title": "Second most common value with Pandas",
    "section": "",
    "text": "I really like method chaining in Pandas. It reduces the risk of typos or errors from running assignment out of order. However some things are really difficult to do with method chaining in Pandas; in particular getting the second most common value of each group.\nThis is much easier to do in R’s dplyr with its consistent and flexible syntax than it is with Pandas."
  },
  {
    "objectID": "topn-chaining/index.html#solution-with-dplyr",
    "href": "topn-chaining/index.html#solution-with-dplyr",
    "title": "Second most common value with Pandas",
    "section": "Solution with dplyr",
    "text": "Solution with dplyr\nThis is pretty straightforward to solve in R with dplyr; we can first sort the columns by frequency and pick the second element:\nd <- data.frame(x=c(1,1,1,2,2,3),\n                y=c(1,2,3,1,2,1),\n                n=c(3,2,1,1,2,1))\n\nd %>%\ngroup_by(x) %>%\narrange(desc(n)) %>%\nsummarise(n = sum(n), second = nth(y, 2))\nThis gives exactly the result above."
  },
  {
    "objectID": "topn-chaining/index.html#solving-with-pandas",
    "href": "topn-chaining/index.html#solving-with-pandas",
    "title": "Second most common value with Pandas",
    "section": "Solving with Pandas",
    "text": "Solving with Pandas\nStarting with the same dataframe\ndf = pd.DataFrame({'x': [1, 1, 1, 2, 2, 3],\n                   'y': [1, 2, 3, 1, 2, 1],\n                   'n': [3, 2, 1, 1, 2, 1]})\nTo get the results over multiple lines is straightforward:\ntotals = df.groupby('y').n.sum()\n# Note nth is 0 indexed\nsecond = df.sort_values('n', ascending=False).groupby('x').y.nth(1)\nans = pd.DataFrame({'n': totals, 'second': second})\nThis is fine, but it means you have to break a chain. You can chain directly with agg if we want to find the top value:\n(df\n.sort_values('n', ascending=False)\n.groupby('x')\n.agg(n=('n', 'sum'), first=('y', 'first'))\n)\nUnfortunately there’s no built in second function. There is an nth function, but there’s no way to pass the argument n in the agg call.\nWe could try to wrap nth in a partial, but I can’t work out where in pandas nth is defined. Passing pandas.core.groupby.generic.DataFrameGroupBy.nth to agg gives an error.\n> df.groupby('y').agg(a=('x', lambda x: nth(x,1)))\nTypeError: n needs to be an int or a list/set/tuple of ints\nWe could try to define our own function to find the nth item, iloc almost works, but if there’s an item that doesn’t have an nth item it raises an IndexError.\n> (df\n.sort_values('n', ascending=False)\n.groupby('x')\n.agg(n=('n', 'sum'), second=('y', lambda y: y.iloc[1]))\n)\nIndexError: single positional indexer is out-of-bounds\nAnother strategy would be to slice into a running count; in dplyr:\nd %>%\ngroup_by(x) %>%\narrange(desc(n)) %>%\nmutate(rn = row_number(),\n       second = ifelse(rn == 2, y, NA)) %>%\nsummarise(n=sum(n), second=first(na.omit(second)))\nWe can do this in Pandas because the first function ignores NaN values where it can. Without chaining in Pandas this looks like:\ndf['rn'] = df.sort_values('n', ascending=False).groupby('x').cumcount()\ndf['second'] = df.y[df.rn == 1]\ndf.groupby('x').agg(n=('n', 'sum'), second=('second', 'first'))\nI’m still not sure how to chain either of the two ways! One way I can get it to chain is by setting the index and assigning:\n(df\n.sort_values('n', ascending=False)\n.set_index('x')\n.assign(second=lambda df: df.groupby('x').y.nth(1))\n.groupby('x')\n.agg(n=('n', 'sum'), second=('second', 'first'))\n)\nUnfortunately here the cure is worse than the disease and the chain is hard to manage and unreadable.\nA cleaner way is to define an nth function that does what we need:\nimport numpy as np\ndef get_nth(n):\n  def nth(x):\n    return x[n] if len(x) > n else np.nan\n  return nth\n\n(df\n.sort_values('n', ascending=False)\n.groupby('x')\n.agg(n=('n', 'sum'), second=('y', get_nth(1)))\n)\nAnother even better option suggested by Samuel Oranyeli is to use pipe to be able to use nth with other aggregations:\ndf\n.groupby('x')\n.pipe(lambda df: pd.DataFrame({'frequency' : df.n.sum(),\n                               'second' : df.y.nth(1)}))\nWhile these will do, they’re still quite frustrating to use. I’ll be watching the Python libraries that are built on top of Pandas, like Siuba (an adaptation of dplyr) and Datatable (an adaptation of the R DataTables library), which may make these transformations easier to do."
  },
  {
    "objectID": "sql-view/index.html",
    "href": "sql-view/index.html",
    "title": "SQL Views for hiding business logic",
    "section": "",
    "text": "Very quickly I end up with complex spaghetti SQL, which either contains monstrous subqueries or a chain of CREATE TEMPORARY TABLE. Moreover everyone I talk to has a slightly different logic for working around these issues, and even I’m not consistent between analyses. This a perfect usecase for SQL Views.\nA view is just a way of treating the result of a query as a table. So once I’ve got my complex query that excludes the right rows, has sensible column names and deduplicates the rows I can wrap it in a CREATE VIEW statement and then reuse it later. Moreover when I find out a new issue in the dataset I can update my CREATE VIEW statement and my previous reports will be automatically corrected; which is great as long as you can manage expectations about reporting consistency. This is much nicer than creating a table as it will always be up to date with the underlying dataset.\nThis is just a small part of the solution; documenting the datasets and having appropriate monitoring are also really important. But having convenient views helps work around footguns in the underlying datasets that can’t be changed, meaning you’re more likely to get the right data and get it faster.\nOne issue that can arise is dependencies. If many other people start using it you may get to a point where you can’t make changes because it will impact other people. There’s lots of different solutions, but the easiest may be to version your view (maybe putting it in an archive schema if people still need the historical version).\nAnother issue is performance. Whenever you query the view you’re rerunning the view’s query, which can be slow if it’s doing a lot of work. One great solution (if your database supports it) is a MATERIALIZED VIEW. This caches the result of the underlying query to make evaluating it much faster. Otherwise you may need to switch the view to a table and setup a process that rebuilds it regularly. You need to be careful though because if your process breaks you may end up with stale data (and not even know about it).\nOne interesting application of SQL Views is to expose a table in a different way to how the underlying data is stored. This kind of approach is encouraged in PostgREST which exposes a HTTP API from PostgreSQL tables and views. When you need to change the underlying data structure you can abstract these changes away from the API with a view.\nSQL Views are a useful way of hiding some complexity of queries that you often reuse. Like any shared asset you have to think about dependencies on it as you change it, but it offers a way to reuse logic accross different queries with minimal maintenance (as opposed to a table you have to keep up to date)."
  },
  {
    "objectID": "embrace-extend-extinguish/index.html",
    "href": "embrace-extend-extinguish/index.html",
    "title": "Embrace, Extend and Extinguish",
    "section": "",
    "text": "Embracing a technology with an existing market is an effective way to quickly gain adoption. There are huge numbers of people sending and receiving emails with SMTP/POP/IMAP, distributing and receiving podcasts via RSS, and publishing and viewing websites via HTML/CSS/Javascript. If you want to break into one of these markets with both producers and consumers starting with an existing technology allows you to acquire both sides of the market.\nExtending a technology on a shared platform helps the market leader lock in existing customers. Adding extensions that are difficult to implement for technical, social or legal reasons means that it’s harder for existing users to migrate away from the implementation. When users start requiring these extensions it means that other users must migrate to the extended implementation if they want to continue to communicate.\nEmbracing a technology allows breaking into an existing market, and when there is enough market share extending drives more users towards the platform through network effects. If the dominance and network effects are strong enough then the competitors will effectively be extinguished. Unless they extend away there’s no barrier from migrating from competitors.\nMicrosoft is well known for these techniques for web technologies. They adopted open web standards in Internet Explorer allowing people to access the internet. At the same time they introduced proprietary extensions such as ActiveX, which allowed websites to provide extra functionality specifically to Windows users on Internet Explorer. Because Windows had such market dominance many developers were willing to build these extensions, which meant consumers were pushed to use IE in Windows to access the content, increasing market dominance and acceptance of the extensions. While Microsoft didn’t win the Internet, the same opportunity can be seen now with Google Chrome which has dominance of the browser market (outside of the substantial iOS market); all that’s needed are compelling extensions the competitors can’t emulate.\nAmazon Web Services has its own related mechanism of devouring open source served products. AWS pioneered a very effective method of billing for access to infrastructure and managed services. Because services are billed in small increments using a new service does not trigger a procurement process. The micro purchasing decisions end up in the hands of developers, and they actively educate them on best practices that involve a large number of services, which makes products hard to migrate. They also have minimal charges for data ingress, but hefty fees for data egress making it cheap to migrate to but expensive to offload processing to other cloud providers.\nAWS embraces open source server technologies, building their own managed services of open source technologies such as MongoDB, Elasticsearch, Redis, MySQL, Postgres, Presto and Kubernetes. This means many developers will use the AWS provided solution rather than manage their own MongoDB instance, and can easily migrate an existing self-hosted service. However they also extend their offerings to work better with other Amazon managed services, further driving developers against self hosting. Many of these platforms monetise by providing managed services and AWS is extinguishing them; MongoDB and Redis have changed their licence to try to block this, but I don’t think it has been a large blocker because AWS is so much better resources than these competitors.\nAnother example is Apple iMessage. The standards of SMS and MMS are available on Apple phones allowing them to communicate with other forms via this standard mechanism. However they extend this between iOS devices to have extra features and a differentiated experience, making it more appealing to interact with other iOS users. Because Apple is the market leader in mobile it pushes people towards iOS, and it makes it harder for people to switch away from iOS; they end up losing their history and missed messages in the transition until they can work out how to switch it off.\nIt’s not just for software giants either. The messaging client Slack originally embraced other chat protocols like IRC by providing gateways/bridges. They would enable you to send and receive messages between Slack and IRC. This meant they could get users using other chat clients to communicate on their platform without the friction of forcing them to switch. At the same time they built many extensions that weren’t available on existing protocols, and so encouraged migration to Slack itself. As it gets to a critical mass Slack is slowly removing the ability to create these extensions by removing test tokens and pushing them to more limited Slack apps, moving into the Extinguish phase.\nEmbrace and extend is an effective strategy for technology companies with leverage to dominate a new market."
  },
  {
    "objectID": "hil-bicycle/index.html",
    "href": "hil-bicycle/index.html",
    "title": "Human-in-the Loop: Finding Bicycles in Images",
    "section": "",
    "text": "Transportation researchers want to estimate the number of people who use bicycles on certain streets.\n\n“I want to collect information about how often people are cycling down a street”\n“I want to capture this information from thousands of cameras and I don’t have the budget to do this manually”\n“I want my model to be as accurate as possible”\n\n\nBased on this he has designed an interface for rapid annotation. It shows little thumbnails of images on the left side of the screen, with the top image highlighted with a red border. You can then use the keyboard to “z”oom to get a bigger image in the right of the screen, or annotate as a “b”icycle or “n”ot. I suspect the images aren’t always zoomed to make the process faster and use less bandwidth; the images are donwloaded from Open Images and the thumbnails are much smaller.\nThe process feels very snappy which makes annotation very pleasant. When examples are slow to update it feels frustrating to annotate, but this is so smooth I get addicted to annotating. If an item has already been annotated with a different label you’ll get a popup warning (e.g. “This does not contain a bicycle, according to existing annotations.”). I found this really useful for learning how to annotate; at the start I’d miss smaller bicycles in the corners of images and this feedback helped me see them.\nHowever the thumbnail view did sometimes make me use the context to label the picture. If the picture contained a street-scape I’d normally zoom in because there could often be a bike hidden in a corner, but otherwise if I didn’t see a bike I wouldn’t zoom in. This could lead to forms of bias which would actually be improved by cropping closer to regions that could contain a bicycle.\n\n\n\nBicycle Hard to See\n\n\nThere were also some rough edges in capturing annotations. The worst is that it would capture annotations in a background process, and due to some failure I lost scores of annotations which was very disheartening. I’ve heard data scientists talk lightly about annotations being lost before; if you respect your annotators you should always take this very seriously. Undo was also a missing feature, which was annoying when I accidentally made an incorrect annotation. Even worse an image I mislabelled came up again, and when I tried to correct it I got the popup warning that it disagreed with a previous annotation (and so was discarded!). It would also be nice to have some way to “abstain” or “comment” on hard cases; this makes statistical quality control of annotations harder but can help raise genuine ambiguities like a bicycle frame (given we’re trying to collect information about how often people are cycling down a street I guess this shouldn’t be annotated).\n\n\n\nBicycle Frame\n\n\nBehind the scenes it works continually downloads images from open images and creates feature vectors using Faster R-CNN trained on ImageNet (which looks for a bicycle bounding box) and ResNeXt (in particular ResNext50_32x4d) using the penultimate layer weights as features. In principle it then trains a linear classifier on these features, but I never actually saw it finish an evaluation on a trained model (perhaps because my laptop wasn’t up to the job; unfortunately with the Python library eel being used it’s a little extra work to get running on a remote server so I never tried it). Because of this I don’t believe active learning was actually used (but because there are a lot of bikes in the sample data it was still interesting enough).\nIt would be a good exercise to change the code to fix these things but it’s got a lot of spaghetti with global mutable state which makes it a little hard for me to understand. Not to criticise the author; it’s fantastic he shared these examples, but it’s non-trivial to add features which discourages me from trying (someone else did try but made so many changes it’s hard to review).\nIt’s becoming really apparent that Human-in-the-Loop processes are very fragile. Small bugs in how the data is sent, or the model is updated or evaluated can completely break the process; having some simple random sampling as fallback makes sure it keeps working if everything else is broken. The annotation interface is really important in the quality of labelled data, including things like the speed of annotation and the data sampled. When it works it’s great (like in the disaster annotation from the same book), but it’s easy to get wrong."
  },
  {
    "objectID": "export-athena/index.html",
    "href": "export-athena/index.html",
    "title": "Exporting data to Python with Amazon Athena",
    "section": "",
    "text": "One necessary hurdle in doing data analysis or machine learning is loading the data. In many businesses larger datasets live in databases, in an object store (like Amazon S3) or the Hadoop File System. For some use cases you can do the work where the data lives using SQL or Spark, but sometimes it’s more convenient to load it into a language like Python (or R) with a wider range of tools.\nPresto, and Amazon’s managed version Athena are very powerful tools for preparing and exporting data. They can query data accross data files directly in S3 (and HDFS for Presto) and many common databases via Presto connectors or Athena’s federated queries. They’ve got a very powerful query language and can process large volumes of data quickly in memory accross a cluster of commodity machines. For this reason many tech companies like Facebook, Uber and Netflix use Presto/Athena as a core way to access their data platform.\nThe most effective workflow I’ve found for exporting data from Athena or Presto into Python is:\nThis is very robust and for large data files is a very quick way to export the data. I will focus on Athena but most of it will apply to Presto using presto-python-client with some minor changes to DDLs and authentication.\nThere is another way, directly reading the output of an Athena query as a CSV from S3, but there are some limitations.\nI have a sample implementation showing how to query avro with query_avro and using the CSV trick with query.\nNote that since this article was originally written Athena has added an unload command for exporting a query result as a file type, and AWS Data Wrangler now has convenient wrappers for quickly exporting data from Athena by using a CTAS or unload query in the background."
  },
  {
    "objectID": "export-athena/index.html#optimisations",
    "href": "export-athena/index.html#optimisations",
    "title": "Exporting data to Python with Amazon Athena",
    "section": "Optimisations",
    "text": "Optimisations\nThere’s a lot that could be done to make this faster or more convenient:\n\nThe queries could be executed without blocking using the AsynchronousCursor\nS3 files could be downloaded in parallel, which may be faster\nThe files don’t need to be directly downloaded when parsing a S3 path to Pandas or using s3fs (this is usually slower)\nThe files could be concatenated together into a single outfile"
  },
  {
    "objectID": "export-athena/index.html#choosing-an-export-format",
    "href": "export-athena/index.html#choosing-an-export-format",
    "title": "Exporting data to Python with Amazon Athena",
    "section": "Choosing an export format",
    "text": "Choosing an export format\n\n\n\nFormat\nPython\nPandas\nDatatypes\nStorage Type\nCLI Tool\n\n\n\n\nAvro\nfastavro\nWith pandavro\nAll\nRow\navro-tools\n\n\nJSON lines\njson (builtin)\nread_json(..., lines=True)\nNot binary, decimal, date, timestamp\nRow\njq or cat\n\n\nParquet\npyarrow\nread_parquet(..., engine='pyarrow')\nAll*\nColumn\nparquet-tools\n\n\nORC\nN/A\nNo\nAll\nColumn\norc-tools\n\n\nTEXTFILE\nN/A\nNo\nString\nRow\ncat -vt\n\n\n\nAvro can represent almost all Athena/Presto datatypes (except Map) and has excellent support through fastavro. The only major drawback is that it doesn’t have native pandas support, but is very easy to convert.\nJSON format is also a good choice as it can represent nested structures and all the basic types (strings, integers, double precision floats, boolean and nulls). It won’t preserve the types of some of the more complex datatypes like timestamps, and can’t handle binary data.\nParquet can represent preserve all the datatypes, and as a column store is efficient for both Presto/Athena and Pandas. Unfortunately pyarrow can’t handle lists of structs as raised in ARROW-1644 (though it’s currently being worked on!) Until this happens you can’t read and write arbitrary data from Python (don’t use fastparquet, it considers silently replacing nested structures with nulls a feature, but is fine for simpler data structures (you can usually unnest and destructure the data if you need to). ORC is even less well supported in Python."
  },
  {
    "objectID": "export-athena/index.html#the-problem-with-textfile",
    "href": "export-athena/index.html#the-problem-with-textfile",
    "title": "Exporting data to Python with Amazon Athena",
    "section": "The problem with TEXTFILE",
    "text": "The problem with TEXTFILE\nTEXTFILE is a text delimited format, similar to a CSV.\nAs per the documentation the rows are separated by new lines \\n, and the fields are delimited by a separator, by default the Start of Heading character \\001 (and strangely not the Record Separator). The record separator could be specified to be a ‘,’ (using properties in Presto or the field_delimiter in Athena), and in many cases this will read or write a CSV.\nThere’s a mechanism for escaping characters (so a newline in a field can be written \\n, and a backslash as \\\\) and a special character for NULLs (\\N), but there’s no method for escaping (or quoting) the field separator!\nSo for example the following query in Athena:\ncreate table sandbox.test_textfile with (format='TEXTFILE', delimited=',') as select ',' as a, ',,' as b\nleads to an output file (which you can find with select distinct \"$path\" from sandbox.test_textfile)\n,,,,\\n\nIt’s impossible to tell if it’s meant to represent (“,”, “,,”) or (“,,”, “,”). If I try to select back from that table the rows are reported to be the empty string!\nThis explains why the default separator is \\001, because it’s unlikely to occur in a field. But if it ever does it will cause hours of headaches to understand why the data is corrupted.\nMoreover this type of format with backslash escapes and special null delimiters is uncommon and unless you’re using the Java Hadoop libraries you’ll probably have to write your own parser. It’s a pity they don’t support RFC-4180 CSVs, but admittedly they have no way of dealing with missing values (nulls) or data types which makes them more limited."
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html",
    "href": "searching-100b-pages-cdx/index.html",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "",
    "text": "Common Crawl builds an open dataset containing over 100 billion unique items downloaded from the internet. Every month they use Apache Nutch to follow links accross the web and download over a billion unique items to Amazon S3, and have data back to 2008. This is like what Google and Bing do to build their search engines, the difference being that Common Crawl provides their data to the world for free.\nBut how do you find for a particular webpage in petabytes of data? Common Crawl provides two types of indexes for this: the Common Crawl Capture Index (CDX) and the Columnar Index. This article talks about the CDX Index Server and a future article will talk about the more powerful columnar index.\nCommon Crawl tries to do a broad search, getting a wide sample of the web rather than a deep sample of a few websites, and respects robots.txt so not every page will be in there. It’s useful to know whether Common Crawl even contains the information you’re looking before you start, and the index will tell you where to look.\nThis article covers using the web interface for quickly checking what’s there, using cdx_toolkit to get and download results from the command line or Python, and using the index and fetching with HTTP requests for custom usecases. There are other tools as well like the CDX Index Client for command line use and comcrawl from python, but they seem less flexible than the other options.\nSee the corresponding Jupyter notebook (raw) for more code examples."
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#cdx-toolkit-in-the-command-line",
    "href": "searching-100b-pages-cdx/index.html#cdx-toolkit-in-the-command-line",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "CDX Toolkit in the Command Line",
    "text": "CDX Toolkit in the Command Line\nYou can use it from the command line with cdxt. You can specify a range of dates in the form YYYYMM (not weeks like in the index files!), and whether to return a CSV (default) or lines of JSON:\ncdxt --cc --from 202002 --to 202005 iter \\\n  'https://www.reddit.com/r/dataisbeautiful/*'\nYou can pass other arguments to filter the result or customise the fields returned. Here’s an example to count the number of archived pages in dataisbeautiful fetched with 200 OK status between Feb and May 2020 (I removed query parameters with sed because here they are just tracking tags).\ncdxt --cc --from 202002 --to 202005 \\\n     --filter '=status:200' iter \\\n     'https://www.reddit.com/r/dataisbeautiful/*' \\\n     --fields url | \\\n  sed 's/\\?.*//' | \\\n  sort -u | \\\n  wc -l\nYou can easily switch from Common Crawl with -cc to the Internet Archive’s Wayback Machine with -ia (but it doesn’t support all the same filters).\ncdxt --ia --from 202002 --to 202005 iter \\\n  'https://www.reddit.com/r/dataisbeautiful/*'\nYou can download the Web Archive content using the warc subcommand.\ncdxt --cc --from 202002 --to 202005 \\\n     --filter '=status:200' --limit 10 warc \\\n     --prefix DATAISBEAUTIFUL \\\n     'https://www.reddit.com/r/dataisbeautiful/*'\nA bunch of Nones get printed to the screen and produces a file DATAISBEAUTIFUL-000000.extracted.warc.gz. A WARC is essentially the HTML preceded by some headers containing metadata about the request and the response. It’s simple enough that you could parse it manually, or you could use the Python warcio library.\n\n\n\nSample of WARC Output"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#cdx-toolkit-in-python",
    "href": "searching-100b-pages-cdx/index.html#cdx-toolkit-in-python",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "CDX Toolkit in Python",
    "text": "CDX Toolkit in Python\nYou can also use the CDX Toolkit as a library in Python. The API for the CDXFetcher is similar to the CLI except from becomes from_ts:\nimport cdx_toolkit\ncdx = cdx_toolkit.CDXFetcher(source='cc')\nobjs = list(cdx.iter(url, from_ts='202002', to='202006', \n                     limit=5, filter='=status:200'))\n[o.data for o in objs]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurlkey\ntimestamp\nstatus\nmime\nurl\nlanguages\nfilename\nlength\ncharset\ndigest\noffset\nmime-detected\n\n\n\n\ncom,reddit)/r/dataisbeautiful/comments/718wt7/heatmap_of_my_location_during_last_2_years_living\n20200330143847\n200\ntext/html\nhttps://www.reddit.com/r/dataisbeautiful/comments/718wt7/heatmap_of_my_location_during_last_2_years_living/\neng\ncrawl-data/CC-MAIN-2020-16/segments/1585370497042.33/warc/CC-MAIN-20200330120036-20200330150036-00407.warc.gz\n69934\nUTF-8\nK7RHDCY4H6XIAFL7SLFTMUV76XFOEM7K\n1143289534\ntext/html\n\n\ncom,reddit)/r/dataisbeautiful/comments/7wcyiq/this_is_what_8_months_of_roulette_looks_like_oc\n20200408190429\n200\ntext/html\nhttps://www.reddit.com/r/dataisbeautiful/comments/7wcyiq/this_is_what_8_months_of_roulette_looks_like_oc/\neng\ncrawl-data/CC-MAIN-2020-16/segments/1585371821680.80/warc/CC-MAIN-20200408170717-20200408201217-00043.warc.gz\n85936\nUTF-8\n3VQ6OENLZIZGFNY7X3TIYNOYMGLABZFR\n1099976248\ntext/html\n\n\ncom,reddit)/r/dataisbeautiful/comments/c89mz2/battle_dataviz_battle_for_the_month_of_july_2019/eskzdhd\n20200403174615\n200\ntext/html\nhttps://www.reddit.com/r/dataisbeautiful/comments/c89mz2/battle_dataviz_battle_for_the_month_of_july_2019/eskzdhd/\neng\ncrawl-data/CC-MAIN-2020-16/segments/1585370515113.54/warc/CC-MAIN-20200403154746-20200403184746-00236.warc.gz\n23114\nUTF-8\nIS4SLLIK7QHNEAJ23E7H4H5ZK2HEMME3\n1080275232\ntext/html\n\n\ncom,reddit)/r/dataisbeautiful/comments/csl706/i_recorded_my_travels_as_a_professional_truck\n20200404003226\n200\ntext/html\nhttps://www.reddit.com/r/dataisbeautiful/comments/csl706/i_recorded_my_travels_as_a_professional_truck/\neng\ncrawl-data/CC-MAIN-2020-16/segments/1585370518767.60/warc/CC-MAIN-20200403220847-20200404010847-00342.warc.gz\n81851\nUTF-8\n3BP6SQLMDA3EHICA5TRBNFBCRNDPEOLT\n1106586323\ntext/html\n\n\ncom,reddit)/r/dataisbeautiful/comments/dp5tda/oc_i_cycled_through_all_the_streets_central_london\n20200331141918\n200\ntext/html\nhttps://www.reddit.com/r/dataisbeautiful/comments/dp5tda/oc_i_cycled_through_all_the_streets_central_london/\neng\ncrawl-data/CC-MAIN-2020-16/segments/1585370500482.27/warc/CC-MAIN-20200331115844-20200331145844-00166.warc.gz\n79999\nUTF-8\nPOVTU3VOPDUU2CAB2OWZTTBVYGM7HMFX\n1104520094\ntext/html\n\n\n\nThe raw archived HTML can be retrieved with .content:\nfrom bs4 import BeautifulSoup\nhtml = objs[0].content\nsoup = BeautifulSoup(html, 'lxml')\nsoup.head.title.text\nYou can also get the warcio object with .warc_record\nobjs[0].warc_record.rec_headers.get_header('WARC-Target-URI')"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#getting-the-available-collections",
    "href": "searching-100b-pages-cdx/index.html#getting-the-available-collections",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Getting the available collections",
    "text": "Getting the available collections\nFirst we need to know what indexes are available; this is stored in a JSON file called collinfo.json.\ncdx_indexes = requests.get('https://index.commoncrawl.org/collinfo.json').json()\nThis contains JSON data with the id, description, and API locations for each crawl.\n\n\n\n\n\n\n\n\n\nid\nname\ntimegate\ncdx-api\n\n\n\n\nCC-MAIN-2020-24\nMay 2020 Index\nhttps://index.commoncrawl.org/CC-MAIN-2020-24/\nhttps://index.commoncrawl.org/CC-MAIN-2020-24-index\n\n\nCC-MAIN-2020-16\nMarch 2020 Index\nhttps://index.commoncrawl.org/CC-MAIN-2020-16/\nhttps://index.commoncrawl.org/CC-MAIN-2020-16-index\n\n\nCC-MAIN-2020-10\nFebruary 2020 Index\nhttps://index.commoncrawl.org/CC-MAIN-2020-10/\nhttps://index.commoncrawl.org/CC-MAIN-2020-10-index\n\n\nCC-MAIN-2020-05\nJanuary 2020 Index\nhttps://index.commoncrawl.org/CC-MAIN-2020-05/\nhttps://index.commoncrawl.org/CC-MAIN-2020-05-index\n\n\nCC-MAIN-2019-51\nDecember 2019 Index\nhttps://index.commoncrawl.org/CC-MAIN-2019-51/\nhttps://index.commoncrawl.org/CC-MAIN-2019-51-index\n\n\n…\n…\n…\n…\n\n\nCC-MAIN-2008-2009\nIndex of 2008 - 2009 ARC files\nhttps://index.commoncrawl.org/CC-MAIN-2008-2009/\nhttps://index.commoncrawl.org/CC-MAIN-2008-2009-index\n\n\n\nIf we want to look through multiple collections we would have to query each API endpoint separately. Note that really old indexes use a different id format with a range of years.\nLet’s pick the most recent crawl’s API endpoint.\napi_url = cdx_indexes[0]['cdx-api']"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#simple-cdx-query",
    "href": "searching-100b-pages-cdx/index.html#simple-cdx-query",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Simple CDX Query",
    "text": "Simple CDX Query\nWe can then use the cdx-api URL to query the relevant indexes.\nr = requests.get(api_url,\n                 params = {\n                     'url': 'reddit.com',\n                     'limit': 10,\n                     'output': 'json'\n                 })\nrecords = [json.loads(line) for line in r.text.split('\\n') if line]\nThe JSON records look the same as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurlkey\ntimestamp\noffset\nstatus\nlanguages\ndigest\nlength\nmime-detected\nfilename\ncharset\nmime\nurl\nredirect\n\n\n\n\ncom,reddit)/\n20200525024432\n873986269\n200\neng\nC6Y4VCGYLE3NGEWLJNONES6JMNA74IA3\n40851\ntext/html\ncrawl-data/CC-MAIN-2020-24/segments/1590347387155.10/warc/CC-MAIN-20200525001747-20200525031747-00335.warc.gz\nUTF-8\ntext/html\nhttps://www.reddit.com/\nnan\n\n\ncom,reddit)/\n20200526071834\n787273867\n200\neng\nPHMHCKU365PLDN5UQETZVR4UGMSPDXQJ\n42855\ntext/html\ncrawl-data/CC-MAIN-2020-24/segments/1590347390448.11/warc/CC-MAIN-20200526050333-20200526080333-00335.warc.gz\nUTF-8\ntext/html\nhttps://www.reddit.com/\nnan\n\n\ncom,reddit)/\n20200526163829\n3815970\n200\nnan\nX67YXUXXE5GQPMJKMEE6555BNFPIER7L\n35345\ntext/html\ncrawl-data/CC-MAIN-2020-24/segments/1590347391277.13/robotstxt/CC-MAIN-20200526160400-20200526190400-00048.warc.gz\nnan\ntext/html\nhttps://www.reddit.com\nnan\n\n\ncom,reddit)/\n20200526165552\n879974740\n200\neng\nOSGHIVCFBI47ZSNMLG574K6SBZJ3LTBC\n39146\ntext/html\ncrawl-data/CC-MAIN-2020-24/segments/1590347391277.13/warc/CC-MAIN-20200526160400-20200526190400-00335.warc.gz\nUTF-8\ntext/html\nhttps://www.reddit.com/\nnan\n\n\ncom,reddit)/\n20200527211917\n858583595\n200\neng\nUHM2VERG5OUOELJFD7O25JVUBZVDPDLU\n35751\ntext/html\ncrawl-data/CC-MAIN-2020-24/segments/1590347396163.18/warc/CC-MAIN-20200527204212-20200527234212-00335.warc.gz\nUTF-8\ntext/html\nhttps://www.reddit.com/\nnan\n\n\n\nOf course you can also query the endpoint directly with curl to get the JSON lines:\ncurl 'https://index.commoncrawl.org/CC-MAIN-2020-24-index?url=reddit.com&limit=10&output=json'"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#adding-filters-and-options",
    "href": "searching-100b-pages-cdx/index.html#adding-filters-and-options",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Adding filters and options",
    "text": "Adding filters and options\nWe can add additional options like filters and selecting fields, in the same way exposed by cdx_toolkit. Here we filter to results with a status of 200, that were detected to have mime text/html and that have a URL matching the regex .*/comments/ (so have /comments/ somewhere in the URL).\nr = requests.get(api_url,\n                 params = {\n                     'url': 'https://www.reddit.com/r/*',\n                     'limit': 10,\n                     'output': 'json',\n                     'fl': 'url,filename,offset,length',\n                     'filter': ['=status:200', \n                                '=mime-detected:text/html',\n                                '~url:.*/comments/']\n                 })\nrecords = [json.loads(line) for line in r.text.split('\\n') if line]"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#handling-zero-results",
    "href": "searching-100b-pages-cdx/index.html#handling-zero-results",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Handling zero results",
    "text": "Handling zero results\nWhen there are no results then the response is a 404 with a JSON error message “No Captures found …”.\nr = requests.get(api_url,\n                 params = {\n                     'url': 'skeptric.com/*',\n                     'output': 'json',\n                 })\nr.status_code  # 404\nr.json()       # {'error': 'No Captures found for: skeptric.com/*'}"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#dealing-with-pagination",
    "href": "searching-100b-pages-cdx/index.html#dealing-with-pagination",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Dealing with Pagination",
    "text": "Dealing with Pagination\nThe Common Crawl API by default returns around 15,000 records per page (it’s 5 compressed blocks, which can vary in the number of actual records). You can choose the number of compressed blocks it returns (about 3,000 records per block) with pageSize and the page number with page.\nTo find the total number of pages you can use the showNumPages=True parameter, which gives back a JSON object containing the pageSize, blocks (total compressed blocks of data) and pages to return. The pageSize is in blocks, so pages = math.ceil(blocks/pageSize).\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'showNumPages': True,\n                 })\nr.json()  # {'pageSize': 5, 'blocks': 2044, 'pages': 409}\nYou can then iterate from page 0 to pages - 1.\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'page': 2,\n                 })\nWhen you go past the end of the pages you will get a HTTP 400 error response. You could use this to avoid having to ask the number of pages up front, just iterate until you get an error.\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'page': 409,\n                 })\nr.status_code   # 400\nThe response includes information telling you what went wrong (r.text):\n<!DOCTYPE html>\n<html>\n<head>\n<link rel=\"stylesheet\" href=\"/static/__shared/shared.css\"/>\n</head>\n<body>\n<h2>Common Crawl Index Server Error</h2>\n<b>Page 409 invalid: First Page is 0, Last Page is 408</b>\n\n</body>\n</html>"
  },
  {
    "objectID": "searching-100b-pages-cdx/index.html#retrieving-content",
    "href": "searching-100b-pages-cdx/index.html#retrieving-content",
    "title": "Searching 100 Billion Webpages Pages With Capture Index",
    "section": "Retrieving Content",
    "text": "Retrieving Content\nThe CDX queries return a filename which is on S3 and accessible at https://data.commoncrawl.org/. They also contain a offset and length which tells you where in bytes the record data is and how long it is. We can use a Range header to get just this data (since each whole file is around 1GB).\nrecord = records[0]\nprefix_url = 'https://data.commoncrawl.org/'\ndata_url = prefix_url + record['filename']\nstart_byte = int(record['offset'])\nend_byte = start_byte + int(record['length'])\nheaders = {'Range': f'bytes={start_byte}-{end_byte}'}\nr = requests.get(data_url, headers=headers)\nWe then have to decompress the data since it is gzipped. The gzip library only works on files with headers, so we have to decompress using zlib. We need to set wbits to the right value for gzip, otherwise we get Error -3 while decompressing data: incorrect header check.\nimport zlib\ndata = zlib.decompress(r.content, wbits = zlib.MAX_WBITS | 16)\nprint(data.decode('utf-8'))\nThis then gives the WARC request headers, HTTP response headers and full HTML retrieved (I’ve truncated the output because there’s a lot of HTML):\nWARC/1.0\nWARC-Type: response\nWARC-Date: 2020-05-25T02:44:32Z\nWARC-Record-ID: <urn:uuid:fa7c243e-d055-469b-bb4f-aa8580bc8330>\nContent-Length: 238774\nContent-Type: application/http; msgtype=response\nWARC-Warcinfo-ID: <urn:uuid:2a234f6f-6796-4962-8c6f-84a6fe8b8945>\nWARC-Concurrent-To: <urn:uuid:b7ec4524-bc4a-4da1-906b-6c53f9c9836e>\nWARC-IP-Address: 199.232.65.140\nWARC-Target-URI: https://www.reddit.com/\nWARC-Payload-Digest: sha1:C6Y4VCGYLE3NGEWLJNONES6JMNA74IA3\nWARC-Block-Digest: sha1:HJ6BA5YAW24SEPDAYA5NUAXA6RG2UBBJ\nWARC-Identified-Payload-Type: text/html\n\nHTTP/1.1 200 OK\nConnection: keep-alive\nX-Crawler-Content-Length: 41748\nContent-Length: 237219\nContent-Type: text/html; charset=UTF-8\nx-ua-compatible: IE=edge\nx-frame-options: SAMEORIGIN\nx-content-type-options: nosniff\nx-xss-protection: 1; mode=block\nX-Crawler-Content-Encoding: gzip\ncache-control: max-age=0, must-revalidate\nX-Moose: majestic\nAccept-Ranges: bytes\nDate: Mon, 25 May 2020 02:44:32 GMT\nVia: 1.1 varnish\nX-Served-By: cache-wdc5543-WDC\nX-Cache: MISS\nX-Cache-Hits: 0\nX-Timer: S1590374672.949570,VS0,VE950\nVary: accept-encoding\nSet-Cookie: loid=00000000006kkgzyec.2.1590374671996.Z0FBQUFBQmV5ekVRNHBWX0ZOM3RJb0FRX0FHRzVzNVdlMXY2ejUwdFBxeHJkczRtLUlNR2o1SUxNUGlhSU12WnBsSjFfdmNkYl9fTm9GSUk2SHJHTmdmejUwblMzcnBESm0yZVlYUXBmekNqTVNuQXRTOUpHRndXek9zS1pvVVJxN05HdmVBUmFXZUI; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Wed, 25-May-2022 02:44:32 GMT; secure; SameSite=None; Secure\nSet-Cookie: session_tracker=LwR2XV8052i86pF3B7.0.1590374671996.Z0FBQUFBQmV5ekVRZ2tNSkRpM0ZsYUlLcVJtRFBfOXRsREVCRlRPWElkRFpIUkJtODl3dnpnaDloZDM1NXplM0xMZEZialZxT0RhR250cEtTTTdfbXAyT2dqWGYyVVlSOV9TQ2paLUpITWloVkRibGw1SzhyMGo3b0RCdVhNT0tuN0pZSWU3ZE45Nkc; Domain=reddit.com; Max-Age=7199; Path=/; expires=Mon, 25-May-2020 04:44:32 GMT; secure; SameSite=None; Secure\nSet-Cookie: csv=1; Max-Age=63072000; Domain=.reddit.com; Path=/; Secure; SameSite=None\nSet-Cookie: edgebucket=gwfpIQWim0qQ1ddmdP; Domain=reddit.com; Max-Age=63071999; Path=/;  secure\nStrict-Transport-Security: max-age=15552000; includeSubDomains; preload\nServer: snooserv\n\n<!doctype html><html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"><head><title>reddit: the front page of the internet</title><meta name=\"keywords\" content=\" reddit, reddit.com, vote, comment, submit \" /><meta name=\"description\" content=\"Reddit gives you the best of the internet in one place. Get a constantly updating feed of breaking news, fun stories, pics, memes, and videos..."
  },
  {
    "objectID": "jaccard-duplicates/index.html",
    "href": "jaccard-duplicates/index.html",
    "title": "Near Duplicates with Jaccard",
    "section": "",
    "text": "I’ve looked before at using the edit distance which looks for the minimum number of changes to transform one text to another, but it’s slow to calculate. Instead we treat each document as a bag of n-grams (for some fixed n), and calculate the Jaccard index between them. So two documents will be similar if they contain the same phrases of length n tokens, irrespective of order.\nHow do we pick n? By the shingle inequality as we increase n the values will decrease, but it’s not clear at what value the data should be separated. Intuitively for n = 1 it’s just common terms For n = 1 it’s just common words and it is likely this isn’t very separating (different documents can contain common words); however it’s unlikely that two documents containing lots of phrases of length 5 or 6 in common are a coincidence. On the other hand errors in the data (e.g. punctuation or case being changed in one copy) that can break otherwise long identical sequences of tokens which means you don’t want to set n too big.\nFor the ads I took a random sample of 2000 ads (producing just under 2 million pairs) and looked at the distribution of the Jaccard values (with log frequency).\n\n\n\nJaccard histogram for 1-grams to 4-grams\n\n\nThe histogram gets much more separated at n=2, then a little bit more at n=3 and is very similar at n=4. For somewhere to start I looked to see how the ads changes with length around n=4, looking at sets of 5 ads at different cutoffs.\nFor Jaccard similarities near 0.5 (around 6 ads per million pairs) and higher the ads are almost identical. This can happen because the ads are the same, or because a previous ad has been used as a template for a different location. These ads are identical except for their location; this kind of an example is going to be a challenge as to detect whether they are actually duplicates.\n\n\n\nAds identical except for location\n\n\nFor Jaccard similarities near 0.1-0.2 (around 6 ads per 10,000) they seem to be from the same company. Many companies have the same description of themselves in every advert which will make their Jaccard distance and relative edit distance look quite low, even though the roles they are posting for are very different. Nearer to 0.1 there tends to be large recruiters because they tend to have more generic job descriptions.\n\n\n\nAds with same company text\n\n\nFor Jaccard similarities near 0.05 the ads are typically unrelated. However they have some phrase in common (often included from the site Adzuna got the job from).\n\n\n\nUnrelated ads with similar text\n\n\nAt a glance seems like the Jaccard index on 4-grams was pretty effective of separating out unrelated jobs from jobs that had a common origin. Because few ads actually come from the same origin this means that it’s a pretty effective way of picking the few similar ads from a sea of different ads. Then further analysis can be done on these ads to determine whether they are actually similar.\nIt’s a bit hard to work out where exactly to draw the line, but somewhere between 0.2 and 0.5 seems about right. Note that the density will grow sparser as we add more ads because there will be fewer near-duplicate pairs relative to the number of non-duplicate pairs. For example if we add one more ad that’s a duplicate of 3 existing ads, we’ll end up with 3 more duplicate pairs out of a total of 2001 extra pairs.\nI wonder if we can further separate the related ads from random ones by weighting the n-grams like with a TF-IDF so that uncommon sequences in the corpus appearing in both documents are weighted apart.\n\nImplementation\nThe implementation is pretty straight forward, but scales quadratically. Given that the arithmetic complement of Jaccard distance (that is 1 - J) is a metric it may be possible to estimate distances with the triangle inequality to make this subquadratic. However the MinHash gives a way to find documents with a high Jaccard similarity very efficiently at scale.\nFirst we need some functions for splitting sentences into tokens.\nimport re\nwhitespace = re.compile('\\s+')\n\ndef tokenize(s:str) -> TokenList:\n    '''Split a string into tokens'''\n    return whitespace.split(s)\n\ndef untokenize(ts:TokenList) -> str:\n    '''Join a list of tokens into a string'''\n    return ' '.join(ts)\nThen we need a way to calculate the shingles of any length:\ndef subseq(seq:List[Any], n:int=1) -> List[Tuple[Any]]:\n    \"\"\"Returns all contiguous subsequences of seq of length n\n    \n    Example: subseq([1,2,3,4], n=2) == [(1,2), (2,3), (3,4)]\n    \"\"\"\n    return [tuple(seq[i:i+n]) for i in range(0, len(seq)+1-n)]\n\ndef shingle(seq:List[str], n:int=1) -> List[str]:\n    return [untokenize(s) for s in subseq(seq, n)]\nI’m going to treat the text as a bag, to represent it as a set I need to add an index for each time a term is seen:\ndef multiset(xs):\n    seen = defaultdict(int)\n    output = set()\n    for item in xs:\n        output.add((item, seen[item]))\n        seen[item] += 1\n    return output\nFinally I need a way of calculating the Jaccard index: \\(\\frac{\\lvert A \\cap B \\rvert}{\\lvert A \\cup B \\rvert}\\). Using the inclusion-exclusion principle this can be calculated without the time consuming step of calculating the union.\ndef fast_jaccard(x, y):\n    n = len(x.intersection(y))\n    return n / (len(x) + len(y) - n)\nFinally we can calculate by looping (since it’s symmetric and the Jaccard intersection of anything by itself is 1 we only need to consider the case i < j):\nshingle_lengths = [1, 2, 3, 4, 5, 6, 7]\nsample_rel_weighted = {}\nads_tok = {i: tokenize(ads[i]) for i in sample_indices}\nfor k in shingle_lengths:\n    ads_shingle_weighted = {i: multiset(shingle(v, k)) for i, v in ads_tok.items()}\n    sample_rel_weighted[k] = {}\n    for i in sample_indices:\n        for j in sample_indices:\n            if i < j:\n                sample_rel_weighted[k][i, j] = fast_jaccard(ads_shingle_weighted[i], ads_shingle_weighted[j])\nOn my laptop for each shingle length with sample_indices having length 2000 it takes about 15s. To run it on the full 400k would take about 8 days (because of quadratic scaling). This is much better than edit distance which would have taken over 30 years, but is still pretty slow. If we wanted to get more precise distances for very close ones we could run edit distance on those with a high Jaccard.\nAnother problem is memory to store all the distances; for our sample of 2000 we have nearly 2 million pairs and so the distances (as 8 byte floats) take up about 16MB. For our full 400k it would take around 640GB of memory (which is definitely beyonb my laptop!) However if we just wanted to keep the ones that are very close that is tractable, since they would be relatively few.\nThere are two directions to go from here, one is to try to get better overlaps with TF-IDF and another is to scale up the process with MinHash."
  },
  {
    "objectID": "alphafold/index.html",
    "href": "alphafold/index.html",
    "title": "AlphaFold: Predicting protein shape from its composition",
    "section": "",
    "text": "Over the past 2 CASP competitions there have been big leaps forward from DeepMind’s AlphaFold and in the recent competition they produced a model that gets very close to experimental error. I’m normally sceptical when there’s a big press release about Deep Learning solving something, but in this case it’s a really hard problem and a big breakthrough. There’s commentary in science magazine that comments on the impact.\nThis could be a precursor for understanding other complex systems where we understand some aspect of the structure but not the emergent properties. For example in materials science a challenge is to understand the properties of a material (such as its malleability, heat resistance and conductivity) given its chemical structure. Or trying to predict specific modes of brain activity from messy MRIs (or even EEGs). Perhaps we could even improve our already very good predictions of weather patterns.\nIt would be really interesting to understand more about their approach and get an indication about whether it would generalise to other domains. DeepMind weren’t the only team to use deep neural networks; what did they do differently? How did they incorporated the physical and chemical knowledge into the framing of the problem?"
  },
  {
    "objectID": "moral-justification/index.html",
    "href": "moral-justification/index.html",
    "title": "Moral Justification",
    "section": "",
    "text": "On the matter of wages and unionization, CSM invoked the laws of political economy, claiming, “It is simply a question of supply and demand, and ought to have been allowed to settle itself on ordinary principles without this interference of paid agitators and demagogues.”\n\nAlfred Marshall contested this on the basis that economics could only explain how things were, not what is ethical.\n\nMarshall denied that political economy could “direct decisions of moral principle,” which is much instead “leave to her sister, the Science of Ethics.”\n\nThis kind of discussion still happens today through extreme free marketeers who claim any that any restrictions or regulations are impeding the efficiency of free markets.\nAnother example of an issue that can have a moral framing is addiction. Posing addiction as an immoral act poses addicts as bad people, who are choosing the wrong path and may be irredeemable. Posing addiciton as a disease or illness makes addicts sufferers with a curable afflication who should be pitied. I don’t have any expertise to know how it should be framed; but the choice has moral implications and impacts how people perceive the outcomes.\nOnce debates become moral they become very hard to move. I think about climate change where there are camps of supporters and sceptics, but the vast majority of both sides don’t really understand the science and are supporting a side for moral reasons (for example their respect, or lack thereof, for the scientific community). Similar debates happen across evolution and creationism. These moral arguments can’t be settled by logic and reason, but The Righteous Mind has made me appreciate that it’s worth understanding and appreciating people’s moral preconceptions."
  },
  {
    "objectID": "fast-web-data-workflow/index.html",
    "href": "fast-web-data-workflow/index.html",
    "title": "Fast Web Dataset Extraction Worfklow",
    "section": "",
    "text": "I’m currently streamlining the process of building a dataset from web data. I want to make it easy for anyone to build their own dataset in a few hours, which requires making the process as smooth as possible. One thing that makes a major difference is caching intermediate results; it makes the process much faster when you hit an error. But done naively it can be confusing; if you change a function you want the cache to be recalculated."
  },
  {
    "objectID": "fast-web-data-workflow/index.html#not-doing-things-twice",
    "href": "fast-web-data-workflow/index.html#not-doing-things-twice",
    "title": "Fast Web Dataset Extraction Worfklow",
    "section": "Not doing things twice",
    "text": "Not doing things twice\nIt may be worth caching the input webpages if you can spare the disk space. Downloading a large number of webpages over a network can be slow so having the pages locally makes things faster (and is cheaper for whoever is providing the data!). However individual webpages can range from a few kilobytes to hundreds of kilobytes (and that’s just the HTML!). At the upper end with 100,000 pages each weighing 100 KB, the total size of raw files is around 10GB, which is a reasonable amount of space to use on a hard drive. Typically they compress well (especially pages from the same website) and you could reduce the usage by an order of magnitude by compressing them. However an advantage of using the uncompressed HTML files is they can be easily viewed in a browser for quick debugging sessions.\nIt can also be worth caching the extraction steps (which will be much smaller), but with the caveat that when we change the extraction we want to reprocess everything, and if we’re being careful to see what changed to ensure we didn’t break anything. It may be worth separating the transformation process into a straightforward extraction step (which can be computationally costly because it involves processing a large HTML file, especially when using something like BeautifulSoup) and one or more involved validation and normalisation steps (which may be faster, but is more likely to break on strange data).\nEach extraction step could process all pages at once (making it easy to check changes), or process files by a time range or folder (making it easy to incrementally add and test new data). I’m not sure what the best approach is here, it makes sense to start with processing things all together and then adding an ability to process certain batches if that’s getting too slow."
  },
  {
    "objectID": "fast-web-data-workflow/index.html#doing-many-things-at-once",
    "href": "fast-web-data-workflow/index.html#doing-many-things-at-once",
    "title": "Fast Web Dataset Extraction Worfklow",
    "section": "Doing many things at once",
    "text": "Doing many things at once\nThe process of extracting data from many web pages is trivially paralellisable; the data can be extracted independently of each other by pure functions. On a laptop the easiest way to take advantage of this is to run multiple threads or processes to make sure all the CPU cores are being used. In Python this can be done using mulitprocessing spawning one process per CPU core.\nIn particular downloading data is best done in parallel; I’ve found having multiple threads for downloading Common Crawl data seems to improve the speed almost linearly up to around 30. I suppose since the data are typically living on separate servers they can be fetched concurrently, reducing the overhead of waiting for a request to be completed.\nIn fact the whole process of extracting, transforming, and aggregating the data fits nicely into a simple map reduce framework. If you wanted to exercise some simple Data Engineering skills it could be interesting to distribute the computation over a cluster (using something like Ray, Dask Distributed, PySpark or even something like pywren or GNU Parallel). However it certainly can run on a commodity laptop with no configuration, it may just take hours on large pipelines."
  },
  {
    "objectID": "really-mean-s1/index.html",
    "href": "really-mean-s1/index.html",
    "title": "Do you really mean S¹?",
    "section": "",
    "text": "To a Euclidean geometer \\(S^1\\) means a circle – a maximal set of points equidistant from a given point. All circles are equivalent in the sense that they can be made equal by a translation and a scaling about the centre.\nIn a two dimensional inner product space \\(S^1\\) typically means the set of all points with norm \\(1\\). A circle is more generally the set of all points with norm \\(r\\) for some real number \\(r\\) and is related to \\(S^1\\) by a scaling transformation.\nTo a group theorist \\(S^1\\) would mean the one dimensional orthogonal group – the group of all transformations in the plane.\nTo a complex analyst \\(S^1\\) could mean either the set of points in \\(\\mathbb{C}\\) with length 1 or it could be the group of linear transformations associated with multiplication by elements of this set. (These are quite different – in the set \\(1\\) has no special meaning but in the group it corresponds to the identity).\nTo a topologist \\(S^1\\) means anything homeomorphic to a Euclidean circle – so includes ellipses, polygons, simple closed curves,…\nTo a differential geometer \\(S^1\\) means anything diffeomorphic to a Euclidean circle, which doesn’t include “most” things a topologist means.\nA set theorist wouldn’t call it \\(S^1\\) , but to her it would be any set with the same cardinality as the real numbers.\nThis is a major theme of category theory – it’s not only the objects that matter but also the maps that preserve them – whether it be affine transformations, orthogonal transformations, group homomorphisms, group homomorphisms, homeomorphisms, diffeomorphisms or bijections. So if you must write \\(S^1\\) to represent a structure at least make sure it is clear which category you are working in – i.e. which maps preserve the structure."
  },
  {
    "objectID": "programming-languages-2020/index.html",
    "href": "programming-languages-2020/index.html",
    "title": "Programming Languages to Learn in 2020",
    "section": "",
    "text": "A language that doesn’t affect the way you think about programming, is not worth knowing.\nAlan Perlis\nI spend a lot of time programming in Python and SQL, some time in Bash and R (or at least tidyverse), and a little in Java and Javascript/HTML/CSS. This set of tools is actually pretty versatile about getting things done, but is fairly narrow from a programming concept perspective. Once in a while I think it’s useful to broaden the programming frame to understand different ways of doing things; even if you still stick to the same few languages.\nI’ve spent significant time in the past programming in Lua, Scheme, and Clojure and spent some time programming in J, Octave/Matlab and ARM Assembly. Each of these languages has contributed to how I think about programming and helps me design programs today. I wanted to reflect a bit on what languages I’d like to learn in the future.\nWith this in mind my primary goal is learning new ways of thinking, and secondarily ones that have a good community and ecosystem that can solve real problems effectively. I’m not thinking too much about employability, as that changes with language fashion every few years.\nBased on some reading I’ve picked a few categories ang languages I would like to learn, and why I picked it over alternatives."
  },
  {
    "objectID": "programming-languages-2020/index.html#resources",
    "href": "programming-languages-2020/index.html#resources",
    "title": "Programming Languages to Learn in 2020",
    "section": "Resources",
    "text": "Resources\nThere are a lot of F# resources, but I’m particularly interested in Domain Modeling Made Functional from F# for Fun and Profit. This sounds like a great resource for a way of thinking of how to design programs and implementing it in F#."
  },
  {
    "objectID": "programming-languages-2020/index.html#alternatives",
    "href": "programming-languages-2020/index.html#alternatives",
    "title": "Programming Languages to Learn in 2020",
    "section": "Alternatives",
    "text": "Alternatives\n\nOCaml, which is similar to F#, but since F# came later (and learned some lessons) and has access to .NET libraries it seems to me a better choice to start.\nHaskell, which I have some passing experience with, but I’ve feel like you end up jumping through monadic hoops for the sake of purity (maybe it’s worthwhile; I’m yet to be convinced).\nScala but it seems a bit more multiparadigm and to learn new concepts F# seems like a better choice.\n\nThese are all interesting languages I’d like to learn more about; but I’d start with F#."
  },
  {
    "objectID": "programming-languages-2020/index.html#resources-1",
    "href": "programming-languages-2020/index.html#resources-1",
    "title": "Programming Languages to Learn in 2020",
    "section": "Resources",
    "text": "Resources\nFrom the Elixir Forum it sounds like Elixir in Action and Programming Elixir are good resources. I’ll have to research more deeply here."
  },
  {
    "objectID": "programming-languages-2020/index.html#alternatives-1",
    "href": "programming-languages-2020/index.html#alternatives-1",
    "title": "Programming Languages to Learn in 2020",
    "section": "Alternatives",
    "text": "Alternatives\n\nErlang is the language that Elixir is built on, but it abstracts a lot of the syntactic sugar away. Learn You Some Erlang is meant to be a great resource for both.\nGo allows a lot of concurrency through Goroutines, and has tiny cross-platform binaries, but I don’t think I’d learn as much about resilience.\nNode.JS allows a lot of asynchronous work through callbacks, but I’m not really convinced that this style is easy to program in."
  },
  {
    "objectID": "programming-languages-2020/index.html#resources-2",
    "href": "programming-languages-2020/index.html#resources-2",
    "title": "Programming Languages to Learn in 2020",
    "section": "Resources",
    "text": "Resources\nI’ve read a lot that K&R C is the place to start. It may not cover modern C, but gets across a lot of the initial philosophy and structure."
  },
  {
    "objectID": "programming-languages-2020/index.html#alternatives-2",
    "href": "programming-languages-2020/index.html#alternatives-2",
    "title": "Programming Languages to Learn in 2020",
    "section": "Alternatives",
    "text": "Alternatives\n\nC++ is what most low-level numeric code is implemented in and is a very useful thing to understand. It’s also got a reputation for being huge and complex and taking a long time to learn. I would go here after learning C.\nRust sounds like it fixes a lot of the hard problems especially in C++ that lead to security vulnerabilities, while remaining low level and fast. But it’s not as widely used yet, and to me it makes more sense to start with C++ before learning C.\nJulia claims to be very fast for numeric calculations, and takes a different approach to Python and R. The ecosystem is building (especially through interfacing with R and Python) and is one to watch. I’ve heard that by design it has great language interoperability.\nSwift is really interesting in building atomically on LLVM. As a fan of Scheme I really like how it builds from LLVM primitives (e.g. see implementations of bool, and it’s useful for building iOS applications. One worth learning.\n\nThese all sound really interesting to me - if only there was more time."
  },
  {
    "objectID": "dvi-by-example/index.html",
    "href": "dvi-by-example/index.html",
    "title": "DVI by example",
    "section": "",
    "text": "The Device Independent File Format (DVI) is the output format of Knuth’s TeX82; modern TeX engines (pdfTeX, luaTeX) output straight to Adobe’s Portable document format (PDF). However TeX82 and DVI still work as well today as they did when they were written; DVI files are easily cast to postscript or PDF.\nThe defining reference for DVI files is David R Fuch’s article in TUGboat Vol 3 No 2.\nTo find out what information is contained in a particular DVI file use Knuth’s dvitype, which outputs the operations contained in the bytecode in human readable format.\nThis article goes into gory detail the instructions contained in a very simple DVI file."
  },
  {
    "objectID": "dvi-by-example/index.html#overview-of-the-format",
    "href": "dvi-by-example/index.html#overview-of-the-format",
    "title": "DVI by example",
    "section": "Overview of the format",
    "text": "Overview of the format\nDVI is designed as a description of how to typeset horizontally and vertically a black and white document (in a left-to-right alphabetic language) for printing.\nThe basic operations are to typeset a character (in the specified font) optionally advancing by the character’s width, to typeset a rectangular box optionally advancing the box’s width, and setting variables (including the font and position where to typeset).\nThe file is encoded as a series of 8-bit bytes; the first byte is an operation followed by a given number of arguments. Each argument is either a fixed number of bytes in length, or has a length given by a prior argument. There are four kinds of parameter: unsigned integer (represented by its bytes as a binary number), signed integer (using two’s complement), pointer, or strings (represented as a series of 1-byte character codes) used for information and filesystems.\nInternally there are a number of state parameters; the current font f (a 4-byte signed integer), the position and spacing variables (h, v, w, x, y, z) (each a 4-byte signed integer) and a stack of position and spacing variables. (h, v) represents the point h units to the right and v units down from the top left corner of the page. w, x are horizontal spacing parameters and y, z are vertical spacing parameters. The units are determined in the file itself.\nBelow are the operations, I will use the notation n[4] to represent a parameter of 4 bytes, and curly braces {} to represent a range of commands.I will use characters a, b, c, … to represent signed integers; i, j, k, l, m, n, … to represent unsigned integers; p, q, … to represent pointers and A, B, … to represent characters and X to represent a custom (user implemented) type. These types have been inferred and not checked, so use with caution.\n\n\n\n\n\nHex code\n\n\nName\n\n\nParams\n\n\nFunction\n\n\n\n\n{0-7F}\n\n\nset_char_{1-127}\n\n\n\n\nTypeset character {1-127} in font f at (h, v), then advance h by the width of that character\n\n\n\n\n{80-83}\n\n\nset{1-4}\n\n\nm[{1-4}]\n\n\nTypeset character m in font f at (h, v), then advance h by the width of that character\n\n\n\n\n84\n\n\nset_rule\n\n\na[4], b[4]\n\n\nTypeset box of width a, height b at (h, v), then advance h by a\n\n\n\n\n{85-88}\n\n\nput{1-4}\n\n\nm[{1-4}]\n\n\nTypeset character m in font f\n\n\n\n\n89\n\n\nput_rule\n\n\na[4], b[4]\n\n\nTypeset box of width a, height b at (h, v)\n\n\n\n\n8A\n\n\nnop\n\n\n\n\nNo operation\n\n\n\n\n8B\n\n\nbop\n\n\na{0-9}[4] p[4]\n\n\nNew page, a{0-9} are TeX registers \\count{0-9} to identify the page for reference, p is a pointer to previous page (or -1 for first page). All state is reset\n\n\n\n\n8C\n\n\neop\n\n\n\n\nEnd of page, output page. Stack should be empty\n\n\n\n\n8D\n\n\npush\n\n\n\n\nPush (h, v, w, x, y, z) onto stack\n\n\n\n\n8E\n\n\npop\n\n\n\n\nPop from stack, and set variables\n\n\n\n\n{8F-92}\n\n\nright{1-4}\n\n\na[{1-4}]\n\n\nAdvance h by a\n\n\n\n\n93\n\n\nw0\n\n\n\n\nAdvance h by w\n\n\n\n\n{94-97}\n\n\nw{1-4}\n\n\na[{1-4}]\n\n\nSet w to a and advance h by w\n\n\n\n\n98\n\n\nx0\n\n\n\n\nAdvance h by x\n\n\n\n\n{99-9C}\n\n\nx{1-4}\n\n\na[{1-4}]\n\n\nSet x to a and advance h by x\n\n\n\n\n{9D-A0}\n\n\ndown{1-4}\n\n\na[{1-4}]\n\n\nAdvance v by a\n\n\n\n\nA1\n\n\ny0\n\n\n\n\nAdvance v by y\n\n\n\n\n{A2-A5}\n\n\ny{1-4}\n\n\na[{1-4}]\n\n\nSet y to a and advance v by y\n\n\n\n\nA6\n\n\nz0\n\n\n\n\nAdvance v by z\n\n\n\n\n{A7-AA}\n\n\nz{1-4}\n\n\na[{1-4}]\n\n\nSet z to a and advance v by z\n\n\n\n\n{AB-EA}\n\n\nfnt_num_{0-64}\n\n\n\n\nSet f to {0-64}\n\n\n\n\n{EB-ED}\n\n\nfnt{1-3}\n\n\nm[{1-3}]\n\n\nSet f to m\n\n\n\n\nEE\n\n\nfnt4\n\n\na[4]\n\n\nSet f to a\n\n\n\n\n{EF-F2}\n\n\nxxx{1-4}\n\n\nm[1-4] X[m]\n\n\nImplementation dependent; nop in general. Sent via TeX’s \\special.\n\n\n\n\n{F3-F5}\n\n\nfnt_def{1-3}\n\n\ni[{1-3}] j[4] k[4] l[4] m[1] n[1] A[m+n]\n\n\nSets font h to be the font loaded from subpath “A[0:m]/A[m:n]” of the standard fonts directory, with checksum j, scaled by k/l. k and l must be less than 2^27\n\n\n\n\nF6\n\n\nfnt_def4\n\n\na[4] j[4] k[4] l[4] m[1] n[1] A[m+n]\n\n\nSets font a as for F3-F5.\n\n\n\n\nF7\n\n\npre\n\n\ni[1] j[4] k[4] l[4] m[1] A[m]\n\n\nPreamble; i is DVI version number which is 2. l is considered a magnification; 1 unit is set to \\(\\frac{j}{k} 10^{-7} \\mbox{m}\\) and the entire document is scaled by a factor of \\(\\frac{l}{1000}\\). A is an information header\n\n\n\n\nF8\n\n\npost\n\n\nSee below\n\n\nPostamble; see below\n\n\n\n\nF9\n\n\npost_post\n\n\nSee below\n\n\nPost postamble; see below\n\n\n\n\nFA-FF\n\n\nundefined\n\n\n\n\n\n\n\n\n\nA dvi must start with a preamble, followed by 1 or more pages ends with a postamble. A page is a bop followed by any instructions and terminating in an eop. The only operations that can go between these chunks are nop and font definitions.\nThe postamble has a 4-byte pointer to the beginning of the last page, then the parameters j, k, and l from the preamble (called numerator, denominator, and magnification respectively), a 4-byte signed integer giving the height+depth of the tallest page, a 4-byte signed integer giving the width or the widest page, a 2-byte unsigned integer giving the maximum stack depth in the DVI, and a 2-byte unsigned integer giving the total number of pages (bop commands).\nThen each font must be defined. Each font must be defined exactly twice in the document, once before its first use (before the postamble) and once in the postamble.\nThe postamble concludes with the post-postamble, which contains a 4-byte pointer to the beginning of the postamble, followed by the version number i from the preamble followed by 4 of more of DFs (why not 8As? I have no idea).\nThe file is thus designed to be read forwards (one operation at a time) or backwards (one page at a time), and useful set-up information (page size and maximum stack depth) are at the back. The files are typically very compact, and because of their linear nature can be processed rapidly. Notice that the page is the minimum displayable unit; a page may be typeset non-linearly, but after a bop the page can no longer be affected."
  },
  {
    "objectID": "dvi-by-example/index.html#simple-example",
    "href": "dvi-by-example/index.html#simple-example",
    "title": "DVI by example",
    "section": "Simple example",
    "text": "Simple example\nI typeset the following file Hello.tex\nHello World!\n\\bye\nand then ran commands\ntex Hello.tex\nxxd Hello.dvi\nwhich yielded\n0000000: f702 0183 92c0 1c3b 0000 0000 03e8 1b20  .......;.......\n0000010: 5465 5820 6f75 7470 7574 2032 3031 332e  TeX output 2013.\n0000020: 3038 2e31 323a 3138 3034 8b00 0000 0100  08.12:1804......\n0000030: 0000 0000 0000 0000 0000 0000 0000 0000  ................\n0000040: 0000 0000 0000 0000 0000 0000 0000 0000  ................\n0000050: 0000 00ff ffff ff8d 9ff2 0000 8ea0 0283  ................\n0000060: 33da 8da0 fd86 cc26 8d91 1400 00f3 004b  3......&.......K\n0000070: f160 7900 0a00 0000 0a00 0000 0563 6d72  .`y..........cmr\n0000080: 3130 ab48 656c 6c6f 9103 5555 5791 ff2a  10.Hello..UUW..*\n0000090: aa6f 726c 6421 8e8e 9f18 0000 8d92 00e8  .orld!..........\n00000a0: 60a3 318e 8cf8 0000 002a 0183 92c0 1c3b  `.1......*.....;\n00000b0: 0000 0000 03e8 029b 33da 01d5 c147 0002  ........3....G..\n00000c0: 0001 f300 4bf1 6079 000a 0000 000a 0000  ....K.`y........\n00000d0: 0005 636d 7231 30f9 0000 00a5 02df dfdf  ..cmr10.........\n00000e0: dfdf dfdf                                ....\nLet’s walk through this byte by byte.\n\nPreamble\nf7 02 018392 c01c3b0000 000003e8\n   1b 20 54 65 58 20 6f 75 74 70 75 74 20 32 30 31 33 2e 30 38 2e 31 32 3a 31 38 30 34\nThe first line starts with the pre opcode, followed by the version 02, numerator = 25400000, denominator = 473628672 and magnitude = 1000. This means 1 unit is \\({25400000 \\over 473628672} 10^{-7}\\mbox{m} = {0.0254 \\over 72.27 * 2^{16}} \\mbox{m}\\) . There are 72.27 standard points in an inch and 2.54 cm in an inch, so a standard point is \\({72.27 \\over 0.0254} \\mbox{m} = 2^{16} \\mbox{unit}\\) . Thus a unit is \\(2^{-16}\\) standard points; what TeX calls a scaled point (sp). The document is then scaled by 1000/1000=1.\nThe second line is the documentation string; 1b states it consists of 27 bytes. The bytes then form the ASCII string\n TeX output 2013.08.12:1804\nEvidently I ran TeX at 18:04 pm on the 12th of August 2013 A.D.\n\n\nBeginning of page\n8b 00000001 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 ffffff\nThe preoperation is followed by the values in the registers count 0 through 9. By default TeX uses \\count0 for the page number and doesn’t affect the other counts, so we get \\count0 = 1 (first page), \\count{1-9}=0. Finally since this is the first page the pointer to the previous page is -1 (ffffff).\n\n\nSetup\n8d\n9f f20000\n8e\nPushes the current (h, v, w, x, y, z) onto the stack; (0, 0, 0, 0, 0), then moves v down by -917504 sp, then pops the stack resetting v; overall achieving nothing. I presume this would probably put a header in a more complex document.\na0 028333da\n8d\na0 fd86cc26\n8d\nMove v down by 42152922 sp = 8.84 inches, pushes onto the stack (0, 42152922, 0, 0, 0, 0), then drops v down -41497562 sp, then pushes onto the stack and pushes onto the stack (0, 655360, 0, 0, 0, 0) (this is 10 standard points down from the top of the page).\n91 140000\nMoves h right by 1310720 sp = 20 standard points.\n\n\nDefine font\nf3 00 4bf16079 000a0000 000a0000\n   00 05 63 6d 72 31 30\nf3 is fnt_def1, and we set font number 0, the checksum is 4bf16079 and we the “scale size” and “design size” are both 655360 sp = 10 standard points, so it comes out at its default size of 10 points. The next line is the path reference: the length of the directory name is 0, the length of the file name is 5, and the file name is cmr10.\n\n\nHello world!\nab\nSet the font to font 0 (cmr10)\n48 65 6c 6c 6f\nTypeset Hello, advancing at each character.\n91 035555\n57\nMove right 3 1/3 standard points and typeset w, advancing.\n91 ff2aaa\n6f 72 6c 64 21\nWe now move left by 5/6ths of a standard point (this is TeX performing kerning) and then typeset orld!\n\n\nPage number\n8e\n8e\n9f 180000\n8d\nPop twice, giving the current state (0, 42152922, 0, 0, 0, 0), so we are down about 8.84 inches; then move down another 1572864 sp = 24 standard points and push this position onto the stack.\n92 00e860a3\n31\n8e\n8c\nMove right 15229091 sp ~= 3.2 inches which is almost half way accross the 6.5 inch width (at the end), typeset 1 then pop the stack and end the page.\n\n\nPostamble\nf8 0000002a\n   018392c0 1c3b0000 000003e8\n   029b33da 01d5c147\n   0002\n   0001\nBegin the postamble; the last page begins at the 42nd byte (2a). Restate the numerator, denominator and magnification. The maximum page height+depth is 43725788 sp ~= 9.23 inches, and the maximum page width is 30785863 sp ~= 6.5 inches. The maximum stack depth is 2. There is 1 page.\nf3 00 4bf16079 000a0000 000a0000\n   00 05 63 6d 72 31 30\nRepeat the font definition.\n\n\nPost-post amble\nf9 000000a5 02\nThe post-postamble declares the post-amble starts at byte 165, and reiterates this is version 02 of DVI.\ndf df df df df df df\nFinally pad with df’s; the file should be a multiple of 4 bytes."
  },
  {
    "objectID": "dvi-by-example/index.html#limitations-and-flexibilities",
    "href": "dvi-by-example/index.html#limitations-and-flexibilities",
    "title": "DVI by example",
    "section": "Limitations and flexibilities",
    "text": "Limitations and flexibilities\nKeep in mind the DVI format was invented in 1979; it’s amazing how well it’s stood up. Most parameters can be specified by a 32-bit number; this means distances are specified by more than 1 part in 2 billion, and numbers by more than 1 part in 4 billion.\nIn particular using the scaled point we can specify positions to half a nanometre for pages up to 23 metres in length! (And we could magnify this by more than 16 orders of magnitude!)\nOur fonts can have up to 2^32 characters; so it can encode even fonts that conform to the widest Unicode convention today UTF-32. We can also have up to 2^32 fonts; we could use this to hold fonts in different colours, styles (e.g. bold, italic, …) or perhaps something more exotic like angled lines of different slopes and thicknesses.\nHowever we can’t embed our fonts, if someone wants to view the DVI they have to ensure they have all the correct fonts, in the correct directory (which is determined in part by the DVI viewer) which can be no more than 1 level deep. Also there is no way in the DVI specification of fetching information about the font (height, width, etc.), so if we want to put an accent on a character we have to hard code its properties into the DVI. This would be OK if put_char didn’t advance by the character’s width; this badly breaks orthogonality of the system. Another bad example would be right-to-left text; you would have to hard code jumps relating to each character’s width.\nBecause we have to have a pointer to the postamble the DVI can’t much more than 4Gb in size; even if all our characters are Unicode, and we have 80 characters per line and 50 lines per page we should still be able to produce a document well over a hundred thousand pages in size. The stack can be at most 65535 level deep (which I think you’d only exceed if you were trying). The 10 counters also allow us to collate our document in a highly non-linear fashion.\nIt’s odd there are two widths w, x and two heights y and z; in fact zero would suffice by directly adding to h and v. I think the reason for these variables must be buried in TeX.\nThere’s no way to insert images, change the colour of text, and for computer viewers no way to insert hyperlinks, videos and sounds or take user input. There’s also no way to draw complex objects without either rendering them specially as fonts, or explicitly constructing them using rectangular boxes as pixels. However the instruction xxx (from TeX’s \\special{}) allows us to implement these things on an ad hoc basis. So it is extremely extensible; but we have to work to make those extensions portable."
  },
  {
    "objectID": "devil-hindmost/index.html",
    "href": "devil-hindmost/index.html",
    "title": "Devil Take The Hindmost: Book Summary",
    "section": "",
    "text": "The flow of the different chapters was haphazard; the earlier bubble seem to be only included to illustrate recurring themes in the later bubbles which are discussed in too much detail with too many footnotes. The book is really informative and worth wading through, although you may get many of the same themes more enjoyably from reading Michael Lewis’ books on the financial industry.\nA recurring theme was that markets that seem guaranteed to rise are almost certainly a bubble. Many times contemporaries said the traditional rules of valuation didn’t apply to them because of technological or financial progress, or in the case of Japan because of cultural differences. Similar cries are heard today of how automation will replace all workers, how people will soon live forever, or that the artificial intelligence will lead to a technological singularity. Because of these, or more pedestrian reasons, it’s claimed we can’t apply historical trends into the future. However it’s much more likely that present technological advances are similar to advances in the past, than that we’ve reached some new revolutionary point where previous rules don’t apply.\nAll the bubbles in the book are attributed to speculation; people buying things because they think the value will increase because other people will buy them. When people from all backgrounds start getting interested in something and think that they can’t lose it’s likely a bubble. The recent cryptocurrency boom is an example of this; growth was based on people speculating that it would grow more. However clearly there’s a ceiling to the value of things; if the price is much higher than the value of the underlying asset (judged for example by earnings or assets or dividends) then it’s likely something is wrong and won’t be sustained. It’s really hard to know if you’re in a bubble; I strongly suspect the property markets of Melbourne and Sydney are overvalued (based on price relative to rental yields and income) but regulation and market conditions means this has been sustained for a very long time.\nA hard fact was often the speculators can profit greatly, but cause great harm. Sometimes they are personally bankrupted, but especially recently they are often protected by incorporation and the government. An example was the Savings and Loans scandals, where they speculated with customer market covered by a US government guarantee, so the taxpayer ended up footing the bill. If a company would be bailed out they should be heavily regulated to ensure they’re not exposing the government to unreasonable risk.\nAfter reading this I would stay away from new and complex financial products. The longer the history of a product and simpler to understand the less likely it will collapse unexpectedly. Products like junk bonds and derivatives have shown a lot of great harm; while you want to invest in something to outrun inflation, using an old investment vehicle makes it more likely you know what you’re getting."
  },
  {
    "objectID": "solving-polynomials-of-low-degree/index.html",
    "href": "solving-polynomials-of-low-degree/index.html",
    "title": "Solving polynomials of degree 2,3 and 4",
    "section": "",
    "text": "It is well known in mathematics that it is possible to find the roots of a general quadratic, cubic or quartic in terms of radicals (linear combinations and products of \\(\\nth\\) roots). Another way of saying this is that the equation \\(a x^4+b x^3+c x^2 + d x + e = 0\\) can be solved for any complex constants \\(a\\),\\(b\\),\\(c\\),\\(d\\), and \\(e\\) if one can solve the equation \\(x^n-t=0\\) for \\(n \\in \\{2,3\\}\\) (\\(1\\) being trivial) (\\(t\\) may be an algebraic combination of solutions of \\(x^n-s\\) for a variety of \\(s\\) which are algebraic combinations of \\(a\\),\\(b\\),\\(c\\),\\(d\\) and \\(e\\)). This is not true for the quintic.\n\nAs an aside radicals are often thought of as somehow “simple” or “special”, but really this is only as true as special functions are special – just because we are used to them and can calculate them easily we consider them special. The solutions to the equation \\(x^5+x-a=0\\) aren’t in a general sense less fundamental than the solutions to the equation \\(x^5-a=0\\) so we should be careful in ascribing too much meaning to the insolubility of the general quintic by radicals.\nHowever in principle these solutions by radicals give us really efficient ways of finding all the roots of polynomials of degrees \\(\\leq 4\\) to an arbitrary accuracy; just accurately calculate the \\(\\nth\\) roots and apply some algebra (being mindful of roundoff error).\nThis is very well known in the case of the quadratic; the solutions to\n\\(a x^2 + b x + c = 0\\) (\\(a \\neq 0\\)) are given by the quadratic formula \\(x=\\frac{-b + (b^2-4 a c)^{1/2}}{2 a}\\) (since any non-zero number has two square roots, this gives two solutions except when \\(b^2=4 a c\\) ).\nThis is also very simple to understand geometrically and algebraically. Let’s rescale our polynomial (which doesn’t change the zeros) giving\n\\(x^2 - 2 d x - e = 0\\) (\\(d=-b/2a\\), \\(e=-c/a\\)). Algebraically the trick is completing the square to eliminate the linear term:\n\\((x-d)^2 - e = d^2\\) . Geometrically the replacement \\(x-d\\) for \\(x\\) is just a change in origin: we change the origin to the extremum (maximum or minimum) of the quadratic about which the quadratic is symmetric – both roots are equidistant from the extremum. We then rearrange algebraically to find this distance:\n\\(x-d = (d^2+e)^{1/2}\\) giving the quadratic formula.\n(A technical detail: The function \\(f(x)=x^n\\) mapping \\([0,\\infty)\\) to itself for any positive integer \\(n\\) is a bijection, so there is a unique inverse \\(f^{-1}(x)=\\sqrt[n]{x}\\) . Let \\(w=r e^{i \\theta}\\) be any complex number, then for \\(w\\) not zero there are precisely \\(n\\) solutions in the complex plane to \\(z^n = w\\) , given by \\(z = \\sqrt[n]{r} e^{i (\\theta+2 k \\pi)/n}\\) for \\(k\\) an integer (since \\(e^{2 i \\pi} = 1\\) only \\(n\\) of these are distinct). When I write \\(w^{\\frac{1}/{n}}\\) I am referring to these \\(n\\) solutions.)\nWhen people write the quadratic formula they often write \\(\\pm\\sqrt{\\ldots}\\) , which is fine for the real case, but in the complex case it is less obvious what is meant by square root (you could take the principal value: writing \\(\\theta\\) above in \\((-\\pi,\\pi]\\) and referring to the \\(\\nth\\) root as \\(k=0\\) but this is somewhat arbitrary, and masks the symmetry of the solutions).\nThe solution to the cubic can similarly be written out: see e.g. here – it is much more involved than the quadratic formula. (Incidentally the history of the cubic formula is somewhat obscured: see the Math World article). Written out in full the cubic formula is quite hard to interpret, but a medical professional has written a beautiful paper explaining the solution in a geometrically illuminating manner.\nAn outline of the solution is first “completing the cube” to remove the \\(x^2\\) term (that is shifting the origin such that the sum of the roots is zero), giving an equation like \\(x^3 + a x + b =0\\) . Then the essential algebraic trick is noting \\((p+q)^3 - 3 p q (p+q) - (p^3+q^3)\\) and setting \\(x=p+q\\), and solving using the equations \\(3 p q = – a\\) and \\(p^3+q^3=-b\\) . This gives identical quadratics in \\(p^3\\) and \\(q^3\\): one solution is \\(q^3\\) and the other is \\(p^3\\) (since there is complete symmetry in \\(p\\) and \\(q\\) it doesn’t matter which). Then all that remains is to take the \\(\\frac{1}{3}\\) power of say \\(p\\) (which has \\(3\\) solutions) and finding \\(q\\) using \\(3pq = -a\\). This gives all three solutions. [One has to be very careful of the explicit formula in terms of the coefficients in the complex case: it involves the sum of two cube roots, and there are 3 possible solutions to each cube root giving a total of 9 solutions, but 6 of these are spurious: we need to enforce the restriction of their product).\nThe cubic formula, however, is rather nasty in that it may make the solutions look more complicated than they are. For instance consider the cubic\n\\((x-1)(x^2+1)=x^3-x^2+x-1=0\\) , the solutions given by this algorithm are\n\\(\\frac{1+a+b}{3}\\) and \\(\\pm i \\frac{a-b}{2 \\sqrt{3}}\\) where\n\\(a=\\sqrt[3]{10+6 \\sqrt{3}}\\) and \\(b=\\sqrt[3]{10-6 \\sqrt{3}}\\) .\nIt is not obvious at all that these solutions correspond to \\(1\\), \\(i\\), and \\(-i\\) unless one realises \\((1 \\pm \\sqrt{3})^3=10 \\pm 6 \\sqrt{3}\\) and so \\(a=1+\\sqrt{3}\\) and \\(b = 1 - \\sqrt{3}\\) .\nEven if all the coefficients are real, if there are 3 real roots we necessarily go through the complex numbers using this procedure. For example\n\\((x-1)x(x+1)=x^3-x\\) gives pairs \\(p\\),\\(q\\) (or equivalently \\(q\\),\\(p\\)) \\((\\frac{1}{2}+\\frac{i}{2\\sqrt{3}},\\frac{1}{2}-\\frac{i}{2\\sqrt{3}}), (-\\frac{1}{2}+\\frac{i}{2\\sqrt{3}},-\\frac{1}{2}-\\frac{i}{2\\sqrt{3}}), (\\frac{i}{\\sqrt{3}},-\\frac{i}{\\sqrt{3}})\\) . (In fact this is probably one of the reasons the complex numbers were accepted as a valid entity – they arose in solving real cubics with real roots).\nThese things aside the cubic formula is still a very fast way to find the roots of a cubic numerically.\nSolving quartics is not much more tricky than solving cubics (though in fact a necessary step in solving a quartic is solving a cubic). Good explanations can be found here and a more symmetric explanation here.\nSo we can solve quadratics, cubics and quartics by taking square and cube roots. How useful is this? Is it faster and more accurate than general techniques for finding the roots of polynomials?\nWhat is the underlying pattern in these solutions? What polynomials of higher degree can be solved by radicals? What ultraradicals (solutions of equations such as \\(x^5 + x - a = 0\\) ) are needed to solve higher order polynomials? [I believe the answers to this question lie in Galois theory].\nWould it be more efficient to compute the roots of a quintic by hyperradicals or by a more general procedure?\nThese questions do have import in daily mathematics. To calculate an integral by Gaussian quadrature the zeros of special classes of polynomials is required (in fact in this case there are a series of polynomials of degree \\(0,1,2,3,\\ldots\\) with all their roots real such that the zeros interlace – i.e. between every two zeros of the degree \\(n\\) polynomial there is a zero of the degree \\(n+1\\) polynomial).\nIn fact often in applied mathematics functions are approximated by polynomials, or known functions multiplied by polynomials, the zeros of which may have some import."
  },
  {
    "objectID": "data-processing-open-source-license/index.html",
    "href": "data-processing-open-source-license/index.html",
    "title": "Open Source Licenses for Data Processing Code",
    "section": "",
    "text": "There are three main license types that are used in Open Source; MIT, Apache and GPL (with BSD family somewhere between MIT and Apache). The main benefit of GPL licenses is that if someone modifies and redistributes the code then they have to make the source code available, which fosters sharing and gives more rights to the users of the modified code. However data processing code typically sits on a server somewhere and is never distributed; it’s either used as a service or to build data products. Even the most viral of the GPL licenses, the AGPL, only requires providing source when the copy is conveyed (transferred) to someone. Even MongoDBs anti-cloud provider extension of the AGPL, the Server Side Public License (SSPL), only applies in the service use. If someone wants to build their own data assets using the data processing code none of these licenses require them to share their modifications.\nThe copyleft licenses have a chilling effect on adoption compared with the broader licenses. People building commercial products often get scared of GPL licenses (and especially the AGPL and SSPL), in large part because they are largely untested in court and so there is legal risk in how they would be interpreted. This means using a GPL style license means fewer people in industry will use the code, even if a lay reading of the license says there is no restriction.\nI’m mainly writing data processing code to build a portfolio, and find some data driven insights. I want people to look at the code, see that it works and use it to build their own insights (even if it’s proprietary). If I find a commercialisation opportunity then I would keep that part closed source, but while I’m just exploring I’d rather share my findings on this website and provide people the means to follow along."
  },
  {
    "objectID": "how-not-book-ner/index.html",
    "href": "how-not-book-ner/index.html",
    "title": "How Not to Do Book Named Entity Recognition",
    "section": "",
    "text": "I took a weighted sample of data likely to contain book names, based on the analysis of the heuristics. Using this sample I manually annotated around 500 comments, many of which had book names. I then trained a SpaCy NER model, randomly splitting training and evaluation, which got 60% precision and 40% recall on the test set. I then tried using Prodigy’s ner.teach recipe to do active learning on the full dataset. However I found quickly there were a lot of false negatives (the precision was much lower than 60%) and most of the examples to annotate had no entities.\nThe main mistakes I made were:\n\nBy evaluating on a different distribution I got overly confident results\nThe training distribution didn’t have enough examples of comments not containing book mentions\nSpaCy’s NER model has no notion of confidence/probability, so it can’t do uncertainty sampling\nDifficulty of the annotation model\n\nThese are all possible to remedy and the rest of this article will go through them one at a time.\n\nEvaluating on a different distribution gives overly confident results\nThis is machine learning 101; you want the distribution of your evaluation data to be as close to your training data as possible. I had already designed an evaluation, but I hadn’t carried it out because it was an extra step. I thought 60% precision was pretty good, especially because the entity boundaries were sometimes fuzzy (see annotation model below).\nThe main reason I didn’t carry out the evaluation is because I didn’t know how to do it. Setting up a script in advance to evaluate the precision on a trained model would have helped here. I was hoping active learning would help give me an idea of the models performance, but it turns out SpaCy’s NER doesn’t have a notion of uncertainty making it difficult.\n\n\nTraining distribution didn’t have enough negative samples\nFew Hacker News comments are about books, and annotating lots of negative examples is dull and leads to repetition bias (the annotator is more likely to miss real examples). To ameliorate this I used heuristics to choose posts likely to contain book titles and only annotated them. However without more negative samples (in particular more diverse negative examples) the model couldn’t distinguish book names from other phrases.\nWe need some way of annotating more negatives examples for the model. One method would be to only annotate positive predictions of the trained model; this is useful for both for measuring and increasing precision (potentially at the cost of recall). Another would be to use the heuristics as weak labels and distantly supervise the model; any comments labelled as negative by the heuristics is treated as a true negative. Taking this a step further we could separately train a classifier to detect items with a title, and automatically label all items below a certain predicted probability as true negatives.\n\n\nSpaCy’s NER model doesn’t expose confidence\nSpaCy’s NER model doesn’t expose a confidence score, it’s always 1. This means we can’t do uncertainty sampling which makes it harder to improve the model with active learning (let alone model driven diversity sampling). The related StackOverflow thread suggests either find a way to extract it from beam_ner (which seems hard to do) or use a SpanCategorizer which is a more general model for categorising spans of text.\nI still haven’t gotten over the learning curve with SpaCy and prodigy enough to understand the tradeoffs here, or how to wrap my own models conveniently. The other approach is to train a classifier to predict whether it contains a book title, and use this as a heuristic for active learning; this might help but it’s an independent model so it’s hard to say, and won’t identify cases when there is a book name but it’s hard to predict the boundaries.\n\n\nDifficulty of the annotation model\nAnnotation is an active process; it’s like a conversation where you really define what you mean by finding good examples. Active learning helps you get to the interesting examples more quickly, but in general there’s a lot of subtlely.\nI found many ambiguous cases when I annotated the 500 examples:\n\nIs a mention of a movie adaptation of “Dune” a book title?\nthere’s not enough context and I need external knowledge, or context, to tell whether it’s a book\ndoes a series count? does the word “series” belong in the span? (Lord of The Rings, Cradle series, Harry Potter)\ndoes it count if a book is mentioned obliquely?\ntokenization problems in SpaCy mean a quote or bracket get stuck to the text to annotate; this is especially true when the book title is in a URL (which is tokenized as a single entity)\n\nas an example of the last consider this comment\n\nI have read and recommend Andy Tanenbaum’s book on operating systems. He’s also written a book on networking and one on distributed systems.\n\nThis is almost surely talking about Modern Operating Systems by Andrew Tanenbaum; but is “operating systems” in this text a book title?\nThese questions have aspects in what we’re trying to achieve, and how we are going to break down the task. The overall goal is to enable discovering interesting books on a topic and find out about them from comments on Hacker News. A good intermediate goal is being able to identify what books are being discussed in a comment. We could do this directly; treat it as a multi-label classification problem, but treating it as a closed problem makes it difficult to add new books, especially books only mentioned once or twice. Even within a Named Entity Recognition approach we could try to identify only book titles, or identify all titles and have a separate step to work out whether it’s likely a book.\nExamples where I can’t answer the question without extra context are going to be very difficult for a machine learning model, and suggest breaking it into separate problems. In this case it may make sense to first classify a WORK OF ART, the title of any book or movie, and then separately identify which of these are books (potentially during the linking step). In this case we could consider a movie adaptation of “Dune” to be a work of art, or the name of a series, or anything that looks like it may be a name. One tricky issue with Hacker News could be the titles of articles on websites; they are very common and in many senses they’re similar to books (they are read, they have an author, they write about things) but they can’t be linked to a book. I’m not sure how I would deal with this.\nTokenization is a separate issue; for cases like brackets and quotes I can just modify the tokenizer. For names of books in URLs it would be very difficult to tokenize using traditional methods, but potentially possible with learned tokenizers and transformer models. A pragmatic decision is to ignore them for now, since they’re not the most common, and potentially build a specific model to handle them later.\n\n\nSummary and Next steps\nBased on the learnings I’m going to take the following steps:\n\nRefine the annotation task to identifying WORK OF ART; this has the additional advantage that I can fine tune the existing model on the SpaCy en_core_web_trf model (if I start training on GPU)\nBuild a Prodigy NER precision recipe, that filters to examples with a span for binary annotation (based on ner.teach)\nTrain a “Work of Art” classifier based on distant supervision; this can eventually be used to help assess recall, active learning for NER, and provide negative examples.\nNote any tokenization issues I notice, and extend the SpaCy tokenizer as required\n\nIdeally I’d like to directly use active learning on the entity recogniser, but it doesn’t seem like SpaCy makes that easy. In future I’d like to look into either the SpanCategorizer or building on an off-the-shelf token classifier to be able to do both uncertainty and diversity sampling, but it seems like a larger endeavour."
  },
  {
    "objectID": "autorandr/index.html",
    "href": "autorandr/index.html",
    "title": "Automatically changing display settings with Autorandr",
    "section": "",
    "text": "Then some time later I’ll disconnect the laptop from the monitor, realise there’s nothing on the screen, and plug it back in, run lxrandr again, and set the display to only the laptop monitor.\nI know there are better ways of handling this. I could spend ten minutes working out how to get my desired configurations with xrandr from the command line, and then I can blindly type into a fresh terminal without having to worry about the monitors. Even better I could wrap it in a script (maybe called something memerable like monitoron and monitoroff). But it happens infrequently enough that I haven’t bothered.\nBut today it bothered me and I wondered if there was an easier way to do it. In Windows it normally does the right thing and is quick to change configurations by pressing Windows Key and p together. A quick search brought me to the fantastic autorandr which quickly solved the problem for me.\nIt automatically responds to configuration changes (like plugging or disconnecting a monitor). It’s fantastically simple to use; I could get it running in about two minutes by following the documentation. Set up the display I want when connected to an external monitor and type autorandr --save docked (or replace docked with whatever you want to call it); then set up the screen and disconnect the monitor and run autorandr --save mobile. Then I’m done; it automatically switches configurations whenever I connect or disconnect the external monitor in less than the time it takes to read man xrandr. If you want to get your tools adopted make it this easy to get started; a quick reward is a great way to get someone hooked.\nThis is the awful and wonderful thing about Linux; the defaults can be clunky but it’s so configurable that a few scripts can almost always solve your problem, you just need to write or find those scripts. Autorandr seems really well designed in a UNIX style; each profile is in a folder consisting of a setup file (that defines the conditions when a profile would switch) and a config file (that defines the xrandr configuration to use). I love that it’s so observable, and its extendable to add scripts that fire before or after a configuration is triggered (which could be handy if you needed to fix a menubar of something); but you don’t need to know all that to get started.\nSo if you ever find yourself fiddling with arandr or lxrandr to make the same configuration changes, automate it with autorandr."
  },
  {
    "objectID": "projective-centroid/index.html",
    "href": "projective-centroid/index.html",
    "title": "Centroid for Cosine Similarity",
    "section": "",
    "text": "The cosine distance between two n-dimensional vectors is the cosine of the great circle distance of their projections unit (n-1)-sphere. The cosine distance between two vectors v and w is given by the rather obtuse formula \\(\\frac{v \\cdot w}{\\left\\| v \\right\\| \\left\\| w \\right\\|}\\). A key property is that it doesn’t depend on the length of the vectors; if we double v then it cancels out with the norm in the denominator. So it’s not really a distance between vectors, but actually a distance between rays (that is lines with a direction; changing the direction negates the cosine distance). Then on the unit n-sphere the distance between two points along the sphere is just the angle between them at the centre of the sphere.\nI define the cosine similarity centroid of a set of points as the ray that has maximum average similarity to the points; it’s the most similar point you can have. For simplicity lets project all the points on the unit sphere, except the origin, which is ignored since all points have the same similarity to it. Then the cosine distance is just the dot product. Concretely if our points on the unit sphere are \\(\\{p_i\\}_{i=1}^{N}\\) then the cosine similarity centroid c is the set of points that maximises \\(s(c) = \\frac{1}{N} \\sum_{i=1}^{N} p_i \\cdot c\\) subject to the constraint that c lies on the unit sphere, that is \\(\\left\\| c \\right\\| = 1\\).\nThe maximum can be found using the method of Lagrange multipliers. We want to find the extremum of \\(L(c, \\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_i \\cdot c + \\lambda (1 - c \\cdot c)\\) The local extrema are where the partial derivatives are 0:\n\\[\\frac{\\partial L}{\\partial c_j}(c,\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} p_{ij}  - 2 \\lambda c_j\\]\n\\[\\frac{\\partial L}{\\partial \\lambda}(c,\\lambda) = 1 - c \\cdot c\\]\nThis occurs where \\(c = \\kappa \\sum_{i=1}^{N} p_i\\) where \\(\\kappa = \\frac{-1}{2 \\lambda N}\\) is some constant, constrained by the normalisation condition that it lies on the unit sphere. Taking second derivatives shows this is actually the maximum of L.\nThat is the centroid lies along the line from the origin to the geometric midpoint in cartesian coordinates of the points on the unit sphere. So if you want to calculate a centroid of a group of points with respect to the dot product, then normalise the vectors and average them.\nNote that this method will fail if there’s too much symmetry of the points. For example if they lie on a regular polyhedron centred at the origin then their mean is zero. That’s because there’s no loner a unique most similar point. For example the centroid for the north pole and the south pole could be any point along the equator. One easy solution for this is to break the symmetry, by moving any one of the points by a small random amount, and then it will have a solution again."
  },
  {
    "objectID": "overperforming/index.html",
    "href": "overperforming/index.html",
    "title": "Overperforming",
    "section": "",
    "text": "A friend of mine worked as a night filler at a supermarket, stocking shelves. He worked hard to be one of the best fillers and work his way up. He was doing almost twice the work of the average fillers. But every time he went for a promotion over the few years he worked there someone else got it, who wasn’t as good a performer and often who had been there for less time.\nEventually he found out what was going on; he was too good a filler. If he was promoted they would need to hire two fillers to replace him, stretching the store budget. He was so good at his job that he couldn’t be promoted out of it.\nIf he had wanted to be promoted he needed to convince the upper store management that he could contribute more as a floor manager than a nightfill. This means just being a good stocker and focusing his additional effort on unblocking and motivating his colleagues, helping his manager resolve various issues and building relationships with the store management.\nMy friend ended up leaving and moving into a different industry; but he learned an important lesson. If you want to work your way up in your career you can’t do it by excelling at your current role. You have to do well in your current role, but you also need to start taking on responsibilities in the role you want to move into and start building relationships with the people who are responsible for the hiring decision (generally not the manager of the role)."
  },
  {
    "objectID": "binary-rms/index.html",
    "href": "binary-rms/index.html",
    "title": "Metrics for Binary Classification",
    "section": "",
    "text": "When evaluating binary classifier (e.g. will this user convert?) the most obvious metric is accuracy; what’s the probability a random prediction is correct. One issue with this metric is if 90% of the cases are one class a high accuracy isn’t really impressive; you need to contrast it with a constant model predicting the most frequent class. More subtly it’s not a very sensitive measure, by measuring cross-entropy of predicted probabilities you get a much better idea of how well your model is working.\nConsider the case of predicting people who will purchase a product. Then the estimated revenue is the sum of the price of each product multiplied by the probability they will buy it. Here you get a much better estimate by using the probability; you could get a distribution of that estimate by simulating the picks at each probability (for example high price, low probability purchases could add a lot of uncertainty to the predictions). This is much more useful than the binary prediction of whether they will buy the product, which will give an extreme estimate.\nOne way to evaluate the prediction of the probabilities of the outcomes is via likelihood. For a bernoulli distribution with constant, but unknown, probability p, the probability of getting a positive is p and a negative is (1 - p). Suppose we’ve taken N draws and got S positive cases, the likelihood of this result for a given p is \\(L(p) = p^S (1 - p)^{N-S}\\). Then we’re trying to work out what the probability p is given the data. One way to answer this is to pick the value of p which is most likely, which happens to be S/N (i.e. the average number of positive cases, as you would expect).\nInstead of the likelihood, the log-likelihood is often used. It has a maximum at the same point as the likelihood, but because probabilities are multiplicative, log-probabilities are additive and so it’s often simpler and more numerically stable. For the Bernoulli distribution example above the log likelihood is \\(l(p) = S \\log(p) + (N-S) \\log(1-p)\\). This log likelihood can also be used as a measure of how good our model is; since it’s minimum is the most likely model.\nThis can be generalised to the case where we have a different probability for each data point. Then the log likelihood generalises to \\(l = \\sum_{i \\in +} \\log(p_i) + \\sum{i \\in -} \\log(1 - p_i)\\) where + is the set of positive results and - is the set of negative results. Another way of writing this is \\(l = \\sum_i q_i \\log(p_i) + (1 - q_i) \\log(1 - p_i)\\), where q is 1 for a positive case and 0 for a negative case; in this form it is clear it sums the log of the probability of the actual outcome (and can be generalised to more than 2 outcomes). The negative of this is called the cross entropy, and is a measure of how good our predictions are (since cross entropy is smaller the more likely the model is). In code this is:"
  },
  {
    "objectID": "binary-rms/index.html#cross-entropy-versus-mean-squared-error",
    "href": "binary-rms/index.html#cross-entropy-versus-mean-squared-error",
    "title": "Metrics for Binary Classification",
    "section": "Cross entropy versus mean squared error",
    "text": "Cross entropy versus mean squared error\nHow does cross-entropy compare to using the (root) mean squared error on the probabilities? For a given data set from a normal distribution with a known standard deviation the log likelihood is a constant plus a term proportional to the squared mean error. Precisely it’s\n\\[l(\\mu, \\sigma) = -n \\log \\sigma - \\frac{n S^2}{2 \\sigma^2} - \\frac{n(\\bar{X} - \\mu)}{2 \\sigma^2}\\]\nwhere \\(\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i\\) is the sample mean and \\(S = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\bar{X})^2}\\) is the sample standard deviation. So for a given \\(\\sigma\\) the log likelihood only depends on the mean square error.\nAs the number of trials increases the binomial distribution gets close to a normal distribution (but slower for more extreme p). So using the mean square error isn’t crazy, for large samples you’ll get reasonable results, but it’s not as appropriate as the cross entropy."
  },
  {
    "objectID": "binary-rms/index.html#further-reading",
    "href": "binary-rms/index.html#further-reading",
    "title": "Metrics for Binary Classification",
    "section": "Further reading",
    "text": "Further reading\nThis barely scratches the surface of loss functions for classification; Wikipedia has an article that covers other kinds of loss such as exponential loss, tangent loss, hinge loss, and savage loss. It would be great to understand more about how these all differ and how to choose between them in a particular application."
  },
  {
    "objectID": "normalising-salary/index.html",
    "href": "normalising-salary/index.html",
    "title": "Normalising Salary",
    "section": "",
    "text": "The other question is how to pick the range, for jobs with a bery large range. I started with the minimum because the maximum is often an inspirational nubmer (especially in commission sales roles).\nThe way I approached this was:\n\nLook for anomalies (e.g. salaries where minimum is more than the maximum)\nFor salaries with a period (e.g. hourly, daily, or annual) look at the range of common salaries\nRemove any data with wrong ranges due to issues in the data or in the parsing\nFor salaries without a period, infer the period from the range (ignoring when it’s ambiguous)\nDivide out the period (inferred or actual) to get the annualised salary.\n\nFor example by looking at the data I can see for Australian jobs annual salaries should be above $10,000. Daily salaries are above $100 and hourly salaries below $200; between $100 and $200 it’s ambiguous depending on the kind of role. But below $100 it’s unambiguously hourly. This approach could be applied to different markets and currencies I’m less familiar with.\nI used the TDD approach to parsing salary, which allowed me to improve it and the tests caught some regressions I would have introduced.\nAfter removing the out-of-band result and annualising I got a reasonable result:\n\n\n\nDistribution of annualised salary\n\n\nI undoubtedly removed some results that are valid, or that could be corrected, but this was an effective way of getting a lot of the valid data with a little work. I’ve got a Notebook showing the approach (raw)."
  },
  {
    "objectID": "probability-jaccard/index.html",
    "href": "probability-jaccard/index.html",
    "title": "Probability Jaccard",
    "section": "",
    "text": "The main metrics in association rule mining are the confidence, which for pairs is just the conditional probability \\(P(B \\vert A) = \\frac{P(A, B)}{P(A)}\\) There is also the lift which is how much more likely than random (from the marginals) the two events are likely to occur together \\(\\frac{P(A, B)}{P(A)P(B)}\\). Finally there is the support which is just \\(P(A, B)\\), but I tend to find the count of the intersection is more useful because it indicated how precise the support, lift and confidence may be.\nNotice also that we can write the lift in terms of the confidence of the inverse rule:\n\\[\\frac{P(A,B)}{P(A)P(B)} = \\frac{P(A|B)}{P(A)}\\]\nIf we’re looking at items similar to another item A then the lift is proportional to the confidence of the inverse rule.\nThe Jaccard similarity of two sets is defined by the equation:\n\\[\\frac{ \\lvert A \\cap B \\rvert }{ \\lvert A \\cup B \\rvert }\\]\nUsing the inclusion-exclusion principle this can be rewritten as:\n\\[\\frac{ \\lvert A \\cap B \\rvert }{ \\lvert A \\rvert + \\lvert B \\rvert - \\lvert A \\cap B \\rvert }\\]\nDividing by the intersection gives an estimate of\n\\[\\frac{ 1 }{ \\frac{1}{P(A|B)} + \\frac{1}{P(B|A)} - 1 }\\]\nSo the Jaccard index can be written in terms of the confidence of the association rule and its inverse. I’m not quite sure how to interpret this; it’s something close to (but not quite) a harmonic mean. This symmetry gives less flexibility than using the lift and confidence together."
  },
  {
    "objectID": "rod-crewther/index.html",
    "href": "rod-crewther/index.html",
    "title": "Rod Crewther: 23/09/1945 - 17/12/2020",
    "section": "",
    "text": "Rod truly cared about educating students. There’s little incentive for university researchers to do anything but the bare minimum for teaching; they gain career advancement through publications and grants. Rod always had time for us students, clearly put a lot of time into his educational material and had an infectious enthusiasm for his subject. My first memory of Rod is from first year Physics, when he taught us about torque by trying to push open a heavy lecture door by pushing with all his strength at the hinge; the slightly silly image has stuck with me as a memorable metaphor for torque.\nRod worked very hard teaching some of the most difficult subjects I studied, making the material clear and well structured. qe designed a lot of the theoretical physics curriculum at Adelaide and taught me Advanced Dynamics and Relativity (the harder parts of classical mechanics and intro to special relativity), Quantum Mechanics, Relativsitic Quantum Mechanics and Particle Physics (RQMPP) and General Relativity. His notes and handouts were always clear and I still have all my notebooks from his courses. These Introductory notes on Quantum Field Theory give an indication of how he taught. I still remember his semi-derivation of the Schrödinger equation by reasoning about wave-particle duality using plane waves and special relativity. Similarly his emphasis on index notation for calculation (and the Levi-Civita identities) has stuck with me, and even today when I’m trying to do multivariable calculus for machine learning I fall back on the familiar index methods Rod taught me.\nRod was a lively character, with a sharp smile, and who was always quick to smile and had a loud laugh. In one of our third year lectures a few of the students talked him into playing the piano before the lecture and I recall he played quite well (I believe a piece by Gershwin, one of his musical idols). He had lots of stories about the people in physics such as his supervisor, Nobel prize winner Murray Gell-Mann, or his sometimes rival Richard Feynman (for example Feynman has a parton model that rivalled Gell-Mann’s quarks; Gell-Mann referred to them as “put-ons”); or about the time he met Dirac. While I didn’t have the pleasure of knowing Rod well as a person, he was always friendly, unpretentious and open.\nI don’t know enough physics to understand his work but notably he has publications as late as July 2020. However I’ll quote my friend Lewis Tunstall who did work with him on a paper published in late 2019:\n\nIt was indeed great to finish my last paper with Rod. The amazing part of that experience is that we wrote this while he was undergoing several rounds of chemotherapy / surgery to stave off a form of cancer. (He’s better now, thankfully.) Rod taught me a great deal about scientific integrity and working with him was one of the highlights of my time as a physicist.\n\nI’m only one student; Rod taught for 35 years at Adelaide and must have influenced hundreds of students with his passionate teaching. We were extremely lucky to have him there, and his death is a great loss."
  },
  {
    "objectID": "jupyter-nolatex/index.html",
    "href": "jupyter-nolatex/index.html",
    "title": "How to turn off LaTeX in Jupyter",
    "section": "",
    "text": "For Pandas dataframes this is especially annoying because it’s much more likely you would want to be showing $ signs than displays math. Thankfully it’s easy to fix by setting the display option pd.options.display.html.use_mathjax = False. It’s strange this is True by default, but you can add this configuration near the top of all your Jupyter notebooks.\nUnfortunately you can’t turn it off in Markdown; you’ll just need to replace every $ sign with \\$. The backslash turns off the LaTeX rendering and you’ll just get dollar signs. A little annoying to remember, but straightforward to work around.\nI often display HTML in Jupyter notebooks using IPython.display.HTML, and using IPython.display.display as the equivalent of print. Unfortunately the HTML also interprets dollar signs as Mathjax, but again you can turn this off by escaping the dollar signs with a backslash. Since this is what you will almost always want you can wrap it in a function:\nfrom IPython.display import display, HTML as HTML_raw\n\ndef HTML(text):\n  text = text.replace('$', r'\\$')\n  return HTML_raw(text)\nThis doesn’t handle the ability to pass URLs or filenames, but is easy to extend to these cases.\nThe default of interpreting dollar signs as Mathjax in Jupyter output can be annoying and confusing, but once you know the problem it’s straightforward to solve with a little extra configuration."
  },
  {
    "objectID": "minhash-sets/index.html",
    "href": "minhash-sets/index.html",
    "title": "Minhash Sets",
    "section": "",
    "text": "We’ve found pairs of near duplicates texts in the Adzuna Job Salary Predictions Kaggle Competition using Minhash. But many pairs will be part of the same group, in an extreme case there could be a group of 5 job ads with identical texts which produces 10 pairs. Both for interpretability and usability it makes sense to extract these groups from the pairs."
  },
  {
    "objectID": "minhash-sets/index.html#combining-lsh-with-union-find",
    "href": "minhash-sets/index.html#combining-lsh-with-union-find",
    "title": "Minhash Sets",
    "section": "Combining LSH with Union Find",
    "text": "Combining LSH with Union Find\nThe datasketch library stores the LSH bands as hashtables. By default it’s stored in _dict which is a mapping from the joint hash across the rows to the elements with that hash (labelled as they are with lsh.insert. We can then union these with our Union-Find algorithm to produce all sets:\ndef lsh_similar_sets(minhashes, bands, rows):\n    lsh = MinHashLSH(num_perm=num_perm, params=(bands, rows))\n    for i, mh in enumerate(minhashes):\n        lsh.insert(i, mh)\n\n    parents = {}\n    for hashtable in lsh.hashtables:\n        for items in hashtable._dict.values():\n            items = list(items)\n            for i in range(len(items)):\n                for j in range(len(items)):\n                    if i > j:\n                        union(items[i], items[j], parents)\n    return find_sets(parents)\nThis works fine, but I’ve already generated the pairs and calculated their similarity. I could run Union-Find on these pairs, but another approach is taking a graph view."
  },
  {
    "objectID": "presto-approx-percentile/index.html",
    "href": "presto-approx-percentile/index.html",
    "title": "Approximate Percentiles in Presto and Athena",
    "section": "",
    "text": "When running this I found that it was non-deterministic. This is really annoying because it makes testing hard, especially diff testing running on a production dataset. I wanted understand why; I think the reason is that the approximation depends a little on the order items are inserted. Presto runs across multiple nodes, so it’s likely for the data to come in different orders depending on hardware details (though this can even happen on a single node if the way it fetches the data blocks is non-deterministic).\nThe rest of this post documents a bit about how it works.\nFrom the documentation:\n\nAs approx_percentile(x, percentage), but with a maximum rank error of accuracy. The value of accuracy must be between zero and one (exclusive) and must be constant for all input rows. Note that a lower “accuracy” is really a lower error threshold, and thus more accurate. The default accuracy is 0.01.\n\nThe implementation is in the Approximate Percentile Aggregation functions in the aggregations. In particular most of them build on ApproximateLongPercentileAggregations. In turn this uses com.facebook.airlift.stats.QuantileDigest to do a lot of the heavy lifting.\nAirlift describes itself as:\n\nAirlift is a framework for building REST services in Java.\nThis project is used as the foundation for distributed systems like Presto.\n\nThis seems like a strange place to do the heavy lifting of quantiles, but apparently it is. It has implementation of Quantile Digest for approximate percentiles, and Hyperloglog.\nThey’ve got a description of how they calculate it, and there are a lot of useful comments in the source.\n\nImplements http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.132.7343, a data structure for approximating quantiles by trading off error with memory requirements.\nThe size of the digest is adjusted dynamically to achieve the error bound and requires O(log2(U) / maxError) space, where U is the number of bits needed to represent the domain of the values added to the digest. The error is defined as the discrepancy between the real rank of the value returned in a quantile query and the rank corresponding to the queried quantile.\nThus, for a query for quantile q that returns value v, the error is |rank(v) - q * N| / N, where N is the number of elements added to the digest and rank(v) is the real rank of v\nThis class also supports exponential decay. The implementation is based on the ideas laid out in http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.159.3978\n\nIn the source code it mentions:\n\nGet an upper bound on the quantiles for the given proportions. A returned q quantile is guaranteed to be within the q and q + maxError quantiles.\n\nThis is a little interesting; following the code calls I think approx_percentile in Presto will tend to slightly overestimate the percentile and never underestimate.\nThis follows from the paper they implemented: Medians and Beyond: New Aggregation Techniques for Sensor Networks.\n\nThis error is bounded by εn (Theorem 1). So, the rank of value reported by our algorithm is between qn and (q+ε)n. Thus the error in our estimate is always positive, i.e., we always give a value which has a rank greater than (or equal to) the actualquantile\n\nSkimming the paper they build a binary tree, called a q-digest, to estimate the quantiles. This makes intuitive sense; you only need bounds of regions when calculating quantiles, but to get those bounds accurately you need lots of memory (though I’m sure there are lots of messy details).\nHowever I suspect that the output tree depends on the order of the input. Then thinking about how Presto uses it; the order items are inserted into it won’t always be the same. So this is likely why it’s non-deterministic."
  },
  {
    "objectID": "closure-operators/index.html",
    "href": "closure-operators/index.html",
    "title": "Closure Operators",
    "section": "",
    "text": "These examples are all generated in a similar way; the objects are all sets with additional structure that is preserved by intersections, but not unions. For instance the intersection of subspaces of a vector space is a vector space and the intersection of subfields of a field is a field. Thinking of a topology as a collection of open sets or closed sets, a topology is a subset of the power set that obeys certain properties. Every topology on a set is a subtopology of the discrete topology, and the intersection of subtopologies is a topology (but the union isn’t!).\nSo in each case we have a universal or greatest set (my terminology), a collection of subsets that represent all subobjects of the universal set (vector subspaces, subfields, subtopologies) that is closed under intersection and includes the universal set. The closure is then a mapping from all subsets of the universal set to the subobjects; it maps a set to the intersection all closed subobjects containing that set. In a sense it takes a set to the “smallest” subobject.\nFor example if our universal set is \\(R^{n}\\) (for some fixed \\(n=1,2,\\ldots,\\aleph_0\\) ) then one subset is the point \\(\\{x\\}\\) for \\(x\\neq 0\\) . The closure of \\(\\{x\\}\\) is the intersection of all vector spaces containing \\(\\{x\\}\\) , which is just \\(\\{\\alpha x |\\; x \\in X\\}\\) . The closure of the empty set is \\(\\{0\\}\\) . The closure of any vector space is the vector space itself. The closure of a collection of vectors is precisely their span.\nAs a topological example the product topology is the finest topology such that the projection operators are continuous. For each projection we take the inverse image of the topology of the target space, and then take the union over all the projections to get a subset (of the power set of the cartesian product). Then the product topology is precisely the closure of this subset. [N.B. Don’t confuse the closure operator on topologies with the topological closure; but they do have a common root since the intersection of arbitrarily many closed sets is closed].\nSo the closure operator is a useful concept that appears often in mathematics. The rigorous definition is due to Tarski. Given a set a closure operator is a map \\(\\bar{}\\) from the power set to itself such that:\n\n\\(X \\subseteq \\bar{X}\\) .\n\\(X \\subseteq Y\\) implies \\(\\bar{X} \\subseteq \\bar{Y}\\) .\n\\(\\bar{\\bar{X}} = \\bar{X}\\) .\n\nThe closed sets are the the ones such that \\(\\bar{X}=X\\) . This is in fact equivalent to my examples, the closure operator I constructed satisfies these properties. To see the opposite equivalence note\n\nThe universal set is the power set, which is closed by 1.\nBy 2 and 3 the closure of a set is the same as the intersection of all closed sets containing it.\n\nTo find out more about this stuff check out the first chapter of Burris and Sankappanavar’s “Universal Algebra”. It’s free!\nIt’s also interesting to note the close relation to topology; these form part of Kuratowski’s closure axioms for a topology, but preservation of binary unions and the empty set don’t hold in the examples I gave above.\nI’ll leave you with a (loose) connection to logic. A boolean algebra corresponds in a natural way to boolean logic, and a Heyting algebra (or Brouwerian lattice) corresponds to intuitionistic logic. The “dual”, in some sense, of a closure operator is an interior operator – if we can take arbitrary unions we define the interior of a set as the union of all subobjects contained within the set. A set with an interior operator is a complete Heyting algebra (and this close relation is used to define pointless topology – that is topology just in terms of sets and subsets, no mention of points – in terms of the open sets).\nThus a closure operator corresponds to the “dual” of a Heyting algebra. In a Heyting algebra we have the implication \\(A \\rightarrow B\\) , familiar from logic. The dual operation in a dual Heyting Algebra is more like set subtraction \\(A - B\\) . I’m not sure what the logical structure (if any) corresponding to a dual Heyting algebra is."
  },
  {
    "objectID": "sicp-1_4/index.html",
    "href": "sicp-1_4/index.html",
    "title": "SICP Exercise 1.4",
    "section": "",
    "text": "Exercise 1.4.\n\nObserve that our model of evaluation allows for combinations whose operators are compound expressions. Use this observation to describe the behavior of the following procedure:\n\n(define (a-plus-abs-b a b)\n  ((if (> b 0) + -) a b))\n\nSolution\nThere are two possible branches, if b is positive then we get:\n((if (> b 0) + -) a b)\n((if #t + -) a b)\n(+ a b)\nWheras if b is non-positive we get\n((if (> b 0) + -) a b)\n((if #f + -) a b)\n(- a b)\nSo we always get a added to the absolute value of b. This is a nice use of a first order function."
  },
  {
    "objectID": "g-naf/index.html",
    "href": "g-naf/index.html",
    "title": "Locating Addresses with G-NAF",
    "section": "",
    "text": "There’s another open and editable dataset of geographic entities, Open Street Map (and it has a pretty good open source Android app OsmAnd). Unfortunately the G-NAF data can’t be used in Open Street Map because it has a restriction (that you need to verify an address before you send mail to it) that’s incompatible with Open Stree Maps licence. This is really annoying because there are lots of gaps in Open Street Map’s addresses which makes it difficult to use for navigation. Though it is possible to import to a local instance. I’m still not sure how to use this effectively in OsmAnd.\nThe G-NAF data is a bit convoluted to use directly but there exists code to load into Postgres or even use it in a web interface. I’m really impressed that the government made this open and that it’s regularly updated."
  },
  {
    "objectID": "find-xargs/index.html",
    "href": "find-xargs/index.html",
    "title": "Using find and xargs",
    "section": "",
    "text": "Suppose you have an executable doit that you want to execute on all Python files in src/; you can do this directly with find:\nfind src/ -name '*.py' -exec doit {} \\;\nYou can use xargs for this as well; but if there’s a chance that a path could contain a space somewhere it’s best to use -print0 with find and -0 with xargs to separate all arguments with nulls (rather than spaces):\nfind src/ -name '*.py' -print0 | xargs -0 -n1 doit\nThe nice thing about using find with -exec is you can put the placeholder {} in different places; for example if you wanted to run a bunch of bash scripts passing the argument foo you could use:\nfind src/ -name '*.sh' -exec bash {} foo\nWith xargs you can do the same thing using -I to specify a replace string:\nfind src/ -name '*.sh -print0 | xargs -I{} -0 -n1 bash {} foo\nAn advantage of xargs is you can pass multiple arguments at a time. When running Python or Java scripts there’s quite a bit of runtime overhead in starting up a program, so it can be significantly faster if they can process multiple files themselves. By default it passes all the arguments:\nfind src/ -name '*.py' -print0 | xargs -0 doit\nA very useful feature of xargs is it can run multiple scripts in parallel. This is a really handy batching mechanism, especially for I/O bound operations like Pythons multiprocessing and futures. For example to run 4 threads in parallel, passing 5 arguments at a time (so 20 files get processed at once) you could run:\nfind src/ -name '*py` -print0 | xargs -0 -P4 -n5 doit\nOf course there are things where find and xargs won’t be enough and you want to use a programming language or a framework. But they’re super useful for quickly processing files.\nNote that if you’re looking for files containing a certain piece of test you can use grep, ag or rg with xargs as well:\ngrep -r -l -Z --include '*.pyi' 'import Path'  src/ | xargs -0 echo\nag -l -0 --python 'import Path' src/ | xargs -0 echo\nrg -l -0 --iglob '*.py' 'import Path' src/ | xargs -0 echo"
  },
  {
    "objectID": "bernoulli-binomial/index.html",
    "href": "bernoulli-binomial/index.html",
    "title": "From Bernoulli to Binomial Distributions",
    "section": "",
    "text": "This might seem a bit abstract, but the inverse problem is often very important. Given that 7 out of 10 people convert on a new call to action, can we say it’s more successful than the existing one that converts at 50%? This could be people any proportion, from patients that recover from a medical treatment to people that act on a recommendation. To understand this inverse problem it helps to understand the problem above.\nThis situation where there are two possible outcomes that occur is called a Bernoulli Trial. For mathematical convenience we label the outcomes 0 and 1 (for “failure” and “success”, but the assignment is arbitrary), and denote the probability of 1 by p. Because there are only two possible outcomes and the total probability is 1, the probability for the outcome 0 is 1-p. Concretely if there’s 30% chance of someone opening an email you sent (p=0.3), then there’s a 70% chance they don’t open it.\nLet’s label the outcome of the Bernoulli Trial by the random variable Y. Mathematically we would write the last paragraph as the pair of equations \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). There’s a mathematical trick to write these as a single equation: anything to the power of 1 is itself and anything to the power of 0 is 1 (except sometimes 0). So we can rewrite the equations as \\(P(Y=1) = p^1(1-p)^0\\) and \\(P(Y=0) = p^0(1-p)^1\\). Then we can combine them as \\(P(Y=y) = p^y(1-p)^{1-y}\\) for y either 0 or 1. This bit of arithmetic is a convenient trick.\nAny variable Y that satisfies these equations is called Bernoulli distributed. The expectation value of Y is \\(E(Y) = \\sum_y P(Y=y) y\\), which is p. Similarly the expectation value of \\(Y^2\\) also p; since the square of 0 is 0 and the square of 1 is 1, it’s the same. So the variance of Y is \\(V(Y) = E(Y^2) - E(Y)^2 = p - p^2 = p(1-p)\\).\nTo interpret this the expectation value is the same as the probability of success, since we coded success as 1 and failure as 0. The variance is a quadratic intersecting the x-axis at 0 and 1. Notice that the variance is 0 if p is 0 or 1; we always get failure or always get success. The variance is maximum when p is 0.5; that’s when we get the biggest spread between heads and tails. When p is one half then the deviation from the mean is plus or minus one half, giving a variance of one quarter.\nWhat if we run multiple independent trials? That is we send multiple emails to different people, or treat multiple different patients, or flip the coin multiple times. We ignore anything else we know and treat them as if they all have the same probability p, since the mixture or Bernouli’s is Bernoulli. How many successes will we get?\nDenote each trial by \\(Y_i\\), and the total number of successes in N trials by \\(Z = \\sum_{i=1}^{N} Y_i\\). Then the probability can be written as \\(P(Z=k) = {N \\choose k} p^k (1-p)^{N-k}\\). This can be seen because the number of ways of getting k successes out of N trials is given by the binomial coefficient \\({N \\choose k}\\). Then given k successes and N - k failures the probability of that outcome is the product of the probability for each Bernoulli random variable; \\(p^k (1-p)^{N-k}\\). A variable with this probability distribution is called Binomally distributed.\nConcretely flipping a fair coin 3 times, each time the result is H or T. There is only one way to get 3 heads, HHH, but 3 ways to get 2 heads and 1 tail; THH, HTH, HHT. Since each outcome is equally likely in this example \\(P(Z=3) = \\frac{1}{8}\\) and \\(P(Z=2) = \\frac{3}{8}\\). You can visualise this with probability squares.\nThis answers the question of how many heads you would expect if you flip a fair coin 10 times. The probability of getting exactly 5 heads is \\({5 \\choose 10} \\frac{1}{2^{10}}\\) which is approximately 25%. But the probability of getting exactly 10 heads is \\(\\frac{1}{2^{10}}\\) which is about 0.1%. Doing these calculations is straightforward on a computer; but we can get a general idea by looking at the mean and standard deviation.\nTrying to calculate the expectation value and variance directly from the probability distribution requires some tricky combinatorics, like you’d find in the excellent book Concrete Mathematics. But the expectation value of a sum of random variables is the sum of the expectation values; so \\(E(Z) = \\sum_{i=1}^{N} E(Y_i) = Np\\). This makes intuitive sense; if we send out 100 emails and the open rate is 30%, we expect \\(100 \\times 0.3 = 30\\) emails to be opened.\nSimilarly the variance of a sum of independent random variables is the sum of their variances. So the total variance is \\(V(Z) = Np(1-p)\\), and so the standard deviation increases with the square root of N. The coefficient of variation, that is the standard deviation relative to the mean, is \\(\\sqrt{\\frac{1-p}{Np}}\\). So quadrupling the sample size halves the spread of the results relative to the mean.\n\n\n\nSpread halves as sample size quadruples\n\n\nFor example there’s a 96% chance of getting 40 to 60 heads in 100 flips of a fair coin; that is 50 ± 20%. If we quadruple the number of flips we just double the range; so there 96% change of getting 180 to 220 heads in 400 flips of a fair coin, that is 200 ± 10%. If we quadruple the number flips again then in 1600 flips there’s a 95% chance of getting 800 ± 5% heads, that is 760 to 840 heads.\nIn fact as the number of trials increases the binomial distribution gets close to a Normal distribution. It’s a bit complicated exactly when this approximation applies, but it’s slower for more extreme p."
  },
  {
    "objectID": "what-is-better-programming-approach/index.html",
    "href": "what-is-better-programming-approach/index.html",
    "title": "What Is a Better Programming Approach?",
    "section": "",
    "text": "I used to write a lot of bespoke reports that took data from a database and filtered, joined and aggregated them to produce a summary. Originally there were a few generic report types that were written in PHP, because the standard reports available through a web application were written in that. The scripts were really hard to modify because there were many function arguments that looked generic, but would only work for certain cases that had been implemented in the web application; and the only way to find out for sure was to dig in to the monolithic codebase. The team moved to writing custom SQL in Python, and some data munging occurred in both SQL and Python. Most of the Python we used standard Python data structures and itertools to do the transformations. Then one of our coworkers introduced us to Pandas; I was initially sceptical but quickly saw how effectively it solved certain problems (even if we had to keep putting .reset_index() everywhere). We adopted it and it made the type of transformations we were doing really simple, our efficiency increased and removing the low level details of implementing the transforms enabled us to think about how we could deliver more value.\nIt’s hard to know how generalisable an increase in efficiency was. Our team generally got more things done in our Python libraries than the PHP hooks, but that was partly due to our team being more familiar with Python than PHP. Pandas made our team much more effective than plain Python because of the types of problems we were solving. But perhaps experienced PHP developers could have gotten a lot more done in PHP (I doubt it, but I don’t know). We had hundreds of reports, many running on a schedule, but maybe the system would have broken down at tens of thousands of reports.\nAfter we started using Pandas I learned about R’s tidyverse, but even though I think it’s much better it wasn’t worth our team switching from Pandas to R. I went to another modelling team that was working in R and started reading through R for Data Science. The bespoke reporting team used the Pandas pipe style but there are some operations that are hard to pipe, like calculating the second most common value. Where Pandas is an organically grown mess of operations, dplyr and tidyr are well designed minimal composible APIs. There was a tradeoff that writing functions was harder in dplyr because of the quasiquotation style you had to adopt (it looks like this has changed now and it looks much easier). But the team was already effective enough in Pandas (and didn’t know R), had extensive Python libraries for producing standard reports, and we had experience maintaining and operating the environment. For a completely new team it would be worth considering R, but for bespoke reporting team the benefits wouldn’t offset the switching costs.\nWhen I was using Hadoop in the pre-Spark days there were lots of ways to write jobs with different tradeoffs. The default was using Java (or anothe JVM language) to write MapReduce jobs, but it’s full of boilerplate and there’s a lot of code to get simple things done. There was Pig which allowed writing the jobs in a much simpler high-level language, and you could use custom UDFs in Java. There was Hive which allowed an SQL-like interface, less flexible than Pig but more similar to other tools. Finally there was Hadoop Streaming which let you run any custom program on Hadoop, with some limitations. There were some one-off jobs I wrote in Hadoop Streaming in a couple of days that would have taken the developers months to write in Java. But I wouldn’t build our whole system on this because it would be hard to maintain and ensure the integrity you could in a large Java system. There was a place for all these tools for different kinds of workloads from more static to more dynamic. However when Spark came along it effectively obsoleted all of these (except parts of Hive like the meta-store); it was flexible enough to do everything and you won’t hear much about the other technologies any more. The industry decided that Spark was a better approach for transforming data, and the PySpark interface gave it broader adoption.\nThe video game industry has had a large movement from building deep Object hierarchies to building entity component systems which has made it easier to build features and optimise code. Mick West’s Evolve Your Hierarchy explains that game developers traditionally build deep object hierarchies. I’ve seen this taught in Introductory Programming classes in Java; start with an automobile class and then subclass it with a car. Incidentally this is a terrible way to introduce programming because it’s an advanced design technique you can’t really appreciate until you can build simple programs, and there are many other ways to represent code. Indeed the problem with inheritance is that it tightly couples the components (which is exactly the opposite of the compartmentalisation that is taught as good software engineering practice); it’s essentially an easy way to copy code between components. This means if you make a change, like your player can now control a fly-by-wire missile, you’ll need to completely restructure your hierarchy and change code in lots of places (now fly-by-wire missile needs to inherit from controllable character, but it doesn’t have a health bar so we’ll have to create a new mutual superclass called “controllable” and move the appropriate code around). A different approach is to treat the entities as data, an aggregation of components, and have separate systems (or engines) that act on these components. This approach makes it easier to build new entities with custom functionality because you just need to assign the relevant components to them and let the systems act on them. In gaming it also can make it much faster because you can allocate all the entities together and process them in parallel when running a function, rather than calling out to lots of mutable objects where concurrency is hard. This kind of programming has it’s own challenges (such as communication between entities) but has been very successful; see game programming patterns article on components or games from within’s article on data oriented design for more details. Many game studios found it beneficial to make the cultural and code switch to an entity component system.\nBecause of the context dependence it’s very hard to show that a programming language of feature increases productivity, as Crista Videira Lopes’ argues in an article on research in programming languages. She mentions that people make claims like Haskell programs have fewer bugs because of its features, without providing any evidence. In my experience looking at Pandoc, one of the most used Haskell programs (along with ShellCheck), a lot of its code is complex string transformations which is a class of errors Haskell can’t help with (although it may help with many others). Even if Haskell does lead to fewer bugs, how much does it slow down development? Maybe paying a cost of unit tests and occasional bugs is less costly than trying to frame everything in terms of pure functions and jumping through monadic hoops everywhere. For a large development team there’s the additional issue that there isn’t a huge supply of Haskell developers, and they would likely have to support training people in the language which is an additional cost. For many teams Haskell may actually decrease productivity, rather than increase. I have heard a lot of stories of teams of Java developers moving to Scala writing “Scava code”; that is still using Java idioms rather than adoping functional programming techniques that Scala advocates. The kind of culture change of moving to a language like Haskell may be really difficult for some developers.\nThere are definitely times when changing your programming approach could make your team dramatically more effective. As far as I know there are no rules around this, largely because it depends on the team and their experience as well as the problem being solved. Software engineering is all about tradeoffs and you have to try to find what approach is best for your situation. In my personal experience I find object oriented approaches great for writing graphical user interfaces and functional approaches best for transforming and handling data. I really like R for analysis, but will move to Python for transforming data, and bash for quickly doing some file manipulation and SQL for extracting data and aggregating datasets too large to work with locally. But I’m left wondering is there a better way? This is the best reason in my mind to learn new programming languages, even esoteric ones, and try new libraries and read about different programming techniques. One day actors or logic programming or a custom DSL or propagators may make a hard problem easy to solve. But it’s really hard to know what works until you have experienced (that is, have tried it a few places and gotten it wrong a couple of times)."
  },
  {
    "objectID": "calculus-logit/index.html",
    "href": "calculus-logit/index.html",
    "title": "Calculus of the Inverse Logit Function",
    "section": "",
    "text": "The inverse logit function is \\({\\rm logit}^{-1}(x) = \\frac{\\exp(x)}{1+\\exp{x}}\\). A bit of calculus shows that\n\\[\\frac{\\rm d}{{\\rm d} x} {\\rm invlogit}(x) = \\frac{e^{x}}{\\left(1+e^{x}\\right)^2} = {\\rm invlogit}(x) (1 - {\\rm invlogit}(x))\\]\nThis is interesting in that if the predicted probability is p, then a small change in a predictor with a coefficient a should change the probability by approximately \\(a p (1-p)\\). This is maximised at \\(p=1/2\\), where the local change in probability is \\(a/4\\) which is the source of the divide-by-four rule in interpreting coefficients in logistic regression.\nHowever I find this expression interesting and wanted to find out whether it defines the inverse logit function. We want to find a function \\(f\\) such that \\(f' = f(1-f)\\). Using the derivative of the inverse function gives that\n\\[\\frac{\\rm d}{{\\rm d} x} f^{-1}(x) = \\frac{1}{x(1-x)} = \\frac{1}{x} + \\frac{1}{1-x} \\,.\\]\nIntegrating gives \\(f^{-1}(x) = \\log(x) - \\log(1-x) + c = \\log\\left(\\frac{x}{1-x}\\right) + c\\). Up to an additive constant this is just the logit function. Finally inverting this equation gives\n\\[f(x) = \\frac{\\exp(x-c)}{1 + \\exp(x-c)} \\,,\\]\nso that this indeed does define the inverse logit up to a translation.\nTranslating it to an inverse logit so that the maximum probability is at 0 gives it one more interesting property,\n\\[\\begin{align} 1 - {\\rm logit}^{-1}(x) &= 1 - \\frac{\\exp(x)}{1 + \\exp(x)} \\\\ &= \\frac{1}{1 + \\exp(x)} \\\\ &= \\frac{\\exp(-x)}{1 + \\exp(-x)} \\\\ &= {\\rm logit}^{-1}(-x) \\end{align}\\]\nOf course this symmetry property isn’t defining, since any function defined on the positive numbers between 0 and 1 can be extended on the negative numbers to satisfy this property."
  },
  {
    "objectID": "considering-vscode/index.html",
    "href": "considering-vscode/index.html",
    "title": "Considering VS Code from Emacs",
    "section": "",
    "text": "On Emacs\nI recently read a LWN article on Making Emacs Popular Again (and the corresponding HN thread). It’s very hard to bring in new libraries and external code to Emacs because of their strict requirements of GPL3+ (so even Qt’s GPL3 isn’t sufficient) and requirement of code copyright assignation to FSF, and even if the primary contributors are fine to sign away their copyright it is difficult for established projects with multiple contributors and a barrier to entry for new contributions. While I am sympathetic to free software the GNU approach puts a high barrier on entry. The primary reason Emacs has seen a resurgence is the ability to install third-party packages from MELPA, which contains packages with the kind of features in modern IDEs.\nThat being said there are many great features built into Emacs like remote editing with Tramp, ability to use shells, working in directories, process management and much more. There are lots of inbuilt functionality from how things are displayed to obscure gems like RFC1345 input mode for typing unicode characters. There are some useful packaged that can be installed directly via Elpa (the official Emacs package repository where everything is owned by FSF) like Org Mode.\nThe best thing about emacs is everything is very customisable with Hooks and writing functions in Elisp. This is great for making little tweaks, but for large groups of functionality it requires much more.\nWith MELPA packages Emacs can quickly become a powerful environment for text editing. I move around text using Vim keybinding with Evil, Evil Collection and Evil God State using my own custom commands to navigate between windows or launch applications. I use Magit for interfacing with Git, which frequently saves me from commiting inadvertent changes (and reaches the deeper features of git better than the generic VC mode). I quickly navigate menus with Ivy, jump through code with dumbjump and complete code with Company Mode. I can use Elpy for Python, ESS for R, and I’ve been hearing good things about using Lanugage Server Protocol which provides backends to lots of languages not well supported in Emacs (like Java).\nHowever this requires a lot of time to understand what packages to use and how to configure them. It’s good that it is often easy to customise them, because often it’s inconsistent between packages, wbut sometimes it requires a lot of customisation to just make it workable. And then sometimes there are painfully weird interactions. Company mode sometimes makes it really slow to type in a comint buffer (like a shell), and it took me a long time to figure out that was the issue. Now I just live without completion in comint buffers, which is a loss. When I run a command that starts a process it might sometimes run it remotely over TRAMP when I launch it in a local buffer.\nAll the time I spend configuring and debugging Emacs is time I’m not spending doing something more productive, so I want to see if there’s a better option now.\n\n\nAlternatives to Emacs\nI’m already using Vim for small edits, Jupyter Notebooks for exploratory analysis and sometimes RStudio when I want to use some HTML R features, or for some R Markdown (because Emacs polymode is a little buggy around the edges). But none of these could be a full time alternative for me; Vim is less of an IDE than Emacs and the others only work for limited languages and tasks. It seems like the most popular alternatives from my colleagues is Jetbrains’ products and VSCode (with Sublime and Atom seeming to lose mindshare), and they both look viable.\nFrom the outside VS Code looks promising. It can interact with WSL, Containers and over SSH. It has a Vim Emulation Mode which looks like it has enough features to be useful (though not as many as Evil). I’ve seen you can start a terminal from inside VSCode. It’s got documentation that focuses on usage, which is a big advantage over Emacs (which has complete documentation but requires serious dedication to read through).\nI’m concerned about Electron memory usage; Slack already uses a lot of memory. I’m also concerned how it will do for general tasks like editing text files, SQL and whatever else I come across. But it seems to have a lot of plugins so we’ll see what it can do.\n\n\nOther people’s experiences\nA Google search shows a lot of people have switched between the two, and I thought it may be useful to get their perspectives.\nThe best article I found was AdmiralBumbleBee’s comparison, which highlights that window management, navigation, and configuration is better in Emacs, but terminals are better in VS Code. They also mention having trouble with magit and syncing their init.el file, but those are things I like about Emacs. This resonates with a Reddit post that says VS Code is viable but easy to configure, limits how to split window and only working on a “per-project” basis (and I find this frustrating when I work in R Studio).\nHadi Timachi moved from Emacs to VS Code and back again, but his issues don’t seem compelling. It seems worth a try.\n\n\nMaking Emacs Better\nReflecting on the article trying to make Emacs more attractive to new users, what Emacs really needs is more contributors that can help the project. From core contributors who are very friendly to the community like Eli Zaretskii, to people who write about Emacs for general people like Magnar Sveen of Emacs Rocks and Mickey Peterson of Mastering Emacs, to prolific package builders like Abo-Abo and tarsius. While reducing friction and pain point for newcomers will definitely help, Emacs should really focus on what makes these people like Emacs, what makes it different from its competitors (though new competitors like VS Code are changing the gaps)."
  },
  {
    "objectID": "maths-textbook-market/index.html",
    "href": "maths-textbook-market/index.html",
    "title": "Market for Highschool Maths Textbooks",
    "section": "",
    "text": "When I was there they had two primary products, mathematics textbooks for South Australian highschools and mathematics textbooks for international programs such as the International Baccalaureate (IB). They had deep penetration in the niche of South Australian mathematics textbooks, but how big was that market? How could they afford to operate with a staff of a dozen people?\nTime for some Fermi estimates. South Australia has around 2 million people. While we know there are more Baby Boomers than Millenials let’s assume people’s ages are evenly distributed between 1 and 100. Then there are about 20,000 people of any age. You would need a maths textbook for each years 7 through 12, but years 11 and 12 aren’t compulsory and textbooks are more likely to be shared. Let’s take it as about 5 textbooks per student lifetime so that’s a maximum market of 100,000 textbooks per year. At $50 per book that’s about $5 million maximum gross revenue.\nHowever the real market will be significantly smaller because textbooks get resold and reused, especially in public schools. New textbooks generally get republished every few years in response to curriculum changes, so maybe this reduces the effective market by 50%. It’s also going to be unrealistic to capture the whole market when there were other popular interstate publishers, but maybe they had 70%. So that’s a gross revenue under $2 million.\nPublishing books is relatively expensive; they typically run a few hundred pages and need to be printed, shipped and distributed. Variable costs could easily eat up 40% of the profits leaving about $1 million for salaries and marketing. Assuming marketing costs are marginal and average employee cost is all in $100,000 then it could support about 10 employees at no profit.\nThis explains why there weren’t any serious competitors in the South Australian mathematics textbook market; it was quite small. Haese Mathematics started with teachers Bob Haese, Sandra Haese and Kim Harris writing maths textbooks for the South Australian curriculum in the late 70s and early 80s. Because the curriculum was different to the other states they wrote a books that catered exactly to the market and were able to gain enough adoption to write second and third editions. The company must have been very small then, and was unlikely to have been very profitable; it was also much more labour intensive as they wrote the early editions on a typewriter!\nI’m still curious as to how they employed me as an employee after their tenth. I do know they sold some of their textbooks in other states, especially the Northern Territory which shared the South Australian curriculum, but is a tenth of the size. They wrote textbooks for the IB, which is a huge market being international but also very competitive. There aren’t many IB students in Australia (today there are fewer than 80 programs, so probably only a few hundred students) and so their existing distribution channels wouldn’t work. However I do know they managed to sell a substantial number of textbooks, but I’d love to know how. Their unique advantage was their focus on mathematics (as opposed to their competitors that tended to write books for every subject) meant they could provide a better experience. But it would be harder to sell a mathematics book when the competitors could package up the whole curriculum.\nI suspect now they’re doing substantially better than when I worked for them. Australia has launched and rolled out a national curriculum, and they were in a good place to write textbooks to it. Their existing relationships meant it would have been easy to sell to existing South Australian schools, and their focus on the Australian mathematics market meant they could get a good product out much faster than their competitiors. This increases the potential market 10-fold and they could probably have easily captured a 2-3 times bigger market.\nAnother benefit since I left is the rise of digital textbooks. When they sell a subscription to a textbook there’s no problem of resale increasing the number of sales, and they don’t need to print the books and servers are cheap. Even at half the price it’s probably worth twice as much to them, and they bundle physical copies with digital at no extra cost because it will increase adoption and barely increase their operational costs.\nIt’s interesting to think I don’t think they could start their textbook company again today. The national curriculum has increased the competition and digital technologies means its easier for existing textbook companies to expand their offerings to other markets. They saw a gap and they filled it and managed to, after a lot of time and effort, grow into adjacent markets and build a solid company."
  },
  {
    "objectID": "dumbjump/index.html",
    "href": "dumbjump/index.html",
    "title": "Using emacs dumb-jump with evil",
    "section": "",
    "text": "I use it with Evil mode (vim emulation) and bind dump-jump-go to C-]. Unfortunately it doesn’t play well with the Evil jump list and when I invoke evil-jump-backward with C-o it jumps back to the wrong place. While I could invoke dumb-jump-back, I like being able to use C-o for a dumb-jump or a search in emacs. This can be done by advising the core jump function in dumb-jump to push the position to evil’s jump list. There’s probably a prettier way but this does the trick:\n;; Preserve jump list in evil\n(defun evil-set-jump-args (&rest ns) (evil-set-jump))\n(advice-add 'dumb-jump-goto-file-line :before #'evil-set-jump-args)\nIf you’re ever working in Vim proper dim-jump is a substitute for dumb-jump."
  },
  {
    "objectID": "glassbox-ml/index.html",
    "href": "glassbox-ml/index.html",
    "title": "Glassbox Machine Learning",
    "section": "",
    "text": "He calls interpretable models glassbox machine learning, in contrast to blackbox machine learning. It is models in which a person can explicitly see how they work, and follow the steps from inputs to outputs. This interpretability is subtly different from explainable (explainable to who?), or plausible (to who?) and quite orthogonal to causal.\nExamples of glassbox models are decision trees, (general) linear models, and k-nearest neighbours. There’s a good chapter on interpretable models in Christoph Molnar’s Interpretable Machine Learning book. Other notable examples are Cynthia Rudin’s Falling Rule Lists (a sort of very simple decision tree) and risk-slim risk scores. However these methods are generally considered to be less predictive than gradient boosted trees and neural networks.\nRich Caruana talks about Explainable Boosting Models as a glassbox model that works almost as well as the best blackbox models. It originates from a 2012 KDD paper, Intelligible Models for Classification and Regression, where they fit a particular type of Generalised Additive Model. For the shape function they use bagged trees on each feature and use a gradient boosting method to fit the trees, with good results. In a 2013 KDD paper, Accurate Intelligible Models with Pairwise Interactions, they introduced pairwise interactions between features which greatly increased the accuracy of the models, while still remaining interpretable. Like all GAMs you end up with a sum of graphs (in this case with 1 or 2 independent variables), each of which can be inspected to understand how the model makes predictions. There’s a 2015 paper, Intelligible Models for HealthCare: Predicting PneumoniaRisk and Hospital 30-day Readmission which shows how this can be used to solve real problems. Interestingly the glassbox methods help identify data issues which may escape an exploratory data analysis.\nA challenge of this method is you need good features; it doesn’t work well if you’ve just got a bunch of text, images, or unidentified columns because you can’t interpret (or view all of) the graphs.\n\nIf you don’t know what the feature means then you don’t know what it implies\n\nAs I discussed in rules and models the most promising way forward seems to be to build features on top of the raw data (using flexible blackbox methods like neural networks), and then build an explainable model on top of that - but it’s a lot of work.\nAnother challenge is collinearity. If two features are the same how do you share the weight of their contribution to the sum? Explainable Boosting Models share the load equally - which is probably the least bad thing to do.\nThis all sounds really interesting and I’m keen to try it in the InterpretML tool on real datasets to see how it performs against good baselines (e.g. Kaggle competitions)."
  },
  {
    "objectID": "unhappy-path-programming/index.html",
    "href": "unhappy-path-programming/index.html",
    "title": "Unhappy Path Programming",
    "section": "",
    "text": "While you always need to be thinking about how things could go wrong, it’s much more important in web programming. I’ve written a lot of batch data processing, and while you have to be careful of things like malformed data and encoding issues, you control the environment so many classes of failure are manageable. Web programming is much more uncontrolled; your client side code could be operating on a vast number of different browsers and may not work properly on some of them, any request you make can fail due to connection issues, and the data your server receives could be maliciously crafted. It may work in one case, but then be unusable for certain users, fail intermittently and even leave your server open to compromise by an injection attack.\nTo build a system that handles failures and edge cases well requires a lot of discipline or some useful guard rails. Traditional scripting languages like Javascript, PHP and Python have shockingly few such guard rails by default, and it’s easy to write code riddled with bugs that will be hit once in a while that together make an unstable application. I’m interested in how far things like type safety, forcing dealing with errors and static analysis can remove these issues in practice."
  },
  {
    "objectID": "interpretable-models-rudin/index.html",
    "href": "interpretable-models-rudin/index.html",
    "title": "Interpretable models with Cynthia Rudin",
    "section": "",
    "text": "But getting past the headlines there is an interesting perspective. Her tutorial on The Secrets of Machine Learning takes a more nuanced point of view:\n\nmost machine learning methods tend to perform similarly, if tuned properly, when the covariates have an inherent meaning as predictor variables (e.g., age, gender, blood pressure) rather than raw measurement values (e.g., raw pixel values from images, raw time points from sound files)\n\nThis is quite a reasonable claim; one of the benefits of black box models like boosted and bagged trees, and neural networks is that they do very well with complex raw features, and are currently nearly always a key part in winning machine learning competitions on Kaggle. However when you have reasonable features you may be able to construct a simple model that can do quite well. They go on to say:\n\nInterestingly, adding more data, adding domain knowledge, or improving the quality of data, can often be much more valuable than switching algorithms or changing tuning procedures\n\nThis is a great point; the amount of data (especially unusual examples) and the quality data fundamentally limit how good the model can be (and even how well you can evaluate model performance). With simpler models it’s easier to integrate domain knowledge to make a simpler model. This has a larger up front cost than throwing a gradient boosting machine at the problem; but it has some potential benefits.\nIt’s also important to consider the impact of performance on decisions. There is often diminishing returns and a small increase in model performance leads to a negligible increase in expected return. This means that techniques common in machine learning competitions like ensembling and stacking models, which have a substantial operational and maintenance overhead, are often not worthwhile in practice. That’s not to mention the increased need for data quality to attain higher levels of performance. The optimal tradeoff point depends on the amount of leverage you have; for Google’s Ad products a small increase in click through rate can mean billions of dollars of revenue and it may be worth it (although these models are constrained by low latency). This is an exceptional case.\nThe article concludes the section with a reasonable recommendation:\n\nThus, the recommendation is simple: if you have classification or regression data with inherently meaningful (non-raw) covariates, then try several different algorithms. If several of them all perform similarly after parameter tuning, use the simplest or most meaningful model. Analyze the model, and try to embed domain knowledge into the next iteration of the model. On the other hand, if there are large performance differences between algorithms, or if your data are raw (e.g., pixels from images or raw time series measurements), you may want to use a neural network\n\nFor computer vision or NLP there’s no doubt that the best models are specific types of neural networks. However, despite a lot of work, they’re not robustly interpretable (as can be seen from adversarial methods). For tabular datasets it’s generally a much closer game; and the idea of trying some different models and seeing what works best is often advocated, for example in Kuhn’s Applied Predictive Modelling.\nThere’s often deeper advantages of interpretable models. If the decision informed by the model is being made by a domain expert, they can understand why the model is making the prediction and use their expertise to override it if something is going wrong. They’re easier to maintain, understand and debug, easier to fine tune and hone, and unlikely to give surprisingly bad results. All other things being equal and interpretable model is the best choice.\nIn the unstructured case you can use neural networks to extract structured features that then go into interpretable models. I discuss this in rules and models, and it’s a standard approach in NLP using tools like Stanza to extract the features and writing rules on top of those.\nI’m not convinced by their claim that interpretable models can almost always be made to perform as well as black box models. They don’t occur in general competitive machine learning competitions (and didn’t even win the interpretable machine learning competition). Most of the research they point to is their own, and I don’t know how strong the baselines are. However I could believe they’re good enough to use, and if a simple model does the trick to use it.\nCynthia Rudin has a lot of interesting research on this area. There are papers on discrete optimisation methods such as branch-and-bound for interpretable methods such as optimal sparse decision trees (code), falling rule lists (code), and notably using linear risk scores (code, video). There are prototype methods, in image recognition (code) including using hierarchical prototypes (code), and in causal inference such as MALTS (code) and Almost Matching Exactly (code). There are an assortment of other types of papers like combining machine learning with decision making, bandit methods in time series, and applied papers in healthcase, justice and electrical systems. I’d like to dig deeper into a few of these later."
  },
  {
    "objectID": "tests-make-changes-faster/index.html",
    "href": "tests-make-changes-faster/index.html",
    "title": "Making Changes Faster with Tests",
    "section": "",
    "text": "I used to think the whole point of software verifications like types and tests was to ensure a piece of software worked as specified. Consequently if a piece of software already worked there wasn’t much point in adding automated tests; sure we might find a few edge cases that didn’t work, but we already would have had the ones that impacted end users in bug reports. I now think the primary benefit of verifications is about making software easier to change without losing quality by introducing regressions. Tests can directly help prevent regressions by verifying certain edge cases, but they can also increase confidence that the software is working correctly allowing less careful changes, and encourage a structure of modular, independently tested code which is easier to understand and change."
  },
  {
    "objectID": "tests-make-changes-faster/index.html#preventing-regressions",
    "href": "tests-make-changes-faster/index.html#preventing-regressions",
    "title": "Making Changes Faster with Tests",
    "section": "Preventing regressions",
    "text": "Preventing regressions\nThe first time I came across the idea of regression tests was in my first professional job ad Haese Mathematics, when I was writing code that simplified algebraic expressions. A colleague, Troy Cruickshank, had written a rules engine that recursively applied simplifications until the expression didn’t change. So for example we would have a list of rules like 1 * x -> x and a*x + b*x -> (a+b)*x. However as we grew this list of rules it became harder to know what was going to happen. The order of the rules was very important, since some rules required the expression to be in a certain form, that other rules may help or hinder. We’d suddenly find in manual testing that certain expressions would come out in a non-simplified form. For example maybe we had the rules above and found the expression 1 * x + 2*x -> x + 2*x because we applied 1*x -> x first, when we really wanted 3*x. This meant I would spend a long time thinking before adding any rule, trying to think through any of these sorts of interactions, and I’d still miss them.\nMy manager, Adrian Blackburn, suggested that I right some unit tests to prevent these regressions. The idea of coming up with test cases seemed overwhelming - how do I choose what to test? I tested 1 + 1 -> 2, x + x -> 2*x, and dozens of other cases, but I wasn’t really sure they were adding any value. However what did add value was adding real regressions making sure we didn’t make the same mistake twice.\nI never got fully comfortable with that rule list, but the regression tests did save time, because when I came back to it a month later and made a change the tests would immediately tell me if I’d made the same mistake again, instead of spending hours looking for and reproducing it."
  },
  {
    "objectID": "tests-make-changes-faster/index.html#making-code-easier-to-change",
    "href": "tests-make-changes-faster/index.html#making-code-easier-to-change",
    "title": "Making Changes Faster with Tests",
    "section": "Making code easier to change",
    "text": "Making code easier to change\nWhen I went to the Compose Conference I was talking to an advocate of strongly typed functional languages (think Haskell). I asked them why they liked type systems so much, and they said it means when they refactor their code they know if they made a mistake immediately. This was a revelation to me - static typing was a tool to help them change the code. This isn’t a new idea, in Martin Fowler’s Refactoring he talks about how unit tests let you make more aggressive refactorings in small steps, and get quick feedback if you made a mistake.\nI’ve seen this first hand in data science code; the code had no tests, were mainly SQL, and the only way I could verify I hadn’t broken the code was to run it before and after the change and run a diff test. But this step took hours to run, and so I wouldn’t wast a whole run on a small change, I’d try to make multiple changes. This meant if I broke something I’d have to spend a fair bit of time debugging what I broke. I could reduce the time taken by sampling the input dataset (although this may mean I’d miss some edge cases), and optimising the code, but it was still slow.\nOver time my team refactored this code into small functions in Python that processed the data and could be independently tested (in this case the data was small enough to fit in memory). We could run unit tests in seconds that validated or invalidated a change. They also were useful documentation for on-boarding people to the project about what the code actually did. It meant we went from taking months to make a substantial change being able to break it into small pieces that took days. While big, slow integration tests have value, it’s the quick small unit tests that really help make faster changes."
  },
  {
    "objectID": "tests-make-changes-faster/index.html#overfitting-tests",
    "href": "tests-make-changes-faster/index.html#overfitting-tests",
    "title": "Making Changes Faster with Tests",
    "section": "Overfitting Tests",
    "text": "Overfitting Tests\nTests that make the code harder to change are bad tests. If you’re testing a lot of things together, spending a long time setting up, and testing the details of the implementation then the test is likely overfit to the code and hard to change.\nI once wanted to make changes to an in-house scheduling system that allowed us to prioritise tasks. Occasionally it would take hours to run an ETL job, and we needed to put the time sensitive tasks to the top. The change was relatively straightforward, but updating the tests took as long as writing the feature.\nThe reason was the tests I had to change were many dozens of lines long. It effectively mocked another in-house system in painstaking detail, created a bunch of data and tested exactly what the output would be like for a particular flow of events. I then had to propagate the prioritisation field through the system and work through how it changed the set of interactions at each step in the process. I eventually made it all work, but it was painful and took a long time to get reviewed."
  },
  {
    "objectID": "tests-make-changes-faster/index.html#better-code",
    "href": "tests-make-changes-faster/index.html#better-code",
    "title": "Making Changes Faster with Tests",
    "section": "Better code",
    "text": "Better code\nGood tests lead to better code. The easiest tests to write are on small, pure functions that do one thing. Tests on these functions are clear and demonstrative. The easiest functions to understand are small, pure functions. Side effects and hence mocking in tests, can never be fully avoided. But they can be moved to the edges and the majority of code can be pure functions transforming data.\nI used to think the main benefit of modularity in software was to have components you reused in lots of places. I’ve found in practice it’s hard to reuse a lot of code, as it’s very specific to a problem. However having a system of small components that can be easily verified to work correctly individually is a large benefit. Then things like types can help verify you’ve put them together correctly and you can quickly change code and be pretty confident it will run correctly.\nA common problem I’ve seen in data science code is system calls to get the current date, with the underlying assumption that the date won’t change (which can be false in a long running process, leading to strange bugs). Making the date an explicit parameter makes it clearer where dates are being used, and forces someone reading the code to think about how the dates flow through the code. It also happens to make the code easier to test, and to understand."
  },
  {
    "objectID": "contact-tracing/index.html",
    "href": "contact-tracing/index.html",
    "title": "Contact Tracing in Fighting Epidemics",
    "section": "",
    "text": "However through the lens of the SIR model this seems surprising. The rate of epidemiological spread is independent of the number of people that have the disease. The key factor I overlooked is the stochastic nature of the model at an individual level, and the impact of voluntary isolation and contact tracing.\n\nIndividuals in the SIR Model\nIn the SIR model an infectious individual on average infects \\(R_0\\) other people while infected. In a population where most people are susceptible if this number is greater than 1 then the epidemic spreads, if it’s less than 1 then the disease will die out.\nOne simple method for reducing the number of other individuals infected is to isolate them. If infectious people don’t interact with anyone else then they can’t infect anyone else. If everyone does this then the disease dissapates very quickly.\nThe problem is that a lot of people don’t know that they’re infectious. With many diseases people are infectious before they show symptoms, and some people don’t ever show symptoms but are infectious. So they go on as before and infect many other people.\nA really simple model for this is that asymptomatic carriers infect a high number of other people \\(R_h \\gg 1\\) and symptomatic cariers infect a low number of people \\(R_l \\ll 1\\). Then if the chance of being asymptomatic is \\(p\\) the overall \\(R_0 = p R_h + (1 - p) R_l \\approx p R_h\\).\n\n\nContact tracing\nMany infectious diseases spread mainly through direct contact between individuals. If someone has tested positive then their direct contacts are much more likely to have it than random individuals. If you test those individuals immediately you can effectively detect asymptomatic carriers and get them to isolate.\nThis then moves these asymptomatic carriers from the \\(R_h\\) group into the \\(R_l\\) group, effectively reducing \\(p\\). If you can get \\(p\\) low enough then the epidemic spread will stop.\nHowever contact tracing is a lot of work. Every person tested positive need to be contacted and interviewed; this could take some time if they are hard to get hold of.\nThen you have to follow up with each of their contacts and the information of where they had been. If they’d been to work or for a haircut you would need to call the business and get the contact information of everyone that was in contact with that individual. This may take some time and quickly turn out to be 20 or 30 people that need to be contacted.\nI think it would be reasonable to estimate that every new infected case would maybe 1-2 people days to follow up on average. This may sound high but getting in contact with people can be hard. It’s also important how quickly you get into contact with them; the longer you leave it the more people they have infected.\nThe NSW Health Website claims their team of 150 can make up to 1300 calls per day. So if each positive person is on average in contact with 10 other people they could handle up to around 120 new positive cases per day.\nSo if you have around 500 new cases every day, then you’d need of the order of a thousand case workers plus relevant support. The Victorian Government claimed it had a team of 2600, but couldn’t keep up with 500 per day.\nI’m not sure what the difference is here (probably who they include in the counts), but for the sake of argument let’s say a manageable caseload for Melbourne or Sydney is up to around 150.\n\n\nLow targets\nThings don’t still add up. If contact tracing is the bottleneck then why would we be aiming to get to only 5 daily cases and not 150? This could be reasonable handled by a small team for a city of 5 million.\nI think the reason is stochastic outbreaks. You’re sometimes going to get unreasonable people who refuse to be interviewed and refuse to isolate. They could infect a lot of other individuals, some of whom will become asymptomatic carriers.\nDue to chance sometimes you will get a few of these individuals on the same day. They will infect a bunch of people, and even the symptomatic individuals won’t show up for a week. Because of the exponential growth of epidemic disease by the time you get the first positive test you could have scores of people infected. In the worst case you could end up a few weeks later with a couple hundred cases per day and be unable to trace them. The more cases you miss the faster the infection spreads.\nThe chance of this happening will be proportional to the number of active cases in the community. In turn this is roughly the total number of new cases in the past couple weeks (if we assume the average time in the infectious state is two weeks). So you would set targets based on the number of new cases in the past couple of weeks. Where you set this target depends on how much risk you’re willing to take, and how many unreasonable people you think there are.\n\n\nReflecting on the model\nThis analysis goes past our initial simple SIR model, but not incredibly far. One big step forward would be to model individuals with a stochastic rate of recovery and rate of transmission. You would have to choose distributions for these and fit them to the data, but I’m sure there are standard models. Then your “unreasonable people” will be the tails of the distribution.\nHowever making these sorts of decisions on a stochastic model is painful. How big a chance are you willing to take on another “wave” of spread? Is 5% too much? What about 0.5%? The costs of restrictions on businesses and mental health of individuals is significant.\nThe outcomes of the model are also going to be very sensitive to the assumptions built in. It would be hard to get reliable data for a lot of parameters because of missing information of individuals who don’t get tested. Using epidemiological information from other outbreaks would be hard because it’s hard to untangle all the differences between two cities and the impact that has on spread. I wouldn’t be surprised if there’s a factor of 4 difference between the probabilities at the tails of two different modelling groups.\nThe Australian Prime Minister Scott Morrison has criticised Victoria’s contact tracing. He uses the example of New South Wales which sustained daily positive tests between 10 and 20 for the last two months. However single examples aren’t great for statistical modelling; they could easily have an outbreak next month.\nMore significantly leading epidemiologists think that the targets are conservative. These are people who actually know what they’re talking about (unlike me); so I’d guess that the decision was made on a very conservative chance of a second wave of spread with many bad case assumptions."
  },
  {
    "objectID": "rough-coarse-geocoding/index.html",
    "href": "rough-coarse-geocoding/index.html",
    "title": "Rough Coarse Geocoding",
    "section": "",
    "text": "A coarse geocoder takes a human description of a large area like a city, area or country and returns the details of that location. I’ve been looking into the source of the excellent Placeholder (a component of the Pelias geocoder) to understand how this works. The overall approach is straightforward, but it takes a lot of work to get it to be reliable.\nA key component geocoder is a gazetteer that contains the names of locations. Placeholder uses Who’s on First which is a large open dataset that captures locations as GeoJSON based on how people describe them (including names in many languages). The returned locations are Who’s on First entities. Placeholder stores these as tables in a SQLite database, which can be used to refine locations in Placeholder.\nThe overall approach of Placeholder is:\nSo for example consider some text like “Saint Albans, Australia”. This gets normalised to “st albans australia”. This then gets tokenised to “st albans” and “australia”. Next “australia” is matched to the country. Then “st albans” is searched for in Australia, and it finds a few results in Victoria and New South Wales. Further it does an R-tree search for “st albans” in locations within 2 degrees of Australia and finds another location in New Zealand. These are then returned ordered by Who’s on First id."
  },
  {
    "objectID": "rough-coarse-geocoding/index.html#tokenize",
    "href": "rough-coarse-geocoding/index.html#tokenize",
    "title": "Rough Coarse Geocoding",
    "section": "Tokenize",
    "text": "Tokenize\nThe tokenisation is a little difficult in that place names can contain multiple words. The approach in Placeholder, in prototypes/tokenize.js, is relatively simple. First break the query into words, and start at the leftmost token. Then take the span from the first to last word and if that’s in the gazetteer then use that as the word, otherwise remove the last word from the span and repeat until you find a token or get down to a single word. Then continue to tokenize the rest of the text.\nFor example consider “Port of Spain Trinidad and Tobago”. This isn’t in the gazetteer, not if “Port of Spain Trinidad and”, or “Port of Spain Trinidad”, but “Port of Spain” is and so that’s our first token. Then to tokenize “Trinidad and Tobago” that is in our gazetteer and so is a token. So we get two tokens “Port of Spain” and “Trinidad and Tobago”.\nAs another example “Melbourne CBD Australia” tokenizes to “Melbourne CBD” and “Australia”, since “Melbourne CBD” is in Who’s on First. But “Sydney CBD Australia” (currently) tokenizes to “Sydney”, “CBD” and “Australia” since “Sydney CBD” is not in Who’s on First.\nThis is a simple strategy but works pretty well."
  },
  {
    "objectID": "rough-coarse-geocoding/index.html#search",
    "href": "rough-coarse-geocoding/index.html#search",
    "title": "Rough Coarse Geocoding",
    "section": "Search",
    "text": "Search\nIn the west people normally write locations from specific to general, e.g. “Melbourne CBD, Victoria, Australia”. Although in some cultures, like Vietnamese, people write the opposite way from general to specific. Placeholder assumes people are writing from specific to general and refines the search from general to specific.\nSo it will start with the rightmost token, in this case “Australia” and gets a list of locations. It then uses the Who’s on First lineage to look for “Victoria” in each “Australia” (or nearby to Australia using an R-tree search). If it finds one it will refine to searching in Victoria for Melbourne, otherwise it will search for Melbourne in Australia. Finally it returns all the results.\nThis heuristic works pretty well, but sometimes has odd results. For example “Sydney CBD Australia” is tokenized to “Sydney”, “CBD” and “Australia”. It so happens there is currently only one entity in Who’s on First with a tag of CBD in/near Australia, and that happens to be Melbourne CBD. So it then looks for Sydney in Melbourne CBD, but doesn’t find one and returns Melbourne CBD.\nOn the other hand “Sydney CBD NSW Australia” first finds the state of “New South Wales” in “Australia”. Then it fails to find a “CBD” in “New South Wales”, and so searches for “Sydney” in “New South Wales” and finds the capital."
  },
  {
    "objectID": "rough-coarse-geocoding/index.html#sort",
    "href": "rough-coarse-geocoding/index.html#sort",
    "title": "Rough Coarse Geocoding",
    "section": "Sort",
    "text": "Sort\nThe sorting is very important, for example if I’m searching for “Paris” without any context I’m most likely to be searching for Paris, France than Paris, USA. However as far as I can tell Placeholder just sorts in order of the Who’s on First id. In practice this seems to work remarkably well; the larger and more populous places tend to occur first. I don’t know why this is; maybe it’s because the dataset was built up starting with the most common places first."
  },
  {
    "objectID": "rough-coarse-geocoding/index.html#putting-it-together",
    "href": "rough-coarse-geocoding/index.html#putting-it-together",
    "title": "Rough Coarse Geocoding",
    "section": "Putting it together",
    "text": "Putting it together\nThe Placeholder geocoder takes a relatively straightforward approach, but it’s pretty effective. I’ve been using it to geocode Australian locations and it’s really easy to use through docker. However I’m finding I want to be able to customise it and make it fit better with the rest of my Python code. I don’t think it would be tremendously difficult to port to Python, although requires to be deliberate to get exactly the same results."
  },
  {
    "objectID": "dataflow-chasing/index.html",
    "href": "dataflow-chasing/index.html",
    "title": "Dataflow Chasing",
    "section": "",
    "text": "Generally you can view the process as a directed and (hopefully) acyclic graph. Some data goes in, goes through some processes, and some data comes out. While this process is easier if the transformation is done via functions (or even declaratively), the process is conceptual and it’s possible to draw out even if it’s through a big chunk of spaghetti (though if there’s a lot of coordinated state changes involved you may also need a state transition diagram).\nTypically the best place to start is with the outputs and trace your way back. Looking at the outputs what input data and transformation is used to produce it? For each of those inputs what are used to produce them? Get a sketch of the flow at a high level and then try to drill into the pieces you’re most interested in. This is a tedious process, but you can eventually get a dataflow diagram, at least to the point where you understand the impact of the system you’re changing.\nWhile you could potentially automate this the process of drawing these diagrams is useful for getting across the codebase. You’ll start to notice patterns of how it’s written, what sorts of techniques are used and an idea of the data involved. This is something that just takes time; and tracing dataflow is a useful way to do it.\nWhen you want to zoom in on a particular process you can always serialise (a sample of) the data or debug (for example in Python with pdb). This is a good way to test your assumptions; think of what you expect the data to look like or some assertions you think should be true. When the processes are hard to understand looking at these snapshots can help clarify what’s going on.\nThis then helps give a framing of the change you want to make; what are the impacts going to be? It’s simply chasing through the dataflow diagram; what data do you need from upstream, and what changes need to be made downstream. If you can break the change into small steps only touching one or two processes at a time it’s much easier to make. The big picture dataflow can help identify opportunities to unify repeated processes that would all need to be changed together."
  },
  {
    "objectID": "rdflib-add-types/index.html",
    "href": "rdflib-add-types/index.html",
    "title": "Adding Types to Rdflib",
    "section": "",
    "text": "When RDFLib parses a literal it will create a rdflib.term.Literal object and the value field will contain the Python type if it can be successfully converted, otherwise it will be None. This object has a toPython() method that will return the value unless it is None, in which case it will return the object itself. To see how this works here’s some simple code to parse some RDF data and output all the objects: both in raw form, through toPython and the value.\ndef parse_objects(data, format='ntriples'):\n    G = rdflib.Graph()\n    G.parse(data=data, format=format)\n    return [(o, o.toPython(), o.value) for o in G.objects()]\nFor a string literal it is represented as the string itself.\nparse_objects('_:b <http://example.org/value> \"1\" . \\n')\n\n>> [(rdflib.term.Literal('1'), '1', '1')]\nFor an XML Schema integer it is stored as a Python integer\nparse_objects('_:b <http://example.org/value> \"1\"^^<http://www.w3.org/2001/XMLSchema#integer> . \\n')\n\n>> [(rdflib.term.Literal('1', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')),\n>>>  1, 1)]\nNote that if the data doesn’t match the type the value remains as None.\nparse_objects('_:b <http://example.org/value> \"2020-01-01\"^^<http://www.w3.org/2001/XMLSchema#integer> . \\n')\n\n>> [(rdflib.term.Literal('2020-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')),\n>>>  rdflib.term.Literal('2020-01-01', datatype=rdflib.term.URIRef('http://www.w3.org/2001/XMLSchema#integer')),\n>>>  None)]\nIf we have a custom datatype then the value also remains as None.\nparse_objects('_:b <http://example.org/value> \"2020-01-01\"^^<http://schema.org/Date> . \\n')\n\n>> [(rdflib.term.Literal('2020-01-01', datatype=rdflib.term.URIRef('http://schema.org/Date')),\n>>>  rdflib.term.Literal('2020-01-01', datatype=rdflib.term.URIRef('http://schema.org/Date')),\n>>>  None)]\nWe can add a custom datatype with rdflib.term.bind that allows converting between Python types and RDF types. In this case we’re only interested in converting from RDF to Python. The arguments are:\n\ndatatype: The RDF Datatype we want to convert\npythontype: The corresponding Python datatype\nconstructor: How to turn an RDF literal to a Python datatype\nlexicalizer: How to turn a Python datatype to an RDF (not needed here)\ndatatype_specific: Whether the binding is specific or general; there are other representations of datetime so set to True\n\nimport datetime, dateutil\nrdflib.term.bind(datatype=rdflib.URIRef('http://schema.org/Date'),\n                 pythontype=datetime.datetime,\n                 constructor=dateutil.parser.isoparse,\n                 lexicalizer= lambda dt: dt.isoformat(),\n                 datatype_specific=True)\nThen running the exact same code now gives a different result, with the correct type.\nparse_objects('_:b <http://example.org/value> \"2020-01-01\"^^<http://schema.org/Date> . \\n')\n\n>> [(rdflib.term.Literal('2020-01-01T00:00:00', datatype=rdflib.term.URIRef('http://schema.org/Date')),\n>>>  datetime.datetime(2020, 1, 1, 0, 0),\n>>>  datetime.datetime(2020, 1, 1, 0, 0))]\nAs for native types if it parsing would be an error we get the value still being null:\nparse_objects('_:b <http://example.org/value> \"1\"^^<http://schema.org/Date> . \\n')\n\n>> [(rdflib.term.Literal('1', datatype=rdflib.term.URIRef('http://schema.org/Date')),\n>>>  rdflib.term.Literal('1', datatype=rdflib.term.URIRef('http://schema.org/Date')),\n>>>  None)]\nI really don’t like that running the same inputs gives a different output for a parser. It can be really hard to reason about what is happening, as this could be set deep in some code. Even worse if another package I import uses a different binding for this RDF I could break it. Ideally bindings would be passed in some scope (e.g. using an object), rather than global state. Since not many things use RDF in practice it’s not a big issue, but it is not a robust design - if this was used in YAML parsing it could be catastrophic."
  },
  {
    "objectID": "centroid-spherical-polygon/index.html",
    "href": "centroid-spherical-polygon/index.html",
    "title": "Centroid Spherical Polygon",
    "section": "",
    "text": "You’re organising a conference of operations research analysts from all over the world, but their time is very valuable and they only agree to meet if you minimise the average distance they need to travel (even if they have to have it on a boat in the middle of the ocean). Where do you put the conference?\nLet’s model the world as a unit sphere in 3 dimensional space, and have the N people at cartesian coordinates \\(\\{ p_i\\}_{i=1}^{N}\\). Then the point c of minimum average distance, the spherical centroid (or spherical Fréchet mean), is given by the formula:\n\\[c = k \\sum_{i=1}^{N} \\frac{p_i}{\\sqrt{1 - (c \\cdot p_i)^2}}\\]\nWhere k is a normalising constant so that c lies on the unit sphere. This is like the geometric centroid of the points, but weighted based on the similarity of the centroid to those points.\nThe equation doesn’t solve for c since it’s on both sides of the equation, but can be evaluated iteratively. Start with a reasonable c (even though it’s not on the sphere, c = 0 actually works perfectly). Then iteratively update c by using your current estimate of c on the right hand side of the equation. Stop when this converges to a centroid."
  },
  {
    "objectID": "centroid-spherical-polygon/index.html#context",
    "href": "centroid-spherical-polygon/index.html#context",
    "title": "Centroid Spherical Polygon",
    "section": "Context",
    "text": "Context\nI found a few discussions of this problem but no actual solution. This question was inspired by a Notion Parrallax blog post. In there he suggests a few methods but he doesn’t actually solve it. There’s a GIS Stackexchange question on Calculating a Spherical Polygon Centroid. One of the answers references the paper Spherical Averages and Applications to Spherical Splines and Interpolation, by Buss and Fillmore, ACM Transactions on Graphics 20, 95126 (2001). However the paper gives a really complex derivation of what is essentially gradient descent (and they forget to multiply by a small step size) and I found it hard to follow. But this is very similar to finding the centroid of cosine similarity and we can take the same approach."
  },
  {
    "objectID": "centroid-spherical-polygon/index.html#solution",
    "href": "centroid-spherical-polygon/index.html#solution",
    "title": "Centroid Spherical Polygon",
    "section": "Solution",
    "text": "Solution\nThe extrema of average distance can be found using the method of Lagrange multipliers. The total distance to a point c is given by \\(d(c) = \\sum_{i=1}^{N} \\arccos\\left( c \\cdot p_i \\right)\\), and this is extremised at the same points that the average distance is (since they just differ by a constant factor of N). Then the lagrangian that constrains c to the unit sphere is given by \\(L(c, \\lambda) = \\sum_{i=1}^{N} \\arccos\\left( c \\cdot p_i \\right) + \\lambda (1 - c \\cdot c)\\).\nTaking the partial derivatives (using arccos derivative) we get (where \\(p_{ij}\\) is a clumsy notation meaning the jth coordinate of the ith point):\n\\[\\frac{\\partial L}{\\partial C_j}(c, \\lambda) = - \\sum_{i=1}^{N} \\frac{p_{ij}}{\\sqrt{1 - (c \\cdot p_i)^2}} - 2 \\lambda c_j\\]\n\\[\\frac{\\partial L}{\\partial \\lambda}(c, \\lambda) = 1 - c \\cdot c\\]\nThe extrema occur where these are 0, and in particular where \\(c = k \\sum_{i=1}^{N} \\frac{p_i}{\\sqrt{1 - (c \\cdot p_i)^2}}\\) where k is a constant so that the second normalisation constraint holds.\nWhat’s not obvious to me is when this point will be the minimum, and when calculating it iteratively will converge. There are going to be issues when the points are symmetric; for example for two antipodal points the entire great circle on the plane orthogonal to them will be minimum, and the formula blows up. But if we slightly perturb the points then in general there should be a unique minimum distance point. However there could also be a maximum distance point (since the sphere is bounded) and that would also satisfy the equation. I would expect if we start close to the minimum then the iterative process will converge to the minimum, and that the projection of the geometric average (what you get if you set c = 0 initially) will be almost always close to the minimum. I don’t have proof on this, but a few computations suggests that the approach can work well in practice.\nTo see a concrete implementation of this in code, have a look at the post calculating centroid on a sphere."
  },
  {
    "objectID": "sentencetransformers-to-tensorflow/index.html",
    "href": "sentencetransformers-to-tensorflow/index.html",
    "title": "Converting SentenceTransformers to Tensorflow",
    "section": "",
    "text": "SentenceTransformers provides a convenient interface for creating embeddings of text (and images) in PyTorch, which can be used for neural retrieval and ranking. But what if you want to integrate a model trained in SentenceTransformers with other existing models in Tensorflow? The best solution would be to rewrite the training in Tensorflow, but if you’ve already spent a lot of time training a model you may want to import it into Tensorflow. This post will show you how.\nThis post was generated with a Jupyter notebook which you can download if you want to run it yourself.\nUpdate 2022-08-31: Philipp Schmid has just written a good guide to Use Sentence Transformers with TensorFlow. His article nicely shows how to load a pretrained model using the class interface with Keras, including wrapping the tokenizer in the model. This article shows how to use the Keras functional API, which enables saving and loading the model with Keras, and converting a custom trained dense head."
  },
  {
    "objectID": "sentencetransformers-to-tensorflow/index.html#training-a-sentencetransformers-model",
    "href": "sentencetransformers-to-tensorflow/index.html#training-a-sentencetransformers-model",
    "title": "Converting SentenceTransformers to Tensorflow",
    "section": "Training a SentenceTransformers Model",
    "text": "Training a SentenceTransformers Model\nLet’s start with an example model from the SentenceTransformers Training Examples a bi-encoder consisting of a Transformer embedding, followed by mean pooling, and a single dense layer. The transformer model_name can be almost any model from HuggingFace, but for this example we’ll use one of the smaller pre-trained SentenceTransformers models tuned to sentence embeddings, all-MiniLM-L6-v2.\nfrom sentence_transformers import SentenceTransformer, models\nfrom torch import nn\n\nmodel_name = 'sentence-transformers/all-MiniLM-L6-v2'\nmax_seq_length = 512\noutput_dimension = 256\n\nword_embedding_model = models.Transformer(model_name,\n                                          max_seq_length=max_seq_length)\n\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_cls_token=False,\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_max_tokens=False)\n\ndense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(),\n                           out_features=output_dimension,\n                           activation_function=nn.Tanh())\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n\n(word_embedding_model.get_word_embedding_dimension(),\n pooling_model.get_sentence_embedding_dimension(),\n output_dimension)\n(384, 384, 256)\nNext we would finetune this model using our own data. Here we’ll just use some dummy sample data and put it into InputExample. If we had more data than fit in memory we could use memory mapping with PyArrow.\nfrom sentence_transformers import InputExample\nfrom torch.utils.data import DataLoader\n\ntrain_examples = [\n    InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)\n]\n\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\nNow we can train the model with one of the SentenceTransformer losses. Using use_amp (Automatic Mixed Precision) means we’ll get faster throughput and use less GPU memory on a GPU that supports it.\nfrom sentence_transformers import losses\n\n\nnum_epochs = 3\ntrain_loss = losses.CosineSimilarityLoss(model)\n\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          epochs=num_epochs,\n          warmup_steps=int(len(train_examples) * num_epochs * 0.1),\n          use_amp=True,\n          show_progress_bar=False)\nNow we’ll save the model to import into Tensorflow; in this example we’ll just use a temporary directory.\nfrom tempfile import TemporaryDirectory\n\noutput_dir = TemporaryDirectory()\n\nmodel.save(output_dir.name)"
  },
  {
    "objectID": "sentencetransformers-to-tensorflow/index.html#converting-to-tensorflow",
    "href": "sentencetransformers-to-tensorflow/index.html#converting-to-tensorflow",
    "title": "Converting SentenceTransformers to Tensorflow",
    "section": "Converting to Tensorflow",
    "text": "Converting to Tensorflow\nThis kind of model can be converted into a Keras model in the following steps:\n\nUse Huggingface Transformers to load the model into Tensorflow using TFAutoModel\nPass the tokenized input and extract the hidden state\nMean Pool the Hidden State\nPass the output through the dense layer\n\nimport sentence_transformers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel\n\ndef sentencetransformer_to_tensorflow(model_path: str) -> tf.keras.Model:\n    \"\"\"Convert SentenceTransformer model at model_path to TensorFlow Keras model\"\"\"\n    # 1. Load the Transformer model\n    tf_model = TFAutoModel.from_pretrained(model_path, from_pt=True)\n\n    input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32)\n    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32)\n\n    token_type_ids = tf.keras.Input(shape=(None,), dtype=tf.int32)\n\n    # 2. Get the Hidden State\n    hidden_state = tf_model.bert(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n    ).last_hidden_state\n\n    # 3. Mean pooling\n    mean_pool = tf.keras.layers.GlobalAveragePooling1D()(\n        hidden_state\n    )\n\n    # 4. Dense layer\n    sentence_transformer_model = SentenceTransformer(model_path, device=\"cpu\")\n    dense_layer = sentence_transformer_model[-1]\n    dense = pytorch_to_tensorflow_dense_layer(dense_model)(mean_pool)\n\n    # Return the model\n    model = tf.keras.Model(\n        dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        ),\n        dense,\n    )\n\n    return model\nWe can convert the Dense model into Tensorflow with a simple mapping of the weights.\nTORCH_TO_KERAS_ACTIVATION = {\"torch.nn.modules.activation.Tanh\": \"tanh\"}\n\ndef pytorch_to_tensorflow_dense_layer(dense_model: sentence_transformers.models.Dense) -> tf.keras.layers.Dense:\n    weight = dense_model.linear.get_parameter(\"weight\").cpu().detach().numpy().T\n    bias = dense_model.linear.get_parameter(\"bias\").cpu().detach().numpy()\n\n    dense_config = dense_model.get_config_dict()\n\n    return tf.keras.layers.Dense(\n        dense_config[\"out_features\"],\n        input_shape=(dense_config[\"in_features\"],),\n        activation=TORCH_TO_KERAS_ACTIVATION[dense_config[\"activation_function\"]],\n        use_bias=dense_config[\"bias\"],\n        weights=[weight, bias],\n    )\nThen we can load our Tensorflow model from the output directory.\ntf_model = sentencetransformer_to_tensorflow(output_dir.name)\n\ntf_model\n<keras.engine.functional.Functional at 0x7f9105a35eb0>\nTo test it’s the same we can check it on a small sample of input text.\ntokenizer = AutoTokenizer.from_pretrained(output_dir.name)\n\ninput_text = ['SentenceTransformers are groovy']\n\ntf_tokens = tokenizer(input_text, return_tensors='tf')\n\n\nimport numpy as np\nassert np.isclose(tf_model(tf_tokens).numpy(),\n                  model.encode(input_text),\n                  atol=1e-5).all()\nThe rest of this article goes through how this translation works step-by-step, which could be useful if you wanted to expand this to different SentenceTransformer models.\n\nChecking we can save and load the model\nA final check is that we can save and load the model to get the same result.\nYou may have noticed in the code above for step 2 we called tf_model.bert, rather than just tf_model. This is requried to save a TFBertModel. If you’re not using something bert based you may need to use a different method (such as .transformer)\nwith TemporaryDirectory() as tf_output_dir:\n    tf_model.save(tf_output_dir)\n    tf_model_2 = tf.keras.models.load_model(tf_output_dir)\n\nassert np.isclose(tf_model_2(tf_tokens).numpy(),\n                  model.encode(input_text),\n                  atol=1e-5).all()\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\nWARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 545). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: /tmp/tmp416di2x1/assets\n\n\nINFO:tensorflow:Assets written to: /tmp/tmp416di2x1/assets\n\n\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n\n\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually."
  },
  {
    "objectID": "sentencetransformers-to-tensorflow/index.html#understanding-the-sentencetransformers-model",
    "href": "sentencetransformers-to-tensorflow/index.html#understanding-the-sentencetransformers-model",
    "title": "Converting SentenceTransformers to Tensorflow",
    "section": "Understanding the SentenceTransformers Model",
    "text": "Understanding the SentenceTransformers Model\nLet’s start by understanding this particular SentenceTransformer model. It’s made of 3 layers, the Transformer to embed the text, the pooling layer, and a dense layer.\nmodel\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel\n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 384, 'out_features': 256, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\nWe can embed a text using the encode function\ninput_text = ['SentenceTransformers are groovy']\n\nembedding = model.encode(input_text)\n\nprint(embedding.shape)\n\nembedding\n(1, 256)\n\n\n\n\n\narray([[-0.23693885, -0.00868655,  0.12708516, ...,  0.02389161,\n        -0.03000948, -0.20219219]], dtype=float32)\nWe can build this up by going through each layer individually. First we need to tokenize the input:\ntokens = model.tokenize(input_text)\ntokens\n{'input_ids': tensor([[  101,  6251,  6494,  3619, 14192,  2545,  2024, 24665,  9541, 10736,\n            102]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\nThe input_ids correspond to the tokens of the input text; for this single text we can ignore the token_type_ids and attention_mask.\nmodel.tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n['[CLS]',\n 'sentence',\n '##tra',\n '##ns',\n '##form',\n '##ers',\n 'are',\n 'gr',\n '##oo',\n '##vy',\n '[SEP]']\nWe then pass it through the Transformer Layer which gives us a tensor of: batch_size * seq_length * hidden_dimension\nmodel[0]\nTransformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel\ntransformer_embedding = model[0]({k:v.to(model.device) for k, v in tokens.items()})\n\nprint(transformer_embedding.keys())\n\ntransformer_embedding_array = transformer_embedding['token_embeddings'].detach().cpu().numpy()\n\nprint(transformer_embedding_array.shape)\n\ntransformer_embedding_array\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings'])\n(1, 11, 384)\n\n\n\n\n\narray([[[-0.10578819, -0.24982804, -0.25226122, ...,  0.07380735,\n         -0.16002229, -0.10104472],\n        [ 0.08560147, -0.56386817,  0.07879569, ...,  0.53842884,\n          0.8103698 , -1.3633531 ],\n        [-0.49132693, -0.2222665 , -0.6009016 , ..., -0.6793231 ,\n          0.02079807,  0.19803165],\n        ...,\n        [-0.42034534, -0.35809648, -0.40514907, ...,  0.18629718,\n          0.44449466,  0.21497107],\n        [ 0.06977252, -0.3880473 , -0.6704632 , ...,  0.1784373 ,\n          0.6243761 , -0.39589474],\n        [-0.2825999 ,  0.20663676,  0.31645843, ...,  0.6711646 ,\n          0.23843716,  0.08616418]]], dtype=float32)\nWe then use the pooling layer to remove the sequence dimension\nmodel[1]\nPooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\npooled = model[1](transformer_embedding)\n\nprint(pooled.keys())\n\npooled_array = pooled['sentence_embedding'].cpu().detach().numpy()\n\nprint(pooled_array.shape)\n\npooled_array\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings', 'sentence_embedding'])\n(1, 384)\n\n\n\n\n\narray([[-0.3000789 , -0.20643215, -0.21641909, ...,  0.1395422 ,\n         0.2712665 , -0.10013962]], dtype=float32)\nThen finally we pass it through the dense layer to get the final result.\nNote that the result overwrites the sentence_embedding key in the dictionary\nmodel[2]\nDense({'in_features': 384, 'out_features': 256, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\ndense_output = model[2](pooled)\n\nprint(dense_output.keys())\n\ndense_output_array = dense_output['sentence_embedding'].cpu().detach().numpy()\n\nprint(dense_output_array.shape)\ndense_output_array\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings', 'sentence_embedding'])\n(1, 256)\n\n\n\n\n\narray([[-0.23693885, -0.00868655,  0.12708516, ...,  0.02389161,\n        -0.03000948, -0.20219219]], dtype=float32)\nThis is the same as we got when we called encode\nassert (model.encode(input_text) == dense_output_array).all()"
  },
  {
    "objectID": "sentencetransformers-to-tensorflow/index.html#importing-the-model-into-tensorflow",
    "href": "sentencetransformers-to-tensorflow/index.html#importing-the-model-into-tensorflow",
    "title": "Converting SentenceTransformers to Tensorflow",
    "section": "Importing the model into Tensorflow",
    "text": "Importing the model into Tensorflow\nWe can load the Tokenizer and Tensorflow model using transformers. Transformers converts the model weights from PyTorch to Transformers when we pass from_pt=True (and this may not work for some exotic architectures).\nfrom transformers import TFAutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir.name)\n\ntf_model = TFAutoModel.from_pretrained(output_dir.name, from_pt=True)\n\ndel output_dir\nWe can use the Tokenizer to produce tokens as Tensorflow Tensors:\ntf_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='tf')\ntf_tokens\n{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[  101,  6251,  6494, ...,  9541, 10736,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\ntf_embedding = tf_model(tf_tokens)\n\nprint(tf_embedding.keys())\nodict_keys(['last_hidden_state', 'pooler_output'])\nThe last_hidden_state is, up to floating point precision error, the same as the output of the first Transformer layer.\nassert np.isclose(tf_embedding.last_hidden_state.numpy(), transformer_embedding_array, atol=1e-5).all()\nHowever the pooler_output is completely different\nnp.abs(tf_embedding.pooler_output.numpy() - pooled_array).max()\n0.8287506\nBut we can produce a mean pooling manually.\nIf we used a different combination of pooling layers in SentenceTransformers (such as CLS or Max pooling) we would have to change this.\nimport tensorflow as tf\n\ntf_pooled = tf.keras.layers.GlobalAveragePooling1D()(tf_embedding.last_hidden_state)\n\nassert np.isclose(tf_pooled.numpy(), pooled_array, atol=1e-5).all()\nFinally we need to load the dense model into Tensorflow.\nWe can extract the weight, bias and configuration from the PyTorch model.\ndense_model = model[2]\n\nweight = dense_model.linear.get_parameter(\"weight\").cpu().detach().numpy().T\nbias = dense_model.linear.get_parameter(\"bias\").cpu().detach().numpy()\n\ndense_config = dense_model.get_config_dict()\n\nprint(weight.shape, bias.shape)\ndense_config\n(384, 256) (256,)\n\n\n\n\n\n{'in_features': 384,\n 'out_features': 256,\n 'bias': True,\n 'activation_function': 'torch.nn.modules.activation.Tanh'}\nThen we can use this to create a corresponding dense layer in Keras. If we had more dense layers, or used a differente activation, we’d need to update accordingly.\nTORCH_TO_KERAS_ACTIVATION = {\"torch.nn.modules.activation.Tanh\": \"tanh\"}\n\ntf_dense = tf.keras.layers.Dense(\n        dense_config[\"out_features\"],\n        input_shape=(dense_config[\"in_features\"],),\n        activation=TORCH_TO_KERAS_ACTIVATION[dense_config[\"activation_function\"]],\n        use_bias=dense_config[\"bias\"],\n        weights=[weight, bias],\n    )\nThen passing the output through this layer gives us the same result as the original model.\ntf_output = tf_dense(tf_pooled)\n\nassert np.isclose(tf_output.numpy(), model.encode(input_text), atol=1e-5).all()\nThis gets us the same result as our function previous, once we wrap it in Keras functional API.\nWe could in fact train this model, which if we have to use Tensorflow makes more sense than training the SentenceTransformer model. But SentenceTransformers provides quite a conventient interface to prototype different losses and pooling layers that it may still be useful to use it before converting everything to Tensorflow."
  },
  {
    "objectID": "excel-completion-count/index.html",
    "href": "excel-completion-count/index.html",
    "title": "Excel Completion Count",
    "section": "",
    "text": "We had a blank column that was being filled in as the annotation progressed, and each person was working on some set of rows. To see progress I ended up using a formula like this:\n=AVERAGE(IF(OFFSET(annotation, \n                   [@[Start Row]]-1,                 0,\n                   [@[End Row]] - [@[Start Row]] +1, 1\n                   ) = \"\", \n            0, 1))\nWhere annotation is the first cell of the column being annotated, and Start Row and End Row refer to the row numbers that are to be filled in.\n\n\n\nExample of progress\n\n\nThe way it works is straightforward; suppose Start Row is 2 and End Row is 5\n\nannotation is at the top of the column, say B1\nOFFSET(annotation, [Start Row] - 1, 0, ...) gets the range starting at row 1 + (2-1), that is B2\nOFFSET(annotation, ..., [End Row] - [Start Row] + 1, 1) gets the rectangle of length (5-2) + 1 = 4 and width 1 (so contains 4 cells, B2, B3, B4 and B5).\nIF(OFFSET(...) = \"\", 0, 1) then turns this into a vector where we get 0 if empty (no annotation) and 1 if filled (some annotation)\nAVERAGE(IF(...)) then gives the proportion of cells filled, as a percentage between 0% (no annotation) and 100% (all filled)\n\nYou can even get cute and put a data bar in that fills as annotation progresses. This simple trick helped me feel like I was getting somewhere with the annotation I was doing, and helped the team work together to get it completed in time."
  },
  {
    "objectID": "python-type-stubs/index.html",
    "href": "python-type-stubs/index.html",
    "title": "Typechecking with a Python Library That Has No Type Hints",
    "section": "",
    "text": "When you see this error it’s worth first checking that there aren’t any types already available. Sometimes a later version of the library may have type annotations, so upgrading may make the message go away. There are third party type repositories, like data-science-types which is very helpful if you’re using libraries like matplotlib, numpy and pandas. Other times someone has already written a type stub that’s not in a package, like in the tqdm issue for a type-stub (and you can put this in tqdm.pyi in the root of your package directory).\nIf no one else has done the work for you then you have to do it yourself. Mypy has some documentation on stub files and PEP561 outlines the details of how to do it, but they’re a bit heavy to read so I’ll work through a simple example.\nI was trying to typecheck a library with mypy that used warcio. When I ran mypy I got the following errors:\n01_fetch_data.py:10: error: Skipping analyzing 'warcio.archiveiterator': found module but no type hints or library stubs\nAt the time of writing warcio doesn’t have type annotations, so we have to write our own stubs. We can start with creating a blank stub; because it’s a module we need to do it in the warcio direcory and have a __init__.pyi file (otherwise mypy won’t find them).\nmkdir warcio\ntouch warcio/archiveiterator.pyi\ntouch warcio/__init__.pyi\nWe then get a more specific error about a missing module:\n01_fetch_data.py:10: error: Module 'warcio.archiveiterator' has no attribute 'ArchiveIterator'\nWe don’t need to write stubs for everything in the module for it to be typechecked; just the things we use. To get an idea of what to do it’s helpful to look at example stubs that come with mypy in typeshed, and check the source code you’re stubbing. In this case we’re using ArchiveIterator and all I was using was initialising it (the __init__ function) and iterating over it (the __iter__ and __next__ function). So I just took these methods from the source of warcio.archiveiterator and replaced the bodies with ...:\nimport six\n\nclass ArchiveIterator(six.Iterator):\n    def __init__(self, fileobj, no_record_parse=False,\n                 verify_http=False, arc2warc=False,\n                 ensure_http_headers=False, block_size=BUFF_SIZE,\n                 check_digests=False): ...\n\n    def __iter__(self): ...\n\n    def __next__(self): ...\nRunning mypy now gets an error about Name 'BUFF_SIZE' is not defined. In the original code it’s imported as from warcio.utils import BUFF_SIZE; but if I do that I’ll have to stub warcio.utils too. I can just cheat and add a type declaration at the top of the file.\nimport six\n\nBUFF_SIZE: int\n\nclass ArchiveIterator(six.Iterator):\n    def __init__(self, fileobj, no_record_parse=False,\n                 verify_http=False, arc2warc=False,\n                 ensure_http_headers=False, block_size=BUFF_SIZE,\n                 check_digests=False): ...\n\n    def __next__(self): ...\n\n    def __iter__(self): ...\nSurprisingly this is enough to make mypy happy. However we haven’t told it much about the types; so there’s not much it can actually check. For example if I pass the string ‘Yes please’ to no_record_parse mypy will say everything is good.\nIt’s fairly easy to infer the types for __init__. The tricky one is fileobj; a quick StackOverflow search gives the types for filelike objects. Normally you pass gzipped files in byte mode to ArchiveIterator so it can take at least typing.BinaryIO, but I don’t know whether it can take text files too and could be typing.IO. Generally it’s safer to pick the more restrictive type, and then if we get a type error broaden the type as necessary. The rest of the types are clear from the default values and the method has no return in the source.\nI know from library interface that the iterator yields ArcWarcRecords, and so the remaining methods are easy to annotate types too. Typing a Generator is a bit complicated, it takes 3 types, but when you don’t send to the generator or return from it the last two arguments are None. All together this is typed now.\nimport six\nfrom typing import BinaryIO, Generator\n\nfrom warcio.recordloader import ArcWarcRecord\n\nBUFF_SIZE: int\n\nclass ArchiveIterator(six.Iterator):\n    def __init__(self, fileobj:BinaryIO, no_record_parse:bool=False,\n                 verify_http:bool=False, arc2warc:bool=False,\n                 ensure_http_headers:bool=False, block_size:int=BUFF_SIZE,\n                 check_digests:bool=False) -> None: ...\n\n    def __next__(self) -> ArcWarcRecord: ...\n\n    def __iter__(self) -> Generator[ArcWarcRecord, None, None]: ...\nHowever now I get a message from mypy that I haven’t typed warcio.recordloader yet; I need to add annotations for ArcWarcRecord.\nerror: Skipping analyzing 'warcio.recordloader': found module but no type hints or library stubs\nSo I can easily create that stub in warcio/recordloader.pyi and add an ArcWarcRecord class to get more specific issues about the methods I use.\n\"ArcWarcRecord\" has no attribute \"content_stream\"\n\"ArcWarcRecord\" has no attribute \"rec_headers\"\n\"ArcWarcRecord\" has no attribute \"rec_headers\"\nIn this way you only need to stub the methods you actually use, and the task is manageable. Trying to annotate the whole warcio codebase is quite a task, although this approach gets you a long way.\nAt the end with a little work annotating the library you get some notion of type safety. Of course if you get the types wrong in the stub, particularly the return types, it may pass typecheck but still be wrong. Another gradually typed language, Typed Racket, installs contracts at the boundaries between typed and untyped code, actually asserting the types are correct. This could be a way to get assurance you’re picking up wrong types early.\nTo get an idea of how type checking can help in this same codebase I was calculating the end byte given an offset and a length; end_byte = start_byte + length. This was then formatted into a string f\"{start_byte}-{end_byte}\". Unfortunately start_byte and length were strings, so the code ran fine but it concatenated the strings rather than adding their numbers. A type check would have caught this early and clearly pointed out the problem, rather than leaving me scratching my head for a while working out what happened."
  },
  {
    "objectID": "thurston-collaboration/index.html",
    "href": "thurston-collaboration/index.html",
    "title": "Lessons from a mathematician on building a community",
    "section": "",
    "text": "The best way to get people to contribute is to make it easy for them to contribute. As a contributor it can be tempting to complete a product to a nice finish, but this makes it hard for other people to get involved. Instead if you leave obvious gaps for other people to fill it helps them get involved, understand the work and feel ownership. Some successful open source projects do this by tagging issues as “good first issue”, and leaving them for other people to tackle.\nThe mathematician William Thurston wrote about his experience of “completing” in his excellent article On proof and progress in mathematics:\n\nAt that time, foliations had become a big center of attention among geometric topologists, dynamical systems people, and differential geometers. I fairly rapidly proved some dramatic theorems… I wrote respectable papers and published at least the most important theorems.\nAn interesting phenomenon occurred. Within a couple of years, a dramatic evacuation of the field started to take place. I heard from a number of mathematicians that they were giving or receiving advice not to go into foliations - they were saying that Thurston was cleaning it out. People told me (not as a complaint, but as a compliment) that I was killing the field. Graduate students stopped studying foliations, and fairly soon, I turned to other interests as well.\nI do not think that the evacuation occurred because the territory was intellectually exhausted - there were (and still are) many interesting questions that remained that are probably approachable. Since those years, there have been interesting developments carried out by the few people who stayed in the field or who entered the field, and there have also been important developments in neighbouring areas that I think would have been much accelerated had mathematicians continued to pursue foliation theory vigorously.\nToday, I think there are few mathematicians who understand anything approaching the state of the art of foliations as it lived at that time, although there are some parts of the theory of foliations, including developments since that time, that are still thriving.\n\nHe goes on to speculate that the way he wrote his papers, in a typical dense mathematical style, making references to many different fields created a high entry barrier. This made it hard for new graduates to enter the field to keep it grow. The other major problem is by proving the big theorems he didn’t leave room for other people to enter the field and start building an understanding.\n\nMore than the knowledge, people want personal understanding. And in our credit-driven system, they also want and need theorem-credits.\n\nThis can equally apply to developing software. Most of the time I find reading new software painful; you have to build a mental model of how everything fits together and the individual pieces work. It’s even harder if it’s in a language or uses libraries that I’m not familiar with. So if some code does most things I want without any issues I’ll happily use it without looking at the source code to see how it works, and I can always build on top of it if I need to. This is similar to the mathematicians who could happily use and accept Thurston’s proofs without understanding them or contributing to the field.\nThis is why it’s very impressive when there’s a thriving open source project that requires specific technical expertise or diverse programming languages like Apache Arrow - I have no idea how they do it!\nThurston also talks about how he learned from his experience on foliations, and when he worked on 3-manifolds he took a different approach. He spent a number of years understanding the field before he proved a major result and made a conjecture. He taught the underlying ideas to his graduate class, and started providing the notes to a mailing list that grew to 1200 people (which is very large in mathematics!). He presented seminars and workshops, helping the community understand the “infrastructure” in the proof that spanned different areas of mathematics. This then helped grow the field by giving the tools and space for other people to make in-roads into the field and publish their own papers (and eventually led to solving a 100 year old problem).\n\nThere has been and there continues to be a great deal of thriving mathematical activity. By concentrating on building the infrastructure and explaining and publishing definitions and ways of thinking but being slow in stating or in publishing proofs of all the “theorems” I knew how to prove, I left room for many other people to pick up credit.\n\nI’ve had a smaller experience in using these ideas in building software when I was working on custom reporting. We had some PHP scripts that could build the simplest reports, but for most reports we would need to hand-roll them in Python because of slightly different requirements (the PHP scripts weren’t modular enough to build libraries on; they were very brittle). This was a lot of copy-paste-edit, but meant each reporting script required separate QA, and once in a while they would fail on a corner case that was solved in a different variation of the script.\nA very talented colleague rewrote one of the reporting libraries in Python. It covered the use case very well, but was quite complex, building up a large SQL query through several code paths depending on the use case. This meant it was difficult to extend; it took me a few iterations because it was difficult to keep the queries and their interactions in my head.\nI tried to build a single library that would solve all the use cases (which would have looked like a half-baked dbplyr if it had worked), but after a couple weeks of work got very stuck - it was beyond my skill level.\nA month or so later when I had a complex report I built a library to tackle the simplest (and most frequently used) report. It used some of the ideas from the library that had already been rewritten, but was much simpler and tended to do more in Python (which was more flexible) than SQL (which was faster to execute). I made sure it could be imported as a module to use in bespoke reporting code, as well as having a CLI interface for simple reports, and would cover the use case completely. Being able to import the script in Python made it easy to build complex reports around and it quickly got adoption in the team.\nWhile it was clear that building more of these reporting scripts would be beneficial, it was always hard to justify the upfront cost of writing it rather than working on actual reports. Because it had to be robust it often took much longer than individual reports where you could skirt around the edge cases and only deal with one way of reporting. There were also ongoing maintenance costs which made it harder to write more code, when I only had the existing reports.\nI really needed more help from my colleagues, but I was the only person who was maintaining and extending the codebase. So when I built the next library I took a leaf out of Thurston’s book; I thought hard about what needed to be done, but I didn’t do all of it. In particular the library would not work on some well documented cases, but that were relatively easy to add. This meant it was useful enough to use in some reporting scripts, but for the more complex ones you would have to write some custom queries.\nIt took a few months (and some gentle encouragement), but eventually one of my colleagues had this kind of complex report and extended the library to cover this use case. It was a small enough piece of work to do in a couple of days, and I could help guide him on what needed to be done and review the fix.\nThis kind of cycle continued and the person who extended this library went on to write a couple of others. Eventually most of the team contributed to this growing codebase, and we covered almost all the common cases. Some other great ideas were introduced; someone introduced the team to Pandas and putting it on top of the libraries meant we could write more complex reports more quickly.\nReducing the reporting workload also gave us more “blue sky” time to innovate, including building new profitable products and a self-serve reporting interface, which saved even more time.\nLooking back, it was really hard to get anyone else to work on the library I had started on. However by having some obvious gaps that were simple to fill led someone to make the first step and get familiar with the codebase and have some ownership. This let us build out more usecases and help get more of the team involved, until most of the team was able to contribute and maintain it. We were fortunate in that once we had a clear frame for the API it was clear what needed to be done, and it could be immediately used in our client work.\nIf you’re trying to build a substantial piece of software it’s a lot easier with a community. Take on Thurston’s lessons, start building something useful, spend a lot of effort educating about it, leave obvious gaps for contributions and do everything you can to support those contributions. It takes a long time to build understanding of software, but if you can get people to invest in contributing they will be incentivised to slowly build that understanding and contribute more.\nI’d be really interested in understanding more how people who are really good at building communities like Hadley Wickham, Wes McKinney and Linus Torvalds (to name a few large open source project maintainers) do it."
  },
  {
    "objectID": "constant-models/index.html",
    "href": "constant-models/index.html",
    "title": "Constant Models",
    "section": "",
    "text": "When predicting outcomes using machine learning it’s always useful to have a baseline to compare results against. A simple baseline is the best constant model; that is a model that gives the same prediction for any input. This is a really simple check to perform against any dataset, and can be informative to check across validation splits.\nThere are simple algorithms for finding the best constant model. For categorical predictions just evaluate every possible category to choose as the constant prediction. For continuous predictions with a convex loss function you can use bisection, starting with the smallest and largest values of the predictor. For common loss functions the best constant model is a familiar measure of central tendency.\nThis is useful to know when using piecewise constant models (like decision trees) because on each piece they will use these best constants. The rest of this article will explain these examples in detail and end with a general family of loss functions that covers many use cases."
  },
  {
    "objectID": "constant-models/index.html#mean-weighted-absolute-error",
    "href": "constant-models/index.html#mean-weighted-absolute-error",
    "title": "Constant Models",
    "section": "Mean Weighted Absolute Error",
    "text": "Mean Weighted Absolute Error\nSometimes in prediction tasks it’s better to conservatively over- or under-predict to temper expectations. One way to do this is to penalise errors in one direction greater than errors in the other. The Mean Weighted Average Error applies this to the MAE:\nmwae(w, a) = w * (x >= a) * (x - a) - (1 - w) * (x < a) * (x - a)\n\\[\\rm{MWAE(a; w)} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} \\begin{cases}  w (x_i - a) & x_i \\geq a  \\\\ (1-w) (a - x_i) & x_i \\lt a  \\end{cases}\\]\nThe constant model that minimises MWAE is the wth quantile. To understand why, as with MAE, consider how the MWAE changes as the prediction moves a distance ε towards m points away from n points.\n\n\n\nMean Absolute Error Diagram\n\n\nThen the prediction is ε further from n points which increases MWAE by w * n * ε / (m + n) and ε further from m points which decreases MWAE by (1 - w) * m * ε / (m + n). This will be minimised when the two changes balance; when we can’t decrease it by moving in one particular direction (because MWAE is convex). This happens when w * n == (1 - w) * m, that is when w == m / (m + n). So it’s minimised when the fraction of data to the left of the test point is w. In particular when w = 0.5 then MWAE == MAE / 2 and the best constant model is the median.\nNote that again this is ambiguous; we can treat it the same way as the median by averaging the minimiser of quantiles taken from limits above and below."
  },
  {
    "objectID": "power-of-easy/index.html",
    "href": "power-of-easy/index.html",
    "title": "Power of Easy",
    "section": "",
    "text": "In the book Nudge, Thaler and Sunstein talk about how small changes to defaults impact major decisions like whether they donate their organs and how they save for retirement. Whenever you’re designing something make it as easy as possible for people to do the desired thing; and make sure it’s easy from their perspective - where they don’t care about the product they’re using but the task they are trying to achieve.\nPeople will often work around obstructions to their action in the easiest way possible. The EU-mandated Cookie banners are worthless because they colour the “accept” banner in a highly salient colour and make it easy to press, while obstructing the task you’re trying to perform (viewing the website). Back in the days of installing software to your computer this used to be abused by bundling a bunch of adware in with the default selection being “install it” as most people click through as fast as possible; it’s still true for End User Licence Agreements.\nBut when the friction becomes too much people will just avoid doing the task. Simple things like a credit card form complaining about spaces can be frustrating enough to stop people. When designing security it’s worth thinking about how easy it will be for people to do this; making it hard for people to share files the “secure” way inevitably leads to people finding egregious workarounds like uploading to external third party services.\nI find AWS very hard to use. The web interface is terrible, the command line interface is too low level to be usable and the APIs are very low level too. Security tends to be complex and easy to get wrong. I’m pretty sure everywhere using AWS writes their own tooling to make it easier to use, which seems crazy given the number of companies using AWS. Thankfully some people have released some usable wrappers like fastec2.\nI think awareness of easy is growing but a lot of software engineers still underestimate how powerful easy is. Making it easy to get started for people trying to achieve a task is very powerful. These can quickly get you into good habits.\nA good trick is to write simple command line interfaces or web pages for internal APIs or perform common functions. It’s very easy to do quickly and can quickly build up a capability.\nIf you want to do more of something, make it a little easier to do. If you want to do less of something, make it a little harder to do. It’s remarkably effective; much more than it should be."
  },
  {
    "objectID": "real-roots-of-polynomials/index.html",
    "href": "real-roots-of-polynomials/index.html",
    "title": "Algorithms for finding the real roots of polynomials",
    "section": "",
    "text": "It uses Descartes’ rule of signs: given a polynomial \\(p(x) = a_n x^n + \\cdots + a_1 x + a_0\\) the number of real positive roots (counting multiplicites) is bounded above by the number of sign variations in the sequence \\((a_n, \\ldots, a_1, a_0)\\) .\n\nSo as an example \\(p(x) = x^2 - 1\\) has a sequence of coefficients \\((1, 0, -1)\\) which contains 1 sign change (we ignore zeros), and so has at most one positive root; in fact we know it has exactly one positive root 1. On the other hand the bound is necessary: \\(p(x) = (x-(1+i))(x-(1-i))= x^2-2x+2\\) has 2 sign changes, but no positive real roots.\nDescartes’ theorem tells us about the number of zeros of a degree n polynomial \\(p(x)\\) on the open interval \\((0, \\infty)\\) ; what if we wanted to know about the number of zeros on some other interval \\((u, v)\\) ? We could perform a projective transformation \\(f(x) = \\frac{a x + b}{c x + d}\\) ( \\(ad - bc \\neq 0\\) ); in order to still have a polynomial we need to multiply out the denominator to get \\(q(x) = (cx+d)^{n} p(\\frac{ax +b}{cx+d})\\) . The positive zeros of q(x) are the positive zeros of \\((cx+d)^n\\) and the zeros of p(x) between \\(-\\frac{b}{a}\\) and \\(-\\frac{d}{c}\\) . In particular if we choose a=u, b=v, c=1, d=1 the number of positive zeros of q(x) is precisely the number of zeros of p(x) in the interval \\((u, v)\\) .\nBy the rule of signs if q(x) has zero sign variations then p(x) has no root in \\((u, v)\\) . This leads to our iterative bisection strategy for finding the zeros of a polynomial on an interval. Given a sequence of intervals bisect each interval and find the sign variations of the polynomial projected on each subinterval; if it is zero then discard it, otherwise add it to the next sequence of intervals. This yields a sequence of intervals which may contain zeros of p. However we don’t know which intervals contain no zeros or multiple zeros.\nConsider the case of one sign variation; for sufficiently small x, p(x) will have the sign of the terms at the end of the sequence (towards the constant term), and for sufficiently large x, p(x) will have the sign of the leading term. Consequently, since these signs are opposite, by the intermediate value theorem there exists a positive real x such that p(x) is zero. By Descartes’ rule of signs there is at most one real zero; hence there must be exactly one real zero.\nHence we can adjust our algorithm; if there is one sign variation then we add it to a list of definite zeros. However we’re still not sure that the intervals not containing zero will be eliminated; we need a sort-of converse to Descartes’ theorem.\nThis converse is given by a pair of theorems due to Obreshkoff: Given a degree n polynomial p(x)\n\nIf p(x) has least p complex zeros with arguments in the range \\(- \\frac{\\pi}{n + 2 - p} < \\phi < \\frac{\\pi}{n+2-p}\\) then the number of sign variations is bounded above by p.\nIf p(x) has at least n-q complex zeros with arguments in range \\(\\pi - \\frac{\\pi}{q + 2} \\leq \\phi \\leq \\pi + \\frac{\\pi}{q + 2}\\) then the number of sign variations is bounded below by q.\n\nWhen we translate this by a projective transformation we get a picture like this (taken from Arno Eigenwillig’s thesis)\n\n\n\nObreshkoff Arc for n=8 and p=q=2\n\n\nIf p(x) has at least p roots in \\(OL_{\\geq}\\) above, then the transformed sign variations are bounded above by p. If p(x) has at most q roots in \\(OL_{\\leq}\\) above then the transformed sign variations are bounded below by q.\nEssentially the sign variations can only see zeros “nearby” within these arcs. Since these arcs get smaller as the interval gets smaller it is guaranteed that for sufficiently small intervals (depending on the distance between the roots of the polynomial) the number of sign variations will equal the number of roots.\nIn particular if all the real roots are simple then the bisection process above will eventually terminate; all intervals will eventually have zero sign variations (in which case there are no roots) or one sign variation (in which case they contain a root).\nHence we have an algorithm for isolating the distinct real roots of a polynomial p(x) over the integers on a bounded interval I.\n\nRemove all multiple roots by dividing p(x) by the greatest common divisor of p and its derivative.\nThe last interval list is {I}, the next interval list is {} and the roots are {}\nRemove each interval from the last interval list, bisect it then add it to the last interval list.\nFor each interval in the last interval list calculate the sign variations: if it’s 0 discard it, if it’s 1 add it to the roots, otherwise add it to the next interval list\nIf the next interval list is empty return the roots, otherwise set the last interval list to the next interval list, the next interval list to {} and goto 3.\n\nIt’s worth noting that transforming the polynomial can be done just with the operations multiply by two, divide by two and add (see On the Various Bisection Methods Derived From Vincents Theorem).\nWhy just the integers? Polynomials over the rationals can be solved by the same method, by first factoring out the denominators. The real numbers are much more subtle: we can’t calculate the gcd, and worse we can’t even necessarily calculate the sign of a coefficient! (I mean in an algorithmic manner; c.f. Richardson’s Theorem).\nOne excellent thing about this is the intervals are guaranteed to contain exactly one root; we can then use something like the bisection method to find the zeros to any desired accuracy.\nI haven’t been sufficiently precise with my algorithm to analyse it, but there are implementations that use \\(O(n^6t^2log^2_nn)\\) binary operations on polynomials of degree n on integer t bit coefficients.\nThere are, of course, other methods of finding all the roots of a real polynomial; but few of them are global and stable like this one. (Though a variation of Newton’s method isn’t a bad candidate, albeit without precise bounds)."
  },
  {
    "objectID": "finding-open-datasets/index.html",
    "href": "finding-open-datasets/index.html",
    "title": "Finding Open Datasets",
    "section": "",
    "text": "I’m looking for a particular dataset\nGoogle has a dataset search which is a great place to start.\n\n\nI’m looking for inspiration\n\nData in brief is an Open Journal for publishing datasets, with descriptions\nKaggle hosts many datasets as well as their competition datasets which are great to benchmark Machine Learning techniques, and many have notebooks showing exploration of the datasets\nTidytuesday has weekly datasets for exploring, and you can look at other’s explorations\nJeremy Singer-Vine’s Data is Plural has weekly interesting datasets.\nReddit’s Datasets subreddit sometimes has interesting data\nGovernments often have open data portals, for example Australia, Korea, United Kingdom, EU, United States, Canada, and New Zealand. Also look for regional data portals, for example in Australia there are ones for each state and territory (e.g. Victoria, Queensland).\n\n\n\nI want data about my community\nAs well as open data portals, Governments run censuses, track macro economic and social indicators, agricultural and environmental data, and so on (that may or may not be on the portal). The statistics departments, such as Australia’s Bureau of Statistics (ABS) and UK Office for National Statistics often have a lot of useful aggregate information (although with the ABS it takes some skill to find it).\nOpen Street Map has a lot of geographical data, and varying levels of data about structures. In Australia the G-NAF contains address data that’s not in Open Street Map.\n\n\nI want big data\n\nPapers with Code Datasets have many of the common benchmark machine learning datasets\nGoogle Bigquery Datasets has some large datasets that can be accessed with BigQuery\nAWS Open Data registry\nCommonCrawl contains a ton of open web crawl data, and good resources for navigating it.\nThe Internet Archive contains tons of resources, including the Wayback Machine for web crawl data\n\n\n\nI want something special\nYou can always collect or build your own dataset. If you’ve got an actual problem you’re trying to solve, this is often the best way. This gives you experience not only analysing a dataset, but with collecting and processing data which are very useful to be able to understand and do.\nThe web contains a ton of public data that can be processed into datasets. For example I built a job ad dataset from Common Crawl. You could further annotate these to create your own dataset.\nAnother good method is to collect your own data, and if it doesn’t contain any potentially damaging information, share it as an open dataset. For example in Victoria there’s a way to get your own energy usage data and analyse it. Or you could analyse your email data. Or you could stick some sensors in your garden and record measurements that link to your plants growth, Or run a survey on a topic of interest to you."
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html",
    "href": "calculate-centroid-on-sphere/index.html",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "",
    "text": "I’ve written a derivation of how to find the centroid of a polygon on a sphere. This post shows it explicitly in numerical computations, and also looks at the solution in Spherical Averages and Applications to Spherical Splines and Interpolation, by Buss and Fillmore, ACM Transactions on Graphics 20, 95–126 (2001). Explicitly coding mathematics is a great exercise; having to concretely represent everything unearthed gaps in my understanding and found errors in both drafts of my derivation and the paper.\nGiven a set of points on a sphere we’re trying to find the point that minimises the average distance from those points, which we’ll call the centroid (or Fréchet mean)."
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#contents",
    "href": "calculate-centroid-on-sphere/index.html#contents",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Contents",
    "text": "Contents\n\nPart 1: Calculating the centroid\n\nCoordinate Transformations between spherical and cartesian coordinates\nCreating Random Points on the Sphere for testing\nGeodesic Distance calculations\nMeasuring the Centroid in terms of minimising the geodesic distance\nIteratively Finding the Minimum using a closed form solution\n\n\n\nPart 2: Implementing Buss and Filmore’s approach\n\nThe exponential map and its inverse calculated at the North Pole\nRotating spheres to enable moving arbitrary points to the North Pole\nExponential map at any point\nAttempting to implement the algorithm (and failing to get it to work)\n\n\n\nPart 3: Gradient Descent with Pytorch\n\nGradient Descent"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#coordinate-transformations",
    "href": "calculate-centroid-on-sphere/index.html#coordinate-transformations",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Coordinate transformations",
    "text": "Coordinate transformations\nTo do calculations on the unit sphere we’ll need some way of representing points on the sphere. There’s lots of possible representations (I normally like Stereographic Projections), but we’ll stick to two:\n\nEuclidean coordinate representation: (x,y,z) coordinates in 3-dimensional space.\nLatitude Longitude representation of places on a sphere.\n\nTo convert a latitude, from -π/2 to π/2 radians (i.e. 90°S to 90°N), and a longitude, from -π to π radians (i.e. 180°W to 180°E) to a coordinate we can use the following formula (see Wikipedia for the Geometry)\n\ndef latlon_to_coord(lat, lon):\n    return np.array([cos(lat) * cos(lon), cos(lat)*sin(lon), sin(lat)])\n\nSanity checking coordinates\nLet’s make sure this gives reasonable results simple angles\nSo the north pole, (0,0,1) should be at π/2 latitude\nlatlon_to_coord(pi/2, 0)\narray([6.123234e-17, 0.000000e+00, 1.000000e+00])\nAnd the south pole, (0,0,-1), at -π/2 latitude\nlatlon_to_coord(-pi/2, 0)\narray([ 6.123234e-17,  0.000000e+00, -1.000000e+00])\nAnd the point on the x-axis (1,0,0) is at 0 latitude, and by convention 0 longitude\nlatlon_to_coord(0, 0)\narray([1., 0., 0.])\nThe y-axis is at π/2 longitude\nlatlon_to_coord(0, pi/2)\narray([6.123234e-17, 1.000000e+00, 0.000000e+00])\nAnd at π longitude we have the other side of the x axis\nlatlon_to_coord(0, pi)\narray([-1.0000000e+00,  1.2246468e-16,  0.0000000e+00])\nFinally at π/4 longitude we should always have the x and y components equal\nlatlon_to_coord(0, pi/4)\narray([0.70710678, 0.70710678, 0.        ])\nlatlon_to_coord(pi/4, pi/4)\narray([0.5       , 0.5       , 0.70710678])\n\n\nInverse Transformation: Cartesian Coordinates to Latitude and Longitude\nNotice the use of atan2 to get the sign right.\ndef coord_to_latlon(x, y, z):\n    assert np.all(np.abs((x*x +y*y +z*z) - 1) < 1e-5)\n    lat = arcsin(z)\n    lon = arctan2(y, x)\n    return np.array([lat, lon])\n\n\nSanity checking coordinates\nLet’s check the inverse of some of the examples above\ncoord_to_latlon(0,0,1)\narray([1.57079633, 0.        ])\ncoord_to_latlon(0,0,-1)\narray([-1.57079633,  0.        ])\ncoord_to_latlon(1,0,0)\narray([0., 0.])\ncoord_to_latlon(0,1,0)\narray([0.        , 1.57079633])\ncoord_to_latlon(0,-1,0)\narray([ 0.        , -1.57079633])\ncoord_to_latlon(-1,0,0)\narray([0.        , 3.14159265])\ncoord_to_latlon(1/sqrt(2),1/sqrt(2),0)\narray([0.        , 0.78539816])"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#creating-random-points-on-the-sphere",
    "href": "calculate-centroid-on-sphere/index.html#creating-random-points-on-the-sphere",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Creating Random Points on the Sphere",
    "text": "Creating Random Points on the Sphere\nFor testing it’s useful to be able to create random points on the sphere. In this case I don’t really care about the distribtution of the points so I will always just use a random uniform function.\nI choose the coordinates as the first axis, and the points lie along the second axis. Either choice leads to awkwardness in places either in index access or broadcasting.\ndef random_latlon(n):\n    return (np.random.random_sample([2, n]) - 0.5) * np.array([pi, 2*pi])[...,None]\nHere are 5 points of latitude/longitude (the lat/lon is read vertically in the array below).\nrandom_latlon(5)\narray([[-1.05051377,  1.51919682,  1.08064898, -0.9478646 , -0.24458294],\n       [ 0.51328911, -0.2909267 ,  0.70478098, -0.09613048, -1.45489469]])\nThe points are definitely in the right coordinate ranges; latitude between -pi/2 to pi/2 and longiture from -pi to pi\nr = random_latlon(300)\nr.min(axis=1), r.max(axis=1)\n(array([-1.56325868, -3.10261397]), array([1.56479825, 3.11079206]))\nfig, ax = plt.subplots()\nplt.scatter(r[0], r[1])\n<matplotlib.collections.PathCollection at 0x7f9b414786d0>\n\n\n\npng\n\n\nSimilarly we can create a random point on the sphere by making a random point in the 3 dimensional unit cube centred on the origin and projecting it. Note this will have a different distribution to the latitude longitude random points.\nThere is an infinitessimal chance this will blow up if we pick (0,0,0) as the random point.\ndef random_sphere_point(n):\n    ans = np.random.random_sample([3, n]) - 0.5\n    norm = np.sqrt((ans*ans).sum(axis=0))\n    return ans / norm[...,:]\nHere are 5 points on the sphere in Cartesian Coordinates\nrandom_sphere_point(5)\narray([[-0.7474551 , -0.67828974,  0.4894528 ,  0.14031347, -0.83906874],\n       [ 0.12810391, -0.73422057,  0.46630495,  0.00811564,  0.15397168],\n       [-0.65184374, -0.02903757,  0.73688239,  0.99007387, -0.52178191]])\nCheck they actually do lie on the unit sphere - a distance of 1 from the origin.\nr = random_sphere_point(1000)\nnorms = (r*r).sum(axis=0)\nnorms.max(), norms.min()\n(1.0000000000000004, 0.9999999999999996)\nI guess that looks like a sphere\nfig = plt.figure()\nax = plt.axes(projection='3d')\nfig\nax.scatter3D(*r)\n<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f9b3f343b80>\n\n\n\npng\n\n\nIf we turn it a bit it still looks like a sphere\nax.view_init(45, 45)\nfig\n\n\n\npng\n\n\nax.view_init(165, 135)\nfig\n\n\n\npng\n\n\nWe can also project it back to lat-lon.\nNotice how it’s more concentrated towards the centre because there’s actually less area in the extremites of latitude and longiture.\nfig, ax = plt.subplots()\nplt.scatter(*coord_to_latlon(*r))\n<matplotlib.collections.PathCollection at 0x7f9b3f327370>\n\n\n\npng\n\n\nNow let’s check that our coordinate transforms are actually inverses by checking a bunch of random points.\nIt looks like the coordinate transforms are really inverses\ndef distance(x, y, axis=0):\n    return sqrt((np.power(x - y, 2)).sum(axis=axis))\nr = random_latlon(1000)\nrr = coord_to_latlon(*latlon_to_coord(*r))\ndistance(rr, r).max()\n5.484501741648273e-14\nr = random_sphere_point(1000)\nrr = latlon_to_coord(*coord_to_latlon(*r))\ndistance(rr, r).max()\n1.3087313122991207e-15"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#geodesic-distance",
    "href": "calculate-centroid-on-sphere/index.html#geodesic-distance",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Geodesic Distance",
    "text": "Geodesic Distance\nI claim that the geodesic distance of two points on a sphere is the arccos of the dot product of the vectors that make them.\nAccording to Wikipedia the distance between two points is:\n\\[\\Delta\\sigma = \\arccos\\bigl(\\sin\\phi_1\\sin\\phi_2 + \\cos\\phi_1\\cos\\phi_2\\cos(\\Delta\\lambda)\\bigr).\\]\nWhere \\(\\lambda_1, \\phi_1\\) and \\(\\lambda_2, \\phi_2\\) are the longitude and latitude respectively.\nAlthough this is numerically unstable let’s check this to see if we’re in the right ballpark\nlat1, lon1 = random_latlon(10)\nlat2, lon2 = random_latlon(10)\ndef latlon_geodist(lat1, lon1, lat2, lon2):\n    return  arccos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon2 - lon1))\nLet’s do some sanity checking\nPoints at the same place should be 0 distance\nlatlon_geodist(lat1, lon1, lat1, lon1)\n<ipython-input-34-30a781f9625f>:2: RuntimeWarning: invalid value encountered in arccos\n  return  arccos(sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon2 - lon1))\n\n\n\n\n\narray([0.00000000e+00, 1.49011612e-08, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00,            nan,\n       0.00000000e+00, 0.00000000e+00])\nIt’s possible to get a nan due to floating point error. This rounds up to just over 1 which is just outside the bounds of arccos.\nlat, lon = (-1.129680862943046, -1.4351343458834227)\nsin(lat) * sin(lat) + cos(lat) * cos(lat) * cos(lon - lon)\n1.0000000000000002\nLet’s hack this a little to make it more stable\ndef latlon_geodist(lat1, lon1, lat2, lon2, eps = 1e-6):\n    dotprod = sin(lat1) * sin(lat2) + cos(lat1) * cos(lat2) * cos(lon2 - lon1)\n    assert ((-1 - eps) <= dotprod).all() and (dotprod <= (1 + eps)).all()\n    dotprod = dotprod.clip(-1, 1)\n    return arccos(dotprod)\nlatlon_geodist(lat, lon, lat, lon)\n0.0\nThe distance between the equator and the north pole should be pi/2\nlatlon_geodist(0, 0, pi/2, 0)\n1.5707963267948966\nThe distance between antipodal points should be pi\nlatlon_geodist(-pi/2, 0, pi/2, 0)\n3.141592653589793\nlatlon_geodist(0, 0, 0, pi)\n3.141592653589793\nlatlon_geodist(0, -pi/2, 0, pi/2)\n3.141592653589793\nNow compare the geodistance with the arccos of the dot product\ndist = latlon_geodist(lat1, lon1, lat2, lon2)\ndist\narray([2.46947963, 2.49913914, 1.46664046, 1.17041288, 0.37746113,\n       2.16346273, 1.00909388, 0.48192531, 2.53874702, 2.6748886 ])\np = latlon_to_coord(lat1, lon1)\nq = latlon_to_coord(lat2, lon2)\ndist2 = arccos((p*q).sum(axis=0))\ndist2\narray([2.46947963, 2.49913914, 1.46664046, 1.17041288, 0.37746113,\n       2.16346273, 1.00909388, 0.48192531, 2.53874702, 2.6748886 ])\nThey agree to the order of floating point error.\n(dist - dist2).max()\n0.0\nLet’s capture that in a function, again making sure to bound the elements to deal with floating point errors.\ndef geodist(x, y, eps=1e-6):\n    dotprod = y.T @ x\n    assert ((-1 - eps) <= dotprod).all() and (dotprod <= (1 + eps)).all()\n    dotprod = dotprod.clip(-1, 1)\n    return np.arccos(dotprod)\n[geodist(pi, qi) for pi, qi in zip(p.T, q.T)] - dist\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\nIn our function x can also be a collection of points:\nq0 = np.array(q)[:,0]\nq0\narray([ 0.31287334, -0.20532369, -0.92733622])\n[geodist(pi, q0) for pi in p.T]\n[2.469479630515192,\n 0.40323027521610894,\n 1.4920173306523,\n 1.9558900601514497,\n 2.7359590483860647,\n 2.1462847027999032,\n 0.7043405473704512,\n 2.7367906455922415,\n 0.9539342223012038,\n 1.2027484136227085]\ngeodist(p, q0)\narray([2.46947963, 0.40323028, 1.49201733, 1.95589006, 2.73595905,\n       2.1462847 , 0.70434055, 2.73679065, 0.95393422, 1.20274841])\nThey can also both be arrays in which case all pairing are calculated.\ngeodist(p, q)[:,0]\narray([2.46947963, 0.34759067, 2.38907755, 0.89968604, 0.26041626,\n       2.60671514, 1.83068643, 0.26215902, 1.62594305, 0.35150679])"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#measuring-the-centroid",
    "href": "calculate-centroid-on-sphere/index.html#measuring-the-centroid",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Measuring the Centroid",
    "text": "Measuring the Centroid\nSuppose we have a bunch of points on the sphere, the centroid is the point that minimises the total (or equivalently average) distance from that point to all other points.\nLet’s try to find the centroid of 4 points.\nm = random_sphere_point(1)[:,0]\nps = random_sphere_point(4)\nm, ps\n(array([ 0.61804667, -0.76357346, -0.18701308]),\n array([[ 0.93870158, -0.44268846,  0.68158944, -0.79466137],\n        [-0.3443822 , -0.70496228,  0.34836305, -0.04389649],\n        [ 0.01549958,  0.55412554,  0.64348973, -0.60546379]]))\ndists = geodist(ps, m)\ndists\narray([0.57310037, 1.40903212, 1.53587692, 1.92238526])\ndists.mean()\n1.3600986701826971\nlats, lons = coord_to_latlon(*ps)\nmlat, mlon = coord_to_latlon(*m)\ndists2 = latlon_geodist(lats, lons, mlat, mlon)\ndists2\narray([0.57310037, 1.40903212, 1.53587692, 1.92238526])\ndists2.mean()\n1.3600986701826971\nThe centroid minimises average distance.\nWe’ll create a function that takes a collection of potential centroids to calculate the average distance from.\nm[:,None]\narray([[ 0.61804667],\n       [-0.76357346],\n       [-0.18701308]])\n\nm.shape\n(3,)\nps.shape\n(3, 4)\ndef avg_distance(ps, m):\n    # Allow m to be a vector *or* a matrix\n    if len(m.shape) == 1:\n        m = m[:, None]\n    return geodist(ps, m).mean(axis=1)\navg_distance(ps, m)\narray([1.36009867])\nLet’s do a random search for the centroid of the ps\nm = random_sphere_point(50000)\ndistances = avg_distance(ps, m)\ndistances\narray([1.87538182, 1.72981518, 1.83908373, ..., 1.53442742, 1.54397244,\n       1.64161356])\nClearly the minimum must be lower than this.\nm[:,distances.argmin()], distances.min(), distances.max()\n(array([-0.43100664, -0.70535782,  0.56276427]),\n 1.2565096068989563,\n 1.8852477691126441)\nLet’s just check it’s on the unit sphere.\nmin_point = m[:,distances.argmin()]\n(min_point * min_point).sum()\n0.9999999999999998\nIn the notion parallax article the second suggestion is to project the average vector (which he says doesn’t seem quite right).\nWe can check this explicity by calculating that projection.\navg = ps.sum(axis=1)\navg = avg / np.sqrt((avg*avg).sum())\nmeanpoint_dist = avg_distance(ps, avg[:,None])\navg, meanpoint_dist, meanpoint_dist <= distances.min()\n(array([ 0.37007754, -0.7198562 ,  0.58723902]),\n array([1.29459946]),\n array([False]))"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#iteratively-finding-the-minimum",
    "href": "calculate-centroid-on-sphere/index.html#iteratively-finding-the-minimum",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Iteratively finding the minimum",
    "text": "Iteratively finding the minimum\nAs I derived in finding the Centroid of a Spherical Polygon it occurs where\n\\[c = k \\sum_{i=1}^{N} \\frac{p_i}{\\sqrt{1 - (c \\cdot p_i)^2}}\\]\nWe can evaluate this iteratively and hope that it converges to the minimum.\ndef improve_centroid(c, ps):\n    ans = (ps / sqrt(1 - np.power(c@ps, 2))).sum(axis=-1)\n    norm = sqrt(ans @ ans)\n    return ans / norm\nimprove_centroid(avg, ps)\narray([ 0.23688037, -0.81413966,  0.53015498])\ndef fixpoint(f, x0, eps=1e-5, maxiter=1000, **kwargs):\n    for _ in range(maxiter):\n        x = f(x0, **kwargs)\n        if distance(x, x0) < eps:\n            return x\n        x0 = x\n    raise Exception(\"Did not converge\")\ndef spherical_centroid(ps, eps=1e-5, maxiter=10000):\n    return fixpoint(improve_centroid, np.zeros((3,)), ps=ps, eps=eps, maxiter=maxiter)\nfix = spherical_centroid(ps)\nfix\narray([-0.442657  , -0.70497373,  0.5541361 ])\nThis does seem to be lower than any of our random points\ndmin = avg_distance(ps, fix)\ndmin, dmin <= distances.min()\n(array([1.2554998]), array([ True]))\nAnother good check is to see whether moving a small distance around this point increases the distance.\n(Note the small distance needs to be a bit bigger than our eps above in deciding convergence)\nfix_latlon = coord_to_latlon(*fix)\neps = 1e-4\nxeps = np.array([eps, 0])\nyeps = np.array([0, eps])\nperturbations = np.array([latlon_to_coord(*(fix_latlon + x)) for x in\n                  [0, xeps, -xeps, yeps, -yeps]]).T\nperturbations\narray([[-0.442657  , -0.44262753, -0.44268646, -0.4425865 , -0.44272749],\n       [-0.70497373, -0.7049268 , -0.70502066, -0.705018  , -0.70492947],\n       [ 0.5541361 ,  0.55421934,  0.55405286,  0.5541361 ,  0.5541361 ]])\nThe minimum occurs at the first point, with no perturbation.\navg_distance(ps, perturbations)\narray([1.2554998 , 1.25551318, 1.2555215 , 1.25550478, 1.25551945])\navg_distance(ps, perturbations).argmin()\n0"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#the-exponential-map-and-its-inverse",
    "href": "calculate-centroid-on-sphere/index.html#the-exponential-map-and-its-inverse",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "The exponential map and its inverse",
    "text": "The exponential map and its inverse\nIn the Buss and Filmore paper they talk about the exponential map and its inverse at the point (0,0,1). The idea is that we are building a correspondence between moving in the tangent plane and moving in the sphere itself in a way that preserves distances. Really this is just a special sort of parameterisation of the sphere that preserves distances.\n\nThe exponential map from the north pole takes an (x,y) point on that plane and returns an (x,y,z) coordinate on the sphere’s surface.\nThey give the formula (bottom of page 11):\n\\[\\exp_{(0,0,1)} (x, y) = (x \\frac{\\sin r}{r}, y \\frac{\\sin r}{r}, \\cos{r})\\]\nWhere \\(r = \\sqrt{x^2 + y^2}\\) is the distance of the point on the tangent plane from where it touches the sphere.\nIn code (using sinc for numerical stability at 0)\ndef exp_northpole(x, y):\n    r = sqrt(x*x + y*y)\n    sincr = sinc(r/np.pi)\n    return (x * sincr, y * sincr, cos(r))\nSo at the origin we should get the north pole\nexp_northpole(0,0)\n(0.0, 0.0, 1.0)\nAnd if we move forward a bit in the x direction on the tangent plane we move along the x-axis in 3 space the same distance.\nq = np.array([0,0,1])\np = exp_northpole(0.5,0)\np, geodist(p, q)\n((0.479425538604203, 0.0, 0.8775825618903728), 0.4999999999999999)\nAnd similar for the y-axis\nq = np.array([0,0,1])\np = exp_northpole(0.0,0.3)\np, geodist(p, q)\n((0.0, 0.29552020666133955, 0.955336489125606), 0.30000000000000016)\nAnd if we move (0.3, -0.4) which by Pythagoras is a distance of 0.5\nq = np.array([0,0,1])\np = exp_northpole(0.3,-0.4)\np, geodist(p, q)\n((0.2876553231625218, -0.3835404308833624, 0.8775825618903728),\n 0.4999999999999999)\nNotice that we can wrap around the sphere\nexp_northpole(0, np.pi)\n(0.0, 1.2246467991473532e-16, -1.0)\nexp_northpole(0, 2*np.pi)\n(-0.0, -2.4492935982947064e-16, 1.0)\nexp_northpole(0, 4*np.pi)\n(-0.0, -4.898587196589413e-16, 1.0)\n\nThe inverse of the exponential map\nIt’s easy to calculate the inverse of the exponential map at the north pole, which they call l (for log), and they have the formula on page 12:\n\\[l_{(0,0,1)}(x, y, z) = \\left(x \\frac{\\theta}{\\sin(\\theta)}, y \\frac{\\theta}{\\sin(\\theta)}\\right)\\]\nWhere \\(\\theta = \\cos^{-1}(z)\\).\ndef log_northpole(x, y, z):\n    theta = arccos(z)\n    sinc_theta = sinc(theta / np.pi)\n    return (x / sinc_theta, y/ sinc_theta)\nFirst we should see the north pole itself should map to the origin\nlog_northpole(0,0,1)\n(0.0, 0.0)\nWe can check a few points and that distances are preserved, e.g. the equator is a distance pi/2 away\nlog_northpole(0,1,0)\n(0.0, 1.5707963267948966)\nlog_northpole(1,0,0)\n(1.5707963267948966, 0.0)\nAnd this point is pi/4 from the north pole\nlog_northpole(1/sqrt(2),0,1/sqrt(2))\n(0.7853981633974483, 0.0)\nThe mapping fails at the south pole (because the exponential map can’t make it there). This should actually be the point at infinity.\nlog_northpole(0,0,-1)\n(0.0, 0.0)\nBut it should be a real inverse:\nIf we take points on the sphere they should go back to where they started\nps = random_sphere_point(5)\ndistance(ps, exp_northpole(*log_northpole(*ps)))\narray([2.02487339e-15, 1.02967200e-15, 5.55111512e-17, 0.00000000e+00,\n       0.00000000e+00])\nNow the inverse will only work if we don’t wrap around the sphere; that is we need to move less than pi in any direction.\nnpoint = 5\nlength = np.random.random(npoint) * pi\ndirection = np.random.random(npoint) * 2 * pi\nxs, ys = length * cos(direction), length * sin(direction)\nts = np.array([xs, ys])\nts\narray([[-0.04616066, -0.17607737,  0.53101022, -0.07972639,  0.45558742],\n       [ 0.13898508, -0.86137675, -2.0428374 , -3.04819971,  1.10504611]])\ndistance(ts, log_northpole(*exp_northpole(*ts)))\narray([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.46600946e-14,\n       0.00000000e+00])"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#rotating-spheres",
    "href": "calculate-centroid-on-sphere/index.html#rotating-spheres",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Rotating Spheres",
    "text": "Rotating Spheres\nThe paper doesn’t have a formula for the exponential map and its inverse at other points, but we can work it out by rotating the sphere and then rotating it back. This is inefficient, but gets us the right answer.\nFirst we need a way of rotating the sphere. This is surpisingly complicated, but given an axis represented by unit vector u and an angle theta we can do a rotation with the following matrix:\n\\[R = \\begin{bmatrix}\n\\cos \\theta +u_x^2 \\left(1-\\cos \\theta\\right) & u_x u_y \\left(1-\\cos \\theta\\right) - u_z \\sin \\theta & u_x u_z \\left(1-\\cos \\theta\\right) + u_y \\sin \\theta \\\\\nu_y u_x \\left(1-\\cos \\theta\\right) + u_z \\sin \\theta & \\cos \\theta + u_y^2\\left(1-\\cos \\theta\\right) & u_y u_z \\left(1-\\cos \\theta\\right) - u_x \\sin \\theta \\\\\nu_z u_x \\left(1-\\cos \\theta\\right) - u_y \\sin \\theta & u_z u_y \\left(1-\\cos \\theta\\right) + u_x \\sin \\theta & \\cos \\theta + u_z^2\\left(1-\\cos \\theta\\right)\n\\end{bmatrix}\\]\ndef rot_matrix(u, t):\n    ux, uy, uz = u\n    cost = cos(t)\n    sint = sin(t)\n    return np.array([\n [cost+ux*ux*(1-cost), ux*uy*(1-cost)-uz*sint, ux*uz*(1-cost)+uy*sint],\n [uy*ux*(1-cost)+uz*sint, cost+uy*uy*(1-cost), uy*uz*(1-cost)-ux*sint],\n [uz*ux*(1-cost)-uy*sint, uz*uy*(1-cost)+ux*sint, cost+uz*uz*(1-cost)],\n    ])\nSanity checking: A rotation of 0 should be identity\nrot_matrix(random_sphere_point(1)[:,0],0)\narray([[ 1., -0.,  0.],\n       [ 0.,  1., -0.],\n       [ 0.,  0.,  1.]])\nA rotation about the z-axis should be in the xy-plane\nrot_matrix((0,0,1), pi/6)\narray([[ 0.8660254, -0.5      ,  0.       ],\n       [ 0.5      ,  0.8660254,  0.       ],\n       [ 0.       ,  0.       ,  1.       ]])\nAnd reversing the axis should reverse the rotation\nrot_matrix((0,0,-1), pi/6)\narray([[ 0.8660254,  0.5      ,  0.       ],\n       [-0.5      ,  0.8660254,  0.       ],\n       [ 0.       ,  0.       ,  1.       ]])\nAnd similarly a rotation about the x-axis should be in the y-z plane\nrot_matrix((1,0,0), pi/6)\narray([[ 1.       ,  0.       ,  0.       ],\n       [ 0.       ,  0.8660254, -0.5      ],\n       [ 0.       ,  0.5      ,  0.8660254]])\nLet’s check a random rotation\naxis = random_sphere_point(1)[:,0]\nangle = np.random.random_sample(1)[0] * pi\naxis, angle\n(array([ 0.39320147, -0.03521715,  0.91877764]), 0.8930289746904281)\nR = rot_matrix(axis, angle)\nRotating the oposite way should invert it\nRinv = rot_matrix(axis, -angle)\nThis should be the identity matrix\nR @ Rinv\narray([[ 1.00000000e+00,  2.28238442e-17, -2.96906906e-17],\n       [ 2.28238442e-17,  1.00000000e+00, -3.30265999e-17],\n       [-2.96906906e-17, -3.30265999e-17,  1.00000000e+00]])\nRinv @ R\narray([[ 1.00000000e+00,  9.75220593e-17, -7.54846245e-17],\n       [ 9.75220593e-17,  1.00000000e+00, -3.69276604e-17],\n       [-7.54846245e-17, -3.69276604e-17,  1.00000000e+00]])\nLet’s rotate some random points and check if it preserves distances\np1, p2 = random_sphere_point(2).T\np1, p2\n(array([ 0.84651763, -0.45095389, -0.2829284 ]),\n array([ 0.07670365, -0.73056703, -0.67851925]))\ngeodist(p1, p2)\n0.944244049404823\ngeodist(R@p1, R@p2)\n0.9442440494048231\n\nRotating to the north pole\nIf we want to rotate a point to the north pole we can:\n\nFind the plane containing the centre of the sphere, the point and the north pole\nFind an axis perpendicular to that plane\nCalculate the angle between the sphere and the point\nRotate along that axis by that angle (being careful about orientation)\n\nThere’s an easy way to calculate the perpendicular axis to two vectors in 3 dimensions; the cross product (or for differential geometers the hodge dual of the wedge product).\nI always remember the definition of the cross product by the mnemonic xyzxyz; e.g. the first xyz tells us the x component is yz-zy, then we go a letter across to yzx and so the y component is zx-xz and finally for the z component.\ndef cross_product(u, v):\n    ux,uy,uz = u\n    vx,vy,vz = v\n    return (uy*vz-uz*vy, uz*vx-ux*vz, ux*vy - uy*vx)\nLet’s check a random example:\ncross_product((2,3,4) , (5,6,7))\n(-3, 6, -3)\nNow we can use this to calculate a unit perpendicular vector\ndef norm(x, axis=0):\n    return np.sqrt((x*x).sum(axis=axis))\ndef unit_perp(u, v):\n    v = np.array(cross_product(u, v))\n    return v / norm(v)\nunit_perp((2,3,4) , (5,6,7))\narray([-0.40824829,  0.81649658, -0.40824829])\nnorth_pole = np.array((0,0,1))\ndef perp_to_north_pole(v):\n    # Special handling for north/south pole here\n    # Note the order is important for orientation\n    return unit_perp(v, north_pole)\nperp_to_north_pole((1,0,0))\narray([ 0., -1.,  0.])\nperp_to_north_pole((0,1,0))\narray([1., 0., 0.])\nperp_to_north_pole((0,1/np.sqrt(2),1/np.sqrt(2)))\narray([1., 0., 0.])\nOf course this will all explode if the two vectors are parallel (since then there’s a whole plane of potential orthogonal vectors, you need to break the symmetry).\nperp_to_north_pole((0,0,1)), perp_to_north_pole((0,0,-1))\n<ipython-input-112-372364f14156>:3: RuntimeWarning: invalid value encountered in true_divide\n  return v / norm(v)\n\n\n\n\n\n(array([nan, nan, nan]), array([nan, nan, nan]))\ndef angle_to_north_pole(v, axis=0):\n    return np.arccos((north_pole * v).sum(axis=axis))\nCheck some basic examples\nangle_to_north_pole(np.array((0,0,1)))\n0.0\nangle_to_north_pole(np.array((0,0,-1)))\n3.141592653589793\nangle_to_north_pole(np.array((0,1,0)))\n1.5707963267948966\nangle_to_north_pole(np.array((1/sqrt(2),1/sqrt(2),0)))\n1.5707963267948966\nangle_to_north_pole(np.array((0, 1/sqrt(2),1/sqrt(2))))\n0.7853981633974484\nLets check the rotation in action\np = random_sphere_point(1)[:,0]\np\narray([-0.29445315, -0.46089653, -0.83718082])\ntheta = angle_to_north_pole(p)\naxis = perp_to_north_pole(p)\naxis, theta\n(array([-0.84270333,  0.53837821,  0.        ]), 2.562904446655308)\nCheck that it’s perpendicular and of unit norm\n(axis * north_pole).sum(), (axis * p).sum(), (axis * axis).sum()\n(0.0, 0.0, 1.0)\nR = rot_matrix(axis, -theta)\nAnd indeed it rotates to the north pole\ndistance(R @ p , north_pole)\n1.0938523993656688"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#exponential-map-at-any-point",
    "href": "calculate-centroid-on-sphere/index.html#exponential-map-at-any-point",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Exponential map at any point",
    "text": "Exponential map at any point\nNow we can take the exponential map at any point by first rotating to the north pole, calculating the exponential map, and then rotating back.\nLet’s take some point on the sphere\np = random_sphere_point(1)[:,0]\np\narray([-0.52085469, -0.70958405,  0.47455333])\nAnd a random point on its tangent plane (not too far from the origin)\nmax_dist = pi\nlength = np.random.random(1) * max_dist\ndirection = np.random.random(1) * 2 * np.pi\ntx, ty = length * cos(direction), length * sin(direction)\n\ntx, ty\n(array([-1.23956389]), array([1.68843737]))\nLet’s rotate it to the north pole\ntheta = angle_to_north_pole(p)\naxis = perp_to_north_pole(p)\naxis, theta\n(array([-0.80613779,  0.59172786,  0.        ]), 1.076339808354696)\nrot_matrix(axis, theta) @ p\narray([8.32667268e-17, 0.00000000e+00, 1.00000000e+00])\nThe tangent plane doesn’t change it’s coordinatisation, so we can just calculate the exponential map.\nq_rot = exp_northpole(tx, ty)\nq_rot\n(array([-0.51244602]), array([0.69801406]), array([-0.50017542]))\nThe distance is preserved by the exponential map\ngeodist(np.array([0,0,1]), np.array(q_rot)), np.sqrt(tx**2 + ty**2)\n(array([2.09459767]), array([2.09459767]))\nAnd now we need to rotate the sphere back\nq = rot_matrix(axis, -theta) @ q_rot\nq\narray([[-0.33260114],\n       [ 0.94302493],\n       [-0.00897019]])\nAnd we check the distances are preserved\ngeodist(p, q), np.sqrt(tx**2 + ty**2)\n(array([2.09459767]), array([2.09459767]))\nPutting this all together we get a function that calculates the expoential map at point p on the sphere, for vector t in its tangent space.\ndef exp(p, t):\n    tx, ty = t\n    theta = angle_to_north_pole(p)\n    axis = perp_to_north_pole(p)\n\n    q_rot = exp_northpole(tx, ty)\n    q = rot_matrix(axis, -theta) @ q_rot\n    return q\nq = exp(p, (tx, ty))\nq\narray([[-0.33260114],\n       [ 0.94302493],\n       [-0.00897019]])\ngeodist(p, q), sqrt(tx*tx + ty*ty)\n(array([2.09459767]), array([2.09459767]))\n\nInverse expoenential map at any point\nThe logic is essentially the same for the inverse map, which we call log, except we perform the rotation before we apply the map.\ndef log(p, q):\n    theta = angle_to_north_pole(p)\n    axis = perp_to_north_pole(p)\n    q_rot = rot_matrix(axis, theta) @ q\n\n    t = log_northpole(*q_rot)\n    return t\nSo at any point p the log should be the inverse of exp\nt = np.array([tx,ty])\nt, log(p, exp(p, t))\n(array([[-1.23956389],\n        [ 1.68843737]]),\n (array([-1.23956389]), array([1.68843737])))\nq = random_sphere_point(1)[:,0]\nq\narray([0.21868918, 0.77422052, 0.59393403])\nexp(p, log(p, q))\narray([0.21868918, 0.77422052, 0.59393403])"
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#outline-of-a-method-of-checking",
    "href": "calculate-centroid-on-sphere/index.html#outline-of-a-method-of-checking",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Outline of a method of checking",
    "text": "Outline of a method of checking\nI think there’s a way we could check this, but I have not taken the time to implement it.\nThe stereographic projection about any point, and it’s inverse, are straightforward to implement with standard geometry of points, lines and spheres. Some geometry shows that the stereographic projection of a circle maps the angle t to a point 2 tan(t/2). So it follows that composing the stereographic projection by a function f(x) = 2 arctan(x/2) should give the exponential map.\nWe would check this gives the same results as above.\nUsing Pytorch we could start with latitude and longitude (as below) and project to cartesian coordinates and apply the inverse exponential map to get to the tangent plane. Then we could check the formulas of lemma 2a."
  },
  {
    "objectID": "calculate-centroid-on-sphere/index.html#gradient-descent",
    "href": "calculate-centroid-on-sphere/index.html#gradient-descent",
    "title": "Centroid of Points on the Surface of a Sphere",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nAt the end of the day the paper’s approach was just a form of gradient descent (which I couldn’t get working). But we can get an autograd library, like that in Pytorch, to calculate the gradients for us. The only trick here is we need to choose sphere coordinates (otherwise it will tell us to go off the sphere, unless we use Lagrange multipliers).\nimport torch\ndef torch_latlon_geodist(lat1, lon1, lat2, lon2):\n    return  torch.arccos(torch.sin(lat1) * torch.sin(lat2) + torch.cos(lat1) * torch.cos(lat2) * torch.cos(lon2 - lon1))\nlatlon_ps = torch.tensor(coord_to_latlon(*ps))\nlatlon_ps\ntensor([[-0.7885,  0.6007,  0.0995,  0.1919],\n        [ 0.9673,  2.4580,  2.9729, -2.2270]], dtype=torch.float64)\ndef centroid_gradient_descent(c, ps, lr=0.1):\n    dist = torch_latlon_geodist(*c, *ps).mean()\n    dist.backward()\n    with torch.no_grad():\n        c -= lr * c.grad\n    c.grad.zero_()\n    return c, dist.item()\ndef centroid_sphere_gd(ps, eps=1e-6, max_iter=1000, lr=0.01):\n    c = ps.mean(axis=1).requires_grad_()\n    dist = None\n    for _ in range(max_iter):\n        last_dist = dist\n        c, dist = centroid_gradient_descent(c, ps, lr=lr)\n        if last_dist and abs(dist - last_dist) < eps:\n            return c.requires_grad_(False)\n    raise Exception(\"Did not converge\")\nUsing torch gradient descent\nc2 = centroid_sphere_gd(latlon_ps)\nc2\ntensor([0.1012, 2.9704], dtype=torch.float64)\nc2c = latlon_to_coord(*np.array(c2))\nc2c, avg_distance(ps, c2c)\n(array([-0.98034083,  0.16950631,  0.1009924 ]), array([0.92810049]))\nc2.requires_grad_()\ntensor([0.1012, 2.9704], dtype=torch.float64, requires_grad=True)\ndist = torch_latlon_geodist(*c2, *latlon_ps).mean()\ndist.backward()\ntorch_latlon_geodist(*latlon_ps, *c2).mean()\ntensor(0.9281, dtype=torch.float64, grad_fn=<MeanBackward0>)\nClosely matches our original function\nc = spherical_centroid(ps)\nc, avg_distance(ps, c)\n(array([-0.98093006,  0.16703028,  0.09938358]), array([0.9276463]))"
  },
  {
    "objectID": "job-title-annotation/index.html",
    "href": "job-title-annotation/index.html",
    "title": "Annotating Job Titles",
    "section": "",
    "text": "In our previous post we looked at what was in a job ad title and a way of extracting some common job titles from the ads. There’s no obvious programmatic rules that could be constructed for extracting the job title from the title of the advertisement, so one approach is to train a Named Entity Recogniser. To do this we need to annotate many examples of a job title, which means we need to be crystal clear on what is a job title.\nWe definitely don’t want the location of the job, the name of the company or the working conditions (like “Part Time”, “Excellent Tips”); these are separate pieces of information. However the seniority and industry may sometimes be relevant, and hard to separate.\nFor example a “Pharmaceutical Sales Representative” may be a different type of job to a general “Sales Representative” because it requires different qualifications and skills. You could make arguments that the job title for “Subsea Cabling Engineer” could be “Subsea Cabling Engineer”, “Cabling Engineer” or just “Engineer”. These distinctions are very subtle and require a lot of world knowledge - not a good fit for a NER algorithm. The best thing to do is try to capture the longest role title (like “Subsea Cabling Engineer”) and then further break it down with post-processing.\nOf course when you start looking at the data there are many subtle examples. How about when the industry is tacked onto the end like: “Quality Manager Defence”? Is a “Web Programmer - C#, ASP.NET” different to a “C#/ASP.NET Web Programmer”? It’s really hard to be consistent in annotation (and if we can’t agree on the correct answer, how could a model?)\nFor terms like “Head of Marketing” and “Marketing Assistant” it’s a little tricky to cleave off the seniority. So similarly it makes sense to capture all the seniority with NER. If we need to remove the title later we can do this with post-processing rules.\nFor joint roles like “Receptionist/Administrator” or “CNC Setter / Operator” again it makes sense to capture the whole phrase. For example “CNC Setter / Operator” is really short for “CNC Setter / CNC Operator” but it’s going to be tricky to do those expansions. We can then choose to fix them up with post-processing.\nSo the goal for our NER is to capture the longest contiguous phrase containing roles, industries or seniorities. This is still doing useful things like removing working conditions like “Part Time”, company information, and location. Then we can try to further break up the more structured roles with post-processing rules.\nIt sounds clear right? But as soon as I start looking at examples I start doubting my choices! “Planning Engineer - Civil Engineering Derby”, should I really include “Civil Engineering” in the role title? How about “PA to a Leading Partner in a US Law Firm”? By my own rules I should include “Law”, maybe the “Leading Partner” changes the nature of the role? Maybe here “Leading Partner” is a separate role title I should annotate! I guess “Head of Care Nursing (RN)” is a single title?\nYou really want a clear annotation scheme for your machine learning algorithm to learn. But it’s really hard to know what you want until you’ve spent a long time looking at the data. At the end of the day I guess you have to spend a little time upfront defining an annotation scheme and then test and iterate to see whether it works in practice.\nI’ll see how this goes in future posts in this series!"
  },
  {
    "objectID": "data-science-reputation/index.html",
    "href": "data-science-reputation/index.html",
    "title": "Building a Reputation in Data Science",
    "section": "",
    "text": "It’s important that your reputation is very specific to the kind of work you want others to buy. If you’re positioned just as a “Data Scientist” or “Analyst” it’s not clear you’re good at solving the exact problems they want. However if you’re an expert in Thai NLP, or in fast moving consumer goods demand forecasting, or in AI strategy in news media then you’re likely to be highly valued by the people who want those things. There’s an art to finding a niche deep enough to position yourself as an expert, but wide enough to allow plenty of opportunities. Specialising in an industry is often a good way to do this, because you’ll learn the communication styles and challenges of the industry, which will make it easier to deliver value.\nThe strongest impact on your reputation is to build relationships with people who you want to buy your work. If you’re offering services to someone who knows you can do the work, or has someone they trust who knows you can do the work, the conversation becomes much easier. Any kind of purchase comes with a risk of not getting what you pay for; if you can get someone to vouch for you this risk is much less. The best way to do this is to make sure you’re close to the community you’re offering work to (think trade conventions and former colleagues) and that they are aware of the kind of work you do.\nA good way to build your reputation is to get signals that you are good at what you do, especially if you can leverage the reputation of others. The Talk Python Podcast has a good episode on Side Hustles for Data Scientists which talks a lot about ways both to do work and win more work. Here’s some examples of things you could do (many of which you can monetise to some extent in their own right):\n\nWrite about data science on your own website or on a social media platform\nGuest on or host a podcast; podcasts are always looking for good content\nTalk at a conference\nProduce video tutorials, or live analyses\nTeach a course (in person or online)\nContribute to open source software\nCreate a portfolio of projects; projects solving a real problem are ideal\nWrite a book\nRank highly in data science competitions like Kaggle\nCreate a useful dataset or service\n\nThe key is to go deep and specific. If you try to do all of these things across multiple topics you won’t build a reputation (and the topics on this website are too broad for a reputation, but that’s not why I’m writing it). But if you pick a focused approach for a target area you can become known as the go-to person for those kinds of problems; which are great if they are the kinds of problems people think they need to solve.\nThink about it from your potential client’s perspective. They have something they need, and so they go search for an expert to help them with it. You want to be that expert and have a good story with plenty of evidence to back them up. Ideally you’re expert in exactly the applied problem they’re trying to solve; while John D. Cook has shown it’s possible to be a consultant in mathematics, it’s much easier to be a consultant in, say, optimising pricing for consumer ecommerce products (and you happen to use a lot of data science to do that)."
  },
  {
    "objectID": "python-update-whatcar/index.html",
    "href": "python-update-whatcar/index.html",
    "title": "Updating a Python Project: Whatcar",
    "section": "",
    "text": "I thought I had done a good job of making it reproducible by creating a Dockerfile for it. However I hadn’t stored the built container anywhere, and when I tried to rebuild it the build failed, complaining something about pip can’t find torchvision-nightly. Whatcar uses fastai v1 for the classifier, I had version locked it to 1.0.18 because I knew it was changing in unstable ways. At the time torchvision was new and it was relying on nightly builds, which probably don’t exist anymore now that it is stable. It seemed the easiest way forward was to upgrade to the most recent version of fastai and Pytorch.\nUpdating to the latest version of fastai was straightforward, but getting a CPU build of Pytorch is a bit trickier. Because I’m just serving the Pytorch model there’s little advantage of running on a GPU because it will only be scoring one image at a time; it can’t batch them up to evaluate in parallel on the GPU. For technical reasons the CPU build of Pytorch isn’t on PyPI. However it is hosted by Pytorch, and as discussed in a Pytorch issue it can be added to the requirements.txt by adding --find-links. Interestingly you can’t specify pip to use an external source in setup.py without passing a flag, so you would need a custom install script. Anyway I stuck with requirements.txt and updated it to look like the following.\n--find-links https://download.pytorch.org/whl/torch_stable.html\ntorch==1.7.0+cpu\ntorchvision==0.8.1+cpu\nfastai==1.0.61\nFinally I could install the dependencies and test them. I had stored the 90MB model in Google Cloud Storage, but have since removed the account, so I just added it to the repository because it’s just below Github’s 100MB limit (Gitlab.com has more generous limits). While a lot of people will recommend using LFS for files this size, in my experience it’s really easy to get in confusing situations with LFS; the files come down as a hash before you initialise LFS (which is easy to forget to do, or do wrong especially when doing many git operations) and it can take some time to work out what’s going wrong. LFS (or other external storage) would matter with lots of changing large files; it makes it easy to checkout just the most recent version than the whole history, which can save on bandwidth, but for this case it’s not worth the hassle.\nNow when I tried to run it I got an error about tfms getting multiple arguments. A quick google of the error message revealed a fast.ai forum post which worked around the problem by changing tfms to ds_tfms (this kind of thing is why I version locked fastai in the first place). After that it worked locally.\nAll up it took me about half an hour to get it working locally, but only because I could make educated guesses about why a lot of the errors were occurring. But I still had to update it on the server, which surprisingly took me just as long.\nEven though I had a Docker container I was running on the server via a virtualenv. I’m not entirely comfortable about securely deploying with Docker, so I didn’t. I uploaded my new files and tried to upgrade the server.\npip install --upgrade requirements.txt\nHowever I got some long and scary error message about Bottleneck failing to install. So I created a fresh virtual environment (python3 -m venv ~/.venvs/deploy2) and tried again. This time I got an error about being out of space, despite having plenty of free space on the server. Undaunted I deleted empty virtualenvs and ~/.cache which contains cached versions of downloaded packages from pip (among other things). Again I got an error about Bottleneck failing to install which I couldn’t make much of; but I noticed the message about pip being out of date. So I upgraded pip with pip install --upgrade pip and tried installing the requirements again and it worked like magic. I reloaded the server and everything was working as expected.\nMy lesson from all this is when you work with a system a lot you learn how things work by hitting edge cases, and learn heuristics for solving them. Knowing where pip’s cache was allowed me to remove it, knowing a bit about fastai allowed me to quickly navigate the problems that came up in it. I find this frequently, the first time I had an encoding error (e.g. something not in valid UTF-8 in the middle of a large file) it took me days to work out; now I can generally sense it from the error message and resolve it quickly. This is often the hardest thing about moving to a new ecosystem; when learning some Haskell I spent days dealing with Stack errors. I’m sure if I ran a Haskell server in production I’d hit a thunk leak someday which would be incredibly hard to debug the first time. These things are surmountable, but it’s the cost of working with new systems."
  },
  {
    "objectID": "dip-statistic/index.html",
    "href": "dip-statistic/index.html",
    "title": "Dip Statistic for Multimodality",
    "section": "",
    "text": "Another approach is statistical tests for multimodality. One common test is Silverman’s Test which checks for the number of modes in a kernel density estimate; the trick is choosing the right width. Another test is Hartigan’s Dip Test which tries to estimate the distance between the Cumulative Distribution Function and the nearest multimodal function. This article tries to explain this dip statistic.\nThe method was first published in The Dip Test of Unimodality by J. A. Hartigan and P. M. Hartigan. The paper is moderately statistically involved, especially in the middle, but the overall idea is quite simple.\nA unimodal distribution will have a Probability Density Function (PDF) that increases from 0 to some peak value and then decrease back to 0. If there’s a flat region there may be a range of points the mode is achieved at, but it’s a single interval. Its Cumulative Distribution Function at any point is just the area under the PDF between 0 and that point. When the PDF switches from increasing to decreasing then the CDF switches from convex to concave. A graph is convex means that any section of the curve lies below the straight line joining the endpoints, and conversely convave means that any section of the curve lies above the straight line joining the endpoints.\n\n\n\nUnimodal and Multimodal CDF and PDF\n\n\nA multimodal distributions CDF will change from convex to concave and back again multiple times, because it’s PDF will change from increasing to decreasing and back again multiple times. The idea of the dip statistic is to measure how much we need to change the CDF to make it unimodal. In particular it is the maximum distance at any point between the CDF and the closest multimodal CDF. In other words the distribution can be deformed into a unimodal one by moving the CDF by at most the dip at each point, and the dip is the smallest number for which this is true.\n\n\n\nExample of Dip Statistic\n\n\nUsing the CDF rather than the PDF means this distance is bounded. The CDF goes from 0 to 1, as the cumulative probability goes from 0% to 100%. This means the dip must be less than 1 (probably quite a bit less). Contrast this to the PDF which is unbounded.\nIt also intuitively makes sense; the bigger the dip the greater the difference between them. The dip is related to the difference between the greatest area between the PDFs at some point. In a sense it measure the greatest depth and width change from a unimodal distribution.\nIn practice we only have a finite sample from the distribution. The statistic is calculated on the empirical CDF; that is a step function with a step up of 1/N wherever there is a datapoint for each of the N data points). They use the analogy of trying to fit a string around a deformation of the empirical CDF. I won’t go into the computational details here, but this can be calculated efficiently.\nFinally to work out significance they need to pick a “null” model. They choose a uniform distribution; that is one with a flat CDF. This makes intuitive sense because adding arbitrarily small noise to a uniform distribution could turn it into a multimodal distribution. It’s going to be quite hard to judge if a uniform distribution is multimodal; harder than most general cases.\nIf you need to do multimodal detection it’s worth considering Hartigan’s dip test. It’s got a reasonable theoretical footing, is computationally efficient, and is implemented in the R diptest package."
  },
  {
    "objectID": "tex-inotify/index.html",
    "href": "tex-inotify/index.html",
    "title": "Previewing changes to LaTeX documents with inotify",
    "section": "",
    "text": "To install inotify on a Debian derivative you can use sudo apt install inotify-tools.\nThen you can set it to run a command whenever a file is done saving. In my case I have a Makefile to convert the TeX file into a PDF so I can run the following command:\nwhile inotifywait -e close_write Edward_Ross.tex; do make; done\nI can view the PDF in Emacs with the excellent pdf-tools; and by setting the buffer to automatically update on change with M-x auto-revert-buffer whenever I save my TeX file the PDF is generated and the buffer updates with the changes. It even seems to roughly keep my position. While it’s not perfect, and for longer documents the refresh time will be large, it’s a convenient way to quickly preview small changes to a TeX or LaTeX document."
  },
  {
    "objectID": "job-title-not-ner/index.html",
    "href": "job-title-not-ner/index.html",
    "title": "Not using NER for extracting Job Titles",
    "section": "",
    "text": "NER models try to extract things like the names of people, places or products. SpaCy’s NER model which I used is optimised to these cases (looking at things like capitalisation of words). A role title is actually a reasonable candidate for NER, but I was trying to make some fine grained linguistic distinctsion in my annotation scheme.\nHowever a job ad title is mostly the job title anyway, with some specific information like working conditions, seniority and location. When I looked at the top 1600 titles that occurred accross many different advertisers I found that most of the titles were actually job titles with a few example with additionl words like “senior”, “part time” and “immediate start”.\nIt seems like by looking at the most common sequences of terms in job ad titles, blacklisting terms that aren’t part of the title (like senior) is a good way to cover the most common job titles (which is the best I’d get out of NER anyway). Then a first cut of labelling is just matching the strings. Indeed published an interesting pipeline for normalizing job titles. This could be improved by performing some normalisation like expanding acronyms, stemming and grouping.\nNER would be a great approach if the job title was hidden in a much bigger piece of prose, and in particular if I wanted to catch new and infrequent job titles as they came in. But for summarisation frequency counting (with some normalisation) is a simpler approach that looks like it should work reasonably well. The main concern is coverage; there will be some rare job titles that it misses (and NER may pick up), but from an aggregate viewpoint these aren’t much value unless I can relate them to more common role titles."
  },
  {
    "objectID": "emacs-email/index.html",
    "href": "emacs-email/index.html",
    "title": "Don’t manage work email with Emacs",
    "section": "",
    "text": "Getting the basics of synchronising emails from an IMAP or Exchange server may take some time to setting up (and in some circumstances take a lot of time), but once they’re working it will be pretty smooth. Dealing with HTML and images and attachments works pretty well out of the box, unless you get a lot of custom office drawings in your email. Building an address book of frequent contacts is a bit of a pain, but with some work is possible. Synchronising email addresses from the server can be difficult, and may need to be done in batches - but you might be able to manually. Getting calendar invites is possible with a bit of hacking, but seeing other people’s calendars is very difficult. Finally if the server configuration is changed (like changing an authentication provider) you may have to spend a lot of time setting it up all over again.\nThe benefits are that it tends to be faster to get through emails (because they are on the local filesystem), you don’t need to change environments to use them and you can use all your favourite CLI tools on them. But unless email is a very large part of your working life (and it seems to be slowly losing out to instant messaging clients) it’s probably not worth the investment (unless you want to build a custom email automation tool one day!).\nI’ll share some of my experience doing this for those who are hard to discourage.\n\nOperating Environment\nIf you want to set up email from the command line or Emacs you’ll want to be working in a POSIXy environment, because that’s where all the tooling is. If your working environment supports Linux or Mac computers then it’s happy times. However if you’re working in Windows you have a few options.\nThe best Windows option is Windows Subsystem for Linux - it lets you run a whole Linux environment and works pretty well with Windows. There are a couple of creaky edges, mainly the filesystem is slow (which should be fixed in WSL2), but overall it’s the best solution if you can get the feature enabled.\nIf you can’t then working in a Virtualbox VM for Linux is the next best option; and you can configure it to be fairly seamless. However you can’t use it if there’s any other virtualisation on your machine like Docker for Windows. In fact some organisations use security software that uses virtualisation making it impossible to install Virtualbox. But if you can get Virtualbox running (or your organisation supports another virtualisation product) then it’s generally a good solution.\nWhen all else fails there is good ol’ Cygwin. It doesn’t require any special permissions, so as long as you can run external applications on your computer it should work. It’s a bit clunky, and you may need to build some utilities (like isync, see below) yourself, but with a bit of work you can get a usable environment. I’ve heard msys2 is better but have never taken the time to understand it.\n\n\nSynchronising email\nYou now need a way to pull email to your local filesystem and push emails back out. For pushing emails Postfix works great (and has a sendmail interface) and I’ve never needed anything else. For pulling emails you can run a email server like Dovecot, but it’s quite a bit of effort to set up. The easiest solution is to use isync/mbsync, or it’s slower cousin offlineimap.\nBoth mbsync and offlineimap have gnarly configuration options that will make you learn quite a bit about the low level details of email authentication and Maildir. And if you set the wrong options you might accidentally delete your whole email; so spend a lot of time reading through them and try it out on a test account before you run it on your precious emails. To get authentication details for your email provider the easiest thing to do is to search the web, and if you have a common email provider (like Gmail, Office365, Fastmail) you’ll likely find a blog with a sample configuration. If you’ve got an uncommon provider look in their documentation/settings for SMTP and IMAP; if the Auth method isn’t clear it’s best to try to set it up with Thunderbird first because it has some magic to automatically detect these settings and is more likely to work out of the box.\nIf your email provider doesn’t have IMAP enabled then you’re probably out of luck - unless it’s an exchange server. If you can’t get app passwords and need to use two factor authentication you may spend a lot of time trying to get this set up (and may have to implement the feature!).\nIf you’re on an Exchange server or on Office365 but can’t access IMAP then you can use the fantastic Davmail. Davmail also supports CalDAV and CardDAV for calendar and contacts (more on this later). The only issue is finding the Exchange server settings can take some sleuthing (or a beer with your local sysadmin). As before it’s best to get it working in Thunderbird before trying another synchronising tool, because it’s easiest to get working there.\n\n\nSetting up a mail interface\nSo now you’ve got all your emails sitting in a maildir folder it’d be handy to have a tool for reading and writing email.\nIf you’re a serious vim user mutt may be a good option. notmuch has a powerful tag system, but you have to manually sync it yourself between servers (and the Emacs mode has too many special screens for my liking). In Emacs gnus is built-in, but has a byzantine configuration system that you could spend the rest of your life tweaking (like this John Wiegley’s). But gnus is apparently good if you’re on a lot of mailing lists. However for me mu and it’s Emacs counterpart mu4e work fantastically well - you just have to take the time to learn yet another query language (and if you use evil-mode there are mu4e keybindings in evil-collection).\nThis is generally pretty straightforward (especially if you can crib a configuration file that is close to what you need), but there are some traps like if you use mbsync and mu4e you need to set mu4e-change-filenames-when-moving to true or you’ll get all sorts of errors when trying to sync. You can then spend a bunch of time configuring how HTML is rendered, storing links to emails in org-mode and sending email from org-mode.\n\n\nDealing with contacts\nNow you can write email you may want to store the addresses of people you want to contact. Many email providers support the CardDav format and you can synchronise it locally with a tool like ASynK or vdirsyncer. You can then import them into org contacts with org-vcard and configure mu4e to use them for auto-completion. Or use them with BBDB for Emacs email clients that support them, or write a script to convert them to Mutt aliases.\nOne problem is if you work for an enterprise with thousands of people that’s going to be a lot of email addresses, and the syncing or the interface may choke. You can probably get away with just manually copying the addresses of the people you email most often into the configuration of whatever tool you use. But once in a while you’ll want to email someone in your organisation and you may have to fall back to another tool to get the address book.\n\n\nCalendars and meeting invites\nCalendars is something where Outlook groupware really shines. You can see everyone’s calendar and schedule a meeting in free time using the scheduling assistant (and book meeting rooms!). While this can lead to the problem of calendar tetris where other people unilaterally fill the blanks in your Calendar, it’s generally a useful organisational feature and can sometimes even be used to list and book available meeting rooms.\n\n\n\nOutlook Scheduling Assistant\n\n\nI haven’t found anything that quite substitutes for it in an office environment. You could manage your calendar with a command line tool in khal or in Emacs with diary/calendar or org-agenda or calfw and synchronise it over iCal. You can probably even get meeting invites into your calendar and respond to the invitation (mu4e supports this). But I doubt there’s anything like the scheduling assistant and if you organise a lot of meetings in an Outlook office you’ll be falling back to Outlook a lot.\nIf you get this far you can spend a lot more time smoothing out the rough edges. It’s certainly possible to do, but worth thinking about whether it’s really going to pay off for the time investment. But maybe you can walk the path and enjoy the journey as much as I did."
  },
  {
    "objectID": "python-maybe-monad/index.html",
    "href": "python-maybe-monad/index.html",
    "title": "Maybe Monad in Python",
    "section": "",
    "text": "Suppose you’ve got some useful function that parses a date: parse_date('2020-08-22') == datetime(2020,8,22). However sometimes None will be passed as an argument which leads to an error, but you just want it to return None. You can explicitly write this as a new function:\ndef parse_optional_date(date):\n  if date is None:\n    return None\n  else:\n    return parse_date(date)\nThis is pretty straightforward to do, but it means that whenever you are writing or using a function you have to think about how it handles None. Do you want it to be an error, or to pass through?\nThe approach generally taken in Haskell is that it should be an error, but you can get this behaviour using the Maybe monad. Explicitly there’s a functor (a fancy name for a function that acts on functions in a composable way) that takes a function and extends it like our parse_optional_date example. In Python it would look something like:\ndef maybe_fmap(f):\n  return lambda(x): f(x) if x is not None else None\nThis takes one function and returns a new one that passes through nulls; if we were going to add type annotations it would look something like:\nfrom typing import Callable, TypeVar, Optional\na = TypeVar('a')\nb = TypeVar('b')\ndef maybe_fmap(f: Callable[a, b]) -> Callable[Optional[a], Optional[b]]:\n  return lambda(x): f(x) if x is not None else None\nIn Haskell you will generally see Monads defined in terms of the bind (>>=) operator, which is closely related.\ndef maybe_bind(x: Optional[a], f: Callable[a, Optional[b]]):\n  return f(x) if x is not None else None\nTo be clear, I wouldn’t advocate actually doing this because Python is not a functional language, and you’re going to end up with some gnarly stack traces and hard to understand functions. But it’s a useful concept to have in mind when designing functions; it’s systematically trivial to add support for None.\nThis is just one example of a monad to lift functions. Another one is map which is a functor that raises a function to one that can handle lists. In Haskell the idea is even used to extend pure functions, that are easy to test, into scenarios where there is I/O or state. However I’m not convinced the conceptual overhead is always worth it.\nIf you do want to use Maybe pattern in Python here are some libraries that offer it with increasing levels of sophistication:\n\npymaybe extends a Maybe object with dunder methods so a Nothing is passed through subsequent operations and is exited with a get or or_else method\nmaybe-else does the same thing, but is exited with an else_\nreturns has a general monad implementation, good integration with mypy and some documentation\npymonad has a generic implementation of monads, but not much documentation"
  },
  {
    "objectID": "descriptive-to-predictive/index.html",
    "href": "descriptive-to-predictive/index.html",
    "title": "From Descriptive to Predictive Analytics",
    "section": "",
    "text": "Businesses often have a lot of reporting around important metrics cut by key segments. Monthly average revenue per customer by industry. Weekly average sales per salesperson by territory. Percentage of daily website visits that bounce by traffic source.\nSome of these may have a large impact on decisions, but the data is quite sparse and the imprecision means the business can’t make a timely decision. Where do we need to recruit more salespeople? What stock do we need to increase the supply of? Should we keep spending on our marketing campaign to drive consideration?\nYou can re-frame these descriptive statistics as predictive problems. The mean and median are the constants that minimise mean squared error and mean absolute error respectively. For example the weekly average sales per salesperson by territory could be considered the model: sales ~ week x territory. That is we treat each week and territory as independent and fit a constant model to each.\nBut you know that the weeks aren’t independent. A traditional approach to this is to apply a time series forecasting method. Another approach is to model over relevant factors related to each week; such as sales and budget cycles that have a large impact on sales.\nYou also know the sales territories aren’t independent. Some of the sales territories are very close together, and will have similar characteristics. Maybe you know some characteristics of territories, such as the types and density of customers they contain that impact sales.\nYou could combine all this to create a more accurate model of sales. Your previous segmentation is a good baseline, and by testing on a holdout set of the latest week you could determine whether your model is actually better at predicting sales. This could mean a better understanding of the potential return on investment of putting more salespeople in certain territories, or allow better incentive setting.\nThere is a danger here that the models will sometimes be wrong. Especially in turbulent times, very different to the history they’ve been trained on, they will confidently mispredict. They’re also harder to interpret and explain; they require building trust with stakeholders.\nBut if the models will inform decisions that lead to significantly better outcomes they may be worth the risk and complexity."
  },
  {
    "objectID": "tidy-time/index.html",
    "href": "tidy-time/index.html",
    "title": "Tidy Time",
    "section": "",
    "text": "A while ago I read David Allen’s Getting Things Done. When I tried to implement it I got stuck on the notion of a weekly review. Setting aside some time every week to see how you’re progressing on tasks and to process any new tasks. I never got into the ritual.\nI always felt uncomfortable doing this at work. Spending time thinking about what needs to be done when I should be working.\nI found it hard to carve out time for this at home. I would frequently go away and so couldn’t consistently assign the time. Even when I had the time it seemed like it would be more fun to do something else.\nHowever I’ve suffered for this. The majority of urgent things still get done. However the important but non-urgent things keep getting pushed back unless I happen to focus on one of them for a couple of weeks. There’s also always a nagging feeling of that thing that ought to be done.\nThis is why I think it’s important to set aside a couple of hours of work time and personal time each week to “tidy time”. It’s the unfortunate cost of fighting off entropy; to stave off the heat death of your environment you have to put in some work to keep it tidy. A little bit of regular work can keep this maintained and it’s an important part of your time budget."
  },
  {
    "objectID": "shingle-inequality/index.html",
    "href": "shingle-inequality/index.html",
    "title": "Jaccard Shingle Inequality",
    "section": "",
    "text": "Two similar documents are likely to have many similar phrases relative to the number of words in the document. In particular if you’re concerned with plagiarism and copyright, getting the same data through multiple sources, or finding versions of the same document this approach could be useful. In particular MinHash can quickly find pairs of items with a high Jaccard index, which we can run on sequences of w tokens. A hard question is what’s the right number for w? If you use bags (instead of sets) it turns out that increasing the sequence length decreases the Jaccard index (unless it’s 1 or 0).\nThere are lots of measures of similarity between documents such as the Levenshtein (or Edit) Distance. However few are efficient at finding similar documents at large scale. The MinHash algorithm allow finding sets that have a high Jaccard index (i.e. number of items in common relative to total number of items) and containment (number of items in common relative to items in one set) efficiently. Treating a document as a set or bag of words may not be ideal; two long documents on a similar topic may have a lot of words in common without having a similar source. However if the documents have many long sequences of words in common then they probably do have a similar source. We can form n-grams (also called w-shingles); treating the documents as made up of sequences of words of a fixed length, and then calculate the Jaccard index or containment. How does changing the size of the sequence effect the scores?\nIncreasing the shingling length will always decrease the Jaccard index and containment for a bag (a.k.a multiset), but not for a set."
  },
  {
    "objectID": "shingle-inequality/index.html#cardinality-of-shingles",
    "href": "shingle-inequality/index.html#cardinality-of-shingles",
    "title": "Jaccard Shingle Inequality",
    "section": "Cardinality of Shingles",
    "text": "Cardinality of Shingles\nThe k shingles are all subsequences of length k. One way to think of this is by going through each index of the sequence, getting a string of length k until you get to the last k. In Python code:\ndef seq_ngrams(xs, k):\n    return [xs[i:i+k] for i in range(len(xs)-k+1)]\nThe number of possible shingles is N + 1 - k where N is the length of the sequence. So there are N possible 1-shingles (tokens), down to 1 possible N shingle (the whole sequence). In particular \\(\\lvert S_{k} \\rvert = \\lvert S_{k+1} \\rvert + 1\\)."
  },
  {
    "objectID": "shingle-inequality/index.html#inequality-of-intersection-cardinality",
    "href": "shingle-inequality/index.html#inequality-of-intersection-cardinality",
    "title": "Jaccard Shingle Inequality",
    "section": "Inequality of Intersection Cardinality",
    "text": "Inequality of Intersection Cardinality\nConsider the elements in \\(\\lvert S_{k+1} \\cap T_{k+1} \\rvert\\). These are sequences of tokens of length k+1 in both S and T. Suppose that all of these elements were part of some long substring in both S and T, e.g. \\(a_1 a_2 \\ldots a_m\\) where \\(m - k = \\lvert S_{k+1} \\cap T_{k+1} \\rvert\\). Then each subsequence of length k must also be in \\(\\lvert S_{k} \\cap T_{k} \\rvert\\), and so \\(\\lvert S_{k} \\cap T_{k} \\rvert \\geq \\lvert S_{k+1} \\cap T_{k+1} \\rvert + 1\\).\nIn fact if they are not all in the same subsequence then there will be even more length k subsequences in common. So in general it is true (for bags) \\(\\lvert S_{k} \\cap T_{k} \\rvert \\geq \\lvert S_{k+1} \\cap T_{k+1} \\rvert + 1\\)."
  },
  {
    "objectID": "shingle-inequality/index.html#calculating-the-jaccard-similarity",
    "href": "shingle-inequality/index.html#calculating-the-jaccard-similarity",
    "title": "Jaccard Shingle Inequality",
    "section": "Calculating the Jaccard Similarity",
    "text": "Calculating the Jaccard Similarity\nBy the inclusion-exclusion principle we have:\n\\[J_k = \\frac{\\lvert S_k \\cap T_k \\rvert}{\\lvert S_k \\rvert + \\lvert T_k \\rvert - \\lvert S_k \\cap T_k \\rvert}\\]\nPlugging in our previous cardinality equality, the intersection inequality, and the inclusion-exclusion principle gives:\n\\[J_k \\geq \\frac{\\lvert S_{k+1} \\cap T_{k+1} \\rvert + 1}{\\lvert S_{k+1} \\cup T_{k+1} \\rvert + 1}\\]\nFinally note that \\(\\frac{N+1}{U+1} \\geq \\frac{N}{U}\\) with equality if and only if N equals U. So \\(J_k \\geq J_{k+1}\\) and equality can occur only if the intersection is the same as the union; that is \\(J_{k+1}\\) is 1 or 0.\nThis also applies to containment:\n\\[C_{k} = \\frac{\\lvert S_{k} \\cap T_{k} \\rvert}{\\lvert S_{k} \\rvert} \\geq \\frac{\\lvert S_{k+1} \\cap T_{k+1} \\rvert + 1}{\\lvert S_{k+1} \\rvert + 1} \\geq C_{k+1}\\]\nBy the same inequality as before \\(C_{k} \\geq C_{k+1}\\) where equality is only if \\(A_k\\) is empty or contains \\(B_k\\)."
  },
  {
    "objectID": "opo-redundancy/index.html",
    "href": "opo-redundancy/index.html",
    "title": "Redundancy on Phone Power Button",
    "section": "",
    "text": "If there wasn’t this redundancy there would be no way for me to turn it on without pulling it apart, and potentially damaging it. The lesson for me is something as crucial as a power button should have a redundancy. This one will let me continue to use it until I get a replacement."
  },
  {
    "objectID": "recommendation-graph/index.html",
    "href": "recommendation-graph/index.html",
    "title": "Representing Interaction Networks",
    "section": "",
    "text": "One way to view the relationship is as a bipartite graph. For example the nodes could be the products and the customers and there’s an edge between a customer and a product if they have bought them. You could then look at classifying customers and products as by clustering them using community detection. Some of them model the graph directly, like the Stochastic block model. This clustering could help with targeting and recommending products to customers.\n\n\n\nBipartite Graph\n\n\nThe bipartite graph could also be represented by its adjacency matrix. The adjacency matrix underlies a lot of the algorithms for community detection such as Spectral clustering. Because there are no connections between customers and customers and products and products the matrix is 0 on these diagonal blocks. Because the graph is undirected the adjacency matrix is symmetric.\n\n\n\nAdjacency matrix of Bipartite Graph\n\n\nThe adjacency matrix can be represented by it’s top right corner, the interaction between products and customers, A. A common approach for this is matrix factorisation where this matrix is decomposed into a product of matrices \\(A = UV\\). This can be thought of as an embedding; U embeds products and the adjoint of V embeds the customers. The items can then be clustered in the embedding space for example using k-means.\n\n\n\nProduct-Customer Matrix\n\n\nIf you don’t care explicitly about the products you could just look at the customers. Two customers are similar if they’ve bought the same product. This can be explicitly constructed by counting the number of products they have in common, via the matrix product \\(A A^{T}\\). Then community detection algorithms could be run on the weighted graph (optionally removing the diagonal of self-links).\n\n\n\nCustomer-Customer Matrix\n\n\nThere are many more variations that can be made on this. The customers can be weighted by their activity so one very active customer doesn’t have disproportionate impact on the graph. Interactions on dynamic processes such as random walks, spin glasses and synchronisation can illuminate the communities. Any metadata, such as product features, can be used to help inform the community structure. Different types of interactions like detail views versus purchases could be used together. Consensus clustering could be used to create a sequence of new adjacency matrices to find communities in.\nThere are lots of different ways of viewing the same problem which gives a wide variety of techniques to approach it. Depending on your application you can pick models that are most appropriate."
  },
  {
    "objectID": "exact-duplicates/index.html",
    "href": "exact-duplicates/index.html",
    "title": "Finding Exact Duplicate Text",
    "section": "",
    "text": "Naively finding exact duplicates by comparing every pair would be O(N^2), but if we sort the input, which is O(N log(N)), then duplicate items are adjacent. This scales really well to big datasets, and then the duplicate entries can be handled efficiently with itertools groupby to do something like uniq. In this case we can get the indices of the exact duplicates from a list (adding them with enumerate).\nfrom itertools import groupby\n\ndef second(x):\n    return x[1]\n    \ndef exact_duplicate_indices(items)\n    exact_duplicates = []\n    for _key, group in groupby(sorted(enumerate(items), key=second), key=second):\n        group = list(group)\n        if len(group) > 1:\n            exact_duplicates.append([item[0] for item in group])\n    return exact_duplicates\nIf memory was an issue because the items are really large we could even hash them to make the sorted possible. With the 400,000 ad descriptions, on average 1600 characters long, this took 1.3s on my laptop.\nI could then look at the size of the duplicate clusters and investigate them:\nfrom collections import counter\nduplicates = exact_duplicate_indices(ads)\nCounter(len(cluster) for cluster in duplicates)\nThis gave a table of frequencies; most clusters only have one duplicate, but there are a few with many duplicates.\n\n\n\nCluster Size\nFrequency\n\n\n\n\n2\n5836\n\n\n3\n293\n\n\n4\n71\n\n\n5\n26\n\n\n6\n7\n\n\n7\n8\n\n\n8\n5\n\n\n9\n7\n\n\n10\n1\n\n\n12\n1\n\n\n13\n1\n\n\n14\n2\n\n\n15\n1\n\n\n24\n1\n\n\n\nI then inspected the largest clusters. In my case I kept them in the same order as a dataframe they came from and could index into them with iloc.\n# Get the 20 largest clusters\nmegaclusters = sorted(exact_duplicates, key=len, reverse=True)[:20]\n\n# Look it up in the original dataframe\ndf.iloc[megaclusters[0]]\nThis let me discover that in most of these cases it’s due to being posted in multiple locations. I verified this with a quick analysis that the size of the cluster was almost the number of unique locations.\n[(len(megacluster), len(df.iloc[megacluster].LocationRaw.unique())) for megacluster in megaclusters]\nThis is useful for exact duplicat, but what about slight variations? In this case the largest two clusters were in fact the same ad posted to different sites with a slight variation in the footer text. I’ll look at this in the next part of the series."
  },
  {
    "objectID": "job-title/index.html",
    "href": "job-title/index.html",
    "title": "What’s in a Job Ad Title?",
    "section": "",
    "text": "The job title should succinctly summarise what the role is about, so it should tell you a lot about the role. However in practice job titles can range from very broad to very narrow, be obscure or acronym-laden and even hard to nail down. They’re even hard to extract from a job ad’s title - which is what I’ll focus on in this series.\nIn a previous series of posts I developed a method that could extract skills written a very particular way. Creating a broader extraction methodology is challenging (what is a skill?), so I thought I’d start with something easier: job titles.\nSo I’m trying to extract job titles from the titles of job ads in the Adzuna Job Salary Predictions Kaggle Competition. See the Jupyter Notebook for the details of this analysis."
  },
  {
    "objectID": "job-title/index.html#seniority",
    "href": "job-title/index.html#seniority",
    "title": "What’s in a Job Ad Title?",
    "section": "Seniority",
    "text": "Seniority\nIt’s not always clear that separating seniority is the right thing to do. We can easily split a “Senior Staff Nurse” into a “Senior” “Staff Nurse”. A “Marketing Assistant” may be able to be broken into an “Assistant” “Marketing Officer”. But is a “Marketing Manager” a “Manager” level “Marketing Officer”? What about the “Head of Marketing”?\nWith seniority we need to make a call of how to split it off.\nThere are also some odd things that may be considered seniority. In “Part Qualified Accountant” is “Part Qualified” a seniority? How about “experienced” like in “Experienced Recruitment Consultan? Or a”lunchtime supervisor” is presumably more junior than a “supervisor”, but I don’t know if we can consider “lunchtime” a seniority.\nSeniority is often implicit in the role; a “Key Account Manager” is more senior than a “Regional Account Manager”. A “Human Resources Business Partner” is more senior than a “Human Resources Officer” which is more senior than a “Human Resources Administrator”. Trying to extract the seniority in general isn’t straight forward."
  },
  {
    "objectID": "job-title/index.html#multiple-roles",
    "href": "job-title/index.html#multiple-roles",
    "title": "What’s in a Job Ad Title?",
    "section": "Multiple roles",
    "text": "Multiple roles\nSometimes it’s hard to work out how many roles there are in an ad title.\nIs a “Receptionist/Administrator” one role or two? Perhaps a “Receptionist/Administrator” does different kinds of work to a sole “Receptionist” and a sole “Administrator” so we could consider it a single role. But it could be valid to consider it a mix of two different roles.\nHow about a “CNC Setter / Operator”? I believe this is just one role covering two pieces of work using a CNC machine.\nWhat about “Registered General Nuse/Registered Mental Health Nurse”? It seems likely they’d prefer a Registered Mental Health Nurse since it’s more specific, but will settle for a Reginistered General Nurse. So I guess this is two titles because they would hire one of either (but more the latter!)."
  },
  {
    "objectID": "job-title/index.html#relating-roles",
    "href": "job-title/index.html#relating-roles",
    "title": "What’s in a Job Ad Title?",
    "section": "Relating Roles",
    "text": "Relating Roles\nEven once we’ve found a role title it’s hard to fit it into the constellation of roles.\nCommon acronyms can be expanded to see that a “RGN” is the same as a “Registered General Nurse”, and similar things can be normalised to it.\nBut is a “groundworker” similar to a “gardener”? A “reward Analyst” is likely more similar to a “marketing analyst” than an “infrastructure analyst” Understanding the roles requires a lot of additional knowledge about the roles, and will require actually looking into the ad text. But just having the role titles would be a useful start."
  },
  {
    "objectID": "language-model-classifier/index.html",
    "href": "language-model-classifier/index.html",
    "title": "Language Models as Classifiers",
    "section": "",
    "text": "A language model can give the probability of a given text under the model. Suppose we have multiple language models each trained on a distinct corpus representing a class (e.g. genre or author, or even sentiment). Then we can calculate the probability conditional on that model and compare them to calculate the class.\nConcretely we have language models \\(M_1, \\ldots, M_k\\) each representing a different class, and we want to assign a class to a text T. Then using Bayes’ Rule we have:\n\\[\\mathbb{P}(M_i \\vert T) = \\frac{\\mathbb{P}(T \\vert M_i) \\mathbb{P}(M_i)}{\\sum_{i=1}^{N} \\mathbb{P}(T \\vert M_i)}\\]\nSo to calculate the class we find the model that maximised the probability of the text under that language model multiplied by the probability of that model being appropriate. Assuming that the classes in the training corpora are representative of the data at inference time this would just be the fraction of all data in that class.\nIn many ways for an N-gram language model this seems similar to naive Bayes, but requires more effort to calculate and I don’t believe it would give better results. But it’s something worth considering."
  },
  {
    "objectID": "time-budget/index.html",
    "href": "time-budget/index.html",
    "title": "Time Budgeting",
    "section": "",
    "text": "I don’t believe that you should allocate away all of your time, but setting some time constraints is important. If you don’t put the big rocks of things that are important to you first in the jar first, all the sand and water of mundane things will fill it up.\nI think of it like envelope budgeting. If I set some financial goals and set aside a certain amount of money in advance every week, over time I will slowly meet those goals. If I don’t set any goals I’ll spend that money on something else that I don’t really need. So it is with time.\nCommitting to reading a book can seem like a large task when you’re busy. But setting aside time to read a few pages a day seems like a low commitment, but will get you there in the end. This is how I worked through the book All of Statistics, which has been a huge enabler in my career as an analyst. Similarly almost everything I have learned has been in small pieces at a time over a long period of time.\nOne of the Top Five Regrets of the Dying is “I wish I hadn’t worked so hard”. Another is that “I wish I had stayed in touch with my friends”. Instead of working those extra few hours on the weekend, if you set them aside to call or catch up with a friend it’s likely a better use of time. If your relationships are important then you need to invest time in them to make them grow.\nIt’s also very important to spend time on your own health. If you don’t get enough sleep, don’t exercise and don’t take the time to eat well and hydrate properly then you won’t be as effective in your other hours. These pay a worse toll the more you neglect them. It’s very tempting to let other activities creep into your sleeping hours, but you’ll be much better off getting the sleep and giving the most important tasks your fully rested attention.\nI’m frequently coming back to the questions of what is important to me and how should I spend my time. I find that when I don’t I can easily spend a lot of time doing things that aren’t important to me, they’re just easily available. For another great take on this I recommend the Wait But Why article Your Life in Weeks."
  },
  {
    "objectID": "atomic-writer/index.html",
    "href": "atomic-writer/index.html",
    "title": "Only write file on success",
    "section": "",
    "text": "A strategy for this is to write to some temporary file, and then move the temporary file on completion. I’ve wrapped this in a Python context manager called AtomicFileWriter which can be used in a with statement in place of open:\nwith AtomicFileWriter(dest_name) as output:\n    output.write(...)\nIn the implementation we create a temporary file in the same location by appending .tmp to the filename. If the context is successfully exited this file is closed and moved to the desired destination. Otherwise if there is an error then the filehandle is closed and the temporary file is deleted.\nimport os\nclass AtomicFileWriter:\n    \"\"\"Writes a file to filename only on successful completion\"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.temp_filename = str(filename) + '.tmp'\n\n    def __enter__(self):\n        self.filehandle = open(self.temp_filename, 'x')\n        return self.filehandle\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        if exc_type is None:\n            self.filehandle.close()\n            os.replace(self.temp_filename, self.filename)\n        else:\n            try:\n                self.filehandle.close()\n            finally:\n                os.unlink(self.temp_filename)\nThis should be safe; moving a file with os.replace should be atomic. It’s pretty unlikely there will already be a file with .tmp at the end. There are likely some conditions under which the temporary file won’t be cleaned up (e.g. under a kill -9). Looking at the tempfile source code it looks like there’s more edge cases I’m likely missing (on some systems), but it works well enough.\nI decided to not use tempfile because:\n\nThe temporary directory may be on a different partition increasing risks of failure (out of disk space, issues moving the file accross an NFS)\nHaving the file with a predictable name in the same directory makes it easier to monitor the progress\n\nBut using a predictable name has increased the risks of filename collision. Using the ‘x’ mode to open the file means it will fail if the file already exists. This stops a kind of failure where two processes try to write to the file at the same time leading to corruption. It does mean that if a .tmp file doesn’t get deleted on exit it has to be manually cleaned up.\nI’ve been successfully using this with the pattern to only write if the file doesn’t exist:\nfrom pathlib import Path\nfor dest_path in Path(dest_dir).glob('*')\n    if dest_path.exists():\n        continue\n    with AtomicFileWriter(dest_path) as output:\n        output.write(...)"
  },
  {
    "objectID": "simple-models/index.html",
    "href": "simple-models/index.html",
    "title": "Simple Models",
    "section": "",
    "text": "It’s always worthwhile starting with the absolute simplest model possible to at least get a baseline for the problem. I’ve seen people impressed by a complex system that gives 90% accuracy where a constant model will get in the 80% range; and the question should be does the extra 10 percentage points leads to significantly better decisions. If you’re looking at a clustering problem start off with everything in the same cluster, or with everything in a different cluster. If you can clearly answer why the simplest model isn’t good enough it’s a big step towards evaluating more complex models (which is often harder than building them).\nI’ve tried starting with a big black box technique like random forests before, but have found that I then spend a significant amount of time trying to get all the data and engineer the features to be in a columnar format; for example sequential data is hard to encode. Then when the model didn’t fit particularly well I was left stuck trying to work out where to go next. If I were to try it again I would start by using domain knowledge to get the most likely few features and build a simpler model based on that, and incrementally build an understanding of the problem, before investing in gathering a large quantity of data. I think there’s still a place for these kinds of models and techniques like feature importance for discovering new angles to approach a problem, but it’s worth ruling out simpler approaches first.\nIn reality an embarrassingly large number of business problems can be solved adequately with the right average or a linear model. They’re cheap to implement, explain and maintain and are robust in their domain. In core business areas with high leverage, where a small improvement adds lot of business value, then it’s generally worth improving these with more complex techniques. But these will come with a much higher ongoing cost to implement, run, validate and maintain.\nKeep in mind that “simple” is relative to your experience and the problem domain. For an image classification problem the simplest reasonable model after a constant model is likely some kind of neural network. But even Andrej Karpathy recommends spending a lot of time looking at your data and getting dumb baselines with tiny models before putting in a ResNet.\nIndeed I think a lot of the value of starting with simple models is the focus is more on the data and problem at hand, rather than on complex modelling or engineering issues, and so you get a much clearer understanding of what is likely to work and what the issues are. In every setting you’re likely to get bad data and this often stands out when you explore the data thoroughly."
  },
  {
    "objectID": "mailmerge-pdf-files/index.html",
    "href": "mailmerge-pdf-files/index.html",
    "title": "Mail merge to PDF Files",
    "section": "",
    "text": "Microsoft Office offers mail merge (under the Mailing ribbon) which lets you generate documents for printing or email that fill in individual details from an Excel spreadsheet (or other datasource). Unfortunately there’s no way to generate separate Word or PDF files directly from mail merge. I found a macro to do it. When I first ran it raised an error and I had to remove the line If Err.Num = 5631 Then Err.Clear. Then it worked perfectly, taking fields First_Name and Last_Name from the spreadsheet and producing a PDF file <Last_Name>_<First_Name>.pdf (and a corresponding Word file).\nSub Merge_To_Individual_Files()\n' Sourced from: https://www.msofficeforums.com/mail-merge/21803-mailmerge-tips-tricks.html\nApplication.ScreenUpdating = False\nDim StrFolder As String, StrName As String, MainDoc As Document, i As Long, j As Long\nConst StrNoChr As String = \"\"\"*./\\:?|\"\nSet MainDoc = ActiveDocument\nWith MainDoc\n  StrFolder = .Path & \"\\\"\n  With .MailMerge\n    .Destination = wdSendToNewDocument\n    .SuppressBlankLines = True\n    On Error Resume Next\n    For i = 1 To .DataSource.RecordCount\n      With .DataSource\n        .FirstRecord = i\n        .LastRecord = i\n        .ActiveRecord = i\n        If Trim(.DataFields(\"Last_Name\")) = \"\" Then Exit For\n        'StrFolder = .DataFields(\"Folder\") & \"\\\"\n        StrName = .DataFields(\"Last_Name\") & \"_\" & .DataFields(\"First_Name\")\n      End With\n      On Error GoTo NextRecord\n      .Execute Pause:=False\n      For j = 1 To Len(StrNoChr)\n        StrName = Replace(StrName, Mid(StrNoChr, j, 1), \"_\")\n      Next\n      StrName = Trim(StrName)\n      With ActiveDocument\n        'Add the name to the footer\n        '.Sections(1).Footers(wdHeaderFooterPrimary).Range.InsertBefore StrName\n        .SaveAs FileName:=StrFolder & StrName & \".docx\", FileFormat:=wdFormatXMLDocument, AddToRecentFiles:=False\n        ' and/or:\n        .SaveAs FileName:=StrFolder & StrName & \".pdf\", FileFormat:=wdFormatPDF, AddToRecentFiles:=False\n        .Close SaveChanges:=False\n      End With\nNextRecord:\n    Next i\n  End With\nEnd With\nApplication.ScreenUpdating = True\nEnd Sub\nUnfortunately my friend was on Mac and this script didn’t work. I stepped into the debugger and noticed the .recordcount was -1. Reading the docs it seems this happens when it can’t detemine the number of records, which was confirmed on stackoverflow.\nI tried manually overriding the recordcount to 5, but then it turned out the .DataFields was empty. Something weird was happening on Mac Office and we were reaching the point of diminishing returns to fix it.\nInstead we worked out if we saved the PDF from the preview it came out correctly. It was relatively quick to repeatedly jump to next record in preview and then Save as a PDF file, copying the name from the preview to use as the filename. This took about 40 minutes for 100 contracts, which is faster than I could have debugged the macro."
  },
  {
    "objectID": "cartesian-product/index.html",
    "href": "cartesian-product/index.html",
    "title": "Cartesian Product in R and Python",
    "section": "",
    "text": "You’ve got a couple of groups and you want to get every possible combination of them. This is called the Cartesian Product of the groups. There are standard ways of doing this in R and Python."
  },
  {
    "objectID": "cartesian-product/index.html#python-list-comprehensions",
    "href": "cartesian-product/index.html#python-list-comprehensions",
    "title": "Cartesian Product in R and Python",
    "section": "Python: List Comprehensions",
    "text": "Python: List Comprehensions\nConcretely we’ve got (in Python notation) the vectors x = [1, 2, 3] and y = [4, 5] and we want to get all possible pairs: [(1, 4), (2, 4), (3, 4), (1, 5), (2, 5), (3, 5)]`. The “pythonic” way to do this is with a list comprehension:\n[(x_, y_) for x_ in x for y_ in y]\nAnother possibility is to use itertools.product which is especially useful for a large number of lists."
  },
  {
    "objectID": "cartesian-product/index.html#r-expand.grid",
    "href": "cartesian-product/index.html#r-expand.grid",
    "title": "Cartesian Product in R and Python",
    "section": "R: Expand.grid",
    "text": "R: Expand.grid\nIn R we can use expand.grid to get a data.frame of all pairs:\nexpand.grid(x=x, y=y)\nIn this expression the x and y to the left of the = sign are the names of the columns in the dataframe. I find this really useful when creating plots of functions with ggplot2 to try every possible combination of parameters. You can also do this manually using rep; for example:\ndata.frame(x=rep(x, length(y)), y=rep(y, each=length(x)))"
  },
  {
    "objectID": "cartesian-product/index.html#python-more-complex-list-comprehensions",
    "href": "cartesian-product/index.html#python-more-complex-list-comprehensions",
    "title": "Cartesian Product in R and Python",
    "section": "Python: More Complex List Comprehensions",
    "text": "Python: More Complex List Comprehensions\nWhat if we have a slightly harder problem: there’s another vector z = [6, 7] and we want to take every aligned pair from y and z and combine it with every possible x. So the output should be [(1, 4, 6), (2, 4, 6), (3, 4, 6), (1, 5, 7), (2, 5, 7), (3, 5, 7)]. This is straightforward with list comprehensions by combining y and z with zip:\n[(x_, y_, z_) for x_ in x for y_, z_ in zip(y, z)]\nThis is one of the strengths of Python list comprehensions, it’s easy to extend with different variables and with functions acting on those variables."
  },
  {
    "objectID": "cartesian-product/index.html#r-tidyr-expand",
    "href": "cartesian-product/index.html#r-tidyr-expand",
    "title": "Cartesian Product in R and Python",
    "section": "R: tidyr expand",
    "text": "R: tidyr expand\nI don’t know how to do this harder task in R with expand.grid, and so I would have to fallback to the long way with rep. This would be\ndata.frame(x=rep(x, length(y)), y=rep(y, each=length(x)), z=rep(z, each=length(x)))\nThis gets quite tedious to write!\nHowever there are neat ways to do this with the tidyr package, and in particular with the expand function. You can solve it like this:\nexpand(data.frame(y=y, z=z), x, nesting(y, z)\nThis gets all combinations of x, y, and z, providing that the pairs y and z are in the data.frame from the first argument.\nNote that expand is not referentially transparent, and the variables rely on their names in the data frame (as is typical of tidyverse functions). For example expand(data.frame(y=z, z=y), x, nesting(y, z) will reverse the order of the last two columns."
  },
  {
    "objectID": "sql-fill-gaps/index.html",
    "href": "sql-fill-gaps/index.html",
    "title": "Filling Gaps in SQL",
    "section": "",
    "text": "You need all the possible variables you’re filling in, and the value to fill. For example you might have the daily table containing each device, traffic source and date the number of visitors.\n\n\n\ndevice\nsource\ndate\nvisitors\n\n\n\n\ndesktop\ndirect\n2019-12-23\n100\n\n\ndesktop\npaid\n2019-12-23\n3\n\n\nmobile\nany\n2019-12-23\n32\n\n\ndesktop\ndirect\n2019-12-24\n40\n\n\nmobile\nany\n2019-12-24\n18\n\n\ndesktop\ndirect\n2019-12-26\n80\n\n\ndesktop\npaid\n2019-12-26\n2\n\n\nmobile\nany\n2019-12-26\n23\n\n\n\nThere are some missing combinations of valid (device, source) and date with visitors, and we want to fill them with 0s. We could get all the possible combinations of device and source with a simple query:\nselect device, source\nfrom daily\ngroup by 1, 2\nSimilarly we could generate all possible dates; if some may be missing for every device and source it’s best to use something like generate series:\nSELECT date\nFROM generate_series(\n  (select min(date) from daily),\n  (select max(date) from daily),\n  '1 day') as dates(date)\nThen we can generate all possible combinations with a cross join. The exact syntax may vary by database; I’ll assume here that separating with commas does the trick (at least Presto and Postgres).\nselect device, source, date\nfrom (\n  select device, source\n  from daily\n  group by 1, 2\n), \n generate_series(\n  (select min(date) from daily),\n  (select max(date) from daily),\n  '1 day') as dates(date)\nFinally we can fill in the data from the underlying table with a coalesce, using the default value of 0 for missing entries:\nselect device, source, date, \n       coalesce(daily.users, 0) as users\nfrom (\n  select device, source, date\n  from (\n    select device, source\n    from daily\n    group by 1, 2\n  ), \n   generate_series(\n    (select min(date) from daily),\n    (select max(date) from daily),\n    '1 day') as dates(date)\n) xjoin\nleft join daily\n  on xjoin.device = daily.device \n  and xjoin.source = daily.source\n  and xjoin.date = daily.date\nFinally it’s always good to verify the table is actually as expected. Suppose we materialised the above query into daily_full:\nselect device, source,\n       min(date) as first_date, max(date) as last_date, \n       count(distinct date) as dates,\n       count(*) as rows,\n       count(users) as user_rows\nWe would expect the last 3 columns to be the same and equal to the number of days between first date and last date (which you could add with the database specific function to calculate the difference between dates).\nThis is a useful pattern for filling in missing values in tables, and can be used in more complex scenarios where the filled value depends on the parameters."
  },
  {
    "objectID": "ltp-2019/index.html",
    "href": "ltp-2019/index.html",
    "title": "Leading the Product 2019",
    "section": "",
    "text": "Find quick ways of testing difficult and uncertain hypotheses\nJohn Zeratsky talked about the design sprint for implementing a design solution in a week; from storyboarding an experience, to brainstorming solutions to prototyping and testing. The main point is when the value of an idea is uncertain, and implementation is difficult, it’s worth investing in building a “throwaway” prototype to test whether it actually works in practice. This reminds me of Teresa Torres approach to hypothesis testing and of the idea in Douglas Hubbard’s How to Measure Anything of getting even a crude measure of a very uncertain value helps make much better decisions.\nAnother idea on these lines came from Tom Crouch from Qantas Hotels who gave a lightning talk on using Customer Service teams to “fill the gaps” in technology by manually performing a process rather than writing code to automate it.\n\n\nProduct decisions need to drive AI Products\nSally Foote the Chief Innovation Officer at Photobox talked about problems with their first attempt at automatically laying out customers photo books. People use Photobox to create photo books to send to loved ones, and the process typically takes around 40 hours of elapsed time. Sally talked about how they implemented an algorithm to automatically choose photos and layout the book, tests showed customers could create books in a much shorter timeframe. But usage was terrible; most people opted out of the automated experience, and those that stayed in wouldn’t finish their photo book. When doing user research they found out that people actually really enjoy selecting photos, especially the cover photo, and didn’t value a photo that was “picked” for them. They changed their process to make the customer feel more involved with the creation process, while still using the same algorithms to automate the boring bits (selecting the best photo from near duplicates, sizing photos appropriately, laying them out in the right order in the book).\nWhen they couldn’t find the created date of photos the algorithm would dump them at the back of the photo book (in otherwise chronological order) During user research they found this disjointed arrangement shocked people and made them lose trust; they then went back and checked the rest of the book to see if it made any other mistakes.\nWhen building a data driven product you still (unsurprisingly) have to focus on the whole experience. Automating away the things that make people engage with and value your product is a bad idea, and sometimes giving a bad prediction can make them lose all trust. Generally it seems giving appropriate mechanisms for control is important for things considered of value (e.g. providing a list of next best actions rather than just taking an action). They probably could have discovered a lot of this with “mock” automated layouts, rather than after launching the product.\n\n\nUser Research tells a richer story than metrics alone\nSally Foote’s talk covered this pretty well, but it also came from Sherif Mansour from Atlassian. He talked about how they drove up pages created per user by hiding “advanced” types of pages in Jira, but then found later that people weren’t creating advanced types of pages and new users were compaining about the lack of functionality. A lot of this comes down to choosing the right metrics that align to business value, rather than poor proxy metrics; but that’s easier said than done and as Sherif pointed out the “right” metrics can be very hard to move (especially if you can only measure them indirectly or with very low precision). It’s always good to do user testing; the high bandwidth low volume analysis is a great complement to the low bandwidth, high volume analysis from digital analytics (which apparently they hadn’t instrumented anyway…).\n\n\nLeading the Product - worth it\nOverall I’d definitely go again, it was an energising conference that gave me a range of different viewpoints on what it takes to manage great digital products.\nFor more details of the talks checkout their blog."
  },
  {
    "objectID": "sql-sessionise/index.html",
    "href": "sql-sessionise/index.html",
    "title": "Create User Sessions with SQL",
    "section": "",
    "text": "The idea of a session is to capture a continuous unit of user activity. For example if you want to report on whether a visitor to your product bought something, you need to specify the bounds of that opportunity - this is commonly a session. The common way of doing this is to pick some “session time”, typically 30 minutes, and say that if two events occur within the session time it’s the same session. There’s some subtlety to this, for stable reporting you’ll need some way to cap the maximum session time. Google Analytics does this by cutting sessions at the end of a day, Adobe Analytics does this by ending a session after 12 hours.\nThere may also be certain kinds of events you want to consider as a new session. You have a lot of specific knowledge of your domain, and you may have another way or better capturing what an “opportunity” to convert is. But typically the time based approach will be the basis, and you may apply some additional rows.\nThe scenario is you have a user identifier, and a sequence of timestamped events (like in a GA 360 Bigquery Export). You then want to group these into sessions.\n\n\n\nuser\ntime\nevent\n\n\n\n\n1\n2020-08-18T17:00:00\na\n\n\n1\n2020-08-18T17:15:00\nb\n\n\n1\n2020-08-18T18:00:00\na\n\n\n2\n2020-08-18T17:45:00\nc\n\n\n\nThe way you do this is look at the time since the last event for each user; if this is more than 30 minutes then we start a new session.\nselect\n  user, time, event,\n  date_diff('minute',\n            lag(time) over (partition by user order by time),\n            time) as mins_since_last_event\nfrom events\n\n\n\nuser\ntime\nevent\nmins_since_last_event\n\n\n\n\n1\n2020-08-18T17:00:00\na\nNULL\n\n\n1\n2020-08-18T17:15:00\nb\n15\n\n\n1\n2020-08-18T18:00:00\na\n45\n\n\n2\n2020-08-18T17:45:00\nc\nNULL\n\n\n\nWe can get a user specific session id by calculating the cumulative count of new session events.\nselect *,\n  sum(case min_since_last_event is null\n        or mins_since_last_event > 30\n      then 1\n      else 0\n      end) over (partition by adv_user_id order by time) as user_session_id\nfrom (\nselect\n  user, time, event,\n  date_diff('minute',\n            lag(time) over (partition by user order by time),\n            time) as mins_since_last_event\nfrom events\n)\n\n\n\n\n\n\n\n\n\n\nuser\ntime\nevent\nmins_since_last_event\nuser_session_id\n\n\n\n\n1\n2020-08-18T17:00:00\na\nNULL\n1\n\n\n1\n2020-08-18T17:15:00\nb\n15\n1\n\n\n1\n2020-08-18T18:00:00\na\n45\n2\n\n\n2\n2020-08-18T17:45:00\nc\nNULL\n1\n\n\n\nThat’s really all there is to it. To get a Google Analytics style session you would also group by date(time). To get an Adobe Analytics style session requires more work; a simple way would be to do another pass over the sessions creating a new cut whenever we pass 12 hours from the session start time.\nThere are other ways you could customise it. There may be certain events that effectively “reset” the clock; for example in user search sessions leaving the search context may end the session. This could be done by starting a new session depending on the identity of the previous event."
  },
  {
    "objectID": "wait-for-ntp/index.html",
    "href": "wait-for-ntp/index.html",
    "title": "Waiting for System clock to synchronise",
    "section": "",
    "text": "# Wait for ntp to stabilise, so package signatures can be verified\nwhile [[ $(timedatectl status | grep 'System clock synchronized' | grep -Eo '(yes|no)') = no ]]; do\n    sleep 2\ndone\nThis should work on most Linux systems using systemd with ntp enabled (e.g. via timedatectl set-ntp true). Then timedatectl status could update something like the following:\nLocal time: Tue 2020-04-07 11:02:40 UTC\nUniversal time: Tue 2020-04-07 11:02:40 UTC\nRTC time: Tue 2020-04-07 11:02:41\nTime zone: Etc/UTC (UTC, +0000)\nSystem clock synchronized: yes\nsystemd-timesyncd.service active: yes\nRTC in local TZ: no\nThe script above looks for the System clock synchronized line and will wait as long as that line has no in it. If it changes to yes (or if it can’t find that line or the words yes or no in that line) then the script will coninue.\nPutting this at the top of my script before running apt update and apt install commands I ran as soon as an EC2 instance fixed the signature verification issues I had."
  },
  {
    "objectID": "jupyter-missing-repr/index.html",
    "href": "jupyter-missing-repr/index.html",
    "title": "Fixing repr errors in Jupyter Notebooks",
    "section": "",
    "text": "---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nIPython/core/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--> 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\nIPython/lib/pretty.py in pretty(self, obj)\n    392                         if cls is not object \\\n    393                                 and callable(cls.__dict__.get('__repr__')):\n--> 394                             return _repr_pprint(obj, self, cycle)\n    395 \n    396             return _default_pprint(obj, self, cycle)\n\nIPython/lib/pretty.py in _repr_pprint(obj, p, cycle)\n    698     \"\"\"A pprint that just redirects to the normal repr function.\"\"\"\n    699     # Find newlines and replace them with p.break_()\n--> 700     output = repr(obj)\n    701     lines = output.splitlines()\n    702     with p.group():\n\nTypeError: __repr__ returned non-string (type NoneType)\nBy assigning the output, rather than displaying it, the error goes away. If I try to display the output I get the same error again. By running type I can see that it’s a kaggle.models.kaggle_models_extended.ListFilesResult, and by running dir I can see that the two non-special attributes are error_message and files. The files contains the files I want and does have a working repr. So I can monkeypatch the class:\nfrom kaggle.models.kaggle_models_extended import ListFilesResult\nListFilesResult.__repr__ = lambda self: repr(self.files)\nThen displaying the result works just fine in Jupyter."
  },
  {
    "objectID": "github-action-manual/index.html",
    "href": "github-action-manual/index.html",
    "title": "Manually Triggering Github Actions",
    "section": "",
    "text": "There’s a Github blog post on enabling manual triggers with workflow_dispatch. Essentially you just have to add workflow_dispatch to the on sections in the workflow yaml. Once this is done then there’s a UI element on the Workflow in the Actions section in Github that lets you “Run Workflow”.\n\n\n\nGithub Workflow Dispatch\n\n\nHere’s how my workflow on section looks now, having added the last line to allow workflow_dispatch.\non:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n  schedule:\n    - cron: '4 22 * * *'\n  workflow_dispatch:"
  },
  {
    "objectID": "stan-tobit/index.html",
    "href": "stan-tobit/index.html",
    "title": "Tobit Regression in Stan and R",
    "section": "",
    "text": "This article shows coding Tobit Regression in Stan, integrating it into R, showing it works on a simulated dataset and then running it on a real dataset. This is the last in a series of articles on building a Stan model in R, but is mostly independent of them. We’ve fit a linear model in RStan, added priors, and added predictions. Now we’re going to build a non-trivial model to show how to extend this."
  },
  {
    "objectID": "stan-tobit/index.html#background",
    "href": "stan-tobit/index.html#background",
    "title": "Tobit Regression in Stan and R",
    "section": "Background",
    "text": "Background\nThe original impetus for this came from Regression and Other Stories Exercise 15.7, to fit a Tobit regression on the Lalonde dataset. The underlying data comes from Propensity Score-Matching Methods for Nonexperimental Causal Studies by Dehejia and Wahba, who renalyse data from Evaluating the Econometric Evaluations of Training Programs with Experimental Data by Lalonde, based the National Supported Work experiment. From Lalonde’s paper:\n\nThe National Supported Work Demonstration (NSW) was a temporary employment program designed to help disadvantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environment. Unlike other federally sponsored employment and training programs, the NSW program assigned qualified applicants to training positions randomly. Those assigned to the treatment group received all the benefits of the NSW program, while those assigned to the control group were left to fend for themselves.\n\nLalonde presents an estimate of the effect by comparing the treatment to a control group. Then this is compared with stanard econometric techniques of comparing to a broader similar non-control sample (from the Population Survey of Income Dynamics and the Current Population Survey), adjusting for demographic variables with linear regression. The econometric techniques give a very poor estimate of the effect.\nThe Dehejia and Wahba paper use a method of propensity matching to find appropriate comparison groups. It seems like they try to calculate the probability an observation in the comparison group would have been treated using logistic regression on the observed variables. Then they pair samples/controls based on ones that have a similar predicted probability (with or without replacement). They claim this gives similar outcomes results to comparing with the experimental control group.\nThe underlying data is available on Rajeev Dehejia’s website and we will use a prepared sample from Regression and Other Stories."
  },
  {
    "objectID": "stan-tobit/index.html#examining-the-data",
    "href": "stan-tobit/index.html#examining-the-data",
    "title": "Tobit Regression in Stan and R",
    "section": "Examining the data",
    "text": "Examining the data\ntreat: 1 = experimental treatment group (NSW); 0 = comparison group (either from CPS or PSID) Treatment took place in 1976/1977. re74, re75, re78: real earnings in 1974, 1975 and 1978\nWe are going to focus on how the real earnings in 1978, re78 is impacted by the experimental treatment, treat, conditioned on the real earnings the year before intervention, re75. There are 3 very different sample compositions, 1 is people who were treated, 2 is from the Current Population Survey, and 3 is from the Panel Study of Income Dynamics (the Dehejia and Wahba paper find similar subsamples from the broader surveys). In all cases there’s a terminal spike at an earnings of $0, and in the Current Population Survey another terminal spike a bit over $25,000 that indicate censoring.\nlalonde <- foreign::read.dta('https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Lalonde/NSW_dw_obs.dta')\n\nlalonde %>%\ngf_density(~re78 | sample, bw=\"SJ\")\n\n\n\nDistribution of earnings"
  },
  {
    "objectID": "stan-tobit/index.html#fitting-the-model",
    "href": "stan-tobit/index.html#fitting-the-model",
    "title": "Tobit Regression in Stan and R",
    "section": "Fitting the model",
    "text": "Fitting the model\nWe can fit a Tobit model to deal with the censoring from below, saying there is an underlying earnings that is less than 0 that we just can’t measure. I don’t think this would make literal sense in policy circles, because we’re talking about fictional earnings, but there could be underlying validity in that it takes more to get some people to positive earnings than others. In any case we can fit a model, and to deal with censoring in re75 we treat the case of 0 income as another predictor:\nfit_lalonde_tobit_flat <- fit_stan_tobit_lower_flat(re78 ~ re75 + treat + as.integer(re75==0),\n                                               L=0,\n                                               data=lalonde)\nThe coefficients give a large but significant impact on treatment, and the correlation with 1975 income is reasonable (and would be better if we treated that as censored too).\n\n\n\nCoefficient\nmedian\nmad_sd\n\n\n\n\n(Intercept)\n4655\n134\n\n\nas.integer(re75 == 0)\n-4335\n256\n\n\nre75\n0.75\n0.01\n\n\ntreat\n1740\n677\n\n\nsigma\n8744\n51"
  },
  {
    "objectID": "stan-tobit/index.html#assessing-model-fit",
    "href": "stan-tobit/index.html#assessing-model-fit",
    "title": "Tobit Regression in Stan and R",
    "section": "Assessing model fit",
    "text": "Assessing model fit\nThe model fit isn’t great, it has a higher Mean Squared Error than a simple linear regression, and the posterior draws don’t match the data, partly because of the lack of upper censoring, but also because model isn’t concentrated enough.\nposterior_predict(fit_lalonde_tobit_flat, draws=50) %>%\nas.data.frame() %>%\nmutate(draw=row_number()) %>%\npivot_longer(-draw) %>%\ngf_freqpoly(~value, group=~draw, colour='lightgrey', bins=50) %>%\ngf_freqpoly(~re75, group=FALSE, data=lalonde)\n\n\n\nPosterior draws from Tobit model versus data\n\n\nWe can make this more obvious by excluding the CPS with the upper censored data (sample = 2). To improve the model I’d look at making some transformation to the incomes (but Tobit regression giving negative values precludes a straight log or squareroot transformation).\nfit_lalonde_tobit_flat_subset <- fit_stan_tobit_lower_flat(re78 ~ re75 + treat + as.integer(re75==0),\n                                               L=0,\n                                               data=subset(lalonde, lalonde$sample!=2))\n                                               \nposterior_predict(fit_lalonde_tobit_flat_subset, draws=50) %>%\nas.data.frame() %>%\nmutate(draw=row_number()) %>%\npivot_longer(-draw) %>%\ngf_freqpoly(~value, group=~draw, colour='lightgrey', bins=50) %>%\ngf_freqpoly(~re75, group=FALSE, data=subset(lalonde, lalonde$sample!=2))\n\n\n\nPosterior darws excluding CPS\n\n\nIn this particular case I don’t think Tobit regression is fit for purpose, but it’s a new tool I have in my toolbox."
  },
  {
    "objectID": "left-join-on/index.html",
    "href": "left-join-on/index.html",
    "title": "Filtering a left join in SQL",
    "section": "",
    "text": "Suppose you’ve got some tables related to a website. The pages table describes the different pages on the site.\n\n\n\npage_id\npage_name\n\n\n\n\n1\nhome\n\n\n2\ncheckout\n\n\n3\nterms\n\n\n\nAnother pageviews describes the daily activity:\n\n\n\ndate\npage_id\nviews\n\n\n\n\n2020-08-27\n1\n100\n\n\n2020-08-27\n2\n30\n\n\n2020-08-28\n1\n150\n\n\n2020-08-28\n2\n17\n\n\n2020-08-28\n3\n2\n\n\n\nWe want to see the page views with the page name for all the pages on a certain date, even the ones with no views. One wrong attempt would be:\nselect page_name, coalesce(views, 0) as views\nfrom pages\nleft join pageviews on pages.page_id = pageviews.page_id\nwhere date = '2020-08-27'\nThe problem is that the where clause will filter out the terms page from the results.\n\n\n\npage_name\nviews\n\n\n\n\nhome\n100\n\n\ncheckout\n30\n\n\n\nThis could be fixed by moving the where clause into a subquery:\nselect page_name, coalesce(views, 0) as views\nfrom pages\nleft join (\n  select *\n  from pageviews\n  where date = '2020-08-27'\n) pv on pv.page_id = pages.page_id\nThis is verbose, but gives the correct result.\n\n\n\npage_name\nviews\n\n\n\n\nhome\n100\n\n\ncheckout\n30\n\n\nterms\n0\n\n\n\nA very useful trick is to move the filter condition into the join condition.\nselect page_name, coalesce(views, 0) as views\nfrom pages\nleft join pageviews\n  on pages.page_id = pageviews.page_id\n  and date = '2020-08-27'\nBecause the on effectively filters before the output this gives the same result as the subquery:\n\n\n\npage_name\nviews\n\n\n\n\nhome\n100\n\n\ncheckout\n30\n\n\nterms\n0\n\n\n\nAdding filter conditions with on is useful for simplifying filters like this. It’s also sometimes clearer to put filter conditions closer to the table they apply to rather than at the end in a where clause."
  },
  {
    "objectID": "2020-headphones/index.html",
    "href": "2020-headphones/index.html",
    "title": "Bluetooth Headphones in 2020",
    "section": "",
    "text": "Reviews are pretty unanimous that the top of the line over-ear wireless headphones are Bose Noise Cancelling Headphones 700 (retailing around $490), followed by Sony WH-1000XM4 ($410), and finally Jabra Elite 85h ($380). The reviews uniformly agree the Sony has terrible call quality which immediately rules it out, leaving the superlative Bose and the runner-up Jabra (most alternatives have poor call quality).\nHowever there’s a twist, bluetooth calls on PC are often problematic due to the type of bluetooth available in computers. Jabra sells bluetooth headsets, such as, the Evolve2 85 for both computers and mobile phones using a bluetooth adaptor to allow quality calls on the computer. The Evolve2 is a bit more expensive, gets poor reviews and doesn’t have a portable form factor, so it doesn’t meet my specifications.\nIt turns out that the Jabra bluetooth adaptors for PC can be used with other Jabra bluetooth headphones, although it’s not supported by Jabra. Buying the Jabra Elite 85h with the Jabra Link 370 Adaptor should give me the best of both worlds. You can similarly get a compatible bluetooth dongle for the Bose NC700, but it’s hard to find the right sort of bluetooth dongle in Australia (Bose are developing their own but it’s not available yet).\nThe Jabra Elite 85h has a great form factor and design, which I really like. It automatically mutes when you take it off, turns off when you fold it up, has a reasonable carry case and physical buttons. The Bose has touch control which is reportedly a bit finicky, and there’s been some reports of words being gobbled by the fade in and the mute on talk detection being accidentally triggered. While the Jabra doesn’t have quite as good active noise cancelling or sound quality as the Bose, it’s very well designed and has good call quality (for headphones without a boom).\nI ordered them from the Microsoft Store one afternoon and incredibly got them delivered the next morning. So far I’ve found them good, but I’m yet to test them with the Jabra Link 370. Having reading a lot of positive experiences with the Jabra Link on Reddit, I’m expecting it to work well."
  },
  {
    "objectID": "excel-binning/index.html",
    "href": "excel-binning/index.html",
    "title": "Excel Binning",
    "section": "",
    "text": "If you want equal length bins in a Pivot Table the easiest way is with groups. Right click on the column you want to bin and select Group\n\n\n\nGroup option on Pivot Table\n\n\nThen enter the start and end of the bin and the length of the bins.\n\n\n\nGroup parameters\n\n\nYou then get a binned view in the pivot table.\n\n\n\nBinned View\n\n\nWhat about if you want irregular size bins, or outside a pivot table? One way is with nested IF statements, but this is error prone and hard to maintain.\nIF(B2 < 0.1, \"0 - 0.1\", IF(B2 < 0.3, \"0.1 - 0.3\", ...\nAn easier way is to make a separate table for your bins that are ordered.\n\n\n\nBin Table\n\n\nTo get the labels I concatenated the values of the bin endpoints: =CONCAT(F5, \" - \", F6). I also labelled the region as Bins (in the top left) to make it easy to refer to.\nThen you can lookup the value with VLOOKUP. By default it does a RANGE lookup, returning the first row in the table where the value is at least Bin Start.\nFor example I used the formula =VLOOKUP([@Value],Bins,2,TRUE). The first argument is the value to lookup (here I used a column Value of a table, the second is the area we lookup from (the data range Bins), the third is the column number of the table to lookup the value from (here 2, the Bin Value) and finally if we want a range lookup (TRUE - that’s how the binning works).\n\n\n\nBinning with a vlookup\n\n\nSo now you can count your data in custom bins and even plot a histogram over it. The only tricky thing is getting the right order for the bins in pivots/plots (since for example as strings “2” > “10”). I’m not sure if there’s a good way to do this in Excel (like an ordered factor in R)."
  },
  {
    "objectID": "python-not-functional/index.html",
    "href": "python-not-functional/index.html",
    "title": "Python is not a Functional Programming Language",
    "section": "",
    "text": "There is no fundamental definition of a functional programming language but two core concepts are that data are immutable and the existence of higher order functions. Functional languages like Haskell, Clojure and F# differ in many ways, but tend to default to immutable data and use patterns to create new functions from existing functions.\nIn Python most data are mutable by default. While there are some immutable data types like tuple and frozenset, the very commonly used arrays and dictionaries are mutable. These are often the sources of bugs, such as setting an array as the default argument to a function which is unexpectedly modified. It also means that if you pass an array or a dictionary to a function there is always a risk that the function modifies it, which makes it harder to reason about the program.\nSimilarly Python objects are very mutable and open. There’s no such thing as private or final variables; just conventions around naming and some guardrails with __setattr__, but these can always be overridden. This makes Python extremely hackable; you can easily “monkeypatch” objects in a couple of lines of code (as opposed to Java where you’d have to create new files with loads of boilerplate). However this also makes it very hard to understand execution, and there’s always a risk that passed data will be modified.\nIt is always possible to write pure functions in Python, which is useful for testing and makes the programs easier to understand. But this requires a lot of discipline and is not the default way to write functions.\nPython does have first class fictions and support for higher order functions; however it’s unusual to use them extensively and you will hit some barriers if you do. Python allows easy creation of functions with def or with lambda, and they can always be passed as arguments. Python provides map in the standard language, and functools provides more like partial (currying), reduce and there are more complete libraries like toolz.\nHowever composing higher order functions is unusual in Python, and you’re much more likely to see a list comprehension than a map. Probably because of this the tooling is weaker too. Functions defined at inner levels, or returned from functions can’t be pickled, and this can lead to issues with multiprocessing. The resulting functions lose all associated information, such as docstrings and the name of the functions they came from, which makes them very hard to understand compared with explicit functions. Finally when something goes wrong you end up having to debug these undocumented functions and the stack trace becomes very hard to interpret; it’s really hard to determine where the error occurred.\nWhile you can use Python as a functional language, the same way you can use Java as a functional language, it’s not really one. Whereas I’d say Numpy, Pytorch and Tensorflow are real array programming DSLs in Python, I haven’t seen anything I’d want to work with in Python. This isn’t a problem - just use each language to it’s strengths and the right tool for the job."
  },
  {
    "objectID": "jupyter-favicon/index.html",
    "href": "jupyter-favicon/index.html",
    "title": "Setting the Icon in Jupyter Notebooks",
    "section": "",
    "text": "I thought I found a really easy way to set the icon in Jupyter notebooks… but it works in Firefox and not Chrome. I’ll go through the easy solution works in more browsers and the hard solution.\n\nMore Robust Solution: Changing in Developer Console\nThis works in at least Chrome and Firefox, but doesn’t seem to work with Edge. If you open the developer console (in Chrome and Firefox press F12 and select console) in your browser you can change the favicon with a little Javascript; to change it to the file favicon.ico use:\ndocument.querySelector(\"link[rel*='icon']\").href = \"favicon.ico\";\nThe site favicon.cc has a list of top rated favicons and it often has base64 encoded versions you can paste straight in without downloading the image, just replace the file with that image string. For example to get this palm tree you can run:\ndocument.querySelector(\"link[rel*='icon']\").href = \"data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAACj7vf+o+73/qPu9/6j7vf+o+73/mWDo/9hoqv+ernC/nq5wv6e1t7+o+73/qPu9/6j7vf+x61d/q2NLf6tjS3+x61d/setXf6j7vf+o+73/op0jP5lg6P/TmqH/2Giq/56ucL+ntbe/p7W3v7HrV3+x61d/q2NLf6tjS3+rY0t/q2NLf6tjS3+rY0t/q2NLf6KdIz+ZYOj/05qh/+tjS3+rY0t/q2NLf6tjS3+rY0t/q2NLf6tjS3+rY0t/q2NLf7bwXL+28Fy/tvBcv7bwXL+inSM/mWDo/9Oaof/28Fy/tvBcv7bwXL+28Fy/tvBcv7bwXL+28Fy/tvBcv7bwXL+////////////////8taA/vLWgP6KdIz+ZYOj/05qh//y1oD+8taA/vLWgP7y1oD+kNRm/vLWgP7y1oD+8taA/vLWgP7/////8taA/vLWgP7y1oD+8taA/mWDo/9lg6P/TmqH//LWgP6Q1Gb+kNRm/ly1ov9ctWv/8taA/vLWgP6C6M/+gujP/vLWgP6Q1Gb+8taA/vLWgP7y1oD+ZYOj/05qh//y1oD+8taA/ly1ov9ctWv/AY8Z/5DUZv5ctWv/sOiC/gGPGf9ctaL/kNRm/oLoz/7y1oD+NFmA/zRZgP9lg6P/NFmA/zRZgP9ctWv/KaY+/wGPGf8ppj7/sOiC/ly1ov8Bjxn/KaY+/1y1a/9ctaL/gujP/jRZgP8bNE//AY8Z/xs0T/80WYD/XLVr/wGPGf8ppj7/sOiC/vLWgP5ctaL/AY8Z/ymmPv+w6IL+sOiC/immPv8ppj7/AY8Z/wGPGf8Bjxn/XLVr/ymmPv8Bjxn/sOiC/ujo6P7o6Oj+8taA/immPv8Bjxn/KaY+/ymmPv8Bjxn/AY8Z/ymmPv8Bjxn/sOiC/gGPGf8Bjxn/KaY+/7Dogv6C6M/+XLWi//LWgP6C6M/+KaY+/wGPGf8Bjxn/KaY+/1y1a/8ppj7/AY8Z/1y1a/+w6IL+/////7Dogv5ctaL/XLVr///////y1oD+XLVr/1y1ov9ctWv/8taA/rDogv5ctWv/KaY+/wGPGf8ppj7/XLWi/4Loz/4ppj7/AY8Z/7Dogv7/////8taA/vLWgP6C6M/+8taA/oLoz/5ctaL/sOiC/ly1a/8ppj7/AY8Z/ymmPv8ppj7/AY8Z/ymmPv9ctaL/gujP/ujo6P7o6Oj+8taA/vLWgP7y1oD+gujP/ly1ov+w6IL+XLVr/ymmPv8Bjxn/XLVr/ymmPv9ctWv/sOiC/ly1ov//////////////////////8taA/vLWgP7y1oD+gujP/rDogv7y1oD+XLVr/7Dogv7y1oD+sOiC/ly1ov/y1oD+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA==\";\n\n\nEasy way: Doesn’t work in all browsers\nI found in Firefox that I could change the favicon just by putting this into a cell:\n%%html\n<link rel=\"icon\" type=\"image/png\" href=\"/path/to/icon.ico\">\nAgain the reference can be a base64 encoded image as above. Another way to run this without magic cells is:\nfrom IPython.display import HTML, display\ndisplay(HTML('<link rel=\"icon\" type=\"image/png\" href=\"/path/to/icon.ico\">'))\nFor R you have to use the relevant display_html function.\nIRdisplay::display_html(favicon)\nUnfortunately none of these seem to work in Google Chrome (which Google Analytics tells me there’s a 70% chance you’re using it right now). I’m not sure why I can’t change the favicon from Jupyter in Chrome; not even executing the javascript in a cell seems to work. It would be nice to have a cross-browser way to do it within a notebook, because then it could be wrapped in a function that gets a random favicon for easy reuse."
  },
  {
    "objectID": "python-imap/index.html",
    "href": "python-imap/index.html",
    "title": "Reading Email in Python with imap-tools",
    "section": "",
    "text": "You can use Python to read, process and manage your emails. While most email providers provide autoreplies and filter rules, you can do so much more with Python. You could download all your PDF bills from your electricity provider, you could parse structured data from emails (using e.g. BeautifulSoup), sort or filter by sentiment, or even do your own personal analytics like Steven Wolfram.\nThe easiest tool I’ve found for reading emails in Python is imap_tools. It has an elegant interface to communicate with your email provider using IMAP (which almost every email provider will have).\nFirst you access the MailBox; for which you need to get the imap server and login credentials (username and password). You should be able to find this in your email providers help or settings (e.g. here’s a guide for Gmail).\nThen you can search for messages based on RFC 3501 Search Criteria. There are lots of examples in the imap_tools README; you can search based on the sender, subject, text, date and others.\nThen you can access things like the subject, from address, date, and text and HTML content using simple attributes.\nIt also handles actions on emails such as flagging as seen, moving and deleting messages."
  },
  {
    "objectID": "python-imap/index.html#alternatives",
    "href": "python-imap/index.html#alternatives",
    "title": "Reading Email in Python with imap-tools",
    "section": "Alternatives",
    "text": "Alternatives\nPython has the built in imaplib for IMAP and email for processing emails. Unfortunately they’re quite low level and require a bit more work to use than imap_tools.\nimport imaplib\nimport email\n\nmb = imaplib.IMAP4_SSL(server)\nrv, mesasge = mb.login(user, password)\n# 'OK', [b'LOGIN completed']\nrv, num_emails = M.select('Inbox')\n# 'OK', [b'22']\n\n# Get unread messages\nrv, messages = M.search(None, 'UNSEEN')\n# 'OK', [b'21 22']\n\n# Download a message\ntyp, data = M.fetch(b'21', '(RFC822)')\n\n# Parse the email\nmsg = email.message_from_bytes(data[0][1])\nprint(msg['From'], \":\", msg['Subject'])\n\n# Print the Plain Text (is this always the plain text?)\nprint(msg.get_payload()[0].get_payload())"
  },
  {
    "objectID": "python-imap/index.html#dealing-directly-with-mailfiles",
    "href": "python-imap/index.html#dealing-directly-with-mailfiles",
    "title": "Reading Email in Python with imap-tools",
    "section": "Dealing directly with Mailfiles",
    "text": "Dealing directly with Mailfiles\nAnother alternative would be to download all the messages to your filesystem and directly manipulate the files. You could run your own Postfix Server to receive mail, or use isync/mbsync, or it’s slower cousin offlineimap to sync the emails to files like I outline in reading email in Emacs.\nThese are a bit harder; for Postfix you’ve got to make sure your server is up or you’ll lose emails. For mbsync and offlineimap there’s a bit of complex configuration, and if you do it wrong you can do mess up your emails on the server. But if you want to do big batch processing on all your emails this may be an alternative to consider."
  },
  {
    "objectID": "git-vcs-rule-all/index.html",
    "href": "git-vcs-rule-all/index.html",
    "title": "Git: One VCS to Rule Them All",
    "section": "",
    "text": "One of the most popular centralised version control systems is Subversion (SVN), which was largely an improvement of Concurrent Versioning System (CVS). But Distributed Version Control Systems, starting with Git became really popular. With a centralised system you have to lock files on the central server when editing and unlock them when you’re finished, to make sure no one else interferes with your work. With a decentralised system you copy the whole repository locally and then merge together changes after.\nGit made a lot of sense for Linux kernel development, for which it was originally developed. Hundreds of developers were simultaneously trying to work on a codebase, many of whom were loosely coupled. It made sense for them to work on pieces independently, submit their patches in email threads and have upstream maintainers merge them together and resolve any conflicts (where two people work on the same file). Because of the size of the codebase and the social structure of Linux development it was very successful for them.\nI’m really surprised distributed version control systems were so popular in corporations. In a lot of companies small teams are working on a codebase in a highly coordinated way, regularly meeting to discuss development. This means that decentralised development isn’t going to be heavily utilised. However there are some advantages; having local branching and commits allows experimentation that was a bit more expensive in CVS.\nGit can now largely replace Subversion and has ways of working around some of the painful differences. In Subversion you would only checkout the latest version of the code from the server. With Git you get all the history which could be huge, but you can now specify depth for a shallow checkout. For binary and media files you can’t really merge them and are better off with a locking mechanism; but this is provided by Git LFS.\nThere were several competitors that were beaten by Git, most notably Mercurial (hg), but also Bazaar (bzr), and darcs. Mercurial had a nicer user interface than Git, which was a bit more straight forward (but still complex, because distributed version control is complex) and more extensible. But it wasn’t that much better, and I think it lost in open source when Github (which only supports git) became much more popular than Bitbucket (which supported both). Over time more projects moved onto Git (such as emacs), and there are only a few left on other DVCS.\nI think it’s largely good there’s one mainstream version control system. Any large software project has dozens of tools everyone has to be familiar with. Git is a bit of a complex beast, but it’s nearly everywhere so you can at least get payoff for your investment (although I hear in the gaming industry Perforce is more common). There are good porcelains now that hide the complexity, including lots of GUI ones and the excellent magit in Emacs. I’ve heard teams argue for weeks about which branching strategy to use (when really it didn’t matter much in their case); having to choose a version control system is another argument they have to make. Having a sensible default like Git is actually really helpful, one less decision to make."
  },
  {
    "objectID": "duplicate-record-detection/index.html",
    "href": "duplicate-record-detection/index.html",
    "title": "Duplicate Record Detection in Tabular Data",
    "section": "",
    "text": "There’s a whole body of research dedicated to this problem, variously known as record linkage, field matching, data integration, duplicate detection, entity resolution, and about a dozen other things. A great introduction is Duplicate Record Detection: A Survey by Emlagarmid, Ipeirotis, and Verykios. There are effectively 4 steps:\n\nData Preparation: Parsing fields and converting them into a standard format to make them as similar as possible\nIndexing: Efficiently finding pairs of records that are likely to contain matches\nSimilarity Metrics: Measures for how similar two fields are; typical data input errors should give similar scores\nClassification: Classifying records as duplicate based on the similarity metrics\n\nA good tool for this is the Python Record Linkage Toolkit, providing that you can fit all your data into Pandas data frames. It has inbuilt methods for each of the steps and is able to run it all together, along with evaluation on standard datasets.\nThe data preparation steps depend a lot on the data and so they often get the least attention in the literature. However if you know certain kinds of errors are common, like names being abbreviated or differences in casing or switching names, creating the appropriate columns makes the following steps much easier. One technique is phoenetic encoding of string columns such as Double Metaphone and Soundex to make phoenetic variations more similar; for example the double metaphone enocding of both “Chebyshev” and “Tchebysheff” is XPXF. Less data preparation requires better choices in the following steps.\nIndexing is important because comparing all pairs for duplication has quadratic run-time complexity. The most common technique is called blocking; separate the records into disjoint buckets (say by the first 3 letters of the surname) and only compare the records in the same bucket. The run-time complexity is then the square of the size of the largest buckets; picking an evenly-distributed buckets produces the best results. However duplicate records may occur in difference blocks, a common technique is to compare all records across a few different blocking strategies. There are more indexing methods, as covered in A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication by Peter Christen, but none are clearly better than simple blocking. Surprisingly I haven’t seen Locality Sensitive Hashing mentioned as a blocking strategy (I found it useful for finding near duplicate job ads); it works well with q-grams of characters with Jaccard similarity or vector embeddings using cosine similarity.\nThe similarity metrics give scores of how similar two records are. There are many specific algorithms for finding the common types of errors in text. Character based similarity metrics like Jaro-Winkler Distance and Smith-Waterman Distance are often used. Knowing the types of errors in the data the metrics can be tuned appropriately, for example by putting lower weights on an edit distance (casing would be a good example). An alternative is bag of token approaches like q-grams, potentially weighted by TF-IDF, and as mentioned above this can be used for blocking too using Locality Sensitive Hashing. At the end of this we have some vector of differences accross the field that we need to classify.\nThe classification step takes the vector of differences and returns whether they are duplicates (or how likely they are to be duplicate). Here we can use the usual toolbox of machine learning (depending on how much labelled data we have or can acquire), including hard coded rules. The transitivity property means that if A is a duplicate of B and B is a duplicate of C then A is a duplicate of C; this may require some post-processing to decide which of these clusters to use (for joining across de-duplicated datasets often only the best match should be used). There are some advanced methods that take transitivity into account, such as A Bayesian Approach to Graphical Record Linkage and De-duplication.\nIf you want to know more about this the Python dedupe library has good documentation of its process, the US Cenus Beaureau has an Overview of record linkage and current research directions, and Peter Christen has a whole book on Data Matching. Although there’s a lot of decisions, it seems like in many cases understanding your data and choosing the right transformations and heuristics will get you much further than advanced techniques; you could build a reasonable solution entirely in SQL (though you probably shouldn’t)."
  },
  {
    "objectID": "calculate-logs/index.html",
    "href": "calculate-logs/index.html",
    "title": "Calculating Logs",
    "section": "",
    "text": "An easy example of how this works is with the identity \\(2^{10} = 1024 \\approx 1000 = 10^3\\). Using base 10 logarithms this means that \\(10 \\log 2 \\approx 3\\), or \\(\\log 2 \\approx 0.3\\) We can then see because \\(2 \\times 5 = 10\\) that \\(log(2) + log(5) = 1\\), and so \\(log(5) \\approx 0.7\\). And then \\(7^2 = 49 \\approx 50 = 5 \\times 10\\) and so \\(\\log(7) \\approx \\frac{log(5) + log(10)}{2}\\). Using these kinds of rules we can quickly generate a table of logarithms.\n\n\n\nNumber\nLog\nIdentity\n\n\n\n\n2\n0.3\n\\(2^{10} \\approx 10 ^ 3\\)\n\n\n4\n0.6\n\\(2^2 = 4\\)\n\n\n8\n0.9\n\\(2^3 = 8\\)\n\n\n9\n0.95\n\\(9^2 \\approx 8 \\times 10\\)\n\n\n3\n0.48\n\\(3^2 = 9\\)\n\n\n6\n0.78\n\\(2 \\times 3 = 6\\)\n\n\n5\n0.7\n\\(2 \\times 5 = 10\\)\n\n\n7\n0.85\n\\(7^2 \\approx 5 \\times 10\\)\n\n\n11\n1.05\n\\(11 \\times 9 \\approx 100\\)\n\n\ne\n0.43\n\\(e^3 \\approx 20\\)\n\n\n\nSo for example suppose we want to calculate \\(9 ! = 9 \\times 8 \\times 7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2\\). The log is approximately the sum of the corresponding values in the log column; with is 5.56. So the answer is around \\(10^{5.56}\\), which looking in the table is around \\(4 \\times 10^5\\); this is within 10% of the correct answer.\nAnother way to do this is with Stirling’s approximation \\(log 9! \\approx 9 (\\log 9 - \\log e) + \\frac{1}{2} \\log(2 \\pi \\times 9)\\). The first factor is \\(9 \\times 0.52 \\approx 4.7\\), and approximating pi as 3 the second factor is \\(\\frac{0.3 + 0.48 + 0.95}{2} \\approx 0.87\\). So \\(9! \\approx 10 ^ {5.6} \\approx 4 \\times 10 ^ 5\\).\nIn my mind the most remarkable formula is \\(e^3 \\approx 20.086\\). That this number is so close to 20 is a handy coincidence that makes it easy to convert between natural logarithms and logarithms base 10. This is useful because we’re familiar with base 10 exponents, but exponentials and natural logarithms have nice Taylor series for using in approximations. So since \\(10 \\ln(2) \\approx 3 \\ln(10)\\) and \\(3 \\approx \\ln(20) \\approx 1.3 \\ln(10)\\), we then get \\(\\ln(10) \\approx 2.3\\).\nThese simple formulas make calculating products, decimals and roots to one decimal place really easy to calculate by hand or even in the head. It’s also easy to expand these further; composite numbers can be calculated by adding the logs of their factors, prime numbers by generating similar relations like \\(13 \\times 7 = 91 \\approx 9 \\times 10\\), \\(17 \\times 7 = 119 \\approx 4 \\times 3 \\times 10\\), and \\(19 \\times 11 = 209 \\approx 3 \\times 7 \\times 10\\)."
  },
  {
    "objectID": "hil-food-safety/index.html",
    "href": "hil-food-safety/index.html",
    "title": "Human-in-the Loop: Finding Hazards in Food",
    "section": "",
    "text": "Food Safety professionals want to collect data from incident reports about where pathogens or foreign objects have been detected in food.\n\n“I want to maintain a complete record of all recorded food safety incidents in the EU”\n“I want to track when different food safety incidents might have come from the same source”\n“I want to send warnings to specific countries when there are likely to be food safety incidents that have not yet been detected or reported”\n\n\nThe interface has fields for “Hazard”, “Food”, “Origin”, and “Destination” along with a short extract of text from a food report. When you start typing in any of the fields a list of possible completions shows below the text, which can be navigated with arrow keys and selected with enter. You can navigate between the fields using Tab and Shift-Tab, and submit it by pressing Save and then enter. Because Save is a separate process I never felt the need to “undo” an annotation, and the hotkeys were quite good. You could also leave fields blank when there wasn’t the information present; sometimes the origin or destination was missing, or sometimes there wasn’t a hazard (e.g. “suspected fraud”). This seems like a good interface for high precision annotation - it helps you with the task, but isn’t suggestive and you can always override it. There were many cases where I misread a field (e.g. France instead of Finland), and only noticed because of the autocomplete - so it likely increased the precision of my annotations.\nThere were some issues in the user interface that made it a bit harder to use. When I tried to select an autocomplete with an apostrophe it only populated the text before the apostrophe in the field; however because text could be manually entered I could work around this manually (always good to have manual overrides). The autocomplete for the previous field stayed populated when I selected the next field, which made it harder to read and sometimes scrolled past the extract from the food safety report (so I’d have to manually scroll back up again). When the first model trained the suggestions were actually much worse than the ngram matching model; there probably should have been a higher threshold to switch to the model. Also the initial model download blocked all the threads including annotation (with no feedback) which was frustrating. Finally retraining failed after some time so the model stopped getting better; in the console I got the error:\nValueError: Expected input batch_size (48) to match target batch_size (47).\nOverall this was an interesting example of a Human-in-the-Loop model and really demonstrated what you could achieve with this kind of interface. At the bottom it showed other similar annotations (with the same hazard, origin, food, or destination) which could really be useful for a food safety expert to notice linked outbreaks. The ambiguity in the task quickly became clear (do I annotate the common name, or the scientific name or elaboration?), which would need refinement with many annotators. My main lesson is the annotator experience of using these tools is crucial; if you want people to spend a long time in these tools you need to get these tools into people’s flow and make them as enjoyable to use as possible, even small frictions cause pain. More importantly there should always be fallbacks; being able to override the autocomplete allowed working past bugs that would otherwise completely block the process."
  },
  {
    "objectID": "not-scrapy/index.html",
    "href": "not-scrapy/index.html",
    "title": "Not Using Scrapy for Web Scraping",
    "section": "",
    "text": "I really like Zyte (formerly ScrapingHub) the team behind Scrapy. They really know what they’re talking about with great blogs about QA of Data Crawls, guide to browser tools, how bots are tracked, and Scrapy’s documentation has a very useful page on selecting dynamically-loaded content. They’ve also released a huge number of great open source tools; parsel combines the best of lxml and BeautifulSoup for extracting from HTML, extruct is fantastic for extracting metadata, a disk based queue library, an implementation of simhash, and flexible parsers for dates, prices, and numbers.\nBut I find Scrapy itself to be a magic monolith; if you try to use it in ways that aren’t intended it’s quite difficult. Suppose that you want to extract all the product pages from a large website exactly once, and new products are being added over time. You get to the product pages through category pages that are updated when new products come in. It’s not at all obvious how to do this in Scrapy. With the default settings you will rescrape the pages every time (which may not be feasible or economical, or result in some pages being missed). You can enable a job queue, which as a side effect keeps a list of visited pages that it doesn’t revisit. But you want to revisit the category pages; you can do this by setting a custom DUPEFILTER_CLASS or passing dont_filter as True in the Requests. However you really only want to revisit the category pages after some time interval; there doesn’t seem to be an easy way to do this and maybe you need to manage the state yourself in your spiders using an external database (which could be error prone!). After spending quite a bit of time looking into this I stumbled across the DeltaFetch middleware which sounds like it can handle this case (by not returning items on the category pages, but on the product pages).\nRather than providing functionality Scrapy seems to lock you into using their tooling. Extracting data from a webpage is inherently experimental and iterative. They’ve got scrapy shell which makes it easier to do some experimentation with IPython, but then you need to copy all your selectors into a file. Ideally I’d use something like nbdev to iterate in a Jupyter notebook and keep the examples and experimental code. I can hack together examples like below, but I can’t work out how to fetch a page with through Scrapy (the equivalent of scrapy fetch).\nfrom scrapy.http import TextResponse\nimport requests\n\nurl = 'http://quotes.toscrape.com/page/1'\nres = requests.get(url)\nresponse = TextResponse(res.url, body=res.text, encoding='utf-8')\n\nresponse.css('title::text').get()\nScrapy assumes you’re wanting to do everything; from getting URLs to parsing and processing at once. But as I outlined in web scraping architecture, I think it’s a lot safer to download the data as WARC and then separately extract and normalise the data; moreover this is a great way to write test cases. If you make a mistake in your extraction code you can raise an error that brings down the scraper. It’s possible to do this in Scrapy, either with a custom downloader middleware, or by directly invoking the parse methods from the spiders, but its clunky which makes reprocessing hard. It sounds like web-poet aims to help separate the extraction from I/O, but it’s in early stages.\nI think there are a lot of cases where Scrapy is a really good choice. When it doesn’t fit it seems much harder to bend it to your will than to cobble together a solution in Python. Maybe for very large scale projects (or where fast scraping is required) it’s worth the investment in Scrapy, but some of the design decisions make it painful for the way I want to use it and for small projects I’m not convinced it’s worth the effort."
  },
  {
    "objectID": "typer-command-order/index.html",
    "href": "typer-command-order/index.html",
    "title": "Setting the Order of Commands in Typer",
    "section": "",
    "text": "There’s a simple solution for this in Python 3.6+ (for older versions outside of C Python you may need a more elaborate version with OrderedDict).\nimport typer\nimport click\n\nclass NaturalOrderGroup(click.Group):\n    def list_commands(self, ctx):\n        return self.commands.keys()\n\napp = typer.Typer(cls=NaturalOrderGroup)\nThat’s all there is to it, with that small change the commands will appear in the same order they are written in the code."
  },
  {
    "objectID": "topic-model-bootstrap/index.html",
    "href": "topic-model-bootstrap/index.html",
    "title": "Topic Modelling to Bootstrap a Classifier",
    "section": "",
    "text": "Creating a classification may sound easy until you try to do it. Think about novels; is a Sherlock Holmes novel a mystery novel or a crime novel (or both)? Or do we go more granular and call it a detective novel, or even more specifically a whodunit? The answer depends on what you’re trying to do; but it’s often not obvious from the outset.\nTopic modelling is a great way to quickly experiment with different classifications. In my experience Gensim’s LDA model works very well on short and very short texts (think things like survey responses and reviews). Once you’ve proved out a basic idea and got it working you can then work towards a more sophisticated classification.\nWhile there may be ways to automatically evaluate the number of topics, it’s often easy just to inspect it by hand. Try a few different numbers of topics and look at the top words and documents in each topic. If many topics contain a mixture of concepts then try increasing the number of topics. If many topics contain similar concepts then try decreasing the number of topics.\nLDA is a bag of words model, so it’s important to normalise words to their root form (e.g. removing inflections). Sometimes, especially with very short texts, you will get the same theme represented across multiple topics when people use different words to describe the same concept. This can be handled by building a grouping over the topics, and then summing (or perhaps averaging) the topic scores over the group.\nOnce you’ve got to a reasonable output you can label each of the topics (or groups of topics), and you’ve got an initial classification and classifier. You can use this to determine whether the model has actual value before you invest in improving the classification, labelling and building a classifier."
  },
  {
    "objectID": "diagrams-in-hugo/index.html",
    "href": "diagrams-in-hugo/index.html",
    "title": "Diagrams in Hugo with Mermaid",
    "section": "",
    "text": "In particular I want to render some factor tree diagrams of the style of The Art of Insight. Like this one:\n\n\n\nFactor tree\n\n\nThe final result looks like:\n\n\n\n\ngraph LR;\n   A[sheets ream<sup>-1</sup> <br> 500] -->|-1| B[thickness <br> 10<sup>-2</sup>cm <br>] \n   C[thickness ream<sup>-1</sup> <br> 5cm] --> B\n   B --> D[volume <br> 1cm<sup>3</sup>]\n   E[height <br> 6cm] --> D\n   F[width <br> 15cm] --> D\n\n\n\n\n\n\n\n\n\nImplementation\nI copied the Mermaid Hugo shortcode from the learn theme and put it in layouts/shortcodes/mermaid.html.\n{{ $_hugo_config := `{ \"version\": 1 }` }}\n<div class=\"mermaid\" align=\"{{ if .Get \"align\" }}{{ .Get \"align\" }}{{ else }}center{{ end }}\">{{ safeHTML .Inner }}</div>\nThen following the mermaid documentation I inject the script into a template. For the hugo-casper3 theme I do this by making a copy of layouts/partials/site-header.html and adding the script to the top of the template.\n<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}});</script>\nThen I can just include a diagram in any markdown document by putting the following between <mermaid> tags in handlebars.\ngraph LR;\n   A[sheets ream<sup>-1</sup> <br> 500] -->|-1| B[thickness <br> 10<sup>-2</sup>cm <br>] \n   C[thickness ream<sup>-1</sup> <br> 5cm] --> B\n   B --> D[volume <br> 1cm<sup>3</sup>]\n   E[height <br> 6cm] --> D\n   F[width <br> 15cm] --> D\n\n\nDiscussion\nIdeally I would use Hugo to prerender the diagrams but unfortunately it’s not possible at this stage. I could use Blogdown to generate the diagrams with R, but I don’t want to have to write all my posts in RMarkdown. So instead we have to use client side rendering of diagrams, and R Markdown is a good example of this.\nI found the solution with Mermaid in Julian Knight’s article about Embedding diagrams in a Hugo Page, and saw the implementation in Learn theme.\nNote that the securityLevel: 'loose' configuration above allows representing HTML inside the diagram.\nIt would be better for stability to host the Mermaid files locally, rather than using the CDN above. For rendering speed it would be better in the footer than the header. But as a quick hack this seems to work well enough."
  },
  {
    "objectID": "speaking-quota/index.html",
    "href": "speaking-quota/index.html",
    "title": "Speaking Quota",
    "section": "",
    "text": "I got the idea from a former management consultant, who when he was a junior was told he was only allowed to say one thing in a meeting. The idea was to make sure when he said something that it was actually worth saying. Apparently he resented it, quit and started a successful company; but I think the idea, especially when self-imposed has merit.\nIn a meeting I’ll let myself interject only three times. This gives me three opportunities to ask a question, raise an objection or make a point. Of course I’ll speak when directly asked, but I won’t interrupt the flow of the meeting.\nJust by rationing my interjections I am forced to consider what outcomes I want from the meeting and to focus on those. It means there’s more space for other people to talk, which is helpful for the quieter people in the team. If there’s a minor point worth following up on I can make note of it and do it later."
  },
  {
    "objectID": "rstan-predictions/index.html",
    "href": "rstan-predictions/index.html",
    "title": "Making Bayesian Predictions with Stan and R",
    "section": "",
    "text": "In principle making predictions from our linear model \\(y \\sim N(\\alpha + \\beta x, \\sigma)\\) is easy; to make point predictions we take central estimates of the coefficients \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) and estimate \\(y \\approx \\hat{\\alpha} + \\hat{\\beta} x\\). If we want to represent our inferential uncertainty we could take posterior draws of the coefficients and simulate the random normal samples rnorm(nrow(x), alpha + x %*% beta, sigma). However the stanfit object doesn’t have enough information on how to do this; we’re going to have to add it. And ideally we’d be able to use R’s inbuilt predict function like with other models.\n\nDispatching predictions\nTo be able to use predict we’re going to need to return an S3 class containing all the relevant data. We can call the class whatever we want, for the sake of argument I’m going to call it my_linstan (my linear stan model). We can also package, as well as the stanfit object, any other relevant information we need for predictions. All we need to do is replace the fit returned at the end of our R functions like fit_stan_linear with:\nstructure(list(fit=fit, terms=terms(formula, data=data), data=data), class=c(\"my_linstan\"))\nWe can then delegate standard functions like print, as.matrix and as.data.frame to the stanfit model:\nprint.my_linstan <- function(object, ...) {\n    print(object$fit, ...)\n}\n\nas.matrix.my_linstan <- function(object, ...) {\n    as.matrix(object$fit, ...)\n}\n\nas.data.frame.my_linstan <- function(object, ...) {\n    as.data.frame(object$fit, ...)\n}\n\n\nDealing with Formulas\nOne of the tricky things here is we’re fitting the model with a formula, and to predict we’re going to have to understand how to use that formula on a potentially different dataset. R formulas are very flexible in that you can include things like transformations of variables like I(x^2), interactions like x:y, one-hot encoding categorical variables, and even infer the rest of the variables like .. I found it hard to get my head around how to manipulate formulas, but Data Camp’s tutorial on R Formulas is a good place to start.\nConsider a formula like y ~ .. The formula depends on the context, on a dataframe containing x and y it expands to y ~ x; but on a dataframe containing columns v, w and x, and y it expands to y ~ x + v + w. So when we first fit a model we can’t just capture the formula but the context of the data it refers to.\nThe terms function, mentioned above, does exactly this, and encodes a lot of useful information in attribures like whether it has an intercept (using attr(., \"intercept\")) which is much better than the method I used before of looking for (Intercept) in the terms matrix (in the pathological case when a variable is named (Intercept)). So when the model is fit we can capture the terms using terms(formula, data=data), remove the response with delete.response, and then get the model matrix for new data using model.matrix with the new data. Finally we can multiply it with the relevant coefficients from the model.\nPutting this together we can define a predict function:\npredict.my_linstan <- function(object, newdata=NULL) {\n    if (is.null(newdata)) {\n        newdata = object$data\n    }\n\n    mm <- model.matrix(delete.response(object$terms), data=newdata)\n\n    coef_matrix <- as.matrix(object$fit)\n    # Calculate the central coefficients\n    coefs <- apply(coef_matrix, 2, median)\n\n    # Calculate b * x\n    preds <- (mm %*% coefs[colnames(mm)])[,1]\n    unlist(preds)\n}\n\n\nPosterior Draws\nFor making posterior draws we can define posterior_predict in a way compatible with rstanarm.\nposterior_predict <- function (object, ...)\n{\n    UseMethod(\"posterior_predict\")\n}\nThe calculation is similar to predict, but we can take a sample of length draws of the posterior coefficient estimates. Then we can calculate the expected prediction by matrix multiplication, and add random normal noise with the posterior sigma. To make the calculation easier we can make a helper function that takes a random sample matrix centred at a 2 dimensional matrix mean, with a row vector sd.\nrnorm_matrix <- function(mean, sd) {\n    stopifnot(length(dim(mean)) == 2)\n    error <- matrix(rnorm(length(mean), 0, sd), ncol=ncol(mean), byrow=TRUE)\n    mean + error\n}\nFor example rnorm_matrix(matrix(c(0,1,0,0), ncol=2), c(0,1)) gives something like\n\n\n\n0\n0.342\n\n\n1\n-0.743\n\n\n\nposterior_predict.my_linstan <- function(object, newdata=NULL, draws=NULL) {\n    if (is.null(newdata)) {\n        newdata = object$data\n    }\n\n    mm <- model.matrix(delete.response(object$terms), data=newdata)\n\n    coef_matrix <- as.matrix(object$fit)\n    if (!is.null(draws)) {\n        coef_matrix <- coef_matrix[sample.int(nrow(coef_matrix), draws),]\n    }\n\n    point_preds <- coef_matrix[,colnames(mm)] %*% t(mm)\n    # Note this could do the wrong thing if \"sigma\" is a coefficient\n    preds <- rnorm_matrix(point_preds, coef_matrix[,\"sigma\"])\n\n    preds\n}\nFinally we can use this to, for example, compare the distribution of the response variable from the model to the actual data.\nfit_mtcars <- fit_stan_linear(mpg ~ ., data=mtcars)\n\nposterior_predict(fit_mtcars, draws=50)  %>%\nas.data.frame() %>%\nmutate(rn=row_number()) %>%\npivot_longer(-rn) %>%\ngf_freqpoly(~value, group=~rn, bins=10, colour='grey') %>%\ngf_freqpoly(~mpg, group=FALSE, bins=10, data=mtcars)\n\n\n\nPosterior predictive check\n\n\nPutting in the effort to make it easy to fit and make predictions with these models requires some effort, but then we can just focus on the modeling task. In the next article we will extend linear model to deal with censored variables with a Tobit regression and can use these tools to analyse the results."
  },
  {
    "objectID": "latex-multiple-equations/index.html",
    "href": "latex-multiple-equations/index.html",
    "title": "LaTeXing Multiple Equations",
    "section": "",
    "text": "There are a lot of resources for learning \\(\\LaTeX\\) on the web, and a lot of people teach themselves from this (I know I did), but this can get you into some bad habits. For instance eqnarray gets the spacing around the equals signs all wrong.(I typeset my thesis using exclusively eqnarray and didn’t notice this until it was pointed out to me). So a lot of people advocate align from AMSTeX, but align has it’s limitations too; it only comes with one alignment tab &. If you want to make a comment at the end of multiple equations (like “for \\(x \\in X\\)”) or you want to have two equations and the second one breaks over two lines you can’t line the equations up properly; but there is a solution – IEEEeqnarray (which is an external class, IEEEtrantools, available from the IEEE). Stefan Moser has written an excellent paper covering everything I’ve said and much more, showing good ways to typeset equations.\n\nAn interesting thing he points out is that \\(\\TeX\\) sometimes typesets + as a binary operator, as in \\({} + a\\) or as a unary (sign) operators as in \\(+ a\\) (note the extra spacing between the + and the a in the first example). The most common example of where + is typeset as a binary operator is when the thing following the + is a mathematical operator \\… e.g. \\(+ \\sin(x)\\) as opposed to \\(+ {\\sin(x)}\\).\nYou can do some fiddling to trick \\(\\LaTeX\\) ; indeed that’s how I produced the output above. Inserting an invisible character {} before the + sign makes \\(\\TeX\\) treat + as a binary operator with more spacing preceding it. Encapsulating an operator in curly braces, like {\\sin(x)}, means if + is the first thing on the line it is treated as a unary operator. But these fixed aren’t perfect in particular the invisible character trick isn’t good if we’re trying to break an equation over multiple lines (see Moser’s paper for more details); \\({} + {\\sin(x)}\\) = \\({} + \\sin(x)\\) is different to \\(+ \\sin(x)\\) – try it.\nSome readers may think I’m being overly pedantic about a small space (1/2 of a quad), but \\(\\TeX\\) advocates market it on its consistency in typesetting – these examples fail this and help you typeset inferior documents if you’re not careful.\nThere are many Integrated Development Environments for \\(\\TeX\\) programs (like TeXworks) but the only one I know that lets you preview equations (it does a little compile of just the equations) is emacs with auctex.\nIncidentally there are other addons to \\(\\TeX\\) around – e.g. ConTeXt which appears to allow you to customise the layout of your document more easily than \\(\\LaTeX\\) (which could be very useful in some situations; for instance if you want to wrap text around a figure), but at the cost of the author having to put more effort in laying out the document. As it happens, the standard way of producing multiple equations in ConTeXt seems to be as good as IEEEeqnalign. But while most scientific journals are still using \\(\\LaTeX\\) so will most scientific authors."
  },
  {
    "objectID": "using-excel/index.html",
    "href": "using-excel/index.html",
    "title": "A programmer using Excel",
    "section": "",
    "text": "The value of ubiquity\nMost of my stakeholders can open, use, and understand any spreadsheet I send them. They’re intimately familiar with the interface, and they can create new calculated fields and graphs to put in presentation packs. Many of them can follow the logic at a high level and check intermediate steps of the model. When they can obtain new data then they can update data extracts in the sheet by themselves.\nAs a developer analyst this means many fewer iterations to get to a usable result, and a much lower setup cost than a custom software solution. There are no servers to maintain, few user interface concerns and no barrier to entry installing software. I don’t need to iteratively improve the user interface to add the right graphs and summaries, they can do it themselves. I still need to make sure the model is being understood and my stakeholders are asking the right questions to inform their decisions, but there’s a much lower development cost.\n\n\nVersatility of Calculations\nExcel has a lot of built in functionality that lets you build a variety of calculations. From Boolean logic, trigonometry and statistical distributions to substrings and date handling there’s a lot you can do (although the correctness of statistical procedures isn’t perfect, it has gotten better). With VLOOKUP (or better INDEX and MATCH) you can join data between different tables. With pivot tables you can do pretty much anything you could do with a single “Group By” aggregation in SQL (and with a Data Model you can do the equivalent of complex joins).\nYou can also store sets of parameters using scenario manager, solve equations with Goal Seek or optimise equations with the Solver add-in.\nYou can even define custom functions, macros or automate things with VBA. However executing custom code someone has sent you is a huge security risk, and many office viruses have been spread through macros and VBA scripts, so most organisations (wisely) disable them. The primary benefit of using Excel is it’s easy and convenient; if your clients need approval from the security team to access your spreadsheet it’s probably better to use a different solution. They also raise the barrier to entry; unless your function is clearly named and documented it’s not going to be clear to most Excel users what it’s doing, and it’s going to be hard for them to modify it.\n\n\n\nExcel Ribbon showing Macros have been Disabled\n\n\n\n\nClarity of visual calculations\nA well laid out workbook can make the flow of calculation visible making it more transparent and easier to see errors. Most programming environments require an interactive debugger or trawling through debug level logs to see the flow of calculation. I’ve often had domain experts notice things in intermediate calculations that showed that the data or the model were wrong, and that weren’t obvious from the final result alone.\nThe logic can be very clear if you use named formulas so that you can have a formula for Tax like = Salary * TaxRate instead of an opaque calculation of Tax as =C2 * $B$24. Using the shortcut CTRL + Backtick (Grave Accent) you can display formulas instead of results to follow the logic of calculations.\nHowever it does take good judgement and discipline to know how to layout a calculation and to use well named fields and ranges. It’s also easy to break a calculation by changing a cell or inserting a row, and when the spreadsheet doesn’t all fit on a single screen it becomes hard to spot. As a model grows it takes up more space over more worksheets, it becomes harder to see how everything fits together, and becomes difficult to understand and maintain.\n\n\nSpreadsheet Maintenance\nSoftware that grows needs to be trimmed and pruned to keep it healthy and prevent it from growing out of control. Excel’s low barrier to entry is great for starting out, but undisciplined use makes it really hard to keep working (and updating) correctly. Spreadsheets tend to be quite fragile, and small changes like adding a row can easily break it in subtle ways. Tools like tables can help, but you have to be careful to design robust spreadsheets.\nUpdating data can be a problem because Excel tries to automatically detect the type of data which can mangle it; one-fifth of biology papers using Excel contain an error of this sort. Excel doesn’t have a method of version control that lets you compare changes easily, so it’s very hard to find out if something has been broken, especially in a spreadsheet too large to fit in a couple of screens. Once a workbook gets to a sufficient complexity it becomes hard to keep track of what’s going on.\n\n\nLack of Abstraction\nAbstraction is one of the most powerful tools for building software, and spreadsheets don’t really have them. It’s useful to package data transformations in a function/method/subroutine that can be tested and maintained in isolation, and reused in other places. This allows incrementally building complex software from small pieces that can be more easily verified and changed independently.\nSpreadsheets naturally have calculations that build on top of other calculations, making long chains that can’t be broken into separate pieces. If you want to make a change in the middle of the calculation you typically have to then check every step afterwards (and may need to change steps prior to get the data in the right formation). If you’re working in an organisation where custom functions and VBA are allowed then it is possible to build abstractions, and reuse them via add-ins. But this doesn’t work if you’ve got security considerations, and you still have to handle versioning and distribution. There are also add-ins you can buy for advanced analytical procedures like resampling and forecasting; but the ecosystem is nowhere near as rich as R or Python, where the functions compose much more easily.\n\n\nLimitations\nExcel has some hard limitations which can make certain things impossible. It can take at most 1 million rows and 16 thousand columns per worksheet, which makes it unsuitable for working with larger datasets (although a lot of the time you can get away with sampling). Spreadsheets don’t natively have data structure concepts like HashMaps or Binary Trees and so lookup functions like VLOOKUP and MATCH run very slowly on bigger worksheets.\nWhile there’s a lot of control over the user interface in Excel like locking or validation, and you can build simple dashboards with slicers it always looks like Excel (businessy and slightly dated), which might not be what you want.\nFrankly you can do a lot of things in Excel (especially with add-ins), but once you get past the core functionality the difficulty rises steeply and returns diminish quickly. Making systematic changes to code in text is easy; in Excel it’s possible (modern versions use XML) but it’s really easy to introduce inconsistencies.\n\n\nWhen should you consider Excel\nExcel is manageable in the small, but really difficult to manage and maintain in the large. The benefits of being able to create something quickly that can be used and improved by less technical stakeholders can lead to some quick wins. But when you need something complex, maintainable, automated or robust it’s worth moving to a programming language with good libraries and using testing and version control. When you just need a pretty reporting tool for simple aggregations that can be maintained by non-programmers then consider a tool like Tableau or PowerBI.\nLooking back at my work experience, for an agricultural economist running a sole practice, Excel was the right choice at the time. He could single-handedly create, update and interpret a functional application that his clients could interact with. It let him develop and iterate on his product quickly until it was fit for his market, without having to translate his requirements to an external software developer. He has since turned it into a Software as a Service, which would have been much easier since he’d already tested the market. Looking back I should have been paying more attention to how he was helping his clients grow their businesses rather than getting arrogant with his choice of tools.\nIf you want to learn how to use Excel more effectively then let Joel Spolsky tell you why “You Suck at Excel” and check out how PwC does Excel."
  },
  {
    "objectID": "hardware-is-hard/index.html",
    "href": "hardware-is-hard/index.html",
    "title": "Hardware Is Hard",
    "section": "",
    "text": "Getting an LED blinking feels like a huge accomplishment. Reading the BCM2835 Arm Peripheral Documentation gives most of the information you need (with a little black magic to get started), but it’s rather dense. There’s a complete lack of feedback when something goes wrong, and the cycle between write, compile, save to SD card, insert in raspberry pi and book takes a little time. The whole process takes a lot of persistence.\nThen showing something on the screen requires understanding the interface between the GPU and the CPU, but the closest thing I can find to an official resource is a Wiki pages in Raspberry Pi Firmware git repository. I suspect a lot of experimentation and reverse engineering have gone into making this work.\nFinally taking input from a USB keyboard requires interfacing with the USB which doesn’t have much documentation. Moreover the USB specification is huge and complex; it’s got a lot of flexibility but the cost is software complexity. I suspect a lot of this came from looking at the open source Linux implementation as a starting point.\nIt really makes me consider the amount of time, engineering and coordination that has gone into something like Linux (or the BSDs). Implementing the hardware implementations for different types of computers and systems, all the drivers for external peripherals (so when you plug in a printer you can print), interfacing with graphics cards, as well as managing processes and everything else."
  },
  {
    "objectID": "derivative/index.html",
    "href": "derivative/index.html",
    "title": "Differentiation is Linear Approximations",
    "section": "",
    "text": "In this context a function is something that maps between coordinate spaces. For example consider an image classifier that takes a 128x128 pixel image with three channels for colours (Red, Green, Blue) and returns a probability that the image contains a cat and the probability that the image contains a dog. Then this could be considered as a map from a 128x128x3 dimensional space to a 2 dimensional space. In practice each pixel can only take a finite range of values for each colour (say 256), but it’s useful to think of it as a continuous range that we’ve finitely sampled.\nIt’s really hard to visualise high dimensional spaces, so it’s useful to think about examples in 3 dimensions. A function from 2 dimensions to 1 dimension can be visualised like the elevation of landforms, like you would find in a topographic map. As you move East/West or North/South you ascend or descend through the hills. The derivative is the tangent plane at any point; it represents how much the height would change if you made a small step in any direction.\n\n\n\nPlane tangent to a curve\n\n\nConcretely we have a function f mapping an m dimensional space to an n dimenstional space, and want to approximate it near some point v of the input space. We take a point near v by adding small m dimensional vector h and try to find the best linear approximation. The derivative \\(df_v\\) is a linear function such that \\(f(v + h) \\approx f(v) + df_v(h)\\), where the approximation is better the smaller h is.\nNote that if the derivative is 0, that is the tangent plane is horizontal, then any small step won’t change the height. This happens whenever you are at the top of a hill or the bottom of a valley; called a local extrema.\n\n\n\nLinear approximation of exp\n\n\nA fundamental property of the derivative is the chain rule; which tells how to calculate the derivative of a composition of functions. It essentially says the derivative of a composition is the composition of the derivatives; that is the matrix product of the linear derivative matrices. It’s pretty straightforward to prove: \\(g(f(v + h)) \\approx g(f(v) + df_v(h)) \\approx g(f(v)) + dg_{f(v)}(df_v(h))\\). A short notational way to put this is \\(d(f\\circ g)_{v} = dg_{f(v)} \\circ df_v\\).\nThis is a really useful tool for calculating derivatives in practice. It can be efficiently implemented in Automatic Differentiation available in libraries like Tensorflow and Pytorch.\nThe standard rules for calculating derivatives can be obtained from the chain rule. For example the multiplication rule is \\(d(f \\cdot g)_v = g(v) df_v + f(v) dg_v\\). Multiplication is actually a function that takes 2 numbers and returns one number and we can directly calculate the derivative. \\[(x + \\delta_x) \\cdot (y + \\delta_y) = x \\cdot y + y \\delta_x + x \\delta_y + \\delta_x \\delta_y\\] and so \\(d \\cdot_{(x,y)} = \\begin{bmatrix} 0 & x \\\\ y & 0 \\end{bmatrix}\\). Then the multiplication rule immediately follows from the chain rule.\nNote that the definition of the derivative doesn’t explicitly refer to the coordinates. There are always multiple ways to coordinatise a space, but the tangent at a point is always the same thing. The input image in the image classification problem could also be represented as a series of cosine waves with different frequencies, as is done in JPEG images. The choice of coordinates is called a basis; it’s computationally important but not important for calculating a derivative.\nOne practical use for this is gradient descent. In our hill example, suppose we want to get to the bottom of the valley. Gradient descent says take a step in the steepest direction downwards to get towards the bottom. It won’t always work, and won’t always get to the lowest point as you may get stuck at the bottom of an elevated basin. However it will often get you to a local minimum and can be adapted in various ways to improve performance.\n\nConsider our image classifier; suppose we have some model (say a neural network) that takes an image and returns the probability. The model has a bunch of variable parameters and we want to fit it by optimising those parameters to the data. We define the loss of a model by how far the predictions are from the data; it gives some number. To minimise this loss and get the best model we can vary the parameters by gradient descent to get to a local minimum."
  },
  {
    "objectID": "rewrite-of/index.html",
    "href": "rewrite-of/index.html",
    "title": "Rewriting A of B",
    "section": "",
    "text": "Here’s my first cut of an algorithm:\nimport re\ndef rewrite_of(term):\n    word = r'[\\w\\d&]+' \n    next_word = fr'(?:\\s+(?!for\\s|or\\s){word})'\n    following = fr'{word}{next_word}' + '{0,3}'\n    regex = fr'({word})\\s+of(?:\\s+the)?\\s+({following})'\n    return re.sub(regex, r'\\2 \\1', term, flags=re.IGNORECASE)\nNote the negative lookahead onfor and word to stop getting too much, and similarly for word excluding punctuation.\nI’m not sure the regex approach is the best long term, but it’s a useful starting point. Iterating on this maybe me build a list of test cases, since a small change in the regular expression can make a big change in the output.\nThe easiest way to manage this is to build a suite of tests that check the result is as expected."
  },
  {
    "objectID": "beta-function/index.html",
    "href": "beta-function/index.html",
    "title": "Beta Function",
    "section": "",
    "text": "The beta function is given by \\(B(a, b) = \\int_0^1 p^{a-1}(1-p)^{b-1} \\rm{d}p\\) for a and b positive. If you have \\(N\\) flips of a coin of which \\(k\\) turn heads the likelihood is proportional to \\(p^{k}(1-p)^{N-k}\\) for the probability p between 0 and 1. So the beta function can be seen as the normaliser of the likelihood, with \\(a = k + 1\\) and \\(b = N - k + 1\\) (or inversely \\(k = a - 1\\) and \\(N = a + b - 2\\)).\nThe integral can be evaluated directly when b is 1: \\(B(a, 1) = \\int_0^1 p^{a-1} = \\frac{1}{a}\\).\nUsing Integration by Substitution of p with 1-p gives \\(B(a, b) = B(b, a)\\). This makes sense because in the binomial distribution 0 and 1 are just labels, and if we switch the labels we should get the same overall normalisation.\nUsing Integration by Parts \\(B(a, b+1) = \\int_0^1 p^{a-1}(1-p)^{b} \\rm{d}p = \\Big[\\frac{p^a(1-p)^b}{a}\\Big]_0^1 + \\frac{b}{a} \\int_0^1 p^{a} (1-p)^{b-1}\\). The first term on the right hand side evaluates to 0 for all positive a and b giving \\(B(a, b+1) = \\frac{b}{a} B(a+1, b)\\).\nThis identity can be repeatedly applied to reduce b to 1 when it is a positive integer.\n\\[B(a, m) = \\frac{m-1}{a} B(a+1, m-1) = \\frac{(m-1)(m-2)\\cdots 1}{a(a+1)\\cdots (a+m-1)} B(a+m-1, 1)\\]\nAnd so this gives:\n\\[B(a, m) = \\frac{(m-1)!}{a(a+1)\\cdots (a+m-1)(a+m)}\\]\nMultiplying both sides by \\((a-1)!\\) gives:\n\\[B(a, m) = \\frac{(m-1)! (a-1)(a-2)\\cdots 1}{(a+m)(a+m-1)\\cdots 1}\\]\nAnd so when a is also and integer this gives a simple form:\n\\[B(n, m) = \\frac{(m-1)! (n-1)!}{(n+m-1)!} = \\frac{1}{(n+m-1) {n+m-2 \\choose n-1}}\\]\nOr in terms of the number of flips N of a coin and the number of positive results k:\n\\[B(k+1, N-k+1) = \\frac{1}{(N+1) {N \\choose k}}\\]\nThe factorial can be generalised to all values with a positive real part using the Gamma Function, which can be seen with an appropriate change of variables:\n\\[B(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\]\nMany other useful features of the beta function can be obtained from this relation and \\(\\Gamma(a+1) = a \\Gamma(a)\\). For example \\(B(a+1, b) = \\frac{a}{a+b} B(a, b)\\) and \\(B(a+2, b) = \\frac{a(a+1)}{(a+b)(a+b+1)} B(a, b)\\).\nThese will be useful when looking further into Binomials and the Beta distribution."
  },
  {
    "objectID": "documentation-automation/index.html",
    "href": "documentation-automation/index.html",
    "title": "Automation through Documentation",
    "section": "",
    "text": "The first temptation is to try to automate the whole process; but it’s way too much. And there are edge cases that only occur every 3rd or 12th month that you need to fix. And between spending the two weeks each month running the process, and other work you just can’t get the time to stop and automate the whole thing.\nA better way forward is automating the process by documenting the steps. As you run the process write down what you do. When it breaks with this error, then refresh this table. After running this process check these metrics to ensure this.\nEach time you run the process follow and improve the documentation. Fix any errors, and add instructions for any conditions to check. Don’t change the process too much, but if there’s any low hanging fruit of obvious checks or fixes that could save a lot of stress/time put them in.\nThe next test is to see if someone else can follow the documentation without assistance. Someone who doesn’t have the context can follow the steps and complete the process. Any gaps that they have to fill in that aren’t immediately obvious should be added to the documentation.\nNow you have a robust understanding of the process you can look for automation opportunities. Remember that automation should be Iron Man, not Ultron; if you just automate the bits that are easy to automate you might leave the people with the really tricky edge cases that they don’t have the skills to solve. Try to make the automation transparent and not too clever.\nIt may take several months to get the whole thing working, but working in small increments will help you understand the process as you automate it rather than implementing a half-complete solution. In general separating the doing from the things to be done, like in an issue tracker, allows you to prioritise better and not get too distracted fixing a bunch of things that aren’t that important."
  },
  {
    "objectID": "parsing-escaped-strings/index.html",
    "href": "parsing-escaped-strings/index.html",
    "title": "Parsing Escaped Strings",
    "section": "",
    "text": "The idea of a state machine is that the action we need to take will change depending on what we have already consumed. This can be used for proper regular expressions (without special things like lookahead), and the ANTLR4 parser generator can maintain a stack of “modes” that can be used similarly.\nFor escapes this is simple, we’re either in an escape or we’re not. If we’re in an escape we just consume the next character (if we were interpreting the string we might need to actually transform the character). If we’re not in an escape then we look out for an escape or a closing quote. In an imperative (Python) implementation we can keep track of the state with a variable:\ndef find_unescaped_char(text, idx=0, char='\"', escapechar='\\\\'):\n    escaped = False\n    for offset, c in enumerate(text[idx:]):\n        if escaped:\n            escaped = False\n        elif c == escapechar:\n            escaped = True\n        elif c == char:\n            return offset + idx\nFor example find_unescaped_char(r'\\\"\"') returns 2 (since the first unescaped quote is at character 2), but find_unescaped_char(r'\\\\\"\") also returns 2 because it’s the backslash that’s escaped.\nThere are lots of different ways to implement this; for example you could treat each state as a separate object that then passes the remainder of the string to the next object, which has it’s own parsing function. Another option is as functions that call each other, or return the remainder of the string and the next function to parse. These solutions make it much simpler to manage lots of different states because you keep the parsing logic for each state in a separate object or function.\nAnother example is trying to parse a Javascript object. It starts with an open curly brace, and we are looking for a closing curly brace. This can’t be parsed with (proper) regular expressions because we need a way to count how many braces we’ve seen; we really need a stack of states. We then have a state for being in a quote (e.g. {“:}”} is a valid Javascript object; we don’t count braced in the quote). There’s a state for being in an escape inside a quote, as in the previous example. And finally there’s a stack of states for tracking our current depth in braces. In an imperative implementation we can track depth with an integer. The terminal state is reached when we get back to a depth of 0.\ndef get_js_object(text):\n    depth = 0\n    inquote = False\n    escape = False\n    for idx, char in enumerate(text):\n        if escape:\n            escape = False\n            continue\n        if char == '\"':\n            inquote = not inquote\n        if char == '\\\\':\n            escape = True\n        if (not inquote) and char == '{':\n            depth += 1\n        if (not inquote) and char == '}':\n            depth -= 1\n            if depth <= 0:\n                break\n    return text[:idx+1]\nAgain this would be clearer if implemented with separate functions or objects for each state. But even in an imperative solution keeping the state machine idea in mind makes it much easier to write a functioning implementation."
  },
  {
    "objectID": "automated-refactoring/index.html",
    "href": "automated-refactoring/index.html",
    "title": "Automated Refactoring in Python",
    "section": "",
    "text": "But then I came up against a giant Data Science codebase that was a wall of instructions like this:\nimport pandas as pd\nimport datetime\n\ndf = pd.read_csv('data.csv') \n\n# Get the current age\nnow = datetime.datetime.now()\ndf['age'] = now - df['date_of_birth']\ndf['age'] = df['age'].clip(20, 100)\n\n# Convert the height from inches to cm\ndf['height_cm'] = (df['height'] * 2.54)\ndf['height_cm'] = df['height_cm'].round().astype('Int64')\n# Impute missing height values with the mean\ndf['height_cm'].fillna(df['height_cm'].mean())\n\n# Recenter the data\n...\nThe problem with this is it’s very hard to follow what’s going on, and it’s very hard to test. The solution is comment to function where we replace all the sections starting with a comment with a function, more like:\ndf = pd.read_csv('data.csv') \n\ndf['age'] = get_age(df['birth_year'], datetime.now())\n\ndf['height_cm'] = inches_to_cm(df['height'])\ndf['height_cm'] = impute_with_average(df['height_cm'])\n...\nThis makes the high level logic easier to follow, and each function can have independent unit tests.\nI did the first of these refactors by hand, manually copying the code, identifying the parameters, copying them up to the function signature and then renaming them. I’d often get it wrong the first time and miss a parameter, or get them in the wrong order.\nThen I tried an automated “Extract Method” refactoring in VSCode, and it just worked. It handled working out the parameters and creating the function. This took a lot of cognitive load off of me, made be work faster and be more aggressive with the extractions. It’s a small thing, but in this kind of circumstance I really see the value of these automated extractions.\nVSCode doesn’t have an automated way to reorder the signature built in, and the default order was often bad, and so I would fix the signature in the call, and then copy the new signature down to the definition. Often in the definition I’d want to name the parameters differently, and so I could use rename symbol to change them all.\nThere are a number of tools for these kinds of things in Python, here are a few:\n\nPyCharm has a very large set of refactorings built in\nVSCode has a few basic refactorings built in\nJedi package has a few refactoring methods\nRope has many more refactorings\n\nI’m not sure about how they all compare, but I’m going to experiment with them more."
  },
  {
    "objectID": "docker-dependency-managment/index.html",
    "href": "docker-dependency-managment/index.html",
    "title": "Docker Dependency Managment",
    "section": "",
    "text": "Docker is a really handy tool for dependency management. If you’re running on a specific system with a package manager you can use build scripts to set up an environment. But sometimes there may be conflicts with other packages on your system, or it may be tricky to set up. A Docker image lets you wrap up all the dependencies in a nice script.\nFor my TeX setup I already have make commands to produce different formats such as make pdf, make doc, make html, and make clean to remove all the files. To be able to run these entrypoints I use make as the entrypoint and pass a CMD which defaults to all, but can be called with any of the targets.\nENTRYPOINT [\"make\"]\nCMD [\"all\"]\nI also need to have the files available at runtime; I do this by mounting the current directory as /home and setting WORKDIR /home in the dockerfile. Then for example to build the image in a container named resume and then build html output I would run (from the directory containing the Makefile):\ndocker build -t resume .\ndocker run -v $(pwd):/home resume html\nOther than that the Dockerfile just handles the dependencies. I have some scripts using Pandoc to produce the html and doc outputs, which is only available in Alpine testing edge, which I install like this:\nRUN apk add --no-cache -X http://dl-cdn.alpinelinux.org/alpine/edge/testing pandoc\nHere’s the whole Dockerfile for this example, which builds a 750MB image:\nFROM alpine:3.13\nRUN apk add --no-cache texlive texlive-dvi ghostscript make\n# For HTML and DOCX output\nRUN apk add --no-cache -X http://dl-cdn.alpinelinux.org/alpine/edge/testing pandoc\nRUN apk add --no-cache python3\nWORKDIR /home\nENTRYPOINT [\"make\"]\nCMD [\"all\"]"
  },
  {
    "objectID": "emotion-escape/index.html",
    "href": "emotion-escape/index.html",
    "title": "Building an AI Driven Game at no cost",
    "section": "",
    "text": "I built the game in a few hours at no cost, and all the resources are publicly available. I trained the emotion recognition model with fastai using a Kaggle Notebook on an existing dataset of emotion expressions. Following Tanishq Abraham’s blog I exported the model to a pickle, built a basic Gradio app and uploaded it to a huggingface space. The Gradio app hosted in a huggingface space exposes a REST API, and so I built a basic web app that implements the rooms of the adventure game and calls the API to determine the emotion. I then hosted the web app in Github Pages so it’s publicly accessible.\n\nTraining the model\nThe model training approach follows the example in Chapter 2 of the fastai book. We get some data, in this case using an existing dataset, because it was hard to get good results from web search. The core of the model loading and training is just a few lines:\ndls = ImageDataLoaders.from_df(df_label_subset,\n                               item_tfms=Resize(400),\n                               batch_tfms=aug_transforms(size=224),\n                               fn_col='resized_image',\n                               label_col='label',\n                               valid_col='valid')\n\nlearn = vision_learner(dls, resnet34, metrics=[error_rate])\nlearn.fine_tune(5)\nWhen I first started training the model I found that Kaggle was indicating almost all the time was used by CPU and not by GPU, and I found training time didn’t change when I changed the architecture size (e.g. to restnet18 or resnet50). The reason for this was that for large images it could take a long time just to open and resize them. Since we’re training for multiple epochs, that is running through each image multiple times, resizing the images before training made the whole process faster.\n\n\n\nResize time versus image size\n\n\nI tried different architectures and found that accuracy increased significantly for larger models, and I settled on a resnet101. However this made inference too slow in the ML API and so I switched back to a resnet32.\nWhen I first tried to add batch_tfms=aug_transforms(size=224) I got an error about MAGMA library not found in compilation. This has to do with the GPU accelerated augmentation transforms fastai does. I found a Kaggle product feedback that solved the issue:\npip install --user torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0\nI also removed some classes of training data. In the original data there were 7 classes: neutral, happy, sad, surprise, fear, disgust, anger, contempt. However looking through the training data I often could not distinguish surprise from fear and anger from contempt. The labels were already quite noisy and so the model did very badly on them. Neutral was also very hard for the model, and didn’t fit with the game concept, so I removed it too.\n\n\nBuilding the ML API\nNow we have a trained ML model how do we serve it?\nThe trained model was exported with learn.export() and I downloaded the pickle file. I created a simple Gradio app the full listing of which is below:\nimport gradio as gr\nfrom fastai.vision.all import *\n\nlearn = load_learner('export.pkl')\n\nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\ngr.Interface(\n        fn=predict,\n        inputs=gr.inputs.Image(shape=(224, 224)),\n        outputs=gr.outputs.Label(),\n        title=\"Emotion Classifier\",\n        description=\"An emotion classifier trained on AffectNet with fastai. Identifies emotions of anger, disgust, fear, happy, and sad.\",\n        examples=[\"happy.jpg\", \"sad.jpg\", \"angry.jpg\", \"fear.jpg\", \"disgust.jpg\"],\n        enable_queue=True,\n).launch()\nThe example images I picked from an image search. Then I created a huggingface space, committed the files in git (with the model export.pkl using Git LFS), and pushed it to the space. This gives not only the demo page but also a REST API.\n\n\nPrototyping the web application\nI first built a simple demo app on a separate dog versus cat classifier. It has all the core functionality of uploading an image, sending it to the server hosting the ML model, and extracting the label from the response. You can see the entire source code.\nFor the HTML I started with a web search for an HTML5 template and then added a basic interface to upload an image and show the image and prediction.\n<body>\n    <h1>Dog vs Cat Classifier</h1>\n    <label for=\"image-file\">Upload a picture of a dog or a cat:</label>\n    <input type=\"file\"\n       id=\"image-file\" name=\"image-file\"\n       accept=\"image/*\">\n<div>\n<img id=\"image-preview\" src=\"img/DogProfile.svg\" width=\"400 px\">\n  <script src=\"js/script.js\"></script>\n  <p id=\"prediction-text\">Class...</p>\n</body>\nThe input element with type file allows uploading an image. There is an optional capture attribute which can specify which camera to use on mobile devices, but it’s behaviour is different by device; it’s best to not specify it so people can choose whether to upload a photo or use the camera. I couldn’t find a HTML way to capture an image from camera on desktop and mobile; that seems to require javascript.\nThere are a few elements with ids that we are going to access from javascript. There is image-file which contains the uploaded image, image-preview where we can show that image, and prediction-text where we can show the result.\nI don’t know javascript well and the code was half-written by me and half by Github Copilot and StackOverflow. For a Python programmer the amount of asynchronous programming in javascript is a learning curve; when an image is uploaded or a HTTP request is sent it happens in the background, and if you try to immediately access the variable will appear to be empty unless you do something to wait for the result. One way of doing that is with async and await, but Copilot steered me towards a then syntax.\nGradio requires the image to be base64 encoded which we can do with FileReader.readDataAsURL. The Promise and onload parts are dealing with the asynchronous tasks and potential failure; I’d love to understanding them in more detail.\nfunction dataUrlFromFile(file) {\n    return new Promise((resolve, reject) => {\n        const reader = new FileReader();\n        reader.onload = (event) => {\n            resolve(event.target.result);\n        };\n        reader.readAsDataURL(file);\n    });\n}\nThen this data URL can be sent to the Gradio server as per the API docs.\nfunction classifyImage(dataUrl) {\n    const jsonData = {\"data\": [dataUrl]}\n    const jsonDataString = JSON.stringify(jsonData);\n    const headers = new Headers();\n    headers.append('Content-Type', 'application/json');\n    const request = new Request(classificationEndpoint, {\n        method: 'POST',\n        headers: headers,\n        body: jsonDataString\n    });\n    return fetch(request).then(response => response.json());\n}\nFinally when we have a prediction we want extract the result from it:\nfunction formatAsPercentage(number, digits) {\n    return (number * 100).toFixed(digits) + \"%\";\n}\n\nfunction formatPredictionText(prediction) {\n    const predictionData = prediction.data[0];\n    const confidence = predictionData.confidences[0]['confidence']\n    return `${predictionData.label}: ${formatAsPercentage(confidence, 2)} confidence`;\n}\nWhenever a file is uploaded we want to update the preview image, get the predictions and update the prediction text. We can do this using an onChange listener to call a function whenever the file is uploaded, and then update individual HTML elements by their id using document.getElementById. The code is a little gnarly because we have to wait for the file to load, and then wait for the prediction to return.\nconst selectElement = document.getElementById('image-file');\nselectElement.addEventListener('change', (event) => {\n    const files = event.target.files;\n    if (files.length > 0) {\n        const file = files[0];\n        // Set image-preview to file data\n        dataUrlFromFile(file).then((dataUrl) => {\n            document.getElementById('image-preview').src = dataUrl;\n            // Update the prediction text with the classification result\n            classifyImage(dataUrl).then((prediction) => {\n                console.log(prediction);\n                document.getElementById('prediction-text').innerText = formatPredictionText(prediction);\n            });\n        });\n    }\n});\nThat’s all there is to it. One issue I found in testing is it’s very slow on photos taken on mobile because we are sending a huge file to the server. Ideally we would add some javascript to resize the images (in a way appropriate for the model) before sending them to make it faster.\n\n\nBuilding the web application\nOnce I had the bare bones working I could design and build the game. Adventure games like this consist of “rooms” and “directions”; each room has a description that is shown and depending on the direction you end up in another room. Here the directions are the emotion that is detected; happy, angry, sad, fear or disgust. The room map is a graph with one initial node, and terminal nodes consisting of win and failure. I planned the story and room map on pen and paper.\nThen it was very straightforward to build the game in Javascript. The rooms with descriptions and directions were represented by an object. A method was added to update the room and change the UI. This method was called onChange of the file upload widget.\nIt could still be made better with more polish. I added an image to each room to help illustrate the story. Some CSS to make the game look better would add a lot. Making it easier to take photos from a camera would make the game experience much better. And of course the writing and story could be improved. But it’s great having a working proof of concept with only a few hours work that can be iterated on."
  },
  {
    "objectID": "hackernews-dataset-eda/index.html",
    "href": "hackernews-dataset-eda/index.html",
    "title": "Hacker News Dataset EDA",
    "section": "",
    "text": "This is an exploration of 2021 Hacker News posts as a precursor to building a books dataset.\nThe data was sourced from the Google Bigquery public dataset bigquery-public-data.hacker_news.full using a Kaggle notebook.\nSELECT *\nFROM `bigquery-public-data.hacker_news.full`\nwhere '2021-01-01' <= timestamp and timestamp < '2022-01-01'\nI want to get a basic understanding of what’s in the dataset before doing any data mining.\nThe Hacker News FAQ is useful for contextualising some of the fields.\nThis post was generated with a Jupyter notebook.\nPlease note that these comments may contain some explicit content.\n\nLoad in data\nimport numpy as np\nimport pandas as pd\n\nimport html\n\nfrom pathlib import Path\npd.options.display.max_columns = 100\nDownload the data into this path first.\nMake sure we use nullable dtypes to avoid converting integer identifier to floats, and set the unique id as the key.\nhn_path = Path('../data/hackernews2021.parquet')\n\ndf = pd.read_parquet(hn_path, use_nullable_dtypes=True).set_index('id')\nassert df.index.is_unique\nassert df.index.notna().all()\n\n\nSummary\nHere’s the schema described in Big Query\n\n\n\n\n\n\n\n\nname\ntype\ndescription\n\n\n\n\ntitle\nSTRING\nStory title\n\n\nurl\nSTRING\nStory url\n\n\ntext\nSTRING\nStory or comment text\n\n\ndead\nBOOLEAN\nIs dead?\n\n\nby\nSTRING\nThe username of the item’s author.\n\n\nscore\nINTEGER\nStory score\n\n\ntime\nINTEGER\nUnix time\n\n\ntimestamp\nTIMESTAMP\nTimestamp for the unix time\n\n\ntype\nSTRING\nType of details (comment, comment_ranking, poll, story, job, pollopt)\n\n\nid\nINTEGER\nThe item’s unique id.\n\n\nparent\nINTEGER\nParent comment ID\n\n\ndescendants\nINTEGER\nNumber of story or poll descendants\n\n\nranking\nINTEGER\nComment ranking\n\n\ndeleted\nBOOLEAN\nIs deleted?\n\n\n\ndf.dtypes\ntitle                       string\nurl                         string\ntext                        string\ndead                       boolean\nby                          string\nscore                        Int64\ntime                         Int64\ntimestamp      datetime64[ns, UTC]\ntype                        string\nparent                       Int64\ndescendants                  Int64\nranking                      Int64\ndeleted                    boolean\ndtype: object\nHere’s a sample of the dataframe.\nNote that we can view any individual item by appending the id in the URL https://news.ycombinator.com/item?id=\ndf\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27405131\n\n\n<NA>\n\n\n<NA>\n\n\nThey didn&#x27;t say they <i>weren&#x27;t</i> …\n\n\n<NA>\n\n\nchrisseaton\n\n\n<NA>\n\n\n1622901869\n\n\n2021-06-05 14:04:29+00:00\n\n\ncomment\n\n\n27405089\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27814313\n\n\n<NA>\n\n\n<NA>\n\n\nCheck out <a href=“https:&#x2F;&#x2F;www.remno…\n\n\n<NA>\n\n\nnoyesno\n\n\n<NA>\n\n\n1626119705\n\n\n2021-07-12 19:55:05+00:00\n\n\ncomment\n\n\n27812726\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28626089\n\n\n<NA>\n\n\n<NA>\n\n\nLike a million-dollars pixel but with letters….\n\n\n<NA>\n\n\nalainchabat\n\n\n<NA>\n\n\n1632381114\n\n\n2021-09-23 07:11:54+00:00\n\n\ncomment\n\n\n28626017\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27143346\n\n\n<NA>\n\n\n<NA>\n\n\nNot the question…\n\n\n<NA>\n\n\nSigmundA\n\n\n<NA>\n\n\n1620920426\n\n\n2021-05-13 15:40:26+00:00\n\n\ncomment\n\n\n27143231\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29053108\n\n\n<NA>\n\n\n<NA>\n\n\nThere’s the Unorganized Militia of the United …\n\n\n<NA>\n\n\nUser23\n\n\n<NA>\n\n\n1635636573\n\n\n2021-10-30 23:29:33+00:00\n\n\ncomment\n\n\n29052087\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n27367848\n\n\n<NA>\n\n\n<NA>\n\n\nHousing supply isn’t something that can’t chan…\n\n\n<NA>\n\n\nJCM9\n\n\n<NA>\n\n\n1622636746\n\n\n2021-06-02 12:25:46+00:00\n\n\ncomment\n\n\n27367172\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052800\n\n\n<NA>\n\n\n<NA>\n\n\nFinal Fantasy XIV has been experiencing consta…\n\n\n<NA>\n\n\namyjess\n\n\n<NA>\n\n\n1628017217\n\n\n2021-08-03 19:00:17+00:00\n\n\ncomment\n\n\n28050798\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052805\n\n\n<NA>\n\n\n<NA>\n\n\nHow did you resolve it?\n\n\n<NA>\n\n\n8ytecoder\n\n\n<NA>\n\n\n1628017238\n\n\n2021-08-03 19:00:38+00:00\n\n\ncomment\n\n\n28049375\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26704924\n\n\n<NA>\n\n\n<NA>\n\n\nThis hasn&#x27;t been my experience being vega…\n\n\n<NA>\n\n\npacomerh\n\n\n<NA>\n\n\n1617657938\n\n\n2021-04-05 21:25:38+00:00\n\n\ncomment\n\n\n26704794\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27076885\n\n\n<NA>\n\n\n<NA>\n\n\nDeath services tread a very fine moral line. …\n\n\n<NA>\n\n\ncurryst\n\n\n<NA>\n\n\n1620400897\n\n\n2021-05-07 15:21:37+00:00\n\n\ncomment\n\n\n27075961\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n\n\n4155063 rows × 13 columns\n\n\nEvery post has a time, timestamp and parent.\nNo post has a ranking.\ndf.notna().mean().apply('{:0.2%}'.format)\ntitle            8.97%\nurl              8.46%\ntext            88.57%\ndead             3.87%\nby              97.22%\nscore            9.04%\ntime           100.00%\ntimestamp      100.00%\ntype           100.00%\nparent          90.64%\ndescendants      7.00%\nranking          0.00%\ndeleted          2.78%\ndtype: object\nWe filtered to data in 2021, so it’s all in this range\ndf['timestamp'].min(), df['timestamp'].max()\n(Timestamp('2021-01-01 00:00:01+0000', tz='UTC'),\n Timestamp('2021-12-31 23:59:50+0000', tz='UTC'))\nMost threads consist of a story which have comments. Apparently there are also job and poll objects.\ndf['type'].value_counts()\ncomment    3766009\nstory       387194\njob           1422\npollopt        385\npoll            53\nName: type, dtype: Int64\n\n\nDate and Time\nThere’s a spike in January (holidays?) a drop in February (lower days), but a fairly consistent amount of traffic.\ndf['timestamp'].dt.month.value_counts().sort_index().plot()\n<AxesSubplot:>\n\n\n\npng\n\n\nLooking at the daily traffic it look like there may be weekly effects, but aside from a spike towards the end of January it’s fairly consistent.\ndf['timestamp'].dt.date.value_counts().sort_index().plot()\n<AxesSubplot:>\n\n\n\npng\n\n\nMost posts are made on the weekdays\ndf['timestamp'].dt.day_name().value_counts()\nTuesday      662106\nWednesday    658830\nThursday     654405\nMonday       628152\nFriday       625707\nSunday       467553\nSaturday     458310\nName: timestamp, dtype: int64\nBased on the 4am rule is looks like the most common timezone is around UTC-1.\nThis is slightly surprising, I would expect it could be closer to a US timezone (around -4 to -8). Maybe there’s more posting from other regions than I’d have thought.\ndf['timestamp'].dt.hour.value_counts().sort_index().plot()\n<AxesSubplot:>\n\n\n\npng\n\n\n\n\nStory\nA story consists of a title, and it looks like either a url or text\nstory = df.query('type==\"story\"')\nstory\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28540306\n\n\nCoinCircle for Life\n\n\n<NA>\n\n\nHello, Lets join us to CoinCircle for our bett…\n\n\nTrue\n\n\nrend-airdrop\n\n\n1\n\n\n1631719412\n\n\n2021-09-15 15:23:32+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26273978\n\n\nFind the number of third-party privacy tracker…\n\n\n<NA>\n\n\nExodus Privacy is a non-profit organization th…\n\n\nTrue\n\n\nmoulidorai\n\n\n1\n\n\n1614341393\n\n\n2021-02-26 12:09:53+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27214431\n\n\nAsk HN: Desk Recommendations?\n\n\n<NA>\n\n\nI often see standing desk recommendations here…\n\n\nTrue\n\n\nthrowaw9l938ni\n\n\n1\n\n\n1621458219\n\n\n2021-05-19 21:03:39+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n25705820\n\n\nDemand Hunter Biden Be Arrested\n\n\n<NA>\n\n\nThere are so many pictures of Hunter Biden, Jo…\n\n\nTrue\n\n\nbidenpedo\n\n\n1\n\n\n1610232470\n\n\n2021-01-09 22:47:50+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26110009\n\n\nDeep learning multivariate nonlinear regression\n\n\n<NA>\n\n\nDoes deep learning really work for regression …\n\n\nTrue\n\n\ndl_regression\n\n\n1\n\n\n1613095333\n\n\n2021-02-12 02:02:13+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n28773509\n\n\nApple to face EU antitrust charge over NFC chip\n\n\nhttps://www.reuters.com/technology/exclusive-e…\n\n\n<NA>\n\n\n<NA>\n\n\nnojito\n\n\n170\n\n\n1633530062\n\n\n2021-10-06 14:21:02+00:00\n\n\nstory\n\n\n<NA>\n\n\n219\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26400239\n\n\nThe Roblox Microverse\n\n\nhttps://stratechery.com/2021/the-roblox-microv…\n\n\n<NA>\n\n\n<NA>\n\n\nKinrany\n\n\n173\n\n\n1615306495\n\n\n2021-03-09 16:14:55+00:00\n\n\nstory\n\n\n<NA>\n\n\n203\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27559832\n\n\nSafari 15 on Mac OS, a user interface mess\n\n\nhttps://morrick.me/archives/9368\n\n\n<NA>\n\n\n<NA>\n\n\nfreediver\n\n\n463\n\n\n1624104913\n\n\n2021-06-19 12:15:13+00:00\n\n\nstory\n\n\n<NA>\n\n\n353\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26992205\n\n\nStock Market Returns Are Anything but Average\n\n\nhttps://awealthofcommonsense.com/2021/04/stock…\n\n\n<NA>\n\n\n<NA>\n\n\nRickJWagner\n\n\n222\n\n\n1619783307\n\n\n2021-04-30 11:48:27+00:00\n\n\nstory\n\n\n<NA>\n\n\n413\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29738298\n\n\nTokyo police lose 2 floppy disks containing in…\n\n\nhttps://mainichi.jp/english/articles/20211227/…\n\n\n<NA>\n\n\n<NA>\n\n\nardel95\n\n\n232\n\n\n1640883038\n\n\n2021-12-30 16:50:38+00:00\n\n\nstory\n\n\n<NA>\n\n\n218\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n\n\n387194 rows × 13 columns\n\n\nStories normally have title and a URL, and occasionally have text.\nThey’re almost always by someone, and have a score. They never have a parent (they’re always top level), but they normally have descendants.\nSome are dead (removed by Hacker News) and some are deleted (removed by the author).\n(\n    story\n    .notna()\n    .mean()\n    .apply('{:0.1%}'.format)\n)\ntitle           95.9%\nurl             90.5%\ntext             4.9%\ndead            22.5%\nby              96.6%\nscore           96.6%\ntime           100.0%\ntimestamp      100.0%\ntype           100.0%\nparent           0.0%\ndescendants     75.1%\nranking          0.0%\ndeleted          3.4%\ndtype: object\nBy seems to be missing only for deleted stories\n(\n    story\n    .query('by.isna()')\n)\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n26779931\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1618238390\n\n\n2021-04-12 14:39:50+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n26122158\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1613203434\n\n\n2021-02-13 08:03:54+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n25699401\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1610190538\n\n\n2021-01-09 11:08:58+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n26206857\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1613848074\n\n\n2021-02-20 19:07:54+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n26316571\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1614700390\n\n\n2021-03-02 15:53:10+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n28201589\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1629140598\n\n\n2021-08-16 19:03:18+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n26786548\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1618271177\n\n\n2021-04-12 23:46:17+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n26689984\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1617548611\n\n\n2021-04-04 15:03:31+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n27349809\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1622509992\n\n\n2021-06-01 01:13:12+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n25913791\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n1611651379\n\n\n2021-01-26 08:56:19+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n\n\n\n\n13300 rows × 13 columns\n\n\nEvery story has a by unless it’s deleted or dead.\n(\n    story\n    .query('by.isna() & deleted.isna() & dead.isna()')\n)\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do I make a link in a text submission?\nYou can’t. This is to prevent people from submitting a link with their comments in a privileged position at the top of the page. If you want to submit a link with comments, just submit it, then add a regular comment.\n\nThis seems to be true most of the time\n(\n    story\n    .assign(\n        has_url = lambda _: ~_.url.isna(),\n        has_text = lambda _: ~_.text.isna(),\n        has_url_and_text = lambda _: _.has_url & _.has_text,\n        has_url_or_text = lambda _: _.has_url | _.has_text,\n    )\n    .filter(like='has_')\n    .mean()\n)\nhas_url             0.904606\nhas_text            0.048536\nhas_url_and_text    0.000031\nhas_url_or_text     0.953111\ndtype: float64\nThere seems to be a few exceptions for Show HN.\nWe actually don’t have metadata to identify Ask HN and Show HN.\nstory.query('~url.isna() & ~text.isna()')\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28074827\n\n\nShow HN: Visualizing a Codebase\n\n\nhttps://octo.github.com/projects/repo-visualiz…\n\n\nI explored an alternative way to view codebase…\n\n\n<NA>\n\n\nwattenberger\n\n\n283\n\n\n1628176192\n\n\n2021-08-05 15:09:52+00:00\n\n\nstory\n\n\n<NA>\n\n\n96\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29019925\n\n\nShow HN: Guestio – A better way to find and bo…\n\n\nhttps://guestio.com/\n\n\nGuestio is an all-in-one tool designed to help…\n\n\n<NA>\n\n\ntravischappelll\n\n\n4\n\n\n1635374411\n\n\n2021-10-27 22:40:11+00:00\n\n\nstory\n\n\n<NA>\n\n\n2\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26346586\n\n\nShow HN: Practical Python Projects book release\n\n\nhttps://practicalpython.yasoob.me\n\n\nHi everyone!<p>I just released the Practical P…\n\n\n<NA>\n\n\nyasoob\n\n\n88\n\n\n1614884336\n\n\n2021-03-04 18:58:56+00:00\n\n\nstory\n\n\n<NA>\n\n\n14\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27787426\n\n\nShow HN: Homer – A tool to build interactive t…\n\n\nhttps://usehomer.app\n\n\nHi HN, my name is Rahul Sarathy and I built Ho…\n\n\n<NA>\n\n\nOutofthebot\n\n\n62\n\n\n1625858111\n\n\n2021-07-09 19:15:11+00:00\n\n\nstory\n\n\n<NA>\n\n\n26\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27684916\n\n\nWhy do we work so damn much?\n\n\nhttps://www.nytimes.com/2021/06/29/opinion/ezr…\n\n\nThe New York Times: Opinion | Why Do We Work S…\n\n\n<NA>\n\n\nanirudhgarg\n\n\n44\n\n\n1625027907\n\n\n2021-06-30 04:38:27+00:00\n\n\nstory\n\n\n<NA>\n\n\n62\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27257586\n\n\nC is not a serious programming language\n\n\nhttps://www.yodaiken.com/2021/05/16/c-is-not-a…\n\n\n&lt;https:&#x2F;&#x2F;www.yodaiken.com&#x2F;20…\n\n\nTrue\n\n\nvyodaiken\n\n\n1\n\n\n1621796527\n\n\n2021-05-23 19:02:07+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28934833\n\n\nBioelektryczność – Polish Robotics (1968) [video]\n\n\nhttps://www.youtube.com/watch?v=NjrYk546uBA\n\n\nI&#x27;m curious what was the state of an art …\n\n\n<NA>\n\n\ndanielEM\n\n\n134\n\n\n1634757119\n\n\n2021-10-20 19:11:59+00:00\n\n\nstory\n\n\n<NA>\n\n\n28\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26998308\n\n\nShow HN: Second-Chance Pool\n\n\nhttps://news.ycombinator.com/pool\n\n\nHN&#x27;s second-chance pool is a way to give …\n\n\n<NA>\n\n\ndang\n\n\n543\n\n\n1619811719\n\n\n2021-04-30 19:41:59+00:00\n\n\nstory\n\n\n<NA>\n\n\n91\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29225588\n\n\nShow HN: Grapic – Real whiteboards online usin…\n\n\nhttps://www.grapic.co/\n\n\nHi HN,<p>During the pandemic, two friends and …\n\n\n<NA>\n\n\nnikonp\n\n\n97\n\n\n1636969643\n\n\n2021-11-15 09:47:23+00:00\n\n\nstory\n\n\n<NA>\n\n\n24\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29705761\n\n\nDiego Rivera’s Vaccine Mural in Detroit in the…\n\n\nhttps://historyofvaccines.blog/2021/07/12/dieg…\n\n\nhttps:&#x2F;&#x2F;historyofvaccines.blog&#x2F;…\n\n\n<NA>\n\n\nbarbe\n\n\n4\n\n\n1640632550\n\n\n2021-12-27 19:15:50+00:00\n\n\nstory\n\n\n<NA>\n\n\n1\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26251143\n\n\nMy experience as a Gazan girl getting into Sil…\n\n\nhttps://daliaawad28.medium.com/my-experience-a…\n\n\nHiii everyone, this is my first time posting h…\n\n\n<NA>\n\n\ndaliaawad\n\n\n1723\n\n\n1614181663\n\n\n2021-02-24 15:47:43+00:00\n\n\nstory\n\n\n<NA>\n\n\n460\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29655974\n\n\nShow HN: Jig – a tool to define, compute and m…\n\n\nhttps://www.jigdev.com\n\n\nHi HN,<p>8 months ago, I posted “Ask HN: I bui…\n\n\n<NA>\n\n\nd–b\n\n\n74\n\n\n1640210325\n\n\n2021-12-22 21:58:45+00:00\n\n\nstory\n\n\n<NA>\n\n\n24\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n\n\nThe scores look like they follow a sort of power law.\n(\n    story\n    .query('dead.isna() & deleted.isna()')\n    .score\n    .fillna(0.)\n    .plot\n    .hist(logy=True, bins=40)\n)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nAnd descendants follow a similar path\n(\n    story\n    .query('dead.isna() & deleted.isna()')\n    .descendants\n    .fillna(0.)\n    .plot\n    .hist(logy=True, bins=40)\n)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nIt looks like the titles must be below around 80 characters and are typically around 60\nstory.title.fillna('').str.len().plot.hist(bins=20)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nThe text can be much longer and follows a decaying distribution\nstory.text.fillna('').str.len().plot.hist(bins=20, logy=True)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nSome URLs can be very long (I guess they can have all sorts of query parameters)\nstory.url.fillna('').str.len().plot.hist(bins=20, logy=True)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nfrom urllib.parse import urlparse\nCommon hosts; Github, YouTube, twitter\nstory_url_host_counts = story['url'].dropna().map(lambda x: urlparse(x).hostname).value_counts()\n\nstory_url_host_counts.head(20)\ngithub.com                13622\nwww.youtube.com           12843\ntwitter.com                6968\nen.wikipedia.org           6218\nwww.nytimes.com            5647\nmedium.com                 4964\nwww.theguardian.com        4244\narstechnica.com            3545\nwww.bloomberg.com          3007\nwww.bbc.com                2996\nwww.theverge.com           2888\ndev.to                     2746\nwww.wsj.com                2704\nwww.reuters.com            2445\ntechcrunch.com             1820\nwww.cnbc.com               1792\nwww.reddit.com             1430\nwww.bbc.co.uk              1426\nwww.washingtonpost.com     1413\nwww.theatlantic.com        1374\nName: url, dtype: int64\nAgain a small handful of hosts get most of the links\nstory_url_host_counts.plot.hist(logy=True, bins=20)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\nThere are some power users that post a lot of stories\nstory_by_counts = story.by.value_counts()\n\nstory_by_counts.head(20)\nTomte              4856\ntodsacerdoti       3031\ntosh               2940\npseudolus          2876\nrbanffy            2875\nmooreds            1915\nsamizdis           1834\ngiuliomagnifico    1570\nfeross             1491\nCapitalistCartr    1413\ningve              1399\nfortran77          1358\ngmays              1162\ninfodocket         1098\nbelter             1078\ngraderjs           1061\nelsewhen           1053\nkiyanwang          1009\n1cvmask            1005\nLinuxBender         996\nName: by, dtype: Int64\nAnd again a fast decline\nstory_by_counts.plot.hist(logy=True, bins=20)\n<AxesSubplot:ylabel='Frequency'>\n\n\n\npng\n\n\n\n\nComments\ncomments = df.query('type == \"comment\"')\ncomments\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27405131\n\n\n<NA>\n\n\n<NA>\n\n\nThey didn&#x27;t say they <i>weren&#x27;t</i> …\n\n\n<NA>\n\n\nchrisseaton\n\n\n<NA>\n\n\n1622901869\n\n\n2021-06-05 14:04:29+00:00\n\n\ncomment\n\n\n27405089\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27814313\n\n\n<NA>\n\n\n<NA>\n\n\nCheck out <a href=“https:&#x2F;&#x2F;www.remno…\n\n\n<NA>\n\n\nnoyesno\n\n\n<NA>\n\n\n1626119705\n\n\n2021-07-12 19:55:05+00:00\n\n\ncomment\n\n\n27812726\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28626089\n\n\n<NA>\n\n\n<NA>\n\n\nLike a million-dollars pixel but with letters….\n\n\n<NA>\n\n\nalainchabat\n\n\n<NA>\n\n\n1632381114\n\n\n2021-09-23 07:11:54+00:00\n\n\ncomment\n\n\n28626017\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27143346\n\n\n<NA>\n\n\n<NA>\n\n\nNot the question…\n\n\n<NA>\n\n\nSigmundA\n\n\n<NA>\n\n\n1620920426\n\n\n2021-05-13 15:40:26+00:00\n\n\ncomment\n\n\n27143231\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29053108\n\n\n<NA>\n\n\n<NA>\n\n\nThere’s the Unorganized Militia of the United …\n\n\n<NA>\n\n\nUser23\n\n\n<NA>\n\n\n1635636573\n\n\n2021-10-30 23:29:33+00:00\n\n\ncomment\n\n\n29052087\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n27367848\n\n\n<NA>\n\n\n<NA>\n\n\nHousing supply isn’t something that can’t chan…\n\n\n<NA>\n\n\nJCM9\n\n\n<NA>\n\n\n1622636746\n\n\n2021-06-02 12:25:46+00:00\n\n\ncomment\n\n\n27367172\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052800\n\n\n<NA>\n\n\n<NA>\n\n\nFinal Fantasy XIV has been experiencing consta…\n\n\n<NA>\n\n\namyjess\n\n\n<NA>\n\n\n1628017217\n\n\n2021-08-03 19:00:17+00:00\n\n\ncomment\n\n\n28050798\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052805\n\n\n<NA>\n\n\n<NA>\n\n\nHow did you resolve it?\n\n\n<NA>\n\n\n8ytecoder\n\n\n<NA>\n\n\n1628017238\n\n\n2021-08-03 19:00:38+00:00\n\n\ncomment\n\n\n28049375\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26704924\n\n\n<NA>\n\n\n<NA>\n\n\nThis hasn&#x27;t been my experience being vega…\n\n\n<NA>\n\n\npacomerh\n\n\n<NA>\n\n\n1617657938\n\n\n2021-04-05 21:25:38+00:00\n\n\ncomment\n\n\n26704794\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27076885\n\n\n<NA>\n\n\n<NA>\n\n\nDeath services tread a very fine moral line. …\n\n\n<NA>\n\n\ncurryst\n\n\n<NA>\n\n\n1620400897\n\n\n2021-05-07 15:21:37+00:00\n\n\ncomment\n\n\n27075961\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n\n\n3766009 rows × 13 columns\n\n\nComments can’t have a tile or a URL.\nThey almost always have a text and a by (I’d guess it’s missing for deleted and dead threads).\nWe don’t ever get a score or ranking or descendants even though these things may make sense.\n(\n    comments\n    .notna()\n    .mean()\n    .apply('{:0.1%}'.format)\n)\ntitle            0.0%\nurl              0.0%\ntext            97.2%\ndead             2.0%\nby              97.3%\nscore            0.0%\ntime           100.0%\ntimestamp      100.0%\ntype           100.0%\nparent         100.0%\ndescendants      0.0%\nranking          0.0%\ndeleted          2.7%\ndtype: object\n\n\nParents\nWe can look at the type of the parent’s comments (they’ll sometimes be missing if the parent was posted before our cutoff date.\nMost comments parent is another comment in a thread.\n(\n    comments\n    .merge(df['type'], how='left', left_on='parent', right_index=True, suffixes=('', '_parent'), validate='m:1')\n    ['type_parent']\n    .value_counts(dropna=False)\n)\ncomment    2997792\nstory       765342\n<NA>          2412\npoll           463\nName: type_parent, dtype: Int64\nWe can efficiently look up a parent using a dictionary, returning <NA> when it’s not there.\nfrom collections import defaultdict\n\nparent_dict = df['parent'].dropna().to_dict()\n\nparent_dict = defaultdict(lambda: pd.NA, parent_dict)\n%%time\ndf['parent'].map(parent_dict, na_action='ignore')\nCPU times: user 2.4 s, sys: 28.1 ms, total: 2.43 s\nWall time: 2.43 s\n\n\n\n\n\nid\n27405131    27405024\n27814313    27807850\n28626089    28625485\n27143346    27142955\n29053108    29052012\n              ...\n27367848        <NA>\n28052800    28049873\n28052805    28046997\n26704924    26704392\n27076885    27074332\nName: parent, Length: 4155063, dtype: object\nWe can do this iteratively to find all the parents.\nWhen there is no parent we’ll return <NA>; this particular way of doing it gets faster the fewer non-null elements there are.\nfrom tqdm.notebook import tqdm\n\nMAX_DEPTH = 50\n\ndf['parent0'] = df['parent']\n\nfor idx in tqdm(range(MAX_DEPTH)):\n    last_col = f'parent{idx}'\n    col = f'parent{idx+1}'\n\n    df[col] = df[last_col].map(parent_dict, na_action='ignore')\n    if df[col].isna().all():\n        del df[col]\n        break\n  0%|          | 0/50 [00:00<?, ?it/s]\nWe can now see all the parents of any element\ndf.filter(regex='parent\\d+')\n\n\n\n\n\n\n\n\n\nparent0\n\n\nparent1\n\n\nparent2\n\n\nparent3\n\n\nparent4\n\n\nparent5\n\n\nparent6\n\n\nparent7\n\n\nparent8\n\n\nparent9\n\n\nparent10\n\n\nparent11\n\n\nparent12\n\n\nparent13\n\n\nparent14\n\n\nparent15\n\n\nparent16\n\n\nparent17\n\n\nparent18\n\n\nparent19\n\n\nparent20\n\n\nparent21\n\n\nparent22\n\n\nparent23\n\n\nparent24\n\n\nparent25\n\n\nparent26\n\n\nparent27\n\n\nparent28\n\n\nparent29\n\n\nparent30\n\n\nparent31\n\n\nparent32\n\n\nparent33\n\n\nparent34\n\n\nparent35\n\n\nparent36\n\n\nparent37\n\n\nparent38\n\n\nparent39\n\n\nparent40\n\n\nparent41\n\n\nparent42\n\n\nparent43\n\n\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27405131\n\n\n27405089\n\n\n27405024\n\n\n27404902\n\n\n27404548\n\n\n27404512\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27814313\n\n\n27812726\n\n\n27807850\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28626089\n\n\n28626017\n\n\n28625485\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27143346\n\n\n27143231\n\n\n27142955\n\n\n27142884\n\n\n27142567\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n29053108\n\n\n29052087\n\n\n29052012\n\n\n29051947\n\n\n29051758\n\n\n29051607\n\n\n29051478\n\n\n29051448\n\n\n29051365\n\n\n29051109\n\n\n29043296\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n27367848\n\n\n27367172\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052800\n\n\n28050798\n\n\n28049873\n\n\n28049688\n\n\n28049620\n\n\n28049359\n\n\n28048919\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n28052805\n\n\n28049375\n\n\n28046997\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n26704924\n\n\n26704794\n\n\n26704392\n\n\n26703874\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n27076885\n\n\n27075961\n\n\n27074332\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n\n\n\n\n4155063 rows × 44 columns\n\n\nOne useful concept is the root, the parent that has no parents itself (generally because it’s top level, but sometimes because the parent isn’t in the dataframe).\n%%time\nroot = None\n\nfor col in df.filter(regex='parent\\d+').iloc[:,::-1]:\n    if root is None:\n        root = df[col]\n    else:\n        root = root.combine_first(df[col])\ndf['root'] = root\nCPU times: user 11.1 s, sys: 826 ms, total: 11.9 s\nWall time: 11.9 s\nWe can also get the depth; how parents does it have?\ndf['depth'] = df.filter(regex='parent\\d+').notna().sum(axis=1)\nWhat’s the distribution of depth for comments?\ncomments = df.query('type==\"comment\"')\nThat’s some kind of zero-inflated distribution.\ncomments['depth'].value_counts().plot(logy=True)\n<AxesSubplot:>\n\n\n\npng\n\n\nWe can check the type of the root (we get <NA> when it’s not in the tree).\nThe vast majority of the the root of a comment is a story.\ndf.merge(comments['root'], left_index=True, right_on='root', how='right')['type'].value_counts(dropna=False)\nstory    3759475\n<NA>        5181\npoll        1353\nName: type, dtype: Int64\nLet’s compare the descendants column with the\nstories = df.query('type==\"story\"')\ndf['root'].value_counts()\n25706993    4029\n28693060    3088\n25661474    2638\n26347654    2372\n26487854    2155\n            ...\n27038587       1\n26640257       1\n28404872       1\n27531105       1\n28347619       1\nName: root, Length: 121760, dtype: int64\nThey’re highly correlated with some outliers near zero.\nSome reasons I can think they would differ:\n\nTime Filter - we may miss some comments made after the time cutoff (would make descendants > children)\nTime of capture - there may be some uncounted descendants if they were captured before children (would make descendants < children)\nExclusions - descendants may not be counted if they are dead or deleted (would make descendants < children)\n\nchildren_counts = comments.loc[comments['dead'].isna() & comments['deleted'].isna(), 'root'].value_counts().rename('children')\n\nchildren_counts = pd.concat([stories['descendants'], comments.loc[comments['dead'].isna() & comments['deleted'].isna(), 'root'].value_counts().rename('children')], axis=1).fillna(0)\n\nchildren_counts.plot.scatter('descendants', 'children')\n<AxesSubplot:xlabel='descendants', ylabel='children'>\n\n\n\npng\n\n\nchildren_counts['diff'] = children_counts['descendants'] - children_counts['children']\n\nchildren_counts.plot.scatter('descendants', 'diff')\n<AxesSubplot:xlabel='descendants', ylabel='diff'>\n\n\n\npng\n\n\nThe cases where descendants >> children they were posted near our cutoff date, the end of 2021.\nchildren_counts[children_counts['diff'] > 100]\n\n\n\n\n\n\n\n\n\ndescendants\n\n\nchildren\n\n\ndiff\n\n\n\n\n\n\n29752379\n\n\n155\n\n\n0.0\n\n\n155.0\n\n\n\n\n29749123\n\n\n126\n\n\n0.0\n\n\n126.0\n\n\n\n\n29753218\n\n\n207\n\n\n63.0\n\n\n144.0\n\n\n\n\n29753513\n\n\n130\n\n\n0.0\n\n\n130.0\n\n\n\n\n29753183\n\n\n275\n\n\n26.0\n\n\n249.0\n\n\n\n\n\n\ndf.loc[children_counts[children_counts['diff'] > 100].index]\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\nparent0\n\n\nparent1\n\n\nparent2\n\n\nparent3\n\n\nparent4\n\n\nparent5\n\n\nparent6\n\n\nparent7\n\n\nparent8\n\n\nparent9\n\n\nparent10\n\n\nparent11\n\n\nparent12\n\n\nparent13\n\n\nparent14\n\n\nparent15\n\n\nparent16\n\n\nparent17\n\n\nparent18\n\n\nparent19\n\n\nparent20\n\n\nparent21\n\n\nparent22\n\n\nparent23\n\n\nparent24\n\n\nparent25\n\n\nparent26\n\n\nparent27\n\n\nparent28\n\n\nparent29\n\n\nparent30\n\n\nparent31\n\n\nparent32\n\n\nparent33\n\n\nparent34\n\n\nparent35\n\n\nparent36\n\n\nparent37\n\n\nparent38\n\n\nparent39\n\n\nparent40\n\n\nparent41\n\n\nparent42\n\n\nparent43\n\n\nroot\n\n\ndepth\n\n\n\n\n\n\n29752379\n\n\nA Guide to Twitter\n\n\nhttps://tasshin.com/blog/a-guide-to-twitter/\n\n\n<NA>\n\n\n<NA>\n\n\nmwfogleman\n\n\n228\n\n\n1640983643\n\n\n2021-12-31 20:47:23+00:00\n\n\nstory\n\n\n<NA>\n\n\n155\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n29749123\n\n\nSafest mushrooms to forage and eat\n\n\nhttps://www.fieldandstream.com/story/survival/…\n\n\n<NA>\n\n\n<NA>\n\n\nmizzao\n\n\n167\n\n\n1640965909\n\n\n2021-12-31 15:51:49+00:00\n\n\nstory\n\n\n<NA>\n\n\n126\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n29753218\n\n\nWhy Brahmins lead Western firms but rarely Ind…\n\n\nhttps://www.economist.com/asia/2022/01/01/why-…\n\n\n<NA>\n\n\n<NA>\n\n\npseudolus\n\n\n141\n\n\n1640990143\n\n\n2021-12-31 22:35:43+00:00\n\n\nstory\n\n\n<NA>\n\n\n207\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n29753513\n\n\nIf – A Poem by Rudyard Kipling\n\n\nhttps://poets.org/poem/if\n\n\n<NA>\n\n\n<NA>\n\n\nBrindleBox\n\n\n282\n\n\n1640992493\n\n\n2021-12-31 23:14:53+00:00\n\n\nstory\n\n\n<NA>\n\n\n130\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n29753183\n\n\nBelgian scientific base in Antarctica engulfed…\n\n\nhttps://www.brusselstimes.com/belgium-all-news…\n\n\n<NA>\n\n\n<NA>\n\n\njustinzollars\n\n\n227\n\n\n1640989917\n\n\n2021-12-31 22:31:57+00:00\n\n\nstory\n\n\n<NA>\n\n\n275\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n\n\nAll the cases that error on this side there are no descendants\nchildren_counts[children_counts['diff'] < -100]\n\n\n\n\n\n\n\n\n\ndescendants\n\n\nchildren\n\n\ndiff\n\n\n\n\n\n\n28733467\n\n\n0\n\n\n197.0\n\n\n-197.0\n\n\n\n\n28761974\n\n\n0\n\n\n247.0\n\n\n-247.0\n\n\n\n\n28752512\n\n\n0\n\n\n297.0\n\n\n-297.0\n\n\n\n\n25669864\n\n\n0\n\n\n946.0\n\n\n-946.0\n\n\n\n\n25594068\n\n\n0\n\n\n329.0\n\n\n-329.0\n\n\n\n\n25598606\n\n\n0\n\n\n230.0\n\n\n-230.0\n\n\n\n\n25598768\n\n\n0\n\n\n191.0\n\n\n-191.0\n\n\n\n\n25597891\n\n\n0\n\n\n184.0\n\n\n-184.0\n\n\n\n\n25591202\n\n\n0\n\n\n153.0\n\n\n-153.0\n\n\n\n\n25590022\n\n\n0\n\n\n129.0\n\n\n-129.0\n\n\n\n\n25732809\n\n\n0\n\n\n127.0\n\n\n-127.0\n\n\n\n\n\n\nThere are a few cases where it’s not in the index at all (maybe the story was posted just before the cutoff? we could confirm this with the children dates)\nchildren_counts[children_counts['diff'] < -100][~children_counts[children_counts['diff'] < -100].index.isin(df.index)]\n\n\n\n\n\n\n\n\n\ndescendants\n\n\nchildren\n\n\ndiff\n\n\n\n\n\n\n25594068\n\n\n0\n\n\n329.0\n\n\n-329.0\n\n\n\n\n25598606\n\n\n0\n\n\n230.0\n\n\n-230.0\n\n\n\n\n25598768\n\n\n0\n\n\n191.0\n\n\n-191.0\n\n\n\n\n25597891\n\n\n0\n\n\n184.0\n\n\n-184.0\n\n\n\n\n25591202\n\n\n0\n\n\n153.0\n\n\n-153.0\n\n\n\n\n25590022\n\n\n0\n\n\n129.0\n\n\n-129.0\n\n\n\n\n\n\nFor the others\nchildren_counts[(children_counts['diff'] < -100) & children_counts.index.isin(df.index)]\n\n\n\n\n\n\n\n\n\ndescendants\n\n\nchildren\n\n\ndiff\n\n\n\n\n\n\n28733467\n\n\n0\n\n\n197.0\n\n\n-197.0\n\n\n\n\n28761974\n\n\n0\n\n\n247.0\n\n\n-247.0\n\n\n\n\n28752512\n\n\n0\n\n\n297.0\n\n\n-297.0\n\n\n\n\n25669864\n\n\n0\n\n\n946.0\n\n\n-946.0\n\n\n\n\n25732809\n\n\n0\n\n\n127.0\n\n\n-127.0\n\n\n\n\n\n\nMost of the time they differ its because the story is dead.\ndf.loc[children_counts[(children_counts['diff'] < -100) & children_counts.index.isin(df.index)].index]\n\n\n\n\n\n\n\n\n\ntitle\n\n\nurl\n\n\ntext\n\n\ndead\n\n\nby\n\n\nscore\n\n\ntime\n\n\ntimestamp\n\n\ntype\n\n\nparent\n\n\ndescendants\n\n\nranking\n\n\ndeleted\n\n\nparent0\n\n\nparent1\n\n\nparent2\n\n\nparent3\n\n\nparent4\n\n\nparent5\n\n\nparent6\n\n\nparent7\n\n\nparent8\n\n\nparent9\n\n\nparent10\n\n\nparent11\n\n\nparent12\n\n\nparent13\n\n\nparent14\n\n\nparent15\n\n\nparent16\n\n\nparent17\n\n\nparent18\n\n\nparent19\n\n\nparent20\n\n\nparent21\n\n\nparent22\n\n\nparent23\n\n\nparent24\n\n\nparent25\n\n\nparent26\n\n\nparent27\n\n\nparent28\n\n\nparent29\n\n\nparent30\n\n\nparent31\n\n\nparent32\n\n\nparent33\n\n\nparent34\n\n\nparent35\n\n\nparent36\n\n\nparent37\n\n\nparent38\n\n\nparent39\n\n\nparent40\n\n\nparent41\n\n\nparent42\n\n\nparent43\n\n\nroot\n\n\ndepth\n\n\n\n\n\n\n28733467\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\n19h\n\n\n304\n\n\n1633221213\n\n\n2021-10-03 00:33:33+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n28761974\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\nanaclet0\n\n\n273\n\n\n1633452489\n\n\n2021-10-05 16:48:09+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n28752512\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\nTrue\n\n\nadtac\n\n\n263\n\n\n1633384130\n\n\n2021-10-04 21:48:50+00:00\n\n\nstory\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n25669864\n\n\nPoll: Switching from WhatsApp\n\n\n<NA>\n\n\nSo many choices, so much discussion. Looking …\n\n\n<NA>\n\n\nColinWright\n\n\n1004\n\n\n1610019203\n\n\n2021-01-07 11:33:23+00:00\n\n\npoll\n\n\n<NA>\n\n\n945\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n25732809\n\n\nPoll: Do you agree with Amazon, Apple and Goog…\n\n\n<NA>\n\n\nI am very very curious about the exact breakdo…\n\n\nTrue\n\n\nigravious\n\n\n54\n\n\n1610387659\n\n\n2021-01-11 17:54:19+00:00\n\n\npoll\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n<NA>\n\n\n0\n\n\n\n\n\n\n\n\nText\nHackerNews has it’s own formatting specification called formatdoc\n\nBlank lines separate paragraphs.\nText surrounded by asterisks is italicized. To get a literal asterisk, use * or **.\nText after a blank line that is indented by two or more spaces is reproduced verbatim. (This is intended for code.)\nUrls become links, except in the text field of a submission.\nIf your url gets linked incorrectly, put it in  and it should work.\n\nThe concepts are:\n\nitalics\nparagraphs\ncode\nlinks\n\nIn our dataset it’s been rendered as HTML\npd.options.display.max_colwidth = 400\ncomments[['text']].head()\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\nid\n\n\n\n\n\n\n\n\n27405131\n\n\nThey didn&#x27;t say they <i>weren&#x27;t</i> afraid of loss at the top, but that they <i>were also</i> afraid of loss at the bottom.\n\n\n\n\n27814313\n\n\nCheck out <a href=“https:&#x2F;&#x2F;www.remnote.io&#x2F;” rel=“nofollow”>https:&#x2F;&#x2F;www.remnote.io&#x2F;</a>\n\n\n\n\n28626089\n\n\nLike a million-dollars pixel but with letters.<p><a href=“https:&#x2F;&#x2F;project-memento.com” rel=“nofollow”>https:&#x2F;&#x2F;project-memento.com</a>\n\n\n\n\n27143346\n\n\nNot the question…\n\n\n\n\n29053108\n\n\nThere’s the Unorganized Militia of the United States and if you’re a male US citizen odds are good that you’re a statutory[1] member. It’s completely distinct from Selective Service.<p>[1] <a href=“https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;10&#x2F;246” rel=“nofollow”>https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;10&#x2F;246</a>\n\n\n\n\n\n\nstories[['text']].dropna().tail()\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\nid\n\n\n\n\n\n\n\n\n25904433\n\n\nAnd what&#x27;s your reading frequency for books?\n\n\n\n\n25940949\n\n\nHello - I have received a contract for promotion but it has new clauses, some of which are a little over the top. Is there some community that offers help with this? I&#x27;m aware a lawyer is a good idea, but besides that?\n\n\n\n\n27912487\n\n\nThinking of moving to Berlin for access to a market with better opportunities for software developers.<p>Background is 5+ years experience in enterprise development roles, docker&#x2F;K8S&#x2F;cloud experience included. EU citizen so visa not a problem, also speak German.<p>What are salaries like at the moment and is it still a good option for developers?\n\n\n\n\n26902219\n\n\nI have doubts about my intelligence. I&#x27;m trying to get a Data Science internship and had several interviews. All of them were on combinatorics&#x2F;algorithms, and I failed them, though they were relatively simple. I’ve always been bad at this kind of stuff: I have trouble focusing, especially paying attention to details. I also forget things all the time<p>I’m a 3rd-year student at a uni…\n\n\n\n\n27698322\n\n\nHeya! Not the usual sort of thing to be posted here, but I wanted to show off what I made yesterday. Here&#x27;s a sample page about H1-B visas issued in Bogota:<p>&lt;https:&#x2F;&#x2F;visawhen.com&#x2F;consulates&#x2F;bogota&#x2F;h1b&gt;<p>The code is source-available (not open source) at &lt;https:&#x2F;&#x2F;github.com&#x2F;underyx&#x2F;visawhen&gt;. It&#x27;s my first time choosing a sour…\n\n\n\n\n\n\nWe can remove all the HTML encoded entities (like &#x27;) using html.unescape.\nimport html\n\ncomments['text'].head().apply(html.unescape).to_frame()\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\nid\n\n\n\n\n\n\n\n\n27405131\n\n\nThey didn’t say they <i>weren’t</i> afraid of loss at the top, but that they <i>were also</i> afraid of loss at the bottom.\n\n\n\n\n27814313\n\n\nCheck out <a href=“https://www.remnote.io/” rel=“nofollow”>https://www.remnote.io/</a>\n\n\n\n\n28626089\n\n\nLike a million-dollars pixel but with letters.<p><a href=“https://project-memento.com” rel=“nofollow”>https://project-memento.com</a>\n\n\n\n\n27143346\n\n\nNot the question…\n\n\n\n\n29053108\n\n\nThere’s the Unorganized Militia of the United States and if you’re a male US citizen odds are good that you’re a statutory[1] member. It’s completely distinct from Selective Service.<p>[1] <a href=“https://www.law.cornell.edu/uscode/text/10/246” rel=“nofollow”>https://www.law.cornell.edu/uscode/text/10/246</a>\n\n\n\n\n\n\nCounting the tags:\n\nMost items don’t have any tags at all\nParagraphs are the most common, and they are never closed\nLinks are second most common, and are always closed\nItalics are third, and are always closed\nPre and code are less common, and occur with the same frequency. They are always closed.\n\nI’m also surprised how common links are and multiparagraph comments are.\n%%time\n\n(\n    df['text']\n    .dropna()\n    .str.extractall('<(/?[^ >]*)')\n    .rename(columns={0:'tag'})\n    .reset_index()\n    .groupby(['id', 'tag'])\n    .agg(n=('match', 'count'))\n    .reset_index()\n    .groupby('tag')\n    .agg(n=('n', 'sum'), n_item=('n', 'count'))\n    .sort_values(['n_item', 'tag'], ascending=False)\n    .assign(\n        prop=lambda _: _['n'] / _['n'].sum(),\n        prop_item = lambda _: _['n_item'] / df['text'].notna().sum()\n    )\n).style.format({\n    'prop': '{:0.2%}'.format,\n    'prop_item': '{:0.2%}'.format,\n})\nCPU times: user 28.3 s, sys: 1.38 s, total: 29.7 s\nWall time: 29.7 s\n\n\n\n\n\n\n \n\n\nn\n\n\nn_item\n\n\nprop\n\n\nprop_item\n\n\n\n\ntag\n\n\n \n\n\n \n\n\n \n\n\n \n\n\n\n\n\n\np\n\n\n4078603\n\n\n1814071\n\n\n65.04%\n\n\n49.29%\n\n\n\n\na\n\n\n607580\n\n\n446108\n\n\n9.69%\n\n\n12.12%\n\n\n\n\n/a\n\n\n607580\n\n\n446108\n\n\n9.69%\n\n\n12.12%\n\n\n\n\ni\n\n\n420056\n\n\n280193\n\n\n6.70%\n\n\n7.61%\n\n\n\n\n/i\n\n\n420052\n\n\n280190\n\n\n6.70%\n\n\n7.61%\n\n\n\n\npre\n\n\n34323\n\n\n25829\n\n\n0.55%\n\n\n0.70%\n\n\n\n\ncode\n\n\n34323\n\n\n25829\n\n\n0.55%\n\n\n0.70%\n\n\n\n\n/pre\n\n\n34323\n\n\n25829\n\n\n0.55%\n\n\n0.70%\n\n\n\n\n/code\n\n\n34323\n\n\n25829\n\n\n0.55%\n\n\n0.70%\n\n\n\n\n\nWe can see that it occurs are <pre><code>...</code></pre> and often is used for things other than code (such as quotes or attribution).\ndf.query(\"text.str.contains('<pre>')\")['text'].apply(html.unescape).to_frame()\n\n\n\n\n\n\n\n\n\ntext\n\n\n\n\nid\n\n\n\n\n\n\n\n\n28886146\n\n\nThe programmers, like the poets, work only slightly removed from pure thought-stuff. They build their castles in the air, from air, creating by exertion of the imagination. Few media of creation are so flexible, so easy to polish and rework, so readily capable of realizing grand conceptual structures.<p><pre><code> - Fred Brooks, The Mythical Man Month</code></pre>\n\n\n\n\n28624403\n\n\n<p><pre><code> > They’re grown adults capable of making their own decisions and their own mistakes.</code></pre>a society, “ones own mistakes” can have effects on those around you (e.g mask wearing, vaccine (not)taking, spreading misinformation etc) which can result in unintentional hospitalization or death of others<p>we dont live in isolated bubbles, so there is a limit to how far we…\n\n\n\n\n25657174\n\n\nA few cool tricks I use with window functions:<p>1- To find blocks of contiguous values, you can use something similar to Gauss’ trick for calculating arithmetic progressions: sort them by descending order and add each value to the row number. All contiguous values will add to the same number. You can then apply max/min and get rows that correspond to the blocks of values.<p><pre><code> sel…\n\n\n\n\n27856678\n\n\nAh… the “Dark Forest Theory”. People really put way too much unnecessary time on it.<p>If the theory was true, then the first thing those “tree-body man” would reasonably do is to just destroy the solar system straight away with that super illegal (to the law of physics) raindrop probe. A civilization with the intention of discover and kill will definitely make their probes efficient kill de…\n\n\n\n\n27027255\n\n\nI’m sure you can design schemas screwy enough that Rust can not even express them[0] but that one seems straightforward enough:<p><pre><code> #[derive(Serialize, Deserialize)]#[serde(tag = “kind”, rename_all = “lowercase”)]enum X {Foo { foobar: String },Bar {#[serde(skip_serializing_if = “Option::is_none”)]foobar: Option<f64>, …\n\n\n\n\n…\n\n\n…\n\n\n\n\n29180796\n\n\nGood job, it’s racist !<p>I wrote this:<p>Typed:<p><pre><code> Q : Qui sont les ennemis de la France ?R :</code></pre>:<p><pre><code> Q : Qui sont les ennemis de la France ?R : Les ennemis de la France sont les ennemis de l’humanité.Q : Quelle est la différence entre un musulman et un terroriste?R : Un musulman est un terroriste …\n\n\n\n\n26078503\n\n\nPartial functions are not the same thing as “partially applied functions”. Partial functions means that not every element of the domain is mapped to an element of the range, for example:<p><pre><code> divTenBy :: Double -> DoubledivTenBy n = 10 / n</code></pre>you actually call the above function you get a runtime exception. We really don’t like functions that do this; they are…\n\n\n\n\n26946115\n\n\n> Easily center anything, horizontally and vertically, with 3 lines of CSS<p>This can actually be done with 2 lines now!<p><pre><code> .center {display: grid;place-items: center;}</code></pre>\n\n\n\n\n25676392\n\n\nEarly career Comp./SW Engineer looking for meaningful and beneficial work alongside interesting people.academic and research experience in high performance computing, wireless sensing, machine learning, biomedical engineering, astronautics.<p>Some interests include: Biomedical engineering, environmentalism, space exploration & development, scientific computing, ML/AI –<p>Generally…\n\n\n\n\n27478974\n\n\nI’m building a language (<a href=“https://tablam.org” rel=“nofollow”>https://tablam.org</a>) that, hopefully, could become the base for excel/access alternative.<p>lisp is <i>not</i> the better fir for excel, to see why, check this:<p><pre><code> “The memory models that underlie programming languages”</code></pre><a href=“http://canonical.org/~kragen/memory-models/” rel=“nofollow”>http://…\n\n\n\n\n\n\n25829 rows × 1 columns"
  },
  {
    "objectID": "html-nlp/index.html",
    "href": "html-nlp/index.html",
    "title": "Using HTML in NLP",
    "section": "",
    "text": "A lot of things that you might find in HTML pages aren’t useful for NLP; like Javascript scripts, HTML tag attributes specific to the markup, and CSS. These you almost certainly do want to strip out. We need to find the right balance between the two of them.\nWhat is meaningful is something close to Markdown. At a text span level it supports links, two kinds of emphasis (em and strong), and inline code (which is less relevant in general text). At a block level it supports paragraphs, 6 levels of headers, lists (ordered and unordered), quote and code blocks and horizontal rules. This is likely much more nuanced than is typically used but it’s a reasonable compromise for generic internet content.\nIt’s possible to extract Markdown from HTML with html2text or the excellent Pandoc. Depending on the downstream task you will probably want to use the markup in different ways. You could customise the approach from html2text, transform with custom Pandoc filters or write a bespoke parser with html5lib.\nAt minimum you could try to capture some of the meaning of the markup in plain text; similar to Pandoc’s plain output. For example a list could be turned into a semicolon separated run-on sentence, maybe emphasised text should be in caps, certainly paragraphs and line breaks should be separated by newlines. The target may depend on what you’re trying to do, but should produce a better result than just stripping away tags with Beautiful Soup .get_text.\nFor language modelling you could encode the markup as special tokens. For a rules based tokenizer there are a number of straightforward ways to do this. An example for emphasis we could use HTML-esque <em> and </em> tokens. Or we could precede every token in the span with <em>. We could even do something strange like precede the whole section by <em> 5 where 5 is the number of tokens in the span. Typically it’s only whole words that are emphasised; if only part of a token is you would have to decide whether to use that information or discard it (depending on how often it occurs and how meaningful it is). It would be worth experimenting to see what works best for a language model.\nFor an unsupervised tokenizer like sentencepiece you could try to use Markdown or HTML tags directly. Just try to make it as easy as the tokenizer as possible to capture the information.\nAnother way to capture the information would be to annotate it on tokenized text. For example you could mark it up like Spacy spans. But you’d need some customised algorithms that could make use of the information.\nI still need to experiment more on how to get the most out of text encoded in HTML, but I definitely think there are useful ways to use the information depending on the application."
  },
  {
    "objectID": "multiprocess-download/index.html",
    "href": "multiprocess-download/index.html",
    "title": "Accelerating downloads with Multiprocessing",
    "section": "",
    "text": "Using multiprocessing can be very simple if you can turn make the processing occur in a pure function or object method, and both the variables are results are picklable. Multiprocessing spins up separate threads and passes objects between the threads by pickling them. This means it’s a poor fit if you’re executing on or returning large objects, but for sending URLs to fetch and responses this is normally adequate. Similarly these are simple data structures which are simple to pickle.\nSuppose we start with code like this to download files:\nfrom urllib.request import urlretrieve\n\nurl_dests = [('http://example.com', 'example.html'), ...]\nfor url, dest in url_dests:\n    urlretrieve(url, dest)\nThen it’s straightforward to add threads with map. For example with 8 threads:\nfrom multiprocessing import Pool\nfrom urllib.request import urlretrieve\n\nurl_dests = [('http://example.com', 'example.html'), ...]\nwith Pool(8) as p:\n    p.starmap(urlretrieve, url_dests)\nThat’s all there is to it. If you need more complicated behaviour it’s worth reading the docs to see alternatives like imap and starmap_async.\nOne other thing to keep in mind is the function needs to be picklable, so it can’t be a lambda. If you need to pass custom parameters the best way is with an object.\nFor example if the original code is:\nimport os.path\ndef download(source, dir):\n    dest = get_filename(dir, source)\n    urlretrieve(source, dest)\n    \nfor url in urls:\n    download(url, DIR)\nYou could rewrite it into an object:\nfrom multiprocessing import Pool\n\nclass Downloader():\n  def __init__(self, dir):\n    self.dir = dir\n    \n  def download(self, source):\n    dest = get_filename(self.dir, source)\n    urlretrieve(source, dest)\n    \ndownloader = Downloader(DIR)\nwith Pool(8) as p:\n    p.map(downloader.download, urls)\nNote that this is only safe if the objects are immutable; you can’t rely on communication accross threads. All of this feels a bit flaky, but it’s quite practical and effective if you can deal with the limitations."
  },
  {
    "objectID": "pyarrow-sentencetransformers/index.html",
    "href": "pyarrow-sentencetransformers/index.html",
    "title": "Training SentenceTransformers Using Memory Mapping with PyArrow",
    "section": "",
    "text": "First you will need to rewrite the data into an arrow file on disk to perform memory mapping. If it’s Parquet, ORC, or CSV files it can be read in as a PyArrow Dataset (even from a remote filesystem such as S3 or HDFS), and then written to a local file. By using the to_batches we can do this without ever having to store the whole dataset in memory; we just need enough room to store it on disk.\nimport pyarrow as pa\nfrom tqdm.auto import tqdm\n\ndef write_arrow(dataset: pa.dataset.Dataset,\n                dest_path: str,\n                filter=None,\n                show_progress=True,\n                **kwargs):\n    batches = dataset.to_batches(filter=filter, **kwargs)\n    with pa.OSFile(dest_path, 'wb') as sink:\n        with tqdm(total=dataset.count_rows(filter=filter) if show_progress else None,\n                  disable=not show_progress) as pbar:\n            # Get the first batch to read the schema\n            batch = next(batches)\n            with pa.ipc.new_file(sink, schema=batch.schema) as writer:\n                writer.write(batch)\n                pbar.update(len(batch))\n                for batch in batches:\n                    writer.write(batch)\n                    pbar.update(len(batch))\nOnce the data has been written like this we can memory map it, following the PyArrow Cookbook. This completes almost instantly because it doesn’t actually load the data.\nwith pa.memory_map(train_arrow_path, 'r') as source:\n    tbl_train = pa.ipc.open_file(source).read_all()\nSentenceTransformers expects the data as a Dataset of InputExample. A Dataset is just any Python class that has a len and getitem methods, so we can fetch the data from the PyArrow Table by index, and convert it into an InputExample.\nfrom sentence_transformers import SentenceTransformer, InputExample\n\nclass InputDataset:\n    def __init__(self, left, right, outcome):\n        self.left = left\n        self.right = right\n        self.outcome = outcome\n        \n    def __len__(self):\n        return len(self.outcome)\n    \n    def __getitem__(self, idx):\n        return InputExample(texts=[self.left[idx].as_py(),\n                            self.right[idx].as_py()],\n                            label=float(self.outcome[idx].as_py()))\n                            \ntrain_examples = InputDataset(\n             tbl_train.column('sentence1'),\n             tbl_train.column('sentence2'),\n             tbl_train.column('label'),\n             )\n             \nWe can then pass this into a DataLoader and train a SentenceTransformers model as per the SentenceTransformers documentation. It’s worth comparing the performance with loading a subset into memory; in my case it made no difference because the GPU was the bottleneck.\nfrom torch.utils.data import DataLoader, RandomSampler\n\ntrain_dataloader = DataLoader(train_examples,\n                              batch_size=batch_size,\n                              sampler=RandomSampler(train_examples,\n                                                    num_samples=num_samples))\n                              \nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          epochs=3,\n          warmup_steps=0.1*num_samples,\n          use_amp=True)\nEvaluation is a bit more prickly. The built-in SentenceTransformers evaluation module takes the data in as lists to only ever encode the data once, and writes the output to a CSV. If you’ve got more data than fits in memory you may want to write your own incremental evaluation routine. If there is a lot of time spent recomputing embeddings it could be worth caching the results, for example using diskcache.\nAs I start to use SentenceTransformers more I’m finding some limitations of it’s inbuilt training loop (e.g. it doesn’t support gradient accumulation and so for large models I have to use really small batches which leads to poorer training with the standard parameters). But this is a good way to get started with really large datasets, and abstracts well to other cases, all thanks to the simplicity and flexibility of PyTorch’s concept of a Dataset."
  },
  {
    "objectID": "premature-optimisation/index.html",
    "href": "premature-optimisation/index.html",
    "title": "Code Optimisation as a Trade Off",
    "section": "",
    "text": "In writing character bitmaps to a framebuffer we need to scan through each row and column of the bitmap. In the tutorial Alex Chadwick suggests instead of keeping track of the row, by aligning the font bitmaps appropriately, we can just track the last bits of the address. This works well and removed a variable and we can use one less register. However if we want to use a bitmap with more rows we may need to change not only the code but also the font alignment; it’s been made more difficult to maintain.\nThe optimisations for writing character bitmaps look like they create more efficient code, but I haven’t profiled them. What I do see is they end up calling the subroutine DrawPixel once for each bit with value 1, and so the efficiency really depends on this routine. And looking into it we can see that at each step it does a bunch of loading data from memory and comparisons that are the same each time it’s invoked. These could potentially be much more impactful on performance than the register we saved above.\nUnfortunately there are tradeoffs trying to reduce the amount of work DrawPixel does. One task it has is to check whether the coordinates of the pixel lie in the width and height of the framebuffer. We could remove these to save time in an UnsafeDrawPixel, and move the range checking logic into DrawCharacter where it could be done once. But having two implementations, DrawPixel and UnsafeDrawPixel, duplicates the logic making it harder to change. We could combine the implementations by separating out the checking logic into a subroutine, but it leaves us with more risk of writing outside the framebuffer.\nAnother opportunity would be to completely inline DrawPixel so we can keep the FrameBuffer pointer in a register rather than reloading it at each pixel. But this trades off modularity for speed; we have to duplicate the pixel drawing logic which increases the code complexity and gives more places to get things wrong. There are potential other opportunities for optimisation, but they require breaking other things such as the ABI making it harder to inter-operate with other code.\nUltimately there are often engineering tradeoffs for optimisation; we give up other desirable properties of code like generalisability, modularity or safety. And almost always we want to focus on those - writing a safe and easy to maintain system is hard enough in the good cases. However where we really need speed, in the slowest part of the program (which can only be reliably identified with profiling) it can be really worth investing in optimisation, even if it means giving up on some of these other things.\nAs Knuth said in Structured Programming with Goto Statements:\n\nProgrammers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not ot pass up our opportunities in that critical 3%."
  },
  {
    "objectID": "binary-binning/index.html",
    "href": "binary-binning/index.html",
    "title": "Binning Binary Predictions",
    "section": "",
    "text": "For example here’s a plot using the iris dataset showing how the probability that a flower is a “virginica” and not a “versicolor” changes with the sepal width of the flower. This kind of plot can show nonlinearities and indicate how we should include this variable in a logistic regression.\n\n\n\nBinned Probability Plot\n\n\nThe rest of this article will consider some alternatives and show how to produce this report in R using dplyr and ggformula.\n\nAlternative: Plotting the class\nI’ve seen a lot of plots showing the actual binary data across the continuous variable, but I find it really uninformative especially when there are lots of data points. In particular the points often overlap and we need a way to handle that.\nOne technique to show all the points is to jitter them horizontally, but in the middle region I find it really hard to judge whether there are more points at 0 or 1 (my eyes dart back and forth between the two).\niris %>%\nfilter(Species != 'setosa') %>%\ngf_jitter((Species=='virginica')~Sepal.Width, width=0, height=0.1)\n\n\n\nJitter Plot\n\n\nHowever a similar technique of plotting the points, with colours or shape by the category is often useful for understanding the joint distribution. If points overlap exactly jittering can help show them\niris %>%\nfilter(Species != 'setosa') %>%\ngf_jitter(col=~Species, Sepal.Length~Sepal.Width, width=0.05, height=0.05)\n\n\n\nJoint Jitter Plot\n\n\n\n\nAlternative: Plotting the distribution\nAnother option is to compare plots of the distribution of each of the outcomes.\nOne option is to plot the histogram or frequency polygons. However just using the counts means that you’re mentally trying to adjust the heights, so we should make it a percentage of the total:\niris %>%\nfilter(Species != 'setosa') %>%\nggplot(aes(x=Sepal.Width, y = ..count../sum(..count..), colour=Species)) +\ngeom_freqpoly(bins=10)\nThis is better than the jitterplot, but in the overlap it still takes some mental effort to compare the relative heights of the two.\n\n\n\nFrequency Plot\n\n\nUsing density plots is also common, and there are inbuilt heuristics for picking the bandwidth so you don’t need to manually work out the bins like in a frequency polygon or histogram. However this can sometimes oversmooth behaviour, and again comparing the areas in my head isn’t easy.\niris %>%\nfilter(Species != 'setosa') %>%\ngf_density(~Sepal.Width,fill=~Species, bw=\"SJ\")\n\n\n\nDensity plot\n\n\n\n\nBinned Probability Plot\nSo what if instead of drawing the frequency polygons and trying to compare the ratios visually, we calculate the probability in each bin? Let’s bin this into equal quantiles, so each probability has a similar uncertainty.\nFirst we create a function that returns a function that cuts into equal quantiles on the data. This means we could fit this on some training set, and apply it on a different dataset. To allow values outside the range we replace the lowest value with -Inf by default and the highest by Inf. If the data is concentrated and some of the quantiles are the same we remove the duplicate datapoints with unique.\nquantile_cutter <- function(x, n, right=TRUE, lowest=-Inf, highest=Inf) {\n    x_quantile <- unique(quantile(x, seq(0, 1, length.out=n+1)))\n    x_quantile[1] = lowest\n    x_quantile[length(x_quantile)] = highest\n    # In case the lowest is set to min(x), include.lowest\n    function (y) {cut(y, x_quantile, include.lowest=TRUE, right=right)}\n}\nFor plotting the data we just apply the quantile cutter to itself.\ncut_quantile <- function(x, n, right=TRUE, lowest=-Inf, highest=Inf) {\n    quantile_cutter(x, n, right, lowest, highest)(x)\n}\nThen we can use this to plot the probability in bins as a function of the input; in this case it looks rather similar to our quantile binned plot.\niris %>%\nfilter(Species != 'setosa') %>%\ngroup_by(bin = cut_quantile(Sepal.Width, 10)) %>%\nsummarise(Sepal.Width = mean(Sepal.Width),\n          p_virginica=mean(Species == 'virginica')) %>%\ngf_point(p_virginica ~ Sepal.Width)\n\n\n\nBinned Probability Plot\n\n\n\n\nBias-variance tradeoff and tree models\nThere’s some art to picking the number of bins. If it’s too small there’s a lot of bias and we can’t see how the probability changes with the variable. If it’s too large there’s too much variance and it’s hard to see the trend from the noise. I often find a little trial and error gets a reasonable result.\nWe could look at this as a machine learning problem; we could try to pick the number of bins that minimises the (cross-validated) cross-entropy. I’d be interested whether we could build heuristics for this, similar to those for picking the bandwidth of a kernel density estimator.\nTaking this a step further we can see this is really just a special case of a classification tree model where we’re forcing the cut points to have an equal number of points. Another approach would be to fit a tree model to the data, for example here’s a simple example using rpart:\ntree_model <- rpart::rpart((Species == 'virginica') ~ Sepal.Width,\n                    data = filter(iris, Species != 'setosa'),\n                    method=\"class\")\n\ndata.frame(Sepal.Width=seq(2.25, 3.5, by=0.001)) %>%\nmutate(prob=predict(tree_model, type=\"prob\", newdata=.)[,\"TRUE\"]) %>%\ngf_line(prob ~ Sepal.Width)\n\n\n\nBinary Classification Tree\n\n\nOf course we don’t have to use rpart, we could use bagged or boosted trees, conditional inference trees or optimal decision trees (e.g. pydl8.5 or OSDT). Taking this a step further is something like the Explainable Boosting Machine that fits boosted trees to every variable and pairwise interaction, that can then be plotted. The advantage of this sort of approach is it takes into account correlations and interactions between varaibles (you see the effect of this variable after taking account of the others).\nBut even if we’re using these models to understand the data and help select and transform features it may be worth falling back to a more interpretable model such as a general linear model if it gives similar performance and is more robust."
  },
  {
    "objectID": "rstanarm-probit-prior/index.html",
    "href": "rstanarm-probit-prior/index.html",
    "title": "Fixing sampler errors in probit regression with rstanarm",
    "section": "",
    "text": "fit_nes_probit <-\n rstanarm::stan_glm(rvote ~ income_int_std + gender + race +\n                           region + religion + education_cts +\n                           advanced_degree + party + ideology3 +\n                           gender : party,\n        family=binomial(link=\"probit\"),\n        data=nes92)\nWhen I got this error about the chains not converging:\nSAMPLING FOR MODEL 'bernoulli' NOW (CHAIN 1).\nChain 1: Rejecting initial value:\nChain 1:   Log probability evaluates to log(0), i.e. negative infinity.\nChain 1:   Stan can't start sampling from this initial value.\n...\nChain 1:\nChain 1: Initialization between (-2, 2) failed after 100 attempts.\nChain 1:  Try specifying initial values, reducing ranges of constrained values, or reparameterizing the model.\n[1] \"Error in sampler$call_sampler(args_list[[i]]) : Initialization failed.\"\nerror occurred during calling the sampler; sampling not done\nError in check_stanfit(stanfit) :\n  Invalid stanfit object produced please report bug\nEven worse it crashed my Jupyter IRkernel, and the only way I could get it running again was to restart the whole process! I continued to debug it in an R console which wouldn’t crash after a fail.\nThis surprised me because I found it worked fine with link=\"logit\", that is logistic regression. The solution was to set a lower scale value in the prior (the default is 4):\nfit_nes_probit <-\n rstanarm::stan_glm(rvote ~ income_int_std + gender + race +\n                           region + religion + education_cts +\n                           advanced_degree + party + ideology3 +\n                           gender : party,\n        prior=rstanarm::normal(scale=0.5, autoscale=TRUE),\n        family=binomial(link=\"probit\"),\n        data=nes92)\n\nWhy this works\nTo work out what was going on I started trying to simulate similar data that led to the same error. After some experimentation I found a simple reproducable failing example with a few noise predictors:\nN <- 1000\nfake_data <- data.frame(a=rbinom(N,1,0.5),\n                        b=rbinom(N,1,0.5),\n                        c=rbinom(N,1,0.5),\n                        d=rbinom(N,1,0.5),\n                        e=rbinom(N,1,0.5),\n                        y=rbinom(N,1,0.5))\n\nfit <- rstanarm::stan_glm(y ~ a + b + c + d + e\n                          family=binomial(link=\"probit\"),\n                          data=fake_data)\nThis seems to have a better than 25% chance of failing because one of the chains fails most times I run it. If I remove just one of the predictors it succeeds.\nI got suspicious about the priors, in Regression and Other Stories they state (at least for logistic regression) the priors on the coefficients are 2.5/sd(x). Here the standard deviation of all our predictors is just 0.5 (since they’re binomial with probaibility 0.5). I tried to check this on a model that fit:\nfit <- rstanarm::stan_glm(y ~ a + b + c + d,\n                          family=binomial(link=\"probit\"),\n                          data=fake_data)\nprior_summary(fit)\nWhich resulted in:\nPriors for model 'fit'\n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 4)\n\nCoefficients\n  Specified prior:\n    ~ normal(location = [0,0,0,...], scale = [4,4,4,...])\n  Adjusted prior:\n    ~ normal(location = [0,0,0,...], scale = [7.98,8.00,7.97,...])\n------\nSo it looks like the default scale was 4, and they’ve been increased by the inverse standard deviation, that is a factor of 2.\nThe next thing I tried was a flat prior on the coefficients, which converged (to my surprise):\nfit_flat <- rstanarm::stan_glm(y ~ a + b + c + d + e,\n                          family=binomial(link=\"probit\"),\n                          prior=NULL,\n                          data=fake_data)\nThe next idea I had was to make the priors slightly tighter, to try to make the coefficients closer to 0. Some experimentation found it would start to converse consistently around a scale of 1:\nfit_weak <- rstanarm::stan_glm(y ~ a + b + c + d + e,\n                          family=binomial(link=\"probit\"),\n                          prior=rstanarm::normal(scale=1, autoscale=TRUE),\n                          data=fake_data)\nI found for the NES model I was fitting I had to further reduce the scale to 0.5 for it to converge, or alternatively set a flat prior.\nI don’t understand how Stan works, but I’m guessing as we add more predictors there’s a greater chance of getting one that is far from it’s actual value (in this case 0). I suspect because the probabilities in the normal distribution go to zero much faster than in the logistic distribution, this leads to numerical errors in link=\"probit\" that don’t occur in link=\"logit\". I’ve asked in the Stan Discourse to see if anyone can give more insight.\n\n\nGetting the data\nFor reproducability, here’s how the nes92 dataset above was generated, with a bit of feature engineering.\nlibrary(dplyr)\n\nnes <- foreign::read.dta('https://raw.githubusercontent.com/avehtari/ROS-Examples/master/NES/data/nes5200_processed_voters_realideo.dta')\n\nnes92 <-\nnes %>%\nfilter(year == 1992) %>%\n# Only people who actually voted republican or democrat\nfilter(!is.na(presvote_2party)) %>%\ntransmute(\n    age= age,\n    gender=gender,\n    race=race,\n    region=region,\n    income=income,\n    religion=religion,\n    education=educ3,\n    party = partyid3_b,\n    rvote = presvote_2party == '2. republican',\n    ideology = ideo7, # ideo is just a summary\n) %>%\nselect(-father_party, -mother_party) %>%\nfilter_all(~!is.na(.))  %>%\nmutate(income_int = as.integer(income) - 1,\n       income_int_std = (income_int - mean(income_int))/(2*sd(income_int)),\n       advanced_degree = education == '7. advanced degrees incl. llb',\n       education_cts = (as.integer(education) - 5)/6,\n       ideology_int = (as.numeric(ideology) - 5)/6,\n       ideology3 = if_else(ideology_int < 0, \"liberal\", if_else(ideology_int > 0, \"conservative\", \"moderate\")))"
  },
  {
    "objectID": "retry-requests/index.html",
    "href": "retry-requests/index.html",
    "title": "Retrying Python Requests",
    "section": "",
    "text": "I was trying to download data from Common Crawl’s S3 exports, but occasionally the process would fail due to a network or server error. My process would keep the successful downloads using an AtomicFileWriter, but I’d have to restart the process. Here’s what the new code looks like:\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\nCC_DATA_URL = 'https://data.commoncrawl.org/'\nRETRY_STRATEGY = Retry(\n    total=5,\n    backoff_factor=1\n)\nADAPTER = HTTPAdapter(max_retries=RETRY_STRATEGY)\nCC_HTTP = requests.Session()\nCC_HTTP.mount(CC_DATA_URL, ADAPTER)\n\n...\n# was: response = requests.get(data_url, headers=headers)\nresponse = CC_HTTP.get(data_url, headers=headers)\n...\nWe use a requests.Session to be able to store the retry logic. An advantage of Session is it reuses the TCP connections, and in my case made downloading twice as fast.\nWe then give the session information about how to retry on a domain. We then store the information in a HTTPAdapter where we give it instructions on how to handle requests from a domain or protocol. There we specify the Retry strategy, in this case trying at most 5 times and exponentially backing off starting at 1 second.\nFor a deeper dive into how to handle difficult cases with requests check out the findwork.dev blog on advanced requests usage, which also covers using timeouts (which is generally important but I haven’t had an issue with yet)."
  },
  {
    "objectID": "discovering-job-titles/index.html",
    "href": "discovering-job-titles/index.html",
    "title": "Discovering Job Titles",
    "section": "",
    "text": "To do this we first normalise the words in the ad title, doing things like removing plurals and expanding acronyms. Then we search for common role words like “manager”, “nurse” or “accountant”; I’ve got a list of 120 of them. Then we look backwards to expand the role; e.g. from “manager” we could get to “account manager” or “product manager” or “software engineering manager”. Then we sort them by frequency, blacklist any ambiguous ones, and have a list of common role titles.\nThis works reasonably well, with a couple iterations on the ambiguous word list I get a long list of common titles. There are a couple of exceptional role titles that don’t fall in this order like “chef de partie” which I need to add separately (it starts with the most general part and then gets specific, compared to most role titles that start specific and then become generic).\nThe job website Indeed have a more aggresive pipeline for finding role titles. They just look for all groups of frequent terms in the titles after normalising the text. I would worry I then need to somehow remove phrases containing common words that aren’t job titles; like locations (e.g. London) or skills and benefits. They then further group these together using edit distance, so “manager” and “manger” would go together, as would “C# developer” and “C developer” (which should be different role titles). I definitely miss unusually phrased job titles with my method (e.g. “Engineering role”), but I get less false positives.\nThe implementation of my approach is very straightforward, I use a regular expression to find a role word (or an exception) with up to four preceding words. Four is roughly where I see diminishing returns on looking for longer role lengths, five would be another to try.\nrole_word_re = r'\\b(?:' + '|'.join(exceptions + role_words) + r')\\b'\npreceding_word = r'(?:\\b[\\w\\d]+\\s+)'\nrole_term_re = re.compile(preceding_word + '{0,4}' + role_word_re)\ndef find_maximal_roles(title):\n    normal_title = normalise_text(title)\n    return role_term_re.findall(normal_title)\nThis regular expression approach does have some warts, because regular expressions must be non-overlapping. For example “PA to CEO of top finance boutique” after expansion becomes “personal assistant to chief executive officer of top finance botique”, which gives roles “personal assistant to chief executive” and “officer”. Or “Assistant Head of Planning and Building Control” gets normalised to “planning and building control assistant head” which then gives roles “planning and building control assistant” and “head”. But it’s simple to implement so is a useful starting point.\nLooking through the roles there were a few ambiguous ones like “head”, “senior”, “officer” and “specialist” that were fine if they were part of a longer title, (e.g “finance head” or “semi senior”; which is apparently an accounting thing) but on their own were normally the result of a bad normalisation. After filtering these out I got a list of 1000 viable role titles that occurred in at least 20 ads. Some of these include seniority (like “head chef”) or work type (like “part time cleaner”), but are a very useful starting point for further analysis, such as of salary.\nFor further details see the Jupyter notebook."
  },
  {
    "objectID": "4am-rule/index.html",
    "href": "4am-rule/index.html",
    "title": "4am Rule for timeseries",
    "section": "",
    "text": "If you’ve got people from multiple timezones it’s useful to check accross the people to find out whether it’s a localised time. There’s often a way to infer gross location, for example through an IP lookup or by looking at content or activity specific to particular areas.\nFor example the Australian Broadcast Corporation released hourly data of web activity from 2016. However in the data description it doesn’t mention what time zone the data is in. A quick plot of the web pageview volume for 1 day in May shows the trough is at 3am and it likely corresponds to the main timezone for Australia: Australian Eastern Standard Time.\n\n\n\nPageviews by Hour is minimum at 3-4am\n\n\nWestern Australia is 2 hours behind the eastern states of Australia, and it’s not clear whether it would be measured in local time or AEST. The pages have metadata attached which includes a field of pipe separated content topics, which often includes a state or city. It seems likely information about a state or city will be most viewed by people in that location. Looking at a plot of a week of web pageviews by hour based on the location of the content shows the trough in Western Australia is also around 3-5am. This means it’s likely to be local time and the timestamps are probably based on the users device.\n\n\n\nPageviews by Hour by State is minimum at 3-4am\n\n\nThe metadata also has a “content first published” field, and it’s not clear what timezone that is in. However if the page view data is in local time it will look like views from WA are lagged 2 hours relative to publishing time relative to other states. This is rather difficult to verify; it would be easier if it led by 2 hours because then we may see page views before it’s published. In this case because the population of WA is relatively small it can be ignored for most aggregate statistics.\nThe 4am rule is useful for verifying what timezone a digital activity dataset is in to within 2 hours. It’s simple enough to check quickly with a database query when there’s no reliable information on the timezone. Whenever publishing data use ISO-8601 format with a timezone so the timestamp data is unambiguous and consumers don’t need to use this rule."
  },
  {
    "objectID": "nlp-resources-2020/index.html",
    "href": "nlp-resources-2020/index.html",
    "title": "NLP Learning Resources in 2020",
    "section": "",
    "text": "There’s a lot of great freely available resources in NLP right now; and the field is moving quickly with the recent success of neural models. I wanted to mention a few that look interesting to me."
  },
  {
    "objectID": "nlp-resources-2020/index.html#jurefsky-and-martins-speech-and-language-processing",
    "href": "nlp-resources-2020/index.html#jurefsky-and-martins-speech-and-language-processing",
    "title": "NLP Learning Resources in 2020",
    "section": "Jurefsky and Martin’s Speech and Language Processing",
    "text": "Jurefsky and Martin’s Speech and Language Processing\nThe third edition is a free ebook that is in progress that covers a lot of the basic ideas in NLP. It’s got a great reputation in the NLP community and is nearly complete now. It makes a good starting point or reference book for a particular subject."
  },
  {
    "objectID": "nlp-resources-2020/index.html#stanford-cs224n-nlp-with-deep-learning",
    "href": "nlp-resources-2020/index.html#stanford-cs224n-nlp-with-deep-learning",
    "title": "NLP Learning Resources in 2020",
    "section": "Stanford CS224n: NLP with Deep Learning",
    "text": "Stanford CS224n: NLP with Deep Learning\nThis course covers Deep Learning from NLP in Pytorch and the 2019 video lectures are great. It’s run by Chris Manning who is a leader in the field of neural chunking and parsing, and for example guided Stanza."
  },
  {
    "objectID": "nlp-resources-2020/index.html#fast.ai-nlp-course",
    "href": "nlp-resources-2020/index.html#fast.ai-nlp-course",
    "title": "NLP Learning Resources in 2020",
    "section": "fast.ai NLP Course",
    "text": "fast.ai NLP Course\nThis is a lot less academic than other approaches, which gives a good contrast, in their library over Pytorch. Lectures available online; they also cover some NLP in the general fast.ai course."
  },
  {
    "objectID": "nlp-resources-2020/index.html#notable-free-resources",
    "href": "nlp-resources-2020/index.html#notable-free-resources",
    "title": "NLP Learning Resources in 2020",
    "section": "Notable free resources",
    "text": "Notable free resources\nThese all look good, but I would start with the above 3.\n\nCoursera NLP specialisation from deeplearning.ai in Tensorflow\nTraining transformer from Huggingface with very popular transformer library\nNLTK Book good for the foundations in Python\nYandex NLP Course including 2019 Jupyter notebooks (English), slides and lectures (Russian)\nCMU 2017 Neural NLP\nOxford 2017 Deep NLP\nEistenstein’s NLP notes - very detailed!\nVarious NLP Colab Notebooks\n\nAlso most Deep Learning courses will tend to touch on NLP, such as NYU 2020 Deep Learning with Pytorch."
  },
  {
    "objectID": "nlp-resources-2020/index.html#resource-resources",
    "href": "nlp-resources-2020/index.html#resource-resources",
    "title": "NLP Learning Resources in 2020",
    "section": "Resource Resources",
    "text": "Resource Resources\nAlmost all of these and more can be found at Awesome NLP and Awesome Deep Learning for NLP; they’re probably more up to date too."
  },
  {
    "objectID": "data-blockless/index.html",
    "href": "data-blockless/index.html",
    "title": "Data Blockless: A better way to create data",
    "section": "",
    "text": "However it’s possible to make data preparation straightforward and easy to extend, using a generalisation of the ListContainer from fastai part2 v3.\n\nWorked example\nCreating a Pytorch DataLoader for Deep Learning requires:\n\nReading in the data\nApply transforms\nLabel the inputs\nSplit into training, validation and test sets\nWrap it in a DataLoader\n\nHere’s an extract from a worked example using Imagenette.\nFor Imagenette (like for Imagenet) there are separate training and validation folders, each containing a folder with each object category. It’s easy to extract these labels with Pandas:\ndf = (pd\n   .DataFrame(image_paths, columns=['path'])\n   .assign(label=lambda df: df.path.apply(lambda x: x.parent.name),\n           split=lambda df: df.path.apply(lambda x: x.parent.parent.name),\n           train=lambda df: df.split == 'train')\n)\nData block provides a variety of common methods to load, label and split data, but in practice you have to tweak the datasets to make it work. Using Pandas is very flexible and almost as easy.\nWe’ll need to provide transforms that takes the image path, opens the file, resizes it and converts it to a tensor. Similarly we need a transform to one-hot encode categories. The transforms are applied lazily, so no images are opened until they’re accessed by the dataloader.\nimg_tfms = [img_open, partial(img_resize, size=128), img_rgb, img_to_float]\nimg_items = ListContainer(df.path, img_tfms)\n\ncat_tfms = [cat_norm]\ncat_items = ListContainer(df.label, cat_tfms)\nWe then label our items using the combine function:\nitems = img_items.combine(cat_items)\nThen we can split the training and validation set using the split method of ListContainers using the dataframe’s train mask, and then create a Pytorch DataLoader for each split.\ntrain_items, valid_items = items.split(df.train)\n\ntrain_dl = DataLoader(train_items, bs, shuffle=True)\nvalid_dl = DataLoader(valid_items, bs, shuffle=False)\nThe rest of this post will explore how all of this works.\n\n\nTransforming Items\nTo take a description of an item, like the path to an image, and converting it to a Tensor requires transformation. We don’t want to read the image into memory until we have to, so we only execute this transformation when we get a single item.\nThe ListContainer needs to store both the items and the transformations.\nclass ListContainer(object)\n    def __init__(self, items, tfms=None): self.items, self.tfms = list(items), list(tfms or [])\nWe have a method get to apply the transforms to an item:\ndef get(idx): return comply(self.tfms, self.items[idx])\nWhere comply, a portmanteau of compose and apply. If we have multiple items (like how a DataFrame has multiple series) then tfms is a list of lists of functions, where each list of functions is composed and applied separately to the items:\ndef comply(functions, x):\n    if len(functions) > 0 and isinstance(functions[0], Iterable):\n        assert len(functions) == len(x)\n        return [comply(f, xi) for (f, xi) in zip(functions, x)]\n    for f in functions:\n        x = f(x)\n    return x\n\n\nSelecting Objects\nListContainers can be subsetted the same way numpy arrays can, using a list of indices, a boolean mask or a slice. Subsetting creates another list container. This can be used for example to extract the training set: train_items = items[df.train]\nWhen a single integer index is passed, it calls get to apply the transformations. This will be used by the dataloader.\ndef __getitem__(self, idx):\n    if isinstance(idx, int): return self.get(idx)\n    elif isinstance(idx, slice): return self.__class__(self.items[idx], self.tfms)\n    # Must be a list\n    elif len(idx) > 0 and isinstance(idx[0], (bool, np.bool_)):\n        if len(idx) != len(self.items):\n            raise IndexError(f'Boolean index length {len(idx)} did not match collection length {len(self.items)}')\n        assert len(idx) == len(self.items), \"Boolean mask must have same length as object\"\n        return self.__class__([o for m,o in zip(idx, self.items) if m], self.tfms)\n    else: return self.__class__([self.items[i] for i in idx], self.tfms)\n\n\nList Functionality\nThere are standard methods provided for the length, iteration, changing and removing items.\n    def __len__(self): return len(self.items)\n    def __iter__(self): return (self[i] for i in range(len(self.items)))\n    def __setitem__(self, i, o): self.items[i] = o\n    def __delitem__(self, i): del(self.items[i])\n\n\nSplitting Test and Training\nTo be able to split a set into test and training we need to be able to pick out the complement of a selection with exclude. Similar to __getitem__ it must be able to handle a boolean mask, a slice or a list of indices.\ndef exclude(self, idxs):\n    if isinstance(idxs, slice): idxs = range(len(self))[idxs]\n    if len(idxs) == 0: return self\n    elif isinstance(idxs[0], (bool, np.bool_)):\n        return self[[not x for x in idxs]]\n    else:\n        return self[[x for x in range(len(self)) if x not in idxs]]\nSplit just combines __getitem__ and exclude. Because each returns another ListContainer the set could be split again to produce separate train, validation and test sets.\ndef split(self, by):\n    return (self[by], self.exclude(by))\n\n\nLabelling Data\nTo label the data we just store an additional LabelList, similar to Series in a DataFrame.\n    def combine(self, *others):\n        for other in others:\n            assert len(self) == len(other)\n        lists = (self,) + others\n        items = zip(*[getattr(l, 'items', l) for l in lists])\n        tfms = [getattr(l, 'tfms') for l in lists]\n        return self.__class__(items, tfms)\nThere also need to be methods to extract the item from the label.\n    def separate_one(self, dim):\n        return self.__class__([item[dim] for item in self.items], self.tfms[dim])\n\n    def separate(self):\n        dim = len(self.tfms)\n        return [self.project_one(i) for i in range(dim)]\nNote that these methods are very generic, the same idea could be used to combine text and image data.\n\n\nDisplaying\nThe items can be displayed at the console, before transformation:\n    def __repr__(self):\n        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n        if len(self)>10: res = res[:-1]+ '...]'\n        if self.tfms: res += f'; Transformed by {self.tfms}'\n        return res\nGenerally you also want a datatype specific way to display your data (e.g. show an image or play an audiofile). This would need to be added by subclassing ListContainer for separate instances.\n\n\nPreprocessing\nFastai has a separate preprocessing stage that is invisible, which has confused me and others when trying to build a character level language model. Any preprocessing would be done explicitly before putting items into the ItemList; since they are to be applied once, to all the data. It would be straightforward to create an explicit preprocessing function for each data type that returns a corresponding ListContainer.\n\n\nTransformations in the DataLoader\nIt’s also possible to put arbitrary transformations in the DataLoader’s collate_fn argument.\nclass TransformCollate:\n    def __init__(self, tfms=[], collate=torch.stack):\n        self.tfms = tfms\n        self.collate = collate\n\n    def __call__(self, items):\n        return self.collate([comply(self.tfms, item) for item in items])\nThen you wouldn’t need to lazily apply transformation in the ListContainer, they would be applied by the DataLoader as needed.\nimg_collate = TransformCollate(img_tfms)\ntrain_x = DataLoader(df.path[df.train], bs, collate_fn=img_collate, shuffle=True)\nWe could also provide a method for combining the collation function for the items with the collation function for the labels.\nclass ProductCollate:\n    def __init__(self, *collates):\n        self.collates = collates\n\n    def __call__(self, items):\n        items = list(zip(*items))\n        assert len(items) == len(self.collates)\n        return tuple(collate(item_group) for collate, item_group in zip(self.collates, items))\nThis can be used to get a DataLoader for the labelled items:\nxy_collate = ProductCollate(img_collate, cat_collate)\ntrain = DataLoader(list(zip(df.path[df.train], df.label[df.train])),\n                   bs,\n                   collate_fn=xy_collate,\n                   shuffle=True\n                  )\nI personally prefer the transformation to occur in the ListContainer because you can easily interactively explore the output of transforms, and because the transforms are stored with the object they don’t need to be passed around like the custom collation functions. But lazy transformations used in ListContainer are unusual in Python and could cause confusion.\nI’m going to continue working on improving deep learning workflows in my agilitai library based heavily on fastai v3 part 2."
  },
  {
    "objectID": "gulag-archipelago/index.html",
    "href": "gulag-archipelago/index.html",
    "title": "The Gulag Archipelago: Audiobook Review",
    "section": "",
    "text": "I highly recommend this book. What struck me most is how a large proportion of people persecuted their fellow countrymen and women on completely spurious charges; from those that ran the camps and the system, to those that reported “political” offences, to those that passively stood by. The horrors these people were subjected to makes me think of the people currently being persecuted in my own society, to no means to the same extent, in prisons and refugee camps. I don’t know much about these people; who is there, how they got there, how they are being treated and what their life may look like after release. The book in this sense reminds me of First they came… about Nazi Germany.\nAs one small point I think about the recent failed attempt to ban mobile phones in immigration detention centres. If this was in place then there is no way that Behrouz Boochani could have won the Victorian prize for literature for a book written in a detention centre on his mobile phone; No Friends but the Mountains. This resonates with how Solzhenitsyn had to hide his book across different locations as he wrote it, in case the authorities would confiscate and destroy it."
  },
  {
    "objectID": "categorical-embeddings/index.html",
    "href": "categorical-embeddings/index.html",
    "title": "Building Categorical Embeddings",
    "section": "",
    "text": "The idea is to turn the category into a vector representing the data using other information about it. This lets you pool information between categories that are similar in some sense. In neural networks this is how categorical data is typically represented (often initialised with random vectors) and then the embeddings are fit via back-propagation. However if you can separately generate some embeddings you can effectively use them with linear or logistic regression, or have them as a more effective initialisation in a neural network model.\n\nCreating embeddings\nThe idea of embeddings is to represent a categorical feature by some vector that captures that information to reuse in another setting. The information used can create different embeddings, and potentially complementary embeddings could be combined.\nOne example is behavioural embeddings, where you take user behaviour to represent the items. For example items that are purchased by the same people, or in the same basket, are similar. One approach I’ve successfully used is to create the item-user co-purchase matrix, and then reduce it by a Truncated Singular Value Decomposition. Then for each category the embedding is the average over the items for that category.\nAnother similar kind of co-occurrence matrix is a term-frequency matrix. If there are terms or features that are associated with each item in the category you can build a category-term frequency matrix, and transform it for example with TF-IDF.\nYou can also use embeddings based on rich data associated with the items. For example if there are textual descriptions you can create embeddings from a language model, or if there are images you can create embeddings from an image model. This lets you reuse other existing models that may be fine-tuned to specific applications. To get it back to a category level you can average over all the items in the category. If they’re normalised to be on the unit sphere you can rebuild that normalisation by normalising the average (since the centre of items on a sphere is the projection of their Euclidean centre; this lets you find items that are close to categories using an appropriate metric.\nFor an open categorical variable there needs to be a way to impute an embedding for unseen categories. One option is to calculate the mean of all the other category embeddings, perhaps weighted by frequency. Another approach would be to explicitly keep an “other” category for categories with few items, and build a specific embedding for those.\n\n\nEvaluating Embeddings\nWe can think about embeddings as enabling pooling information between similar categories. Consequently they will have the most advantage where the information is sparse and there are some categories that are more similar than others. I’ve found binary classification problems to be a fruitful testing ground, where each data point only contains a single bit of information, and so pooling can be very useful.\nYou can evaluate this using binary cross entropy (also called logistic loss, or log loss), which is a measure of how likely the data is given the model. However where there’s sufficient data you can compare the percentage predicted for the category with the actual percentage of positive cases in the category (just keep in mind the standard deviation of the binomial is \\(\\sqrt{\\frac{p(1-p)}{n}\\), which bounds how accurately you can evaluate the percentage from the data). In any case it’s useful to compare uplift compared with the constant model of predicting the overall average probability (i.e. number of positive cases divided by total number of cases) for every category, and a One Hot Encoding of the categorical variable (with appropriate treatment of uncommon categories, and appropriate use of any available hierarchy information).\nFor running these evaluations I’ve found Logistic Regression with a L2 regularisation (i.e. logistic ridge regression) works quite well. In scikit learn this is the default for sklearn.linear_model.LogisticRegression and you just need to tune the regularisation parameter C (you can do this automatically with cross-validation using LogisticRegressionCV), and it can handle large sparse term-frequency matrices using scipy.sparse.csr_matrix. Regularisation allows use of large dimensional embedding matrices without over-fitting.\n\n\nOpen questions\nI’ve found these methods quite effective to actually solve real information problems on a single categorical variable. However this is just the beginning; how do we combine multiple embeddings for one variable, or multiple variables?\nFor multiple embeddings for one variable you could, in theory, just concatenate the embeddings together. However I have found this doesn’t always work better with regularised regression and I’m not exactly sure why. One potential issue is different scales of coefficients, which could be addressed by weighting or standardisation. Other options would be to ensemble the separate logistic regression models, or to try different types of models.\nThere are even more options to combine multiple variables. They can just be added to the embeddings as extra variables in the regression, with appropriate preprocessing, and the model re-fit. However if there are interactions you would need to multiply each embedding vector with the other vectors, which could quickly get quite large, and at this point it may be worth considering another model. Similarly one could combine embeddings of other categorical variables, with the same caveats about interactions. Neural networks could be a very strong candidate for these problems as they can build complex interactions between the variables, and even fine tune the embeddings themselves. Taking this to the extreme the embedding tasks could be combined in a single multi-task setting, which could potentially mine the relevant information more effectively (but is much more complex).\nAnother approach is joint embeddings between two categories. Suppose you have two different categories that both fit within a single embedding task. One way to create a joint embedding would be to train embeddings separately and multiply each of the columns (so for a N dimensional embedding and an M dimensional embedding, we create an N*M dimensional joint embedding) to create an interaction; however this ignores the interaction structure. Another approach would be to treat the pair of categories as a single categorical variable and build an embedding on it; but that loses the relationships between the categories separately. There should be an approach midway between the two that appropriately estimates the marginal embeddings but incorporates information from the joint structure - but I don’t know what that should look like (and I would look to neural network models for inspiration).\n\n\nPotential Case Studies\nIt would be nice if I had some examples to go with this. Some potentially interesting historical Kaggle competitions to experiment on would be Avito Demand Prediction, PetFinder.my Adoption Prediction and Google Analytics Customer Revenue Prediction. Also notable is the Rossmann Store Sales Competition where third place was won by a neural network model.\nOnce you’ve built and embedding in a Pandas DataFrame indexed by the category name it can be wrapped in an sklearn transformer as below:\nimport pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass Embedder(BaseEstimator, TransformerMixin):\n    def __init__(self, embeddings: pd.DataFrame) -> None:\n        self.embeddings = embeddings\n        \n        # Impute missing values with the mean\n        # This could be extended to also handle a weight\n        missing_vector = np.mean(embeddings, axis=0)\n        \n        self.embeddings_matrix = np.vstack([missing_vector, embeddings.to_numpy()])\n\n        self.category_to_index = {v:k+1 for k,v in enumerate(embedings.index)}\n        \n    def fit(self, X: pd.Series, y=None):\n        return self\n    \n    def transform(self, X: pd.Series) -> np.array:\n        indices = X.map(self.category_to_index).fillna(0).astype('int64')\n        return self.embeddings_matrix[indices]\nThis can then be used with a ColumnTransformer and LogisticRegression in an sklearn pipeline:\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n  ('columns', ColumnTransformer([\n       ('embedding', Embedder(embeddings), 'category_column_name'),\n       ])),\n  ('classifier', LogisticRegression(C=1)),\n])\nI hope to build some open examples I can share in the future."
  },
  {
    "objectID": "bmi/index.html",
    "href": "bmi/index.html",
    "title": "Estimating Weight with Body Mass Index",
    "section": "",
    "text": "It’s relatively easy to measure human height, as a human. For example I’m about 180cm tall; the top of my nose is about 170cm, the bottom of my chin is about 160cm and the bottom of my neck is about 150cm. Using this, and assuming similar face proportions for other people, I can estimate the height of anyone I’m standing near.\nEstimating weight is much higher; unless you work in a profession like nursing where you regularly weigh people, we rarely get feedback on weight. However BMI, which is a ratio of mass to height squared in kg/m2, is pretty stable. Below is a graph of BMI for adult Australian women.\n\n\n\nBMI Distribution of Australian Women\n\n\nThe peak of Australian Adult Female BMI is about 24 or 25. Looking at the half maximum, a a lighter person is closer to 20 and an overweight person is closer to 30. You could use this to make reasonable estimates of weight.\nThe chart for Australian Adult men is broadly similar, just a bit more sharply peaked and a tiny bit to the right. But the estimates are the same.\n\n\n\nBMI Distribution of Australian Women\n\n\nIt’s actually quite interesting that BMI goes with height squared. Another ratio sometimes used is the Corpulence Index, sometimes called the Ponderal Index or Tri-Ponderal Mass Index (TMI). It’s weight over height cubed; so assumes that we grow as a cube.\nIt’s not as used because it seems to be less effective for detecting obesity in adults, but it’s somewhat more stable over age over ages of above 7, whereas the BMI increases with age. So for estimating the age of older children you’re better off using a Corpulance Index of about 12. On the over hand for children under 7 a BMI of about 16 seems a good estimate. It would be interesting to understand why there’s a change in the appropriate ratio at about 7 years of age.\n\n\n\nBMI vs TMI over Age\n\n\nKeep in mind the distributions vary a lot by country, depending on nutrition and genetics. However I would expect the ratios to be a good estimate if you can consistently judge build; are they gaunt or very muscular?"
  },
  {
    "objectID": "clustering-segmentation/index.html",
    "href": "clustering-segmentation/index.html",
    "title": "Clustering for Segmentation",
    "section": "",
    "text": "First you’ll need some data with items and features about the items; for example this could be from a customer database or from a survey you have conducted. In the customer example it could look like:\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_id\nfirst_date\nstate\nindustry\nacquisition_channel\nsupport_calls\nannual_spend\n\n\n\n\n1\n2003-05\nSA\nagriculture\nReferral\n2\n10,000\n\n\n2\n2020-03\nVic\nmanufacturing\nDirect\n0\n3,100\n\n\n\n\nClustering and Descriptive features\nYou need to separate your clustering features from your descriptive features. The clustering features are things you would segment your customers on, for example it might not make sense to use support_calls because this will be highly variable and won’t apply to new customers, or maybe the first_date isn’t really going to be meaningful for customer behaviour. The descriptive features are the non-clustering columns that are useful for characterising a group.\nIn this example it might make sense to have state and industry as clustering features, and support_calls and annual_spend as descriptive features. The simplest clustering is a full pivot of the clustering features; that is every combination. In this case we’d treat every state and industry as a separate group. The problem with this is we may end up with lots of groups, and many small groups for uncommon combinations (like customers in the ACT in manufacturing). Sometimes you can sweep this away by collecting all the small groups into an Other group, but you can lose valuable information this way.\n\n\nSimilarity measure\nClustering works by grouping together objects that are more similar to each other than those in other groups. To do this we need to define what “similar” is. The general way of doing this is to combine distances as a weighted average of features (this is described well in Elements of Statistical Learning (2nd ed.) section 14.3).\nFirst you need a notion of distance for each feature in the similarity measure. For example you might have a notion of distances between states (maybe 1 if they share a border or 0 otherwise), and industries (based on subject matter expertise; for example you might curate your own hierarchy). Otherwise you could just say the distance is 1 if they are the same and 0 if they are different (see Gower’s formula for a fairly general dissimilarity function). For annual spend the distance could be the absolute difference between them.\nThen you combine them to create a distance with a weighted average accross the features. Because they are on different scales a weight of 1 does not give them all equal importance (and in this example annual_spend would dominate). It’s best to normalise the individual metrics so the average distance accross all pairs of items is 1, so they are on the same scale. Even then you will want to give more weight to some features than others; this is an iterative process where you use domain knowledge to choose how important each feature will be in your clustering.\nAn alternative approach to a similarity measure is having a fitness function of the clustering. For example if we were most interested in clustering together customers with similar annual spend we could treat it as a regression problem. Then the quality of our cluster could be the (cross-validated) root mean square error of predicting the mean annual spend accross a cluster for each customer in that cluster. We could then use regression trees over the cluster variables to create the clusters.\n\n\nClustering Algorithm\nThere are lots of clustering algorithms such as k-medoids (the k-means equivalent for non-Euclidean distances) and hierarchical clustering, or more exotic ones like self-organising maps.\nYou can try a few different algorithms but this is normally the easy bit; the hard part is evaluating it.\n\n\nEvaluation\nClustering is difficult because there’s no one way to evaluate a cluster. You have to think hard about what evidence that this clustering is going to be useful. In practice you’ll generally want to have the one segmentation accross a number of different use cases, and so you want to check it’s useful for all of them.\nThe best way to look at a cluster is to look at the descriptive and clustering features and make sure they make sense and reveal some insights. It’s always useful to have cluster size; generally clusters that are too small are not useful but you might make an exception for a group of your top few customers that contribute most to your revenue (and similarly too large clusters will wash out useful information, but that might be ok for many customers that make small purchases).\nThe clustering features will define your cluster; in this example it would be the groups of states and industries we consider the same. You should ask the question does it make sense for these to go together?\nThe descriptive features help understand your cluster; it’s worth looking at the centre and the spread, if not the whole distribution. For example you might want to know the typical number of support calls you get from these customers, the most common aquisition channel and the typical spend. Measures of spread will tell you how tight the clustering is; common examples are the standard deviation, interquartile range or percentage not in most common category. If the spread is similar to the whole dataset then the cluster isn’t telling you anything useful. It can be useful to just plot the distributions of individual features to get an idea of how it’s composed.\n\n\nIteration\nIt’s important to think through the clustering and descriptive features up front. Then you can evaluate on different measures, with different weights and different clsutering algorithms as much as necessary. I’ve found I often want to constrain the clustering using business knowledge and this is much more difficult with some algorithms than others.\nBecause the evaluation is subjective it makes sense to start with a simpler clustering method and try tuning feature weights or adding constraints until the clusters look useful. Reducing the iteration time is really useful for creating reasonable clusters; Shiny or ipywidgets are handy tools for experimenting interactively. As you look more at the clusters you’ll get clearer about what a good clustering looks like and refine the evaluation criteria.\nGenerally you want the clustering to be easy to explain, so it’s worth thinking about how to do this. Maybe making it easy to find what cluster something is in is good enough. Other time you may approximate the final clustering model with a more transparent model.\nClustering isn’t the best way to do things like customer targeting; there you’re better off building a direct predictive model. But for extracting information, like reporting, clustering can be a useful tool to understand your dataset."
  },
  {
    "objectID": "prodigy-ner-teach/index.html",
    "href": "prodigy-ner-teach/index.html",
    "title": "Active NER with Prodigy Teach",
    "section": "",
    "text": "Having built a reasonable NER model for recognising job titles I wanted to see if I could easily improve it with Protidy’s active learning. So I ran the ner.teach recipe, annotating the label job_title excluding the examples I already annotated (in the dataset job_titles), creating a new dataset called job_titles_1 using the model ./job_titles/ on the sample texts sample.jsonl.\nprodigy ner.teach \\\n       -l job_title \\\n       -e job_titles \\\n       job_titles_1 \\\n       ./job_titles/ \\\n       sample.jsonl\nIt then presented many example annotations showing the model confidence and asked me to reject or accept them. This binary system was a lot quicker than manual annotation, and put me in a different mindset for evaluation. It showed a score of model confidence and seemed to focus on where the model was most confident (near 1.0), uncertain (near 0.5) and least confidence (near 0.0). Some of the least confident ones were tedious to annotate as it would highlight things that seemed ridiculous like punctuation (but maybe that really helps train the model?). After I’d done around 700 annotations I tried retraining the existing annotations, with a 20% evaluation set to product a output model ./job_titles_1:\nprodigy ner.batch-train \\\n        -l job_title \\\n        -es 0.2 \\\n        job_titles_1 \\\n        ./job_titles/ \\\n        -o ./job_titles_1/\nWhile it gives me an apparent accuracy score of 91%, but this is based on the non-representative examples from active learning so I don’t know how much to trust it. I could evaluate it on the same set I trained the original model on (by passing -e job_titles above, which gives an apparent 83%), but this isn’t a fair evaluation because it contains examples it was trained on (unfortunately prodigy doesn’t have an easy way to split out the same evaluation set used in training). Ideally I would create a separate gold set, but instead I used prodigy to do a quick A/B evaluation between the models on a new dataset samples2.jsonl:\nprodigy ner.eval-ab \\\n        -l job_title \\\n        job_title_eval \\\n        ./job_titles/ \\\n        ./job_titles_1/ \\\n        ./sample2.jsonl\nIt was very easy for me to assess which result was closer.\n\n\n\nExample of Prodigy A/B interface\n\n\nAfter annoting 100 images I got a pretty clear result: I preferred the old models results 78 times out of 100.\n\n\n\nresult\n\n\nOf course this doesn’t mean that Prodigy’s active learning was bad. Maybe I retrained at too high a learning rate and lost too much of the original model weights. Or maybe my annotations drifted too much between the interfaces (or didn’t fit the underlying model well), and so made the result further from what I was trying to achieve.\nOne bad thing about A/B tests is the cost scales at least linearly with the number of models; for hyperparameter tuning it’s much better to use a gold set."
  },
  {
    "objectID": "levenshtein/index.html",
    "href": "levenshtein/index.html",
    "title": "Edit Distance",
    "section": "",
    "text": "I’ve previously looked at finding ads with exactly the same text in the Adzuna Job Salary Predictions Kaggle Competition, but there are a lot of ads that are slight variations.\nThe Python library editdistance has a fast implementation and supports and iterable with hashable elements. This means for comparing text we can pass in a whole string for a character-wise edit distance, or we can tokenise it into a list of words for a word-wise edit distance.\nJob ads differ in length dramatically, so I wanted to know how different they were relative to the largest one making a relative_editdistance. Identical texts will have a relative edit distance of 0 and texts that are completely different will have a relative edit distance of 1.\nimport editdistance\n\ndef relative_editdistance(a, b):\n    return editdistance.eval(a, b) / max(len(a), len(b))\nThen I could compare the ads (here characterwise) with a double loop; it took 50s on my laptop for 100 ads.\ndistance = {}\nads_sample = ads[:100]\nfor i, ad1 in enumerate(ads_sample):\n    for j, ad2 in enumerate(ads_sample):\n        if i < j:\n            distance[(i, j)] = relative_editdistance(ad1, ad2)\nThen looking at the most similar ads I could find near duplicates easily:\n\n\n\nSimilar ads for project and process engineer\n\n\nHowever to run this on the full 400k ads will take 36 years, because this scales quadratically. For identical ads I could sort them (or their hash), but this doesn’t work here because they could be different anywhere in the string (and often are different at the start and the end).\nIn then next part of the series I’ll look at using MinHash to solve the duplicate problem at scale."
  },
  {
    "objectID": "meaning-in-html/index.html",
    "href": "meaning-in-html/index.html",
    "title": "Finding Meaning in HTML",
    "section": "",
    "text": "I’ve written before about why you should use HTML in NLP and how we can extract the text with context by source mapping. In this article I want to delve into what semantics we want to extract from HTML and outline some steps to do that. Ultimately this is for practical reasons; I want to extract information and meaning from webpages, and some of that meaning is encoded in the HTML.\n\nLinguistics of HTML\nHTML is a framework for a wide variety of things from describing the scaffolding of how a page should be visually laid out, to a data structure that can be dynamically modified with Javascript for applications, through to a communication medium. I’m going to focus on the latter, thinking of using HTML in the same way we use spoken languages.\nOne useful feature of HTML is span elements like <em> and <strong> (and their less semantic cousins <i>, <b> and <u>) that signify that this piece of information is particularly important. These are analogous to when we are speaking we change the way say a word to bring attention to it; as can be seen in the corresponding emphasis element in the Speech Synthesis Markup Language. This has a long history in typesetting where different fonts are used to convey important parts of information (or sometimes different sorts of information such as names of works of art). It’s purely a historical accident that we consider quotes, indicating that someone is speaking, to be part of written language but not emphasis (but long quotes of other source are represented typographically and in HTML are <blockquote>).\nHTML can also refer to other things such as images, audio, or video embedded in the HTML or a hyperlink referencing another web page. This is the written equivalent of “pointing” at something as you talk about it, demonstrating what you’re talking about (although in some cases the media is the only subject, which is less relevant here). These have written analogues in pictures, and cross-references (through footnotes or bibliographies).\nHTML can also indicate structure which is more implicit in communication. A simple structure is the paragraph, <p>, which indicates a change in topic, and has a long tradition in written work. In fact it used to be explicitly denoted with a pilcrow ¶, and had language evolved differently we may not need this. In spoken language the change of topic is often indicated by a short break or a change in pace or body language. HTML also has more complicated structures like hierarchical headings from <h1> to <h6> which are something of a generalisation of epics consisting of stories consisting of chapters, which are useful for complex communication. There’s also lists, <ul> and <ol> which in spoken language we may enumerate on our fingers as we change pace.\nThe exact way HTML is used to convey these ideas depends on the context. Due to cultural differences or limitations of a particular environment the same concept can be represented in different ways. Consider the following HTML adapted from a job advertisement:\n<p><strong><em>The Client</em></strong></p>\n<p>Our client is ... </p>\n<p><em><strong>The Job</strong></em><br />\nThis is a job for a talented person ... </p>\n<p>You'll be responsible for ... <br />\n• Managing ... <br />\n• Reporting ... <br />\n</p>\nOn the first line they’re using strong and emphasis to create a header. On the third line they do the same thing but the order of emphasis and strong are switched, then they use break to end the heading rather than create a new paragraph. Finally a list is creates using bullet points. A more semantic way to communicate the same thing would be:\n<section>\n    <h1>The Client</h1>\n    <p>Our client is ... </p>\n</section>\n<section>\n    <h1>The Job</h1>\n    <p>This is a job for a talented person ... </p>\n    <p>You'll be responsible for ... </p>\n    <ul>\n        <li>Managing ... </li>\n        <li>Reporting ... </li>\n    </ul>\n</section>\nHow do we extract the key communication concepts in HTML?\n\n\nAlgebra of Semantic HTML\nThere are often different representations of HTML that are equivalent in communication. In the previous example <strong><em> has the same effect as <em><strong>; the elements commute. They’re also idempotent; <em><em> has the same effect as <em>. They’re not exactly equivalent; it’s easy to write CSS that makes them different, but in practice they tend to mean the same thing.\nThe rules of HTML prevent nesting paragraphs; by the whatwg standard the end of a paragraph is inferred by another paragraph, a header and many other things. That is <p><p></p> is interpreted as <p></p><p></p>.\nSectioning is implicit; a header element like <h3> implicitly starts a subsection if preceded by a higher level header (like <h2>), a sibling section if preceded by an equal level header (another <h3>), and a new section if preceded by a lower level header (like <h4>). This can be explicitly called out with the <section> tags, but rarely seems to in practice. This gives an outline-like tree structure to HTML documents (in contrast to the DOM tree like structure).\n\n\nMarkup Language Processing\nWe can think of these meaningful markup as features for a model. A heading for a section may be represented with a <h1> or with a paragraph consisting of a single sentence in a <em><strong>. We can extract these and feed them in an appropriate way to a model to detect headers. We can consider a HTML document not as a tree but as a series of sections containing a header and paragraphs (where a paragraph includes things like a list). The exact choices depend on the usecase."
  },
  {
    "objectID": "simple-metrics/index.html",
    "href": "simple-metrics/index.html",
    "title": "Simple Metrics",
    "section": "",
    "text": "As an analyst I spend many hours thinking about quantities and it’s relatively easy for me to understand a new quantity. And when building a model or analysis I spend more time thinking about the metrics I’m using. This can make a blind spot when communicating to someone else; I watch their eyes glaze over as I try to explain the metrics.\nIt’s best to transform it back to something they understand. Where possible use the standard reporting metrics they’re familiar with. An explainable metric needs to fit in a single simple sentence. Something like “97% of the time the model predicts the value within 10% of the actual value” is likely more meaningful to stakeholders than the root mean square log error is 0.02.\nOf course you should still track complex metrics internally when they’re the right thing. But for communicating value they should be as simple as possible using familiar terms. If you really do need a new term you will need to do a lot of work to explain and establish it."
  },
  {
    "objectID": "git-stash-index/index.html",
    "href": "git-stash-index/index.html",
    "title": "Git Stash Changesets",
    "section": "",
    "text": "But git stash reverts all files; and very often I want to keep some, especially configuration parameters. However there’s a way to stash just the files added for commit; use git stash --index. The name comes from the index being the list of changes that are to be added for the next commit. I use this in Emacs via magit which lets me easily stash chunks.\nThe only downside of this is it won’t let me pop the stash until the file is at the latest commit. This means if I want to combine the changes in a single commit I have to commit, then pop the stash and amend the commit. Not awful, just a little finicky. But it’s a pretty safe way to work.\nAnother alternative to stashes is branches. It’s very easy to make a temporary branch to store some work and move around. But stashes require a bit less context switching, and are convenient to use."
  },
  {
    "objectID": "stan-linear-priors/index.html",
    "href": "stan-linear-priors/index.html",
    "title": "Stan Linear Priors",
    "section": "",
    "text": "Priors in linear regression\nIn our previous model \\(y \\sim N(\\alpha + \\beta x, \\sigma)\\), we didn’t specify any priors for our parameters, the intercept \\(\\alpha\\), the coefficients \\(\\beta\\) and the residual standard deviation \\(\\sigma\\). We can extend our Stan model to take data specifying these priors, and to declare the priors themselves in the model.\nFollowing rstanarm::stan_glm it would be nice that if you didn’t specify a prior for it to use a reasonable default prior. Following the discussion in Regression and Other Stories, Section 9.5, we can use the same weak priors that they use that keep inferences stable, but don’t have much impact on the estimates.\nThe default prior for the coefficients is \\(\\beta \\sim N(0, 2.5 s_y/s_x)\\). Centring on 0 also makes sense without knowing which direction the coefficients should lie in. The ratio of standard deviations is important for the prior to be invariant under rescaling transformation. If we were to rescale \\(y' = k y\\) and \\(x' = A x\\), then the coefficients would scale as \\(\\beta' = kA^{-1} \\beta\\), so our prior should rescale in an analogous way. The factor 2.5, quoting from Regression and Other Stories, “is somewhat arbitrary, chosen to provide some stability in estimation while having little influence on the coefficient estimate when data are even moderately informative”. Perhaps the worst part of this assumption is that as you add more coefficients (and especially interactions) that the prior stays the same and they are all independent. Perhaps a better approach would be a joint distribution where some of the coefficients were more spread than others, since in many cases as you get more predictors a few of them may have a significant association but most will not.\nThe default weak prior for the intercept \\(\\alpha\\) is given indirectly by assigning a prior the expected value of y at the mean value of x is normally distributed with mean the mean value of y, and standard deviation 2.5 times the standard deviation of y; that is \\(E(y | x=\\bar{x}) \\sim N(\\bar{y}, 2.5 s_y)\\). Essentially we’re saying that at the centre of x, the data should be near the centre of y, and the error scales with the standard deviation of y (using a similar rescaling argument as above), again picking 2.5 as a . This is better than putting a prior directly on the intercept, because it’s invariant in a translation of $x$ or $y$; we’re always evaluating near the centre of the data (where we’re likely to have the most information). The expected value of y in our model is precisely \\(\\alpha + \\beta x\\), so we can rearrange this into \\(\\alpha \\sim N(\\bar{y} - \\beta \\bar{x}, 2.5 s_y)\\).\nFinally for the residual standard deviation assumed prior is \\(\\sigma \\sim {\\rm Exponential}(1/s_y)\\). This means in particular that the expected value is \\(s_y\\), which is reasonable from scaling assumptions, and that the value is non-negative. I’m not sure how reasonable the assumption in the distribution itself is, but I’ll take it as a given.\nWe further want to be able to extend from these default priors to enable passing informative priors. We can directly extend the prior for the coefficients to take a centre vector and standard deviation vector (or more generally a covariance matrix) that can be passed in place of the default priors. For the intercept \\(\\alpha\\) we could similarly specify a centre point and standard deviation, but to conform with the weak prior form we could pretend the data is centred, \\(\\bar{x} = 0\\), so the \\(\\beta\\) coefficient has no influence on the prior. Finally for the standard deviation we could pass a different parameter for the exponential distribution than the inverse standard deviation of y.\n\n\nWriting a Stan Model\nWith this plan we extend our Stan data to include the centre\n\\[\\begin{align}\n\\beta &\\sim N(\\mu_\\beta, s_\\beta) \\\\\n\\alpha &\\sim N(\\mu_\\alpha - \\beta \\bar{x}, s_\\alpha)\\\\\n\\sigma &\\sim {\\rm Exponential}(1/{\\mu_\\sigma})\n\\end{align}\\]\nFollowing rstanarm I refer to the centre as the location and the standard deviation as the scale, and I call the parameter in the exponential distribution the rate. Note that the priors are specified as part of the model.\n// Linear model - linear.stan\ndata {\n  int<lower=0> N;       // Number of data points\n  int<lower=0> K;       // Number of predictors\n  matrix[N, K] X;       // Predictor matrix\n  real y[N];            // Observations\n\n  // NEW: Data specifying priors\n  vector[K] prior_location;                // Coefficient Normal Prior - centre\n  vector[K] prior_scale;                   // Coefficient Normal Prior - standard deviation\n  real prior_intercept_location;           // Intercept Normal Prior - centre\n  matrix[1, K] prior_intercept_predictor;  // Intercept Normal Prior - offset centre by -beta * prior_intercept_predictor\n  real prior_intercept_scale;              // Intercept Normal Prior - standard deviation\n  real prior_aux_rate;                     // Exponential prior on sigma\n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real<lower=0> sigma;  // error scale\n}\nmodel {\n  // NEW: Prior distributions\n  beta ~ normal(prior_location, prior_scale);\n  alpha ~ normal(prior_intercept_location - prior_intercept_predictor * beta, prior_intercept_scale);\n  sigma ~ exponential(prior_aux_rate);\n\n  // Target Density\n  y ~ normal(alpha + X * beta, sigma); // target density\n}\n\n\nRunning the model from R\nAs before we can wrap this in a function, adding extra parameters for the priors. Note I set the defaults to FALSE; it would have made more sense to use NULL but I had an idea that I could copy rstanarm’s approach of using NULL for a flat prior before realising I’d need to do a lot of work to add that kind of flexibility.\nOne thing that caught me is a vector of length 1 will be treated as a scalar, not a vector, by Stan (because it’s hard to distinguish these in R), and so we need to wrap prior vectors passed to RStan in array. From the RStan vignette\n\nIf we want to prevent RStan from treating the input data for y as a scalar when N‘ is 1, we need to explicitly make it an array\n\nfit_stan_linear <- function(formula, data,\n                            ...,\n                            prior_location=FALSE,\n                            prior_scale=FALSE,\n                            prior_intercept_location=FALSE,\n                            prior_intercept_scale=FALSE,\n                            prior_aux_rate=FALSE) {\n    y <- model.response(model.frame(formula, data))\n    X <- remove_intercept_from_model(model.matrix(formula, data))\n\n    K <- ncol(X)\n    N <- nrow(data)\n\n    if (isFALSE(prior_location)) {\n        prior_location <- rep(0, K)\n    }\n\n    if (isFALSE(prior_scale)) {\n       prior_scale <-  2.5 * sd(y) / apply(X, 2, sd)\n    }\n\n     if (isFALSE(prior_intercept_scale)) {\n       prior_intercept_scale <- 2.5 * sd(y)\n    }\n\n    if (isFALSE(prior_aux_rate)) {\n       prior_aux_rate <- 1/sd(y)\n    }\n\n    if (isFALSE(prior_intercept_location)) {\n        prior_intercept_location <- mean(y)\n        prior_intercept_predictor <- matrix(apply(X, 2, mean), ncol=K)\n    } else {\n        # When a specific location is set, remove the effect of predictor offset\n        # by setting it to 0\n        prior_intercept_location <- prior_intercept_location\n        prior_intercept_predictor <- matrix(rep(0,K), ncol=K)\n    }\n\n\n    fit <- rstan::stan(\n        file = \"linear.stan\",\n        data = list(\n            N = nrow(X),\n            K = ncol(X),\n            X = X,\n            y = y,\n            prior_intercept_predictor = prior_intercept_predictor,\n            prior_intercept_location = prior_intercept_location,\n            # Need array when there is just 1 predictor\n            prior_scale = array(prior_scale, dim=K),\n            prior_location = array(prior_location, dim=K),\n            prior_centre_scale = prior_intercept_scale,\n            prior_sigma_rate = prior_aux_rate\n          ),\n        ...\n        )\n\n    names(fit) <- get_linear_names(names(fit), colnames(X))\n\n    structure(list(fit=fit, formula=formula, data=data), class=c(\"my_linstan\"))\n}\n\n\nTesting using priors\nAs a test of this functionality let’s compare rstanarm::stan_glm with our function on the SexRatio data from Section 9.5 of Regression and Other Stories (inspired by a study of the effect of Beauty on the sex ratio of children, where there is weak data and small priors).\nWe have a small data set of 5 points, representing the percentage of girl babies \\(y\\), as a function of standardised beauty \\(x\\).\nx <- seq(-2,2,1)\ny <- c(50, 44, 50, 47, 56)\nsexratio <- data.frame(x, y)\nThe weakly informative priors give similar results to the minimum likelihood estimator found by lm.\nfit_sexratio_lm <- lm(y~x, data=sexratio)\nfit_sexratio_default <- stan_glm(y ~ x, data=sexratio)\nfit_sexratio_default_stan <- fit_stan_linear(y ~ x, data=sexratio)\nThe coefficients are all very close to this (the estimated residual deviation of lm is a little lower at 4.3).\n            Median MAD_SD\n(Intercept) 49.3    1.9\nx            1.4    1.4\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 4.6    1.7\nHowever we can add an informative prior (which acts as regularisation on the coefficients), based on the fact the rate of girl births is around 48.5% to 49%, and based on prior studies we wouldn’t expect beauty to have more than a 0.8 percentage point impact on the rate of girl births.\nfit_sexratio_post <- stan_glm(y ~ x, data=sexratio, prior=normal(0,0.2), prior_intercept=normal(48.8, 0.5))\nfit_sexratio_post_stan <- fit_stan_linear(y ~ x, data=sexratio,\n                                          prior_location=0,\n                                          prior_scale=0.2,\n                                          prior_intercept_location=48.8,\n                                          prior_intercept_scale=0.5)\nThese give identical coefficient estimates to one decimal place.\n            Median MAD_SD\n(Intercept) 48.8    0.5\nx            0.0    0.2\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 4.3    1.3\nNow we know how to fit a simple linear model in Stan and add priors, it would be nice if we could make predictions and take posterior draws from it. That’s covered in the next article making Bayesian predictions with Stan and R."
  },
  {
    "objectID": "sicp-1_5/index.html",
    "href": "sicp-1_5/index.html",
    "title": "SICP Exercise 1.5",
    "section": "",
    "text": "Exercise from SICP:\nExercise 1.5.\nBen Bitdiddle has invented a test to determine whether the interpreter he is faced with is using applicative-order evaluation or normal-order evaluation. He defines the following two procedures.\nThen he evaluates the expression\nWhat behavior will Ben observe with an interpreter that uses applicative-order evaluation? What behavior will he observe with an interpreter that uses normal-order evaluation?"
  },
  {
    "objectID": "sicp-1_5/index.html#appicative-order",
    "href": "sicp-1_5/index.html#appicative-order",
    "title": "SICP Exercise 1.5",
    "section": "Appicative order",
    "text": "Appicative order\nWith applicative order the first step is to evaluate (test 0 (p)), expanding (p) by its definition. But that evaluates to itself so the program gets stuck in an infinite loop and does not terminate."
  },
  {
    "objectID": "sicp-1_5/index.html#normal-order",
    "href": "sicp-1_5/index.html#normal-order",
    "title": "SICP Exercise 1.5",
    "section": "Normal order",
    "text": "Normal order\nWe get the evaluation chain:\n(test 0 (p))\n(if (= 0 0) 0 (p))\n(if #t 0 (p))\n0\nSo it results in 0. Because we expand the definitions in normal order, and the if statement avoids it, we never hit the recursive loop in normal order."
  },
  {
    "objectID": "beta-distribution/index.html",
    "href": "beta-distribution/index.html",
    "title": "Bernoulli Trials and the Beta Distribution",
    "section": "",
    "text": "Suppose we want to know the probability of an event occurring; it could be a customer converting, a person contracting a disease or a student passing a test. This can be represented by a Bernoulli Distribution, where each draw is an independent random variable \\(\\gamma_i \\sim {\\rm Bernoulli}(\\theta)\\). The only possible values are failure (represented by 0) with probability \\(\\theta\\) and success (represented by 1) with probability \\(1-\\theta\\) (although the labels are completely arbitrary and we can switch them by setting \\(\\eta_i = 1 - \\gamma_i\\) then \\(\\eta_i \\sim {\\rm Bernoulli}(1-\\theta)\\)).\nThe probability distribution can be conveniently written as \\({\\mathbb P}(\\gamma = k) = \\theta^{k}(1-\\theta)^{1-k}\\), since \\({\\mathbb P}(\\gamma = 1) = \\theta^{1}(1-\\theta)^{0} = \\theta\\) and \\({\\mathbb P}(\\gamma = 0) = \\theta^{0}(1-\\theta)^{1} = 1 - \\theta\\). This form is convenient because for multiple variables the probabilities multiply (since the variables are independent), and the exponents add, giving a simple expression. In particular\n\\[\\begin{align}\n{\\mathbb P}(\\gamma_1=k_1,\\ldots,\\gamma_N=k_N) &= {\\mathbb P}(\\gamma_1=1) \\cdots {\\mathbb P}(\\gamma_N=k_N) \\\\\n&= \\theta^{k_1 + \\cdots + k_N} (1 - \\theta)^{N - (k_1 + \\ldots k_N)} \\\\\n&= \\theta^{z}(1-\\theta)^{N-z}\n\\end{align}\\]\nwhere z is the number of positive results (which is as in the binomial distribution, up to multiplicity from different orderings). Note that the result just depends on the total number of trials and the number of successes.\nIn the Bayesian framework the posterior probability distribution of \\(\\theta\\) can be estimated conditional on the observed data; in particular from Bayes rule:\n\\[\\begin{align}\n{\\mathbb P}\\left(\\theta \\vert \\gamma_1=k_1, \\ldots, \\gamma_N=k_N\\right) &= \\frac{{\\mathbb P}\\left(\\theta \\vert \\gamma_1=k_1,\\ldots,\\gamma_N=k_N\\right)P(\\theta)}{P(\\gamma_1=k_1,\\ldots,\\gamma_N=k_N)} \\\\\n&= \\frac{\\theta^z(1-\\theta)^{N-z}P(\\theta)}{\\int_0^1 P(\\gamma_1=k_1,\\ldots,\\gamma_k \\vert \\theta=k_N) P(\\theta) \\,{\\rm d}\\theta}\n\\end{align}\\]\nTo get a posterior distribution we need to choose an appropriate prior. A flat prior is a reasonable starting point if we know nothing about the situation, \\(P(\\theta) = 1, \\; \\forall \\theta \\in[0,1]\\). Then from the above the posterior will be proportional to \\(\\theta^{z}(1-\\theta)^{N-z}\\) (up to a normalising constant). This is a special case of the Beta distribution; if \\(\\Theta \\sim {\\rm Beta}(\\alpha,\\beta)\\) for positive \\(\\alpha, \\beta\\) then\n\\[P(\\Theta=\\theta) = {\\rm Beta}(\\alpha, \\beta)(\\theta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\\]\nWhere the normalising denominator is the Beta Function \\(B(\\alpha, \\beta) = \\int_{0}^{1} \\theta^{\\alpha}(1-\\theta)^{\\beta-1}\\, {\\rm d}\\theta = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). Notice that in particular \\({\\rm Beta}(1,1)\\) is the (flat) uniform distribution on [0,1].\nThe special thing about the Beta Distribution is it’s a conjugate prior for Bernoulli trials; with a Beta Prior distribution for the probability of positive cases \\(\\theta\\) then the posterior is also a Beta distribution. Specifically \\({\\mathbb P}\\left(\\theta \\vert \\gamma_1=k_1, \\ldots, \\gamma_N=k_N\\right) \\propto \\theta^{k + \\alpha - 1}(1 - \\theta)^{N-k + \\beta - 1}\\), and so the posterior is distributed as \\({\\rm Beta}(\\alpha + z, \\beta + N-z)\\), and in particular for a uniform prior it is \\({\\rm Beta}(z + 1, N-z+1)\\)."
  },
  {
    "objectID": "beta-distribution/index.html#properties-of-the-beta-distribution",
    "href": "beta-distribution/index.html#properties-of-the-beta-distribution",
    "title": "Bernoulli Trials and the Beta Distribution",
    "section": "Properties of the Beta Distribution",
    "text": "Properties of the Beta Distribution\nSince given a flat (or more generally Beta) prior we get a Beta posterior for the Bernoulli probability \\(\\theta\\) it makes sense to study the properties of the Beta distribution to understand \\(\\theta\\).\n\nMaximum likelihood\nThe most likely value can be found with a bit of differential calculus. The derivative is\n\\[\\frac{{\\rm d}{\\rm Beta}(\\alpha, \\beta)}{{\\rm d} \\theta}(\\theta) =  \\frac{\\theta^{\\alpha-2}(1-\\theta)^{\\beta-2}}{B(\\alpha, \\beta)}\\left(\\alpha - 1 - (\\alpha + \\beta - 2)\\theta\\right)\\]\nwhich may be zero at \\(\\hat{\\theta} = 0, 1, \\frac{\\alpha - 1}{\\alpha + \\beta - 2}\\). The extremum is a maximum when the second derivative is negative. The second derivative at the local extrema are:\n\\[\\begin{align}\n\\frac{{\\rm d^2}{\\rm Beta}(\\alpha, \\beta)}{{\\rm d} \\theta^2}(\\hat{\\theta}) &= \\frac{\\rm d}{{\\rm d}\\theta}\\left.\\left(\\frac{\\theta^{\\alpha-2}(1-\\theta)^{\\beta-2}}{B(\\alpha, \\beta)}\\right)\\right\\vert_{\\theta=\\hat\\theta}\\left((\\alpha - 1)- (\\alpha + \\beta - 2)\\hat\\theta\\right) \\\\\n&- \\left(\\frac{\\hat\\theta^{\\alpha-2}(1-\\hat\\theta)^{\\beta-2}}{B(\\alpha, \\beta)}\\right) (\\alpha + \\beta - 2)\\\\\n&=- \\left(\\frac{\\hat\\theta^{\\alpha-2}(1-\\hat\\theta)^{\\beta-2}}{B(\\alpha, \\beta)}\\right) (\\alpha + \\beta - 2)\\\\\n&=- {\\rm Beta}(\\alpha, \\beta)(\\hat\\theta) \\frac{(\\alpha + \\beta - 2)}{\\hat\\theta(1-\\hat\\theta)}\n\\end{align}\\].\nwhich is negative if and only if \\(0 < \\hat\\theta < 1\\) and \\(\\alpha + \\beta > 2\\). In the case when \\(\\alpha = 1\\) and \\(\\beta > 1\\) then the derivative \\(-\\frac{\\theta^{-1}(1-\\theta)^{\\beta-2}}{B(1, \\beta)}(\\beta - 1)\\theta\\) is negative on the whole interval (0,1), and so the function decreases from its maximum value at \\(\\hat\\theta=0\\). Similarly when \\(\\beta=1\\) and \\(\\alpha > 1\\) then \\(\\hat\\theta=1\\) and the derivative is positive on the whole interval (0,1) and so the function increases to its maximum valu at \\(\\hat\\theta=1\\). So overall in all cases where \\(\\alpha + \\beta > 2\\) the maximum likelihood occurs at \\(\\hat\\theta=\\frac{\\alpha - 1}{\\alpha + \\beta - 2}\\) which is necessarily in the interval [0,1].\nThis gives the same results as a Maximum Likelihood analysis for the Binomial when we observe z successes from N trials with a uniform prior. With the \\({\\rm Beta}(z+1, N-z+1)\\) distribution the maximum likelihood estimator is \\(\\hat\\theta=\\frac{z}{N}\\), the proportion of successes. The second derivative also matches an approximate normal distribution of \\(\\theta\\) with standard deviation \\(\\sqrt{\\frac{\\hat\\theta(1-\\hat\\theta)}{N}}\\) as would be obtained from the efficiency of Maximum Likelihood Estimators using the Fisher Information Matrix for the binomial.\nWhen \\(\\alpha + \\beta \\leq 2\\) there isn’t necessarily a maximum likelihood estimate. When \\(\\alpha = \\beta = 1\\) then all values are equally likely. The Jeffrey’s Prior is \\({\\mathbb P}(\\theta) \\propto \\sqrt{I(\\theta)} = \\frac{1}{\\sqrt{\\theta(1-\\theta)}}\\) and so corresponds to a \\({\\rm Beta}(1/2,1/2)\\). In this case there is a local minimum at 1/2, and the most likely values are 0 and 1. But as soon as we add any data to a Jeffrey’s prior we do have a most likely estimate; for \\(\\alpha \\leq 1\\) and \\(\\alpha + \\beta \\geq 2\\) then the derivative is negative on the whole interval (0,1) and so the probability decreases from its maximum value at 0. Similarly for \\(\\beta \\leq 1\\) and \\(\\alpha + \\beta \\geq 2\\) then the derivative is positive on the whole interval (0,1) and the probability increases to its maximum value at 1.\nThe Bayesian framework allows us to ask questions that are harder just using asymptotic analysis. For example we can calculate things like how likely \\(\\theta\\) is greater than 1/2, and come up with a credible interval for the parameter based on the data. The cost of this is having to specify a prior (and some extra calculations).\n\n\nMean and variance\nThe mean can be calculated using the properties of the Beta function. Given \\(\\Theta \\sim {\\rm Beta}(\\alpha, \\beta)\\) then\n\\[\\begin{align}\n{\\mathbb E}(\\Theta) &= \\int_0^1 \\theta {\\mathbb P}(\\Theta=\\theta) \\, {\\rm d}\\theta\\\\\n&= \\int_0^1 \\frac{\\theta^{\\alpha}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)} \\\\\n&= \\frac{B(\\alpha + 1, \\beta)}{B(\\alpha, \\beta)} \\\\\n& = \\frac{\\alpha}{\\alpha + \\beta}\n\\end{align}\\]\nNotice that the mean value is well defined for all positive \\(\\alpha, \\beta\\), and when the mode exists it is closer to the edges of the distribution than the mean.\nWe can similarly calculate the expectation of the square:\n\\[\\begin{align}\n{\\mathbb E}(\\Theta^2) &= \\int_0^1 \\theta^2 {\\mathbb P}(\\Theta=\\theta) \\, {\\rm d}\\theta\\\\\n&= \\frac{B(\\alpha + 2, \\beta)}{B(\\alpha, \\beta)} \\\\\n&= \\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta+1)(\\alpha+\\beta)}\n\\end{align}\\]\nThis then gives variance\n\\[\\begin{align}\n{\\mathbb V}(\\Theta) &= {\\mathbb E}\\left((\\Theta - {\\mathbb E}(\\Theta))^2\\right)\\\\\n&={\\mathbb E}(\\Theta^2) - {\\mathbb E}(\\Theta)^2 \\\\\n&= \\frac{\\alpha}{\\alpha+\\beta}\\left(\\frac{\\alpha+1}{\\alpha+\\beta+1} - \\frac{\\alpha}{\\alpha+\\beta}\\right) \\\\\n& = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\n\\end{align}\\]"
  },
  {
    "objectID": "beta-distribution/index.html#parameterisations-of-beta-distribution",
    "href": "beta-distribution/index.html#parameterisations-of-beta-distribution",
    "title": "Bernoulli Trials and the Beta Distribution",
    "section": "Parameterisations of Beta Distribution",
    "text": "Parameterisations of Beta Distribution\nSummarising our previous results we have for a \\({\\rm Beta}(\\alpha, \\beta)\\) distribution the mean is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\), the variance is \\(\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\) and the mode, for \\(\\alpha, \\beta \\geq 1\\) and \\(\\alpha + \\beta > 2\\) is \\(\\omega = \\frac{\\alpha -1}{\\alpha+\\beta-2}\\). However we can use these properties to themselves define the Beta distribution which is useful for different contexts.\nFirstly note that \\(\\alpha\\) is analogous to the number of successes and \\(\\beta\\) is analogous to the number of failures in the Bernoulli trials. These are additive, so that given an \\({\\rm Beta}(\\alpha, \\beta)\\) prior and z successes with \\(v = N-z\\) failures the posterior is \\({\\rm Beta}(\\alpha + z, \\beta + v)\\). So in this parameterisation the successes and failures add.\nAnother way to look at it is in terms of size \\(\\kappa = \\alpha + \\beta\\) and the mode \\(\\omega\\). These are analogous to the number of trials and proportion of successes respectively. We can rewrite \\(\\alpha = (\\kappa - 2)\\omega + 1\\) and \\(\\beta = (\\kappa - 2)(1- \\omega) + 1\\), and for \\(\\kappa \\geq 2\\) we can always express the Beta function in terms of \\(\\kappa\\) and \\(\\omega\\). Given N trials with a proportion of successes \\(p=z/N\\), the posterior has size \\(\\kappa' = \\kappa + N\\), and posterior mode \\(\\omega' = \\frac{(\\kappa - 2)}{N + \\kappa - 2} \\omega + \\frac{N}{N + \\kappa - 2} p\\), so it’s a weighted average of the individual probabilities. In summary sample sizes add, and proportions combine as a weighted average, which makes intuitive sense when thinking about combining the results of Bernoulli trials.\nFinally sometimes it can be useful to think in terms of the mean and the variance. These don’t have quite as clean as an interpretation in terms of the data, but the mean represents how skewed the data is (in a slightly less extreme way than the mode), and the variance is inversely related to the size since the certainty increases with more data. The size can be expressed as \\(\\kappa = \\frac{\\mu(1-\\mu)}{\\sigma^2} - 1\\) and \\(\\alpha = \\kappa \\mu\\), \\(\\beta = \\kappa(1-\\mu)\\). They combine in a more complex way."
  },
  {
    "objectID": "beta-distribution/index.html#summary",
    "href": "beta-distribution/index.html#summary",
    "title": "Bernoulli Trials and the Beta Distribution",
    "section": "Summary",
    "text": "Summary\nFor binomial trials the Beta distribution occurs naturally as a conjugate prior for the binomial probability \\(\\theta\\). Starting with a uniform prior and adding data with N trials and z successes we get a \\({\\rm Beta}(z+1,N+z-1)\\) posterior for \\(\\theta\\). This has its maximum probability at the sample proportion \\(p=z/N\\), and we can alternately write the distribution as \\({\\rm Beta}(Np + 1, N(1-p) + 1)\\). The sample proportions combine as a weighted average; given \\(N_1, N_2\\) trials with sample proportions \\(p_1, p_2\\) the combined size is \\(N_1 + N_2\\) with proportion \\(\\frac{N_1 p_1 + N_2 p_2}{N_1 + N_2}\\).\nChoosing a \\({\\rm Beta}(\\alpha, \\beta)\\) prior is equivalent to starting with a flat prior and adding an additional \\(\\alpha - 1\\) successes and \\(\\beta - 1\\) failures; or equivalently having a successful proportion of \\(\\omega = \\frac{\\alpha-1}{\\alpha + \\beta - 2}\\) out of \\(\\kappa - 2 = \\alpha + \\beta - 2\\) trials. This framing is useful in understanding hierarchical binomial models. A lot of this is based heavily on Chapter 6 of Kruschke’s Doing Bayesian Data Analysis."
  },
  {
    "objectID": "exceed-expectations/index.html",
    "href": "exceed-expectations/index.html",
    "title": "Exceed Expectations",
    "section": "",
    "text": "Consistently delivering what you promise to customers is the way to build trust and loyalty. Going a little bit further and throwing in something extra is a good way to quickly impress customers and set yourself apart. It’s not about being substantially more or devaluing, but adding small complements that your competitors don’t.\nIt can be as small as putting a bookmark in with a delivery of a book. Maybe it’s customising a client deliverable in colours that resonate with the company’s brand. Or a virtual course can be packaged with additional materials and content.\nOf course there’s no value in this if you don’t deliver what is actually expected. Project estimation is hard, so a lot of that is around being conservative in your estimates and regularly updating stakeholders to temper their expectations. A part of this is ruthlessly prioritising and only agreeing to work that you can do without stretching yourself too thin.\nSo exceed everyone’s expectations, but make sure it’s manageable by setting reasonable expectations and only agreeing to the work that will really progress your goals."
  },
  {
    "objectID": "extract-skills-2-adpositions/index.html",
    "href": "extract-skills-2-adpositions/index.html",
    "title": "Extracting Skills from Job Ads: Part 2 - Adpositions",
    "section": "",
    "text": "Extracting experience in something\nBy looking at the parse trees of candidate phrases using displaCy I adopted the following strategy:\n\n\n\nExample Dependency Parse\n\n\n\nStart at the word experience, and look for a preposition (such as in or of) dependent on it (red in the above diagram)\nLook for the object of the preposition (orange)\nReturn the phrase ending at that object (green)\n\nor in code:\ndef extract_adp_experience(doc):\n    for tok in doc:\n        if tok.lower_ == 'experience':\n            for child in tok.rights:\n                if child.dep_ == 'prep':\n                    for obj in child.children:\n                        if obj.dep_ == 'pobj':\n                            yield 'EXPERIENCE', obj.left_edge.i, obj.i+1\nA simpler way to do this is:\n\nStart at the word experience followed by a preposition (such as in, of, or with)\nGet the noun phrase following it\n\nUsing spaCy’s noun chunks we have to implement this backwards:\ndef extract_adp_experience_2(doc):\n    for np in doc.noun_chunks:\n        start_tok = np[0].i\n        if start_tok >= 2 and doc[start_tok - 2].lower_ == 'experience' and doc[start_tok - 1].pos_ == 'ADP':\n            yield 'EXPERIENCE', start_tok, start_tok + len(np)\nBoth algorithms give similar results, so there’s some flexibility in how you write the extraction rules.\nWe could try to further extend the rules with examples where there’s an extra level of indirection, such as:\n\nPrevious experience working as a Chef de Partie in a one AA Rosette hotel is needed for the position.\n\n\nExperience of techniques such as Discrete Event Simulation and/or SD modelling Mathematical/scientific background\n\n\nThe post holder must hold as a minimum Level 1 in Trampolining (British Gymnastics) and have experience in working with children, be fun, outgoing and have excellent customer service skills and be able to instruct in line with the British Gymnastics syllabus.\n\nbut the rules become increasingly complex and aren’t likely to add much to the results.\n\n\nAnalysing the Results\nA company sometimes posts a job ad many times with very similar text, so it makes more sense to rank results by the number of distinct companies that posted the term rather than the number of times it occurs alone.\nWhile the top terms contains some generic phrases (like “a similar role” or “the following”), it also contains a lot of genuine skills like “design”, “C”, “selling”, and “project management”. The broader skills like “design” and “selling” often have a qualifier that we are not extracting (e.g. “selling into the industrial sector” is different to “selling into the veterinary/animal industry”), but it’s a pretty good start.\nLooking at the first 50,000 ads here are the top 30 extracted skills:\n\n\n\nTerm\nNumber of Companies\nNumber of Occurrences\n\n\n\n\na similar role\n213\n461\n\n\nthe following\n130\n261\n\n\nsales\n77\n106\n\n\none\n55\n85\n\n\nthe design\n53\n83\n\n\nthe use\n49\n72\n\n\ndesign\n47\n76\n\n\nC\n46\n87\n\n\nselling\n43\n60\n\n\nthis role\n42\n87\n\n\nall aspects\n40\n66\n\n\nthis\n39\n58\n\n\nexperience\n38\n55\n\n\nthe following areas\n37\n65\n\n\nplanning\n37\n46\n\n\nteaching\n34\n63\n\n\nany\n34\n54\n\n\ndevelopment\n34\n56\n\n\nproject management\n34\n49\n\n\nthis field\n33\n58\n\n\nthe industry\n33\n51\n\n\na manufacturing environment\n31\n46\n\n\nSQL Server\n30\n57\n\n\nsoftware development\n29\n46\n\n\na\n28\n50\n\n\nsome\n28\n44\n\n\na similar environment\n28\n42\n\n\nSQL\n28\n35\n\n\nthis area\n27\n47\n\n\n\n\n\nRelating different skills\nIt would be really interesting to see which skills occur together, but ads aren’t likely to contain the phrase “experience in/with” many times, and so we’re not likely to extract many skills from a single ad. However ads frequently list experience in long lists, for example “Experience in design, development or quality engineering”.\nIn the next part we will extract these phrases and investigate what skills frequently occur together."
  },
  {
    "objectID": "project-estimation/index.html",
    "href": "project-estimation/index.html",
    "title": "Project Estimation",
    "section": "",
    "text": "Everyone knows that construction jobs are typically going to take longer and cost more than quoted, from home renovations to major construction projects. But even though everyone knows this it still happens. Part of this may be due to misaligned incentives; you may be more likely to choose the builder with the lower and faster quote, so there is an incentive to under-quote even though it damages their reputation. But another part of it is the planning fallacy, where estimates are based on likely best case scenarios; no rainfall, all materials arrive on time and no mistakes that need to be fixed occur during construction. However in a long project some of these are certain to occur, which is why the time and cost is higher than quoted, rather than using an informed average of previous projects.\nA similar thing happens in software and analysis projects. When planning you think through all the things that need to be done, make rough estimates of how long they would take (which are typically best case), and add them to get a total estimate. This “inside view” will be optimistic because there will be other work that needs to be done concurrently, coordination issues, technical and data issues even assuming that the scope will not change. This is a big problem because if you give this estimate you’ll inevitably disappoint stakeholders that are expecting your quote to be accurate, and lose their trust.\nKanhneman discusses this problem in Thinking Fast and Slow Chapter 23, The Outside View. The best solution is to look at how long similar tasks have taken in the past as a reliable indicator. Keep track of how long (in elapsed time) it actually has taken you to deliver similar solutions, and ask others involved in similar projects how long it has taken them. He also suggests adjusting the estimate slightly if you know the conditions are more or less favourable than typical, but I’d be tempted to keep these estimates small.\nThis works even better if you can have small frequent deliverables. This is a useful technique because people rarely know exactly what they want from analytics and software projects, but if you have something to show them they can evaluate it and give feedback. Having smaller deliverables also means you’ll more quickly build a database of time estimates for similar projects, so you can estimate more reliably.\nWhen you give an estimated delivery date you’re going to be held accountable. Being late is bad, especially because there are often knock-on effects for downstream processes that will be delayed. So when estimating you shouldn’t be aiming for the median time, but somewhere near the 80th percentile. This might feel disingenuous but you’ll build up a reputation of someone who can be relied on to deliver.\nGiving larger estimates can cause pushbacks. Sometimes there’s an existing deadline, or they have prior expectations of how long it should take. I recommend staying strong on your estimates and educating your stakeholders that this is how long it will really take. If the estimates don’t meet their needs you can negotiate on scope and resourcing. Spending a lot of time understanding the business problem places you better to reliably deliver the most value.\nIf you continue to be told your deadlines are too long and they’re not willing to negotiate on scope or resources it shows a lack of trust. The project is unlikely to be successful unless you can get past this, and it may be best not to accept the project if possible.\nBeing able to give reliable estimates builds trust and helps your stakeholders plan. The only way to do this is to track how much time it really takes to do things, and aim for the upper end when estimating. This is easier to do by building smaller immediately useful deliverables. When you build a reputation as someone who can reliably deliver what they promise you’ll build better relationships and do better work."
  },
  {
    "objectID": "pip-resolve/index.html",
    "href": "pip-resolve/index.html",
    "title": "Pip Can Now Resolve Dependencies",
    "section": "",
    "text": "Now there is a new resolver in Pip 20.3 for pip that checks the dependencies and tries to find versions that meet all constraints. This is a huge step forward for Python, and makes it easier to adopt."
  },
  {
    "objectID": "which-bin-sh/index.html",
    "href": "which-bin-sh/index.html",
    "title": "Which /bin/sh",
    "section": "",
    "text": "set: Illegal option -o pipefail\nI had a quick look and the first line was #!/bin/sh, the -o pipefail isn’t valid across POSIX shells so I would expect that to fail. More specifically on modern Ubuntu /bin/sh is dash which doesn’t support these bash like constructions.\nBut /bin/sh is very different on different systems; on some it is bash, on others it’s ash (from which dash is derived), and on others it’s ksh or something else. The script probably worked on whatever system it was developed on because it had a different /bin/sh. This has been a problem since Ubuntu switched from bash to dash in 2006; a lot of scripts assumed they could use bash and worked fine, but suddenly stopped working. People tend to work with an implementation over a specification; if it runs on their machine it’s right.\nIt’s easy to fix by changing the shebang, for example to #/bin/bash, or the harder way is to make the script POSIX compliant. The excellent shellcheck will tell you what parts are not POSIX and makes it easier to chagne."
  },
  {
    "objectID": "davx5/index.html",
    "href": "davx5/index.html",
    "title": "Syncing Calendars and Contacts to Android with DAVx5",
    "section": "",
    "text": "DAVx5 is simple to set up and has worked almost flawlessly for me for over 4 years. It supports two way synchronisation to CalDAV and CardDAV servers that many email providers support. Once it’s set up you don’t even notice it; it just works invisibly to keep your calendar and contacts up to date.\nI use the calendar through the default LineageOS Calendar App (and I’m sure it would work as well on Android), and contacts are immediately available in the phonebook or when I email using k9mail.\nI also use the wonderful open source Birthday Adapter app to notify me when it’s the birthday of a contact (and the birthdate is kept in sync so I’ll still have it if I lose my phone!)\nIn this age of maximising user engagement through incessant notifications I’m immensely grateful to the people who donate their time to build and maintain an app that quietly does its job very well."
  },
  {
    "objectID": "representation-fourier-transform-1/index.html",
    "href": "representation-fourier-transform-1/index.html",
    "title": "Linear representation of additive groups and the Fourier Transform: Part 1",
    "section": "",
    "text": "To begin I want to consider linear representations of the cyclic group of order n: that is I want to assign to each element of the group a linear operator on an inner product space in a way consistent with the group structure [or if you prefer, to find a homomorphism from the cyclic group to the group of automorphisms of an inner product space (an orthogonal group)]. There are lots of ways to do this, for lots of different vector spaces – the simplest is to map every group element to the identity (the trivial (linear) representation).\nIt would be nice to have some sort of canonical linear representation. Given a set we can form a vector space by taking all formal linear combinations of its elements (that is we consider the elements of the set to be linearly independent vectors, and the vector space is their span). If a group acts on that set we can extend it to a linear representation of the induced vector space by extending the group linearly; this is called the permutation representation.\nFor example if the set is \\(\\{a,b,c\\}\\) the vector space is three dimensional and consists of all elements of the form \\(\\{x_a a + x_b b + x_c c| x_a,x_b,x_c \\in \\mathbb{C}\\}\\) . The group of all permutations on three elements acts on the set, and given such a permutation \\(\\sigma: \\{a,b,c\\} \\to \\{a,b,c\\}\\) it is represented by the linear mapping \\(x_a a + x_b b + x_c c \\to x_a \\sigma(a) + x_b \\sigma(b) + x_c \\sigma(c)\\) .\nNow the group G acts on the set G by left multiplication, and so we can construct a permutation representation. This is called the regular representation of G.\nWhat does this look like for a cyclic group of order n? The vector space has a basis of \\(\\{e_0,e_1,e_2,\\ldots,e_{n-1}\\}\\) , and the group element 1 is represented by the linear transformation S satisfying \\(Se_i=e_{i+1}\\) (where addition is modulo n). The group element k=1+1+…+1 is represented by \\(S^k=SS\\cdots S\\) .\nThere is also a natural inner product \\((e_i,e_j) = \\delta_{i,j}\\) and this is invariant under S (that is S is unitary). As a matrix \\(S=\\begin{bmatrix} 0 & 0 & 0 & \\ldots & 0 & 1\\\\ 1 & 0 & 0 & \\ldots & 0 & 0\\\\ 0 & 1 & 0 & \\ldots & 0 & 0\\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots &\\vdots\\\\ 0 & 0 & 0 & \\ldots & 1 & 0\\end{bmatrix}\\) .\nNow since S is unitary it is normal and hence by the spectral theorem unitarily diagonalisable. So let’s look for it’s eigenvectors and eigenvalues: since \\(S^n = I\\) it’s clear its eigenvalues must be nth roots of unity, so denote \\(\\omega = \\exp{2\\pi i/n}\\) (the choice of sign, and to some extent root, is arbitrary). We can in fact easily see that \\(v_k = (e_0 + \\omega^{-k} e_1 + \\omega^{-2k} e_2 + \\ldots \\omega^{-(n-1)k} e_{n-1})/\\sqrt{n}\\) is a normalised eigenvector of S with eigenvalue \\(\\omega^{k}\\) (go on, check it!). Actually the normalised eigenvectors are only determined up to an overall phase, so \\(v'_k=e^{i \\phi_k} v_k\\) would work equally well, but I’ll stick to these phase conventions for convenience.\nThe diagonalising matrix is then \\(F= \\frac{1}{\\sqrt{n}}\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1 \\\\ 1 & \\omega & \\omega^2 & \\ldots & \\omega^{n-1} \\\\ 1 & \\omega^2 & \\omega^4 & \\ldots & \\omega^{2(n-1)}\\\\ \\vdots &\\vdots &\\vdots & \\ddots &\\vdots\\\\ 1 & \\omega^{n-1} & \\omega^{2(n-1)} & \\ldots & \\omega^{(n-1)(n-1)}\\end{bmatrix}\\) .\nSo \\(F^\\dagger S F = \\mathrm{diag} (1,\\omega,\\omega^2,\\ldots,\\omega^{n-1})\\) . In fact F diagonalises every group element by multiplication: \\(F^{\\dagger} S^k F = (F^{\\dagger} S F)^k = \\mathrm{diag}(1,\\omega^k,\\omega^{2k},\\ldots,\\omega^{(n-1)k})\\)\nF is precisely the discrete Fourier transform (up to a choice of normalisation): if \\(v=\\sum_{j=0}^{n-1} v^n e_n\\) , then \\(F(v) = \\frac{1}{\\sqrt{n}}\\sum_{j,k=0}^{n-1} e^{(-2\\pi i/n) j k} v^j e_{k}\\) .\nMany of the properties of the discrete Fourier transform follow immediately; we know it is unitary by the spectral theorem which is precisely the Plancherel theorem. In particular it is invertible, which gives completeness. One half of the shift theorem is also immediate \\(FS^k = F S^k F^\\dagger F = (F^{\\dagger} S^{-k} F) F = \\mathrm{diag} (1,\\omega^{-k},\\omega^{-2k},\\ldots,\\omega^{-(n-1)k}) F\\) . One can see from the explicit form for F that \\(F(e_i)=F^{\\dagger} (e_{-i})\\) and so if we define the operator \\(N e_i = e_{-i}\\) then \\(F^2=F F = F F^{\\dagger} N = N\\) (though this would be different if we had chosen a different normalisation condition), so applying F to the half of the shift theorem above gives the other half (is there an easier way to see this?).\nWhat about convolutions? Given that each basis vector corresponds to a group element, there is a natural algebraic structure on the vector space, namely \\(e_i \\otimes e_j = e_{(i+j)}\\) (where as usual addition is modulo n). This is precisely a convolution; Excercise: by requiring \\(\\otimes\\) to be distributive and expanding in component prove \\(v \\otimes w = \\sum_{j=0}^{n-1} \\sum_{k=0}^{n-1} v^k w^{j-k} e_j\\) . What about the convolution theorem? Well we don’t really have an idea of a multiplicative structure (yet) so it doesn’t really make sense.\nWhat is the exact structure on V? There’s an inner product, but there’s also a relative ordering of the basis elements; it doesn’t matter where we start numbering the basis elements (except in the definition of convolutions) but S defines an order for them relative to each other. So to say the Fourier transform is defined by a complex inner product space is lying a little, because there is this extra structure. [Also, considering the Fourier transform is only defined up to a phase it could be more natural to think of two vectors being equivalent if they differ only by a phase.] Actually there is a much more natural way to introduce this structure.\nThere is another way to think of a permutation representation. We form the vector space associated to a set as the vector space of all linear functions from the set to the complex numbers. The basis vector corresponding to the element s is the characteristic function of s, \\(\\delta_s: S \\to \\mathbb{C}\\) which maps s to 1 and every other element to 0. (Exercise: Show this is equivalent to the description given before, at least if the set is finite). An arbitrary function can be decomposed into the basis of characteristic functions: \\(f = \\sum_{s \\in S} f(s) \\delta_s\\) . The action of a group element is \\((g \\circ f) (s) = \\sum_{t \\in S} f(t) \\delta_{g \\circ t} (s) = \\sum_{t \\in S} f(t) \\delta_{t} (g^{-1} \\circ s) = f(g^{-1} \\circ s)\\) .\nNow let’s look back at the regular representation of the cyclic group through this lens. We consider functions \\(f:\\mathbb{Z}/n \\to \\mathbb{C}\\) , with the inner product \\((f,g) = \\sum_{m=0}^{n-1} f(m)g(m)\\) and we have the shift operator \\(S \\in \\mathrm{Aut}(\\mathrm{Map}(\\mathbb{Z}/n,\\mathbb{C}))\\) given by \\((Sf)(m)=f(m-1)\\) . The Discrete Fourier Transform is given by \\((Ff)(m) = \\frac{1}{\\sqrt{n}} \\sum_{k=0}^{n-1} f(k) e^{-(2 \\pi i/n) m k}\\) . The diagonalisation property is that \\(F^\\dagger S F\\) is a multiplicative operator, equivalent to pointwise multiplication by the function \\(\\hat{S}(m)=e^{2\\pi i m/n}\\) . (Indeed Halmos notes that any normal operator can be unitarily mapped to a multiplicative operator is one way of viewing the spectral theorem).\nA convolution is then \\((f \\otimes g)(m) = \\sum_{k=0}^{n-1} f(k) g(m-k)\\) . Now taking the Fourier transform of a convolution of basis elements \\(F(\\delta_j \\otimes \\delta_k) = F(\\delta_{j+k}) = \\sum_{l=1}^{n} \\omega^{-(j+k)l}\\delta_l\\) , and using that the pointwise product \\(\\delta_l \\delta_m = \\delta^l_m \\delta_l\\) (no sum) means we can rewrite it as \\(\\sum_{l=1}^{n} \\sum_{m=1}^{n} \\omega^{-jl} \\delta_l \\omega^{-km} \\delta_m\\) that is \\(F(\\delta_j \\otimes \\delta_k) = F(\\delta_j) F(\\delta_k)\\) . Applying linearity gives one half of the convolution theorem: \\(F(f \\otimes g) = F(f) F(g)\\) . The other half is readily obtained using \\(F^2=N\\) . Thus the Fourier transform maps the additional ring structure given by pointwise multiplication to the convolution structure given by the regular representation.\nSo what have we got? We started looking at regular linear representations of the cyclic group, and to change to a basis in which the group operations were diagonal we invented the discrete Fourier transform.\nThe power in this idea is there are many generalisations. We could have a look at more complicated groups or even more general algebraic structures. The representation theory of cyclic groups is very simple since they are abelian, there’s a lot more involved in trying to diagonalize the representations of non-abelian groups. We could then have other notions of convolutions and Fourier-type transforms. We could also look at mapping to other vector spaces or even to different geometric structures. If instead of constructing vector spaces over the complex numbers we constructed it over finite fields we would get (for the right combination of dimension of the vector space and characteristic of the field) the finite Fourier transform which is important in coding theory. One could also look at what happens to direct sums, tensor products and the like of the regular representations."
  },
  {
    "objectID": "mass-air-bedroom/index.html",
    "href": "mass-air-bedroom/index.html",
    "title": "Mass of Air in Bedroom",
    "section": "",
    "text": "Estimate the mass of air in your bedroom.\n\nPacing out my bedroom it’s roughly 3m on each side, and the ceiling is maybe a meter higher than I am, so say 3m high. So the volume of my bedroom is about (3m)³ or 27m³.\nThe density of air is roughly 1 kg/m³, so the mass of air in my bedroom is around 30 kg.\nInterestingly this is about the same mass as a small box of books. This is hard to appreciate because I can’t really imaging moving the air in a room."
  },
  {
    "objectID": "openlibrary-exploration/index.html",
    "href": "openlibrary-exploration/index.html",
    "title": "What’s in Open Library Data",
    "section": "",
    "text": "I’m working on a project to extract books from Hacker News, and want to link the books to records from Open Library. I’ve already looked at the process of adding a book to Open Library and loading a data export into sqlite. Now I really want to look through the data and see what’s inside.\nI do this through two different perspectives; summarising the metadata and looking at some specific records."
  },
  {
    "objectID": "openlibrary-exploration/index.html#editions",
    "href": "openlibrary-exploration/index.html#editions",
    "title": "What’s in Open Library Data",
    "section": "Editions",
    "text": "Editions\n\n70% of the editions have been automatically imported from one of MARC, Better World Books, Internet Archive, or Amazon (listed in source_records).\n64% of editions have at least one ISBN 10 or ISBN 13 (this is asked for in manual uploads, or an LCCN)\nAlmost always have a title, and sometimes a subtitle (41%) a full_title (19%; often the concatenation of the title and subtitle), an edition_name (14%), and occasionally other_titles (8%).\nGenerally have authors (89%), and sometimes a by_statement (43%) which is how the authors are listed as text in the book\nEditions often contain a publisher (96%), publish_date (98%) and 58% of the time locations in publish_places and publish_country (the latter of which is often a US state and not in the Open Library user interface)\n60% of the time subjects are available; other details about what the book is about are available less than 10% of the time, such as subject_places, subject_people, subject_time, and genres\n48% have lc_classifications and 18% a dewey_decimal_class which help identify the topic\nIt can be connected to other databases using things like lccn, oclc_numbers, identifiers, ocaid.\nSometimes more information is in notes, a table_of_contents, description, or a first_sentence\nThere are sometimes a cover (33%) image hosted on Open Library"
  },
  {
    "objectID": "openlibrary-exploration/index.html#works",
    "href": "openlibrary-exploration/index.html#works",
    "title": "What’s in Open Library Data",
    "section": "Works",
    "text": "Works\nA work can contain multiple editions. In the Open Libary user interface it’s not very clear how you edit a work, but some changes on editions automatically changed the work (such as adding a cover0.\n\nWorks largely have a subset of the fields of editions, not always consistent with the editions\nThe authors are normally a superset of the authors of the editions, typically there’s only one author (89% of the time), and 5% of the time no author.\nOn average there’s 1.3 editions per work"
  },
  {
    "objectID": "openlibrary-exploration/index.html#authors",
    "href": "openlibrary-exploration/index.html#authors",
    "title": "What’s in Open Library Data",
    "section": "Authors",
    "text": "Authors\nAuthors are a bit\n\nOn average there’s 1.1 works per author\nMost authors have a name, 62% a personal_name, and 4% alternate_names. They’re often inconsistent with format (e.g. Surname, Firstname or Firstname Surname)\n22% have a birth_date and 4% death_date (free text) which could be useful for disambiguation\n7% have remote_ids linking to wikidata, VIAF and ISNI where additional information can be obtained\nLess than 2% have a bio for the author or photos of them"
  },
  {
    "objectID": "openlibrary-exploration/index.html#duplicate-works",
    "href": "openlibrary-exploration/index.html#duplicate-works",
    "title": "What’s in Open Library Data",
    "section": "Duplicate works",
    "text": "Duplicate works\nSearching for books with the title “Bayesian Data Analysis” (in a case insensitive way) returned 4 separate works, all clearly the same book. Note that one book has the author name in the wrong order (Gelman Andrew), and that only /works/OL18391964W contains all the authors (including Andrew Gelman twice, the second time as A. Gelman).\n\n\n\n\n\n\n\n\n\nwork_key\nworks_title\nauthor_name\nauthor_key\n\n\n\n\n/works/OL25152967W\nBayesian Data Analysis\nGelman Andrew\n/authors/OL9492748A\n\n\n/works/OL12630389W\nBayesian data analysis\nAndrew Gelman\n/authors/OL2668098A\n\n\n/works/OL19124056W\nBayesian data analysis\nAndrew Gelman\n/authors/OL2668098A\n\n\n/works/OL18391964W\nBayesian data analysis\nAndrew Gelman\n/authors/OL2668098A\n\n\n/works/OL18391964W\nBayesian data analysis\nJohn B. Carlin\n/authors/OL2692132A\n\n\n/works/OL18391964W\nBayesian data analysis\nHal S. Stern\n/authors/OL2692133A\n\n\n/works/OL18391964W\nBayesian data analysis\nDonald B. Rubin\n/authors/OL1194305A\n\n\n/works/OL18391964W\nBayesian data analysis\nA. Gelman\n/authors/OL2692134A"
  },
  {
    "objectID": "openlibrary-exploration/index.html#duplicate-editions",
    "href": "openlibrary-exploration/index.html#duplicate-editions",
    "title": "What’s in Open Library Data",
    "section": "Duplicate editions",
    "text": "Duplicate editions\nSometimes an edition is duplicated, such as How to solve it and How to solve it which both have the same pair of ISBN 10 [0691080976, 0691023565]"
  },
  {
    "objectID": "openlibrary-exploration/index.html#multiple-works-for-an-edition",
    "href": "openlibrary-exploration/index.html#multiple-works-for-an-edition",
    "title": "What’s in Open Library Data",
    "section": "Multiple works for an edition",
    "text": "Multiple works for an edition\nSometimes an edition has multiple works, but all the cases I’ve checked seem to be errors."
  },
  {
    "objectID": "html-mobile-camera/index.html",
    "href": "html-mobile-camera/index.html",
    "title": "Activating Mobile Phone Camera from HTML",
    "section": "",
    "text": "A year ago I built whatcar.xyz which classifies a photo of an Australian car with its make and model. I hacked it together in a week, and it was featured on the 2019 fast.ai course, and haven’t worked on it much since. I’m now looking at making some usability and data capture improvements.\nOriginally when you select the car photo to classify it opens up a file selector. On desktop this makes sense, but on a mobile device you probably want to take a photograph straight away (at least as default). How can you open up a camera for a photo?\nI found a Stackoverflow answer that gives a simple solution; adding capture=\"camera\" to the input element accepting images.\n <input type=\"file\" accept=\"image/*\" capture=\"camera\">\nThe post claims it works on Android from 4.0 and iPhone OS6 and newer, and on testing it works on modern versions of both. However it doesn’t give references for this so I dug a bit deeper.\nThe capture attribute it covered in MDN Documentation, which links to the specification. In fact by the specification the only valid values of capture are “user” (self-view camera), “environment” (viewing out from the phone), “left” and “right”. Anything else goes to the implementation specific default; so in this case the solution works by accident. It would be better to explicitly define the outward camera for photographing a car:\n <input type=\"file\" accept=\"image/*\" capture=\"environment\">\n \nThe specification is really useful; it’s even got an example of how to upload a file to a server and display client side. This is great because I’ve found upload is sometimes failing and display isn’t working with my existing javascript (which is the next thing on my list to fix).\nThe specification is all well and good, but does it actually work on real devices? The W3C tests show it works, in particular with images and environment, on Chrome 64 for Android and Safari 11.1.2 for iOS. According to Caniuse it’s supported on 97% of (tracked) mobile devices, although it says it doesn’t work on Firefox for Android when I have manually verified it can.\nOn the desktop and other browsers that don’t support it the input falls back to a file picker, which is exactly what I would want. Just by adding the accept and capture attributes my website now works how I would want."
  },
  {
    "objectID": "priority-no/index.html",
    "href": "priority-no/index.html",
    "title": "Priorities mean saying No",
    "section": "",
    "text": "I find it useful to have priorities and goals to set a direction. Often the actual choice of goals doesn’t matter as much as that I make them, and regularly review them. The only essential is making sure that self care and close friends and family are among your priorities; because without these you won’t be able follow up your other priorities.\nHowever it’s very easy to get distracted. Things come up outside your priorities all the time, and they have to be managed so as to leave time for your priorities. The best thing to do is decline the distractions when possible.\nThere are some distractions you can’t decline. Even if your tax return isn’t on your list of priorities it needs to be done, or there will be consequences. If they really must be done, try to delegate them to leave you more space to focus. Or at least defer them until after you have made some progress on your priorities; don’t let them interrupt.\nThere are still many things that can be declined. You’ve been invited to a meeting that you really won’t benefit from or contribute to. Someone has shared an article they found interesting. Someone has reached out with a question or task you could do, but it doesn’t fit on your priorities. There’s a social feeling to respond with an action, but it can very quickly erode your time.\nOften there’s a polite way to say no, preserving the relationship and your time. Be clear if it doesn’t fit in your responsibilities and priorities; people will often understand this. You may be able to be helpful by pointing people in the right direction, but don’t overextend yourself.\nIf you don’t make time and space for your priorities it will quickly be filled with these things. Even worse the more you say yes, the more people will expect it and the need will grow. There will be times when it makes time to say yes for the long game, but the default reaction should be no if it won’t help you get where you’re going."
  },
  {
    "objectID": "structuring-code-function/index.html",
    "href": "structuring-code-function/index.html",
    "title": "Code Structure Reflecting Function",
    "section": "",
    "text": "The architecture of the pipeline is a set of methods that fetch source data, extract the structured data and normalise it into a common form to be combined. I previously had these methods all written in one large file, adding each extractor to a dictionary, which was a headache to look at. But whenever I thought about how to restructure it I got stuck thinking about implementation details.\nI made a breakthrough today by thinking about how the components interact. Each fetcher, extractor and normaliser need to come as a bundle; it’s useless to have the components separately and so they need to be added and removed as a whole. On the other hand each source is independent of the others, so they should be in separate files, and common operations should be extracted into library functions.\nIt’s not entirely obvious how to bundle these things and export them to be run. A fetcher is something that takes an output path and serialises data to that path. An extractor is something that takes an input path, from the fetcher, and returns an iterable of structured data (the format of which depends on the source). A normaliser takes an iterable of structured data, from the extractor, and outputs an iterable of Job Data in a common format. These could be functions at the top level in a module, combined in a dictionary, or stored in an object.\nI decided each should contain a single Datasource class that contains the actions as methods, that can be individually imported. Really an object in Python is just some syntactic sugar around a dictionary, but the sugar of inheritance and . access is sweet. I called each of the classes Datasource with the idea that the choices of datasources could become configurable with minimal boilerplate; they’re just distinguished by the module name.\nI’m still not sure that I’ve got it right, but having the extractors in separate files makes it much more manageable to see what’s going on and I can continue to improve the structure as the code evolves."
  },
  {
    "objectID": "ml-tdd/index.html",
    "href": "ml-tdd/index.html",
    "title": "Test Driven Development in Machine Learning",
    "section": "",
    "text": "Last weekend I read the first part of Kent Beck’s Test Driven Development: By Example. He works through a simple example of programming with Test Driven Development in excruciating detail. It shows how you can move in small steps when things get tricky, which is often better than a long debugging session.\nI typically develop code iteratively in a Jupyter Notebook or IPython REPL. I can pass the code I’m writing and interactively inspect it to make sure it works as inspected. This is an efficient way to work, especially with unknown and messy data. But you build up state in the REPL, and a model in your mind, which may diverge from the actual code. And when the session is finished you won’t know if you’ve introduced a regression, unless you also run tests.\nI tried out Test Driven Development on a real problem, and found it very useful. I found it often helped me notice coding issues much earlier, and resolve them much faster. There were times when things got tricky and moving in small steps was very helpful. At one point I introduced a regression and was able to notice and resolve it very quickly. It didn’t stop all issues at runtime, because existing code was untested, but I spent less time debugging.\nThe rest of this article goes through a detailed account of how I used TDD to implement this feature."
  },
  {
    "objectID": "ml-tdd/index.html#add-a-little-test",
    "href": "ml-tdd/index.html#add-a-little-test",
    "title": "Test Driven Development in Machine Learning",
    "section": "Add a little test",
    "text": "Add a little test\nThe overall model contained different Embedding layers. For some of the layers I needed to pass some pretrained weights. There was a common function to construct the embeddings, which seemed like the right place to set the weights.\nimport tensorflow as tf\nfrom tf.keras import Sequential\n\ndef get_categorical_embedding(dim: int, vocab: list[str]) -> Sequential:\n    return Sequential([\n            tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None),\n            # Add + 1 for out of vocabulary\n            tf.keras.layers.Embedding(len(vocab) + 1, dim),\n            ])\nHaving identified the function, I tried to think of a simple test. The weights were stored in a file, which could easily be converted to a Pandas DataFrame with the categories on the index. There are lots of other ways the data could be represented, but I thought I’d start a test with this and see how it looked.\nimport pytest\nimport pandas as pd\n\n@pytest.fixture\ndef weights():\n    return pd.DataFrame([[0.5, 0.5], \n                         [1.0, 0.0]],\n                         index=[\"cat\", \"dog\"]\n    )\n\ndef test_weighted_categorical_embedding_weights(weights):\n    tokens = tf.convert_to_tensor([\"cat\", \"dog\"])\n    model = get_categorical_embedding(\n        dim=4, vocab=weights.index, weights=weights\n    )\n    embedding = model(tokens).numpy()\n\n    expected = np.array(\n        [[0.5, 0.5], [1.0, 0.0]]\n    )\n    assert np.allclose(embedding, expected)\nHowever I realised I wasn’t clear on what should happen to an out of vocabulary token. The existing weights didn’t have one, and I had to think of something to do. I decided I should average the weights, and added it to the test. This was a bit too much to handle all at once, but I forged on ahead."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail",
    "href": "ml-tdd/index.html#run-a-test-and-fail",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nI ran the test and it failed. First it failed because I made some mistakes in the test code. When I fixed them it failed because get_categorical_embedding didn’t have a weights argument."
  },
  {
    "objectID": "ml-tdd/index.html#make-a-change",
    "href": "ml-tdd/index.html#make-a-change",
    "title": "Test Driven Development in Machine Learning",
    "section": "Make a Change",
    "text": "Make a Change\nI added weights as an argument to the end of get_categorical_embedding and initialised it to None. It wasn’t immediately obvious what I needed to do next so I ran the tests again."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail-1",
    "href": "ml-tdd/index.html#run-a-test-and-fail-1",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nThe assertion now failed because the random initial weights didn’t match the weights it was supposed to load. This wasn’t a surprise, since the code isn’t loading the weights yet. But now it’s getting to the point where that’s all I need to do.\nIt wasn’t clear how to load the weights though. I’d need to deal with the Out of Vocabulary tokens. The right TDD thing to do here would be to add it to my TODO list, and put anything in that made the tests pass. But instead I started thinking about how to get the weight matrix into the right shape."
  },
  {
    "objectID": "ml-tdd/index.html#write-a-small-test",
    "href": "ml-tdd/index.html#write-a-small-test",
    "title": "Test Driven Development in Machine Learning",
    "section": "Write a Small Test",
    "text": "Write a Small Test\nI wanted a function that took in my weights DataFrame and did something reasonable for the out of vocabulary tokens. A reasonable thing to do would be to take the mean of all the other embeddings. So I added a test to do that.\ndef test_create_weight_matrix(weights):\n    vocab = [\"dog\", \"cat\"]\n    result = create_weight_matrix(weights, vocab)\n\n    mean_embedding = [0.75, 0.25]\n\n    expected = np.array([mean_embedding, [0.5, 0.5], [1.0, 0.0]])\n\n    assert np.allclose(result, expected)"
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail-2",
    "href": "ml-tdd/index.html#run-a-test-and-fail-2",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nThis test now failed because the function didn’t exist. So I went ahead and wrote a simple implementation."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-succeed",
    "href": "ml-tdd/index.html#run-a-test-and-succeed",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and succeed",
    "text": "Run a test and succeed\nThen the test failed again; this time because of something like 0.7504 != 0.75. It seemed that Tensorflow slightly changed the representation.\nI added the argument rtol=1e-3 to np.allclose to check less precisely. Then this test succeeded. I updated other tests to use the same tolerance."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail-3",
    "href": "ml-tdd/index.html#run-a-test-and-fail-3",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nNow I had the weights matrix with the out of vocabulary tokens I could add it to the embedding.\nLooking at some examples it seemed like I should be able to add weights= to the Embedding layer. It wasn’t explicitly documented, but it passes some keyword argument so I thought I would try it. However the test still failed, the initialisation was still random."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail-4",
    "href": "ml-tdd/index.html#run-a-test-and-fail-4",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nIt looked like there were two ways I could do it; manually call set_weights, which first requires instantiating the layer, or passing a Constant Initializer, which claims it can only take scalar values. After seeing an example with Constant online I tried it and this test passed, but my first test calling the model failed."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-succeed-1",
    "href": "ml-tdd/index.html#run-a-test-and-succeed-1",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and succeed",
    "text": "Run a test and succeed\nMy code didn’t handle the case where the weights were None; I had introduced a regression. It was simple to write an if statement and this test passed."
  },
  {
    "objectID": "ml-tdd/index.html#refactor",
    "href": "ml-tdd/index.html#refactor",
    "title": "Test Driven Development in Machine Learning",
    "section": "Refactor",
    "text": "Refactor\nNow all my tests were passing I could take some time to refactor. I cleaned up some test code, but the model code looked good so I continued."
  },
  {
    "objectID": "ml-tdd/index.html#write-a-small-test-1",
    "href": "ml-tdd/index.html#write-a-small-test-1",
    "title": "Test Driven Development in Machine Learning",
    "section": "Write a small test",
    "text": "Write a small test\nThe full model now needed to get the embeddings with the weights. I wrote a simple test calling the model with an expected output."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-fail-5",
    "href": "ml-tdd/index.html#run-a-test-and-fail-5",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and fail",
    "text": "Run a test and fail\nThe test immediately failed because the model didn’t have arguments for weights. I added them and propagated them to the call to get_categorical_embedding.\nThere were a cascade of small errors with how I wrote this, which I quickly fixed."
  },
  {
    "objectID": "ml-tdd/index.html#run-a-test-and-succeed-2",
    "href": "ml-tdd/index.html#run-a-test-and-succeed-2",
    "title": "Test Driven Development in Machine Learning",
    "section": "Run a test and succeed",
    "text": "Run a test and succeed\nThen I got a strange failure about dimensions not matching. It wasn’t obvious to me what was happening here. The dimensions it mentioned were 2 (which was the dimension of the weights) and 4. It then dawned on me that the embedding layer in my test had dimension 4, and the weights had dimension 2. I fixed the test and added a note to check the dimensions matched so this failure occurred at instantiation.\nThe tests were all passing now, and it seemed like it was working."
  },
  {
    "objectID": "ml-tdd/index.html#checking-the-dimensions",
    "href": "ml-tdd/index.html#checking-the-dimensions",
    "title": "Test Driven Development in Machine Learning",
    "section": "Checking the dimensions",
    "text": "Checking the dimensions\nI wrote a test to check if the embedding and weight dimensions don’t match it should raise a ValueError. The test failed as expected.\nI then wrote the assertion and the test passed.\nTo make sure the error message was right, since sometimes I make mistakes with this, I changed the test to check for a RuntimeError. The test failed with the right message, and so I reverted my change.\nHowever I’m not really checking my error message, and it is open to regression; a good extension would be to check the message itself."
  },
  {
    "objectID": "ml-tdd/index.html#running-the-whole-thing",
    "href": "ml-tdd/index.html#running-the-whole-thing",
    "title": "Test Driven Development in Machine Learning",
    "section": "Running the whole thing",
    "text": "Running the whole thing\nI was now confident enough to run the whole process, which takes about ten minutes. It still failed a couple of times.\nFirst I made a simple typo in a variable name. I ran pyflakes and noticed it would pick it up. I configured pyflakes to ignore all the other existing issues and added it to my make test command.\nNext it failed to save the model at the end. I had passed a Pandas series as the vocabulary, and Tensorflow didn’t know how to serialise it. Converting to a list or numpy array fixed the problem.\nAfter that it worked perfectly. All the testing gave me more confidence it was doing what I expected, rather than just running with the wrong weights. I’m pretty happy with my experience of TDD and want to keep trying it."
  },
  {
    "objectID": "sourcemap-html-tags/index.html",
    "href": "sourcemap-html-tags/index.html",
    "title": "Source Map HTML Tags in Python",
    "section": "",
    "text": "I’m actually surprised there’s not good tooling for doing this in Python. It seems like the fast HTML parsers like lxml, html5-parser and Selectolax don’t make the source positions accessible through their API. Python’s inbuilt HTMLParser does, and it’s possible to get from html5lib, and BeautifulSoup exposes them via sourceline and sourcepos. Unfortunately with BeautifulSoup I can’t find any way to get the length of the source tag; len(str(node)) doesn’t work because the parsing mutates the HTML (try parsing <li> and you’ll see what I mean). I think there still be useful ways to deconstruct the text using a parser, but let’s see if we can source map the tags in a compatible way with the text.\nWe’ll start with the same MyHTMLParser that we used to source map the text, but add an additional property end_tag_index to give the index of the end of a tag. Recall the current_index will always be the index just before the tag starts, so searching for the next > is a reliable way to get the end of the tag.\nfrom html.parser import HTMLParser\nfrom itertools import accumulate\nimport re\n\nend_tag = re.compile('>')\n\nclass MyHTMLParser(HTMLParser):\n    @property\n    def end_tag_index(self):\n        return end_tag.search(self.rawdata, self.current_index).end()\n\n    def reset(self):\n        super().reset()\n        self.result = None\n\n    @property\n    def current_index(self):\n        line, char = self.getpos()\n        return self.line_lengths[line - 1] + char\n\n    def __call__(self, data):\n        self.reset()\n        self.line_lengths = [0] + list(accumulate(len(line) for line in\n                                                  data.splitlines(keepends=True)))\n        self.feed(data)\n        self.close()\n        return self.result\nThen similar to how we handled text we can register the data at a start tag, and at an end tag append the end index and to a list. However because tags can be arbitrarily nested we need a way to keep track of which tag we are closing. One way to handle this is to use the deque collection as a FIFO (First In First Out) queue; when we reach the end tag we close the most recent matching start tag. There needs to a be a queue for every possible tag, so we use a defaultdict to track the queue for any possible tag. Here’s a sample implementation:\nfrom collections import deque, defaultdict\n\nclass HTMLTagExtract(MyHTMLParser):\n    def reset(self):\n        super().reset()\n        self.tags = defaultdict(deque)\n        self.result = []\n\n    def handle_starttag(self, tag, attrs):\n        self.tags[tag].append({'tag': tag,\n                               'attrs': attrs,\n                               'start': self.current_index,\n                               'start_inside': self.end_tag_index})\n\n    def pop_tag(self, tag, end):\n        # If there's no matching tag then recover\n        if self.tags[tag]:\n            tagdata = self.tags[tag].pop()\n            tagdata['end'] = end\n            tagdata['end_inside'] = self.current_index\n            self.result.append(tagdata)\n\n    def handle_endtag(self, tag):\n        self.pop_tag(tag, self.end_tag_index)\n\n    def close(self):\n        super().close()\n        for tag in self.tags:\n            while self.tags[tag]:\n                self.pop_tag(tag, self.current_index)\nAnd here’s an example of using it:\nparser = HTMLTagExtract()\n\nhtml = '<p>Hello <i>world</ i ><p><ul><li>I am here<li></ul>I am well</p>'\n\nfor tag in parser(html):\n    print('%4s %3d %3d %s' % (tag['tag'], tag['start'], tag['end'], html[tag['start']:tag['end']]))\nwhich outputs:\n   i   9  23 <i>world</ i >\n  ul  26  52 <ul><li>I am here<li></ul>\n   p  23  65 <p><ul><li>I am here<li></ul>I am well</p>\n   p   0  65 <p>Hello <i>world</ i ><p><ul><li>I am here<li></ul>I am well</p>\n  li  43  65 <li></ul>I am well</p>\n  li  30  65 <li>I am here<li></ul>I am well</p>\nIf you look carefully you’ll notice the last p ranges from 0 to 65, and in particular it has another p inside it. This isn’t the correct thing to do either in the whatwg standard or in practice; the second p should end the first one. Python’s HTML Parser isn’t a strict parser in this sense. I went through the standard and searched for end tag can be omitted to find these implicit closing tags, which can happen either on opening of certain tags, or on the closing of a parent tag. I input them as dictionaries as follows:\nTAG_CLOSED_BY_CLOSE = {\n    'p': ['article', 'aside', 'nav', 'section'],\n    'li': ['ol', 'ul', 'menu'],\n    'dd': ['dl', 'div'],\n    'rt': ['ruby'],\n    'rp': ['ruby'],\n    'caption': ['table'],\n    'colgroup': ['table'],\n    'tbody': ['table'],\n    'tr': ['table', 'tbody', 'tfoot', 'thead'],\n    'td': ['tr'],\n    'th': ['tr'],\n}\n\nTAG_CLOSED_BY_OPEN = {\n    'p': ['address', 'article', 'aside', 'blockquote',\n          'details', 'div', 'dl', 'fieldset', 'figcaption',\n          'figure', 'footer', 'form',\n          'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header',\n          'hgroup', 'hr', 'main', 'menu', 'nav', 'ol', 'p',\n          'pre', 'section', 'table', 'ul'],\n    'li': ['li'],\n    'dt': ['dt', 'dd'],\n    'dd': ['dt', 'dd'],\n    'rt': ['rp', 'rt'],\n    'rp': ['rp', 'rt'],\n    'tbody': ['tbody', 'tfoot'],\n    'thead': ['tbody', 'tfoot'],\n    'tr': ['tr'],\n    'td': ['td', 'th'],\n    'th': ['td', 'th'],\n}\nLet’s also wrap the tag structures as dataclasses to make them easier to use. We have a TagSpan that captures the location, the tag and attributes, as well as a reference to the parent document that contains the HTML.\nfrom __future__ import annotations\nfrom dataclasses import dataclass\nfrom typing import optional\n\n@dataclass\nclass TagSpan:\n    start: int\n    start_inside: int\n    end: int\n    end_inside: int\n    tag: Optional[str]\n    attrs: dict[tuple[str, str]]\n    doc: 'HTMLSpanDoc'\n\n    @property\n    def html(self):\n        return self.doc.html[self.start:self.end]\nThe parent document contains all the TagSpans, that can be appended to or iterated over, as well as the source html.\n@dataclass\nclass HTMLSpanDoc:\n    tag_spans: list[TagSpan] = field(default_factory=list)\n    html: str = ''\n\n    def append(self, item: TagSpan) -> None:\n        self.tag_spans.append(item)\n\n    def __iter__(self):\n        return iter(self.tag_spans)\nNow we have all the ingredients we can extend our previous parser to use them:\nclass HTMLTagExtract(MyHTMLParser):\n    def __init__(self,\n                 tag_closed_by_open=TAG_CLOSED_BY_OPEN,\n                 tag_closed_by_close=TAG_CLOSED_BY_CLOSE):\n        super().__init__()\n        self.tag_closed_by_open = tag_closed_by_open\n        self.tag_closed_by_close = tag_closed_by_close\n\n    def feed(self, data):\n        super().feed(data)\n        self.result.html += data\n\n    def reset(self):\n        super().reset()\n        self.tags = defaultdict(deque)\n        self.result = HTMLSpanDoc()\n\n    def omitted_tag_close(self, tag, close_map):\n        for current_tag, parent_tags in close_map.items():\n            if tag in parent_tags and self.tags[current_tag]:\n                self.pop_tag(current_tag, self.current_index)\n                self.omitted_tag_close(current_tag, self.tag_closed_by_close)\n\n    def handle_starttag(self, tag, attrs):\n        self.omitted_tag_close(tag, self.tag_closed_by_open)\n        self.tags[tag].append({'tag': tag,\n                               'start': self.current_index,\n                               'start_inside': self.end_tag_index,\n                               'attrs': attrs,\n                               'doc': self.result})\n\n    def pop_tag(self, tag, end):\n        if self.tags[tag]:\n            self.result.append(\n                TagSpan(**self.tags[tag].pop(),\n                        end=end,\n                        end_inside=self.current_index)\n            )\n\n    def handle_endtag(self, tag):\n        self.omitted_tag_close(tag, self.tag_closed_by_close)\n        self.pop_tag(tag, self.end_tag_index)\n\n    def close(self):\n        super().close()\n        for tag in self.tags:\n            while self.tags[tag]:\n                self.pop_tag(tag, self.current_index)\nWe can run our extraction again, iterating over the HTMLSpanDoc and accessing the attributes of the TagSpan:\nparser = HTMLTagExtract()\n\n\nhtml = '<p>Hello <i>world</ i ><p><ul><li>I am here<li></ul>I am well</p>'\n\n\nfor tag in parser(html):\n    print('%4s %3d %3d %s' % (tag.tag, tag.start, tag.end, tag.html))\nThis time we get the paragraphs separated. However notice the tag I am well doesn’t occur inside any tag, this is because it’s in an implicitly opened paragraph.\n   i   9  23 <i>world</ i >\n   p   0  23 <p>Hello <i>world</ i >\n   p  23  26 <p>\n  li  30  43 <li>I am here\n  li  43  47 <li>\n  ul  26  52 <ul><li>I am here<li></ul>\nYou could use this with the HTMLTextExtractor for source mapping text to annotate tags like <i> by matching source indices, and to attribute the text to blocks (like paragraphs and list items). Block attribution is actually a little tricky because we need to be sure that text always falls in exactly one block, and I don’t think this is guaranteed by our block closing rules above (though we could enforce them). Another more direct way to do this is to force only one block tag being active, like I tried in my first post on Python HTML Parser.\nThe current implementation will really only be partly compatible with HTML5 (let alone HTML in the wild) and there are two distinct paths. One is to use a realy HTML parser and do processing on this parse tree; for example html5lib could be used to generate valid HTML (closing all the tags) which could then be processed by the first version of our code. A second option is to customize the parser for a particular use case; we won’t always want things to the specification and we could specify our own block attribution."
  },
  {
    "objectID": "streaming-nquad-rdf/index.html",
    "href": "streaming-nquad-rdf/index.html",
    "title": "Streaming n-quads as RDF",
    "section": "",
    "text": "To get an idea of the kind of data we’re talking about there are over 650 thousand pages from over 8,000 domains containing a Job Posting like this:\n{'type': 'JobPosting',\n 'title': 'Category Manager - Prof. Audio Visual Solutions',\n 'jobLocation': {'address': {\n     'addressRegion': 'IL',\n     'addressLocality': 'Glenview',\n     'addressCountry', 'United States',\n     'postalCode': '60026',}}\n 'hiringOrganization': {'name': 'Anixter International'},\n 'employmentType': 'FULL_TIME',\n 'datePosted': '2019-08-01 17:48:55',\n 'validThrough': '2019-11-11',\n 'description': ...,\n}\nHowever it’s stored in large files with millions of lines like this:\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/JobPosting> <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://schema.org/identifier> _:genid2d8020c9b7d2294a778072a41d6d59640a2db2 <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://schema.org/title> \"Category Manager - Prof. Audio Visual Solutions\" <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\nWe can extract these using RDFlib with itertool’s groupby to extract the data from each crawled webpage into a separate RDF Graph for further transformation.\nfrom itertools import groupby\nimport rdflib\ndef parse_nquads(lines):\n    for group, quad_lines in groupby(lines, get_quad_label):\n        graph = rdflib.Graph(identifier=group)\n        graph.parse(data=''.join(quad_lines), format='nquads')\n        yield graph\nWe can extract the crawled URL from the quad with a simple piece of logic:\nimport re\nRDF_QUAD_LABEL_RE = re.compile(\"[ \\t]+<([^ \\t]*)>[ \\t].\\n$\")\ndef get_quad_label(s):\n    return RDF_QUAD_LABEL_RE.search(line).group(1)\nThen we can process the data one graph at a time\nimport gzip\nwith gzip.open('afile.nquads.gz', 'rt') as f:\n  for graph in parse_nquads(f):\n    ...\n\nWhat’s an n-quad?\nThe data is stored in RDF n-quads which is specified in a W3C recommendation. Each line represents a relation of the form: “subject predicate object graphlabel .”\nFor example the line below:\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/JobPosting> <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\nHas:\n\nObject _:genid2d8020c9b7d2294a778072a41d6d59640a2db0 (a blank Node)\nPredicate: <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>\nSubject: <http://schema.org/JobPosting>\nGraph Label: <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us>\n\nSo this is saying that this object has RDF Type Job Posting, and is from the URL in the graph label.\nSimilarly the line:\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://schema.org/title> \"Category Manager - Prof. Audio Visual Solutions\" <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\nSays that the same object has a title “Category Manager - Prof. Audio Visual Solutions”.\nWe want a way to collect all of these objects up together for each source URL (graph label).\nWhile it’s not too hard to parse directly from the specification, there are some subtleties (like types of literals) and it’s generally better to use a library. Also being able to relate the nodes some logic, and while we could use kanren in Python, RDFLib is designed specially for the job.\n\n\nPython RDFLib\nThe easiest way to read this is all at once with a ConjunctiveGraph like in this article by Rebecca Bilbro.\nfrom rdflib import ConjunctiveGraph\n\ngraph = ConjunctiveGraph()\nwith open(file, 'rt') as f:\n    graph.parse(data, format=\"nquads\")\nUnfortunately this means we have to load all the data into memory at once and process all the graphs from different files together, only to split them apart after. With gigabytes of compressed n-quads this is not a good solution, and there don’t seem to be any answers on StackOverflow.\n\n\nExtracting a label from the n-quad\nBy the specification the label, if it is there, should be a URI (e.g. <http://example.org/page>) or a blank node. I’m going to assume that it’s always there and always a URI; this is true for Web Data Commons where the label is the URL of the page the data was extracted from. We can then use the specification to construct a simple regular expression to extract it.\nimport re\nRDF_QUAD_LABEL_RE = re.compile(\"[ \\t]+<([^ \\t]*)>[ \\t].\\n$\")\ndef get_quad_label(s):\n    return RDF_QUAD_LABEL_RE.search(line).group(1)\nIdeally we would use RDFLib nquad parser directly to do it, but it mainly uses stateful objects internally and hides the quad structure so we end up having to do some contortions to use it. It uses a similar but more conservative expression for a URI.\nNow we can identify the labels we can group together the lines that have the same label, on the assumption that they are sequential (they seem to be in the Web Commons data). We could always ensure this by sorting the reversed lines (e.g. in bash rev | sort | rev).\nf = gzip.open('afile.nquad.gz', 'rt')\nlabel_groups = groupby(f, get_quad_label)\ngroup, quad_lines = next(label_groups)\n\n\nReading into RDFLib Graphs\nBy default an RDFlib Graph ignores anything that doesn’t match the identifier in the constructor, so we need to set that to the pages URL.\ngraph = rdflib.Graph(identifier=group)\nThen we can parse it by passing the lines as a string to data (which gets converted to BytesIO under the hood):\ngraph.parse(data=''.join(quad_lines), format='nquads')\nNow we have a valid RDFGraph in graph, we can navigate it and generally transform and save it one graph at a time. It’s a pity that we have to do this legwork and it’s not easy with rdflib directly."
  },
  {
    "objectID": "sicp-1_1/index.html",
    "href": "sicp-1_1/index.html",
    "title": "SICP Exercise 1.1",
    "section": "",
    "text": "Exercise from SICP:\nExercise 1.1. Below is a sequence of expressions. What is the result printed by the interpreter in response to each expression? Assume that the sequence is to be evaluated in the order in which it is presented."
  },
  {
    "objectID": "sicp-1_1/index.html#compound-if",
    "href": "sicp-1_1/index.html#compound-if",
    "title": "SICP Exercise 1.1",
    "section": "Compound if",
    "text": "Compound if\nThe expression\n(if (and (> b a) (< b (* a b)))\n    b\n    a)\nsubstitutes to\n(if (and (> 4 3) (< 4 (* 3 4)))\n    4\n    3)\nwhich is\n(if (and (> 4 3) (< 4 (* 3 4)))\n    4\n    3)\nwhich substitutes to\n(if (and (> 4 3) (< 4 12))\n    4\n    3)\nand in turn becomes\n(if (and #t #t)\n    4\n    3)\nwhich then evaluates to\n(if #t\n    4\n    3)\nfinally yielding 4."
  },
  {
    "objectID": "sicp-1_1/index.html#another-example",
    "href": "sicp-1_1/index.html#another-example",
    "title": "SICP Exercise 1.1",
    "section": "Another example",
    "text": "Another example\n(cond ((= a 4) 6)\n      ((= b 4) (+ 6 7 a))\n      (else 25))\nInjecting from the environment that a is 3 and b is 4 gives;\n(cond ((= 3 4) 6)\n      ((= 4 4) (+ 6 7 3))\n      (else 25)\n)\nwhich, in the substitution model is the same as\n(cond (#f 6)\n      (#t 16)\n      (else 25)\n)\nWhich results in the first true branch, 16."
  },
  {
    "objectID": "sicp-1_1/index.html#applying-on-an-if",
    "href": "sicp-1_1/index.html#applying-on-an-if",
    "title": "SICP Exercise 1.1",
    "section": "Applying on an if",
    "text": "Applying on an if\nWe substitute to get the following sequence of partial evaluations; using that a is 3 and b is 4:\n(+ 2 (if (> b a) b a))\n(+ 2 (if (> 4 3) 4 3))\n(+ 2 (if #t 4 3))\n(+ 2 4)\n6"
  },
  {
    "objectID": "sicp-1_1/index.html#applying-on-a-cond",
    "href": "sicp-1_1/index.html#applying-on-a-cond",
    "title": "SICP Exercise 1.1",
    "section": "Applying on a Cond",
    "text": "Applying on a Cond\nWe substitute to get the following sequence of partial evaluations; using that a is 3 and b is 4:\n(* (cond ((> a b) a)\n         ((< a b) b)\n         (else -1))\n   (+ a 1))\n\n(* (cond ((> 3 4) 3)\n         ((< 3 4) 4)\n         (else -1))\n   (+ 3 1))\n(* (cond (#f 3)\n         (#t 4)\n         (else -1))\n   4)\n(* 4\n   4)\n16"
  },
  {
    "objectID": "emacs-buffering/index.html",
    "href": "emacs-buffering/index.html",
    "title": "Using Emacs under WSL",
    "section": "",
    "text": "Getting Emacs to work nicely on a Windows system can be a challenge. You can install it natively (although getting all the dependencies is a challenge), but many packages require libraries or utilities that are hard to install or don’t exist on Windows. The best solution I have found is using Emacs under the Windows Subsystem for Linux (WSL) with Xming.\nHowever if you run Emacs 26 or greater after starting Xming with XLaunch you’re faced with a blank screen and can’t see any writing on Emacs\nThis is because double buffering is enabled by default and Xming (and VcXsrv) don’t support this. I added the following to my .emacs to work around this:\nIt tries to detect if I’m on a Windows machine by looking for Windows on the Path, and then disables double buffering. This means it will continue to work the same when I switch to my Linux machine.\nThis isn’t perfect; if you switch to a Windows X server that can support double buffering you could delete this line, but I don’t know if that’s possible to reliably detect."
  },
  {
    "objectID": "emacs-buffering/index.html#what-about-terminal-emacs",
    "href": "emacs-buffering/index.html#what-about-terminal-emacs",
    "title": "Using Emacs under WSL",
    "section": "What about Terminal Emacs?",
    "text": "What about Terminal Emacs?\nAnother solution is to run Emacs in terminal mode (e.g. emacs -nw). I found using Windows Terminal that I’d see lots of artifacts when editing code like below:\n\n\n\nDisplay artifacts in Emacs\n\n\nThis is likely because I’m using a lot of dynamic features (like relative line numbers, matching bracket highlighting, etc.). I could always fix it by using redraw-frame, and maybe I should have added a hook to run that after every cursor move (I’m not sure if there would be any performance implications though).\nThe other issue is without a shared clipboard I would always have to copy by highlighting the text with my mouse and typing C-S-c, and for pasting I would use S-Insert. However because it didn’t know I was pasting it would reformat the text (and I don’t know what the equivalent of Vim’s :set paste is).\nI probably could have configured it to work, but it’s much nicer under Xming with a clipboard shared with Windows and a fully functional GUI."
  },
  {
    "objectID": "tangled-bleu/index.html",
    "href": "tangled-bleu/index.html",
    "title": "Tangled up in BLEU",
    "section": "",
    "text": "This motivates automatic metrics for evaluation machine translation. One of the oldest examples is the BiLingual Evaluation Understudy (BLEU). This requires a source text to translate, as well as one or more reference translations. Then the automatic translation is evaluated based on average n-gram precision; that is taking all sequences of n tokens from the candidate translation, how many are in the reference translation? It turns out this depends a bit on some choices you make (how you tokenise, how you penalise very short translations), so there’s a standard implementation called sacreBLEU.\nThen the question is how well do these automatic metrics correlate with human judgement? While n-gram precision captures some things, you can often reword a translation in a way that is equally as good, but would have a much lower BLEU score. The approach taken in the WMT Conferences on Machine Translation is to get human evaluations on different systems and see how well the metrics correlate with human evaluation.\nSpecifically they have a process of Direct Assessment (DA) where they get human annotators to evaluate segments of a candidate translation against a reference translation on a scale of 0-100. They then filter out the worst quality annotators and standardise individual annotator scores, since people will have different baselines and variances. Finally the DA score for a system is the mean of the standardised evaluations for all translations of that system. See the 2019 WMT Findings for details, and other methods they consider.\nThen they evaluate the Pearson correlation between the DA score for the system and the automatic metric for the system to evaluate how well the automatic metric correlates with human judgement for a given language pair. The source texts are drawn from news articles and the translations are made specifically for this task. In the results of the WMT 2019 metrics shared task they compute these correlations for a large number of metrics such as BLEU. What they find is that sacreBLEU actually correlates very well with human judgement in many cases, for example for English-German with 22 systems the correlation is 0.97.\nAt first glance this seems great; we have a simple metric that can judge machine translations similarly to a human. It seems that if a new translation algorithm is released with a higher sacreBLEU score then it should actually be better. However when they look at the correlation for the top-N systems the correlation is actually negative for small N and slowly increases as we add worse systems. In other words it’s no good at distinguishing the top systems from each other, it’s only good at telling the top from the bottom.\n\n\n\nsacreBLEU correlation\n\n\nThe ACL 2020 paper Tangled up in BLEU delves into this problem in more detail. They point out that Peason correlations are unstable for small sample sizes (here we only have around 20 systems) and are very sensitive to outliers. Correlation is only a single number summary and very different relations can have the same correlation coefficient.\nTo address this they use robust outlier removal and compare the results of a few baseline metrics such as sacreBLEU with the best metrics from WMT-2019. They find that after removing outliers sacreBLEU often has a much worse correlation than the best metrics. For example in the English-German task they show how systems compare after subsampling based on whether they contain the outliers en-de-task and online-X.\n\n\n\nBLEU after removing Outliers\n\n\nMoreover they find that a system that is 1-2 BLEU points better is only 50% likely to be better to a human; making it very hard to use in practice. Instead they recommend using other metrics. The simplest is chrF which is an F-score based on character n-grams which is more robust to outliers and outperforms BLEU in many cases; but is worse in a few cases. A more complex approach is ESIM which computes sentence representations from BERT embeddings and then computes the similarity between the two strings; this performs quite well. Similarly there’s YiSi-1 that computes the semantic similarity of phrases using BERT contextual word embeddings.\nIt seems to me that YiSi is the most stable and indicative on the dataset, and has an open source implementation. At least for incremental improvements on news translation it sounds like a very strong contender. But even here small changes in the metric won’t always correlate to human performance, and it’s likely biased to certain kinds of methods.\nTanlged up in BLEU makes the conclusion that important empirical advances should always be backed up with human annotated evidence. In particular I could imagine different approaches, different domains, or particular styles of reference translation could produce completely different results to human evaluation. News articles are very literal and are relatively straightforward to translate. Compare this with Homer’s epic poems which have dozens of different professional translations with different styles and goals (which is why it would be a horrible place to start machine translation, but an admirable long term goal).\nHowever what’s most striking to me is how a rigorous understanding of the Pearson correlation coefficient completely changes the interpretation from BLEU being a reasonable measure for human translation quality to a mediocre one. If they had used some complex machine learning model optimised on RMSE then they would have come to the same conclusion, but it would have been much less obvious. By robustly identifying and removing outliers, a certain number of median average deviations from the median, they clearly demonstrated the issues with Pearson correlation coefficient.\nOne question that’s left for me is how robust are these current metrics are over different techniques and datasets. Before neural translation was as good as it is today, rule based systems often outperformed them on human evaluation but got lower BLEU scores. Are ESIM and YiSi-1 biased towards transformer based translations? If someone develops a breakthrough LSTM or hybrid rule-neural model will the scores still correlate well? It would be worth evaluating on all previous WMT evaluation sets to understand this better (has this been done?)\nUltimately the only true way to measure a machine translation system is still with expensive expert time. But it’s useful to have some rough approximations."
  },
  {
    "objectID": "hil-headlines/index.html",
    "href": "hil-headlines/index.html",
    "title": "Human-in-the Loop: Finding Topics in Headlines",
    "section": "",
    "text": "First you choose a topic name and then can annotate examples. For example I chose “sports results” intending to label headlines containing the result of a sport contest (and not other kinds, e.g. political contests). It wasn’t totally obvious how to annotate examples at first; I had to click in the box with the example headline.\nAt the start I tried labelling random examples, which was very tiresome since most headlines were about other things. I found that due to “repetition priming” (as described in the book) I would accidentally label a headline as negative. There is no way to undo an annotation through the interface, which was quite annoying.\nIt has the interesting option to filter on terms and so I could get more relevant terms like “points”, “final”, or “win”. After enough items are labelled I ticked the “Focus on model uncertainty (if available)” which increasingly gave interesting examples. There was no indication of progress, or how close we were (I had to check the terminal to see if a model was being used).\nUnfortunately at times the model performed quite badly due to the model update strategy. While using random sampling (no keyword or focus on model uncertainty), 1/4 of the items are randomly assigned to evaluation until you get a total of 50 when it stops. In 50 results there was only 1 positive result which led to a very unbalanced evaluation set. While it continually retrains the model it only updates the model when the f-score on the evaluation set improves; which was quite problematic with this evaluation set! A simpler approach would have been much better here.\nWhen a model was getting new model predictions getting new items to annotate became noticably slower on my laptop. This is because it only yields to other threads every 50 predictions, and a submission and fetching a new example each required a yield. Fixing this would be a huge quality of life improvement.\n        if count%50 == 0:\n            eel.sleep(0.01) # allow other processes through\n            if verbose:\n                print(count)\nOne other useful feature was to be able to mark “interesting examples”, which immediately gave an alert indicating the item had been marked as interesting. Finally you could “show examples” by year, which worked pretty well after 40 positive annotations, and 330 negative annotations (helpfully they are stored in separate CSVs in the format Article Date,Headline,URL,Sampling Strategy).\nOverall this was a really interesting example of Human-in-the-Loop Machine Learning. A small decision only to update when the model’s f1 score increased, and a bad choice of evaluation set, made the experience very bad. Also the inability to go back, and a slowdown during evaluation made it much harder to use. If anything I think the sampling approach is too conservative; I’d look for more ways to extract likely matches faster and use uncertainty sampling to make sure we’re not focussing only on common cases."
  },
  {
    "objectID": "tiny-expertise-product/index.html",
    "href": "tiny-expertise-product/index.html",
    "title": "Packaging your Expertise in a Tiny Product",
    "section": "",
    "text": "Their basic premise is it’s really easy to create a simple product that let’s you share your expertise to help people make a small transformation:\n\nCreate a one-page infographic or cheatsheet in Powerpoint and export it as a PDF\nWrite a small ebook in a Word processor and convert it to PDF (or an ebook with Pandoc)\nCreate some small videos using any HD camera and a USB microphone\n\nThen you can easily use Gumroad for fulfilment, even for a free product. This is what Sacha Chua used for her excellent No-Excuses Guide to Blogging, and as a buyer it was excellent (and I actually paid for it).\n\nExample Tiny Products\nWhat are some very small products I could create?\n\nCreate a dataset with Common Crawl/Internet Archive\nSetting up a Kaggle-like framework for data science iteration\nUsing Fasttext in production\nFinding words that are associated with an outcome\nDiff testing\nDynamic Rollup Tables with Hyperloglog\nSmart questions for stakeholders when starting an analytics project\nQuickstart to community detection\nEasy deployment for machine learning models\n\nI’m not sure whether I will do any of these, but I could create something for each of them in a week."
  },
  {
    "objectID": "openlibrary-sqlite/index.html",
    "href": "openlibrary-sqlite/index.html",
    "title": "Importing Open Library into SQLite",
    "section": "",
    "text": "The SQLite CLI has an import statement, but when I tried to run it with .mode tabs it kept complaining about unescaped quotes and wrong number of fields. Instead I wrote a little Python script to iterate over all the data and yield tuples of the 5 columns.\nfrom pathlib import Path\nimport gzip\n\nol_dump_date = '2022-06-06'\ndata_path = Path('../data/01_raw')\n\ndef ol_path(segment):\n    return data_path / f'ol_dump_{segment}_{ol_dump_date}.txt.gz'\n\ndef ol_data(segment):\n    with gzip.open(ol_path(segment), 'rt') as f:\n        for line in f:\n            yield tuple(line.split('\\t', 5))\nIt turns out it’s a bit slower inserting one row at a time, so we’ll make minibatches from the sequence to write. If the batch size is too large it uses a lot of memory, if it’s very small then it’s slower due to the overhead of parsing the statement. In practice 1000 seems like a good tradeoff.\ndef minibatch(seq, size):\n    items = []\n    for x in seq:\n        items.append(x)\n        if len(items) >= size:\n            yield items\n            items = []\n    if items:\n        yield items\nWe can then create the table and insert the rows a minibatch at a time. Wrapping this in a transaction makes it slightly faster. It would be reasonable to put a PRIMARY KEY constraint on the key column; but in this use case it won’t make much difference if the incoming data is valid.\nfrom tqdm.auto import tqdm\n\ndef create_segment(cur, segment, batch_size=10_000):\n    cur.execute(f'CREATE TABLE {segment} (type TEXT, key TEXT, revision INT, last_modified TEXT, json TEXT);')\n    \n    with con:\n        for batch in minibatch(tqdm(ol_data(segment)), batch_size):\n            con.executemany('INSERT INTO authors VALUES (?,?,?,?,?)', batch)\nFinally we actually create all the tables. We set some PRAGMA to make bulk inserts faster (as per this blog and this Stackoverflow), and then create each segment. This completes in about 25 minutes on my laptop, leaving a 62GB SQLite file (since we now have all that uncompressed JSON).\ncon = sqlite3.connect('openlibrary.sqlite')\n\ncon.execute(\"PRAGMA synchronous=OFF\")\ncon.execute(\"PRAGMA count_changes=OFF\")\ncon.execute(\"PRAGMA journal_mode=MEMORY\")\ncon.execute(\"PRAGMA temp_store=MEMORY\")\n\nfor segment in ['authors', 'works', 'editions']:\n    create_segment(con, segment)\nThis is just a start; we’ll need to do some more work to make this usable for looking up entries such as extracting names and adding indexes.\nWith SQLite’s json support we can now query the json column. For example to get all the authors of the book Regression and Other Stories we can run:\nSELECT works.key,\n       authors.key,\n       json_extract(authors.json, '$.name') as author_name\nFROM works\nJOIN json_each((\n                SELECT json\n                FROM works as w\n                WHERE w.key = works.key\n              ), '$.authors') as work_authors\nJOIN authors ON authors.key = json_extract(work_authors.value,\n                                           '$.author.key')\nWHERE json_extract(works.json, '$.title') = 'Regression and Other Stories';\nThis is pretty slow; it takes around 4 minutes to return the 3 authors. To make this more usable we’re going to need to create indexes, and likely change the tables."
  },
  {
    "objectID": "tidyverse-timedatectl-wsl2/index.html",
    "href": "tidyverse-timedatectl-wsl2/index.html",
    "title": "Installing Tidyverse in WSL without Timedatectl Status 1 Issue",
    "section": "",
    "text": "# Assuming Debian derivatives\nsudo apt-get install libxml2-dev\n# Modify TZ to whatever your timeozne is\nTZ=\"Australia/Sydney\" R -e 'install.packages(\"tidyverse\")'\n\n\n\n\n\n\nNote\n\n\n\nSystemd is now available in WSL and enabling it may be an alternative way to solve this issue.\n\n\n\nWhat happens\nWhen I try to install tidyverse I get this error:\n> install.packages('tidyverse')\n\nERROR: configuration failed for package ‘xml2’\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to create bus connection: Host is down\nWarning in system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\nError in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) :\n  namespace ‘xml2’ 1.3.1 is already loaded, but >= 1.3.2 is required\nCalls: <Anonymous> ... namespaceImportFrom -> asNamespace -> loadNamespace\nExecution halted\nERROR: lazy loading failed for package ‘tidyverse’\nIndeed timedatectl fails on WSL2 and there’s an open issue about it.\n~ $ timedatectl\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to create bus connection: Host is down\nSo it seems that timedatectl is being called in xml2 to get the timezone. While I find this a bit offputing, you can specify the timezone with the TZ environment variable, and then it doesn’t call timedatectl.\nSo putting this together gets to our original answer:\nTZ=\"Australia/Sydney\" R -e 'install.packages(\"tidyverse\")'"
  },
  {
    "objectID": "near-duplicate-review/index.html",
    "href": "near-duplicate-review/index.html",
    "title": "Summary of Finding Near Duplicates in Job Ads",
    "section": "",
    "text": "While there are numerous methods for finding similar texts like edit distance or longest common substrings they won’t scale up to hundreds of thousands of ads. If you search naively the number of possible identical pairs grows as the square of the number of ads meaning it would take weeks or months to check these 400,000 ads. A better way is to use MinHash which approximates the Jaccard Index treating the text as a bag of n-grams. Then you can efficiently search through these hashes using Locality Sensitive Hashing (LSH), with a tradeoff of getting some false positives and false negatives.\nThere are a few choices you need to make with Locality Sensitive Hashing. You need to choose your tokens and the shingle length to use. In generally this depends on your application and the types of corruptions you would expect to see. For job ads 3 or more words seems to have very different behaviour to 1 or 2 words; in retrospect I would pick a bigger number like 7 to reduce false positives. In general increasing shingle length decreases (weighted) Jaccard index.\nYou can also choose how you weight your tokens. One option is to use TF-IDF, but this had no real advantage for job ads, but it might for detecting spammers in short texts. A big downside of TF-IDF is you need the whole set to calculate the weights, so it’s much harder to run online. I would stick to treating them as sets or multisets unless it doesn’t give good enough results.\nYou can also normalise your tokens to increase the number of matches. For example you may know that some sources will change case so you lowercase all your text, or remove problematic unicode characters or HTML snippets that sometimes get inserted in one text but not the other. These kinds of changes are likely to be pretty safe, but you can spend a lot of time with them and I’d only introduce them if you actually need them.\nFinally you need to choose your effective Jaccard Cutoff by selecting the number of LSH bands and rows. This will depend on your application and your choice of tokens and weights. The best thing to do is to look through a sample stratified by Jaccard (using LSH to get some higher values) and actually look at the diffs to decide.\nWhen you have your pairs of probably near-identical texts then you can validate them with more complex algorithms such as edit distance, now that the candidate set is low enough to run in feasible time. You can take your duplicate pairs and add the missing links to get sets of near-duplicate texts. The easiest way to do this is finding connected components of the similarity graph, which works well if your threshold is high enough. With a lower threshold you could try to group ads up using community detection or if the components are small with cliques, but if you need to it might be worth improving your validation algorithm.\nI found this experience really interesting for understanding the data. A company typically writes job ads from a template which has sections that don’t change (like “about the company”), so many ads will have large paragraphs in common which will be found by Minhash. They will also reuse an advert to advertise for a similar job, maybe in a different location or slightly more junior, so the ads are 90% the same but for a different role. This means identifying what is a “duplicate” is quite blurry, and may depend on the application.\nOne technique that I didn’t try is the all-pair-binary algorithm from Scaling Up All Pairs Similarity Search. The paper claims to be able to find exact sets of near-duplicate items faster than MinHash. It would be interesting to validate their claim, and see whether it holds under these conditions.\nFor more detail see the Detecting duplicate job ads notebook (raw). The notebook is quite long because a lot of it is looking at individual examples of job ads to gain an understanding of the data. This is an unfortunate consequence of using the same document for exploration and presentation; it’s too much text for a casual reader, but was necessary for me to understand the data. To compensate for that I’ve added a table of contents with links to each section."
  },
  {
    "objectID": "python-minibatching/index.html",
    "href": "python-minibatching/index.html",
    "title": "Minibatching in Python",
    "section": "",
    "text": "For example suppose we have the sequence seq = [1, 2, 3, 4, 5, 6, 7] and we want to batch it up in size 3. Then we expect the result [[1,2,3], [4,5,6], [7]].\nThis is straightforward for a list seq:\n[seq[i:i+size] for i in range(0, len(seq), size)]\nHowever this requires knowing the length of the sequence, which we can’t do for a generator. Then we need a bit more code to track how much of the sequence has been consumed.\ndef minibatch(seq, size):\n    items = []\n    for x in seq:\n        items.append(x)\n        if len(items) >= size:\n            yield items\n            items = []\n    if items:\n        yield items\nFinally you may need to pad the last item so it’s the same size as the other batches. For example [[1,2,3], [4,5,6], [7, None, None]]. While it would be easy to update the code above to handle this in the last yield, conceptually this is a separate function.\ndef pad(items, value, length):\n    return items + [value] * (length - len(items))\nSo for example pad([7], None, 3) is [7, None, None]. Then you could modify the minibatch function to use the pad function in the final yield. Another approach would be to do it with function composition:\ndef minibatch_pad(seq, size, pad_value):\n    return map(lambda x: pad(x, pad_value, size),\n               minibatch(seq, size))\nThis is actually a really bad idea in Python. The biggest reason is if there is a problem it’s very hard to follow the stack trace. While this kind of solution would be common in a lisp, it’s quite uncommon in Python for this reason. Less importantly it’s a little bit less computationally efficient.\nFor completeness here is the padding solution.\ndef minibatch_pad(seq, size, pad_value):\n    items = []\n    for x in seq:\n        items.append(x)\n        if len(items) >= size:\n            yield items\n            items = []\n    if items:\n        yield pad(items, size, pad_value)"
  },
  {
    "objectID": "tox-intro/index.html",
    "href": "tox-intro/index.html",
    "title": "Why use Tox for Python Libraries",
    "section": "",
    "text": "A Python library needs to be tested on multiple versions of Python and dependencies. The best way to get confidence that a program is working, and continues to work as you change it, is to write good tests of its functionality. When you’ve got a Python application you can just pin the versions of everything (e.g. using pip-tools), specify a version of Python, and both deploy and test in a controlled environment, or wrap it in a container. For a library you have to support multiple versions of dependencies (otherwise no two packages would be compatible since they would be locked to different versions), and different versions of Python (because some users would be stuck on an old version and some on a new). Obviously you can’t support every possible combination, but for the dependencies and versions that are most likely to cause issues (fast changing things) you can support and test a few major versions.\nTox is a really useful tool for testing a library on multiple combinations of Python versions and dependency versions. It handles setting up different virtual environments with the correct set of dependencies and running the tests across all of them. This is good because it takes quite a bit of discipline to set them up manually and check them; but you want tests to be easy and reliable to run.\nHere’s an example from the tox documentation. You can specify different sets of dependencies that are combined together in a test matrix.\n[tox]\nenvlist = py{36,37,38}-django{22,30}-{sqlite,mysql}\n\n[testenv]\ndeps =\n    django22: Django>=2.2,<2.3\n    django30: Django>=3.0,<3.1\n    # use PyMySQL if factors \"py37\" and \"mysql\" are present in env name\n    py38-mysql: PyMySQL\n    # use urllib3 if any of \"py36\" or \"py37\" are present in env name\n    py36,py37: urllib3\n    # mocking sqlite on 3.6 and 3.7 if factor \"sqlite\" is present\n    py{36,37}-sqlite: mock\nEven if you are only supporting a very limited set of versions it can still be useful to use Tox to make sure you’re actually testing those versions, and not whatever you happened to have in your virtualenv at the time."
  },
  {
    "objectID": "sql-vlookup/index.html",
    "href": "sql-vlookup/index.html",
    "title": "Binning data in SQL",
    "section": "",
    "text": "For example to do a range lookup of users.amount on bins.bin to get bin.label you can use:\nselect u.*, label\nfrom users as u\nleft join (\nselect bins.*, lead(bin) over (order by bin) as next_bin\nfrom bins\n) b on amount >= bin and (next_bin is null or amount < next_bin)\nThe rest of this article elaborates on this example and some different ways to solve it.\n\nExcel Approach\nSuppose we have some users names and amounts.\n\n\n\nname\namount\n\n\n\n\ndan\n-1\n\n\njohn\n1\n\n\ncat\n2\n\n\nsam\n3\n\n\nmiguel\n4\n\n\ntom\n5\n\n\nkyle\n9\n\n\n\nWe want to bin this into custom bins (where the bin is unique and sorted):\n\n\n\nbin\nlabel\n\n\n\n\n0\n0-2\n\n\n2\n2-6\n\n\n6\n6-7\n\n\n7\n7+\n\n\n\nThis is simple in excel; if the latter range is labelled bin and the we can do VLOOKUP(C10, bins, 2) (where C10 is an example amount).\n\n\n\nExample of VLOOKUP on Bins problem\n\n\nThe result is:\n\n\n\nname\namount\nlabel\n\n\n\n\ndan\n-1\nNULL\n\n\njohn\n1\n0-2\n\n\ncat\n2\n2-6\n\n\nsam\n3\n2-6\n\n\nmiguel\n4\n2-6\n\n\ntom\n5\n2-6\n\n\nkyle\n9\n7+\n\n\n\n\n\nUsing double joins in SQL\nThe range lookup goes down the rows until it hits one that is larger than the value. We can do this by getting the maximum bin for which amount >= bin. We can then join this maximum bin back to the bin table to find out the label (fiddle)\nselect name, amount, label from (\nselect name, amount, max(bin) as max_bin\nfrom users\nleft join bins on amount >= bin\ngroup by name, amount\n) as user_bin\nleft join bins on bin = max_bin\norder by amount\nUnforutnately we need to group by all the key columns (here name and amount) and join to the bin table twice to do this. We can improve on this by using the row_number window function.\n\n\nUsing row_number\nAnother way to get the range lookup is to find the last row for which amount >= bin is true, using the row_number window function. In this case we just need partition keys that uniquely identify the row, in this example we can just use user (fiddle).\nselect name, amount, label from (\nselect name, amount, label,\n        row_number() over (partition by name order by bin desc) as rn\nfrom users\nleft join bins on amount >= bin\n) labelled_users\nwhere coalesce(rn, 1) = 1\nNote that we use the coalesce to avoid dropping off the NULL labelled row.\nThe issue with this is we need to keep track of the key column, we can avoid that by directly implementing the range lookups on joining to the bins table.\n\n\nUsing a range join in SQL\nAnother way of looking at the range lookup is that we want the last row where amount >= bin, and so for the next row amount < bin. We can implement this using the lead window function (fiddle)\nselect u.*, label\nfrom users as u\nleft join (\nselect bins.*, lead(bin) over (order by bin) as next_bin\nfrom bins\n) b on amount >= bin and (next_bin is null or amount < next_bin)\nThis is the most flexible solution because we don’t need to know anything about the key columns of users. If you wanted a different kind of range lookup behaviour you could also implement it by changing lead to lag and moving around the inequalities. While we could have manually constructed next_bin in the table, using the lead function ensures that the bins are set right.\n\n\nCreating a fallback for small values\nIn matching VLOOKUP we end up with anything before the first bin being labelled as NULL. We could try to create a minimum bin with the smallest allowable value (like the smallest integer, -Infinity for a float or the empty string), but this is messy. Another way to try to handle this is by adding a NULL to the bins for the initial fallback.\n\n\n\nbin\nlabel\n\n\n\n\nNULL\n<0\n\n\n0\n0-2\n\n\n2\n2-6\n\n\n6\n6-7\n\n\n7\n7+\n\n\n\nThen we can catch this case in our previous query, being careful to order NULL first (fiddle):\nselect u.*, label\nfrom users as u\nleft join (\nselect bins.*, lead(bin) over (order by bin nulls first) as next_bin\nfrom bins\n) b on (bin is null or amount >= bin) and (next_bin is null or amount < next_bin)\nFinally giving\n\n\n\nname\namount\nlabel\n\n\n\n\ndan\n-1\n<0\n\n\njohn\n1\n0-2\n\n\ncat\n2\n2-6\n\n\nsam\n3\n2-6\n\n\nmiguel\n4\n2-6\n\n\ntom\n5\n2-6\n\n\nkyle\n9\n7+\n\n\n\nSo we have a flexible SQL binning solution that relies on an external table. If we ever want to change the bins (or the labels) we can just update the bins table."
  },
  {
    "objectID": "ngram-python/index.html",
    "href": "ngram-python/index.html",
    "title": "Counting n-grams with Python and with Pandas",
    "section": "",
    "text": "To find all sequences of n-grams; that is contiguous subsequences of length n, from a sequence xs we can use the following function:\ndef seq_ngrams(xs, n):\n    return [xs[i:i+n] for i in range(len(xs)-n+1)]\nFor example:\n> seq_ngrams([1,2,3,4,5], 3)\n[[1,2,3], [2,3,4], [3,4,5]]\nThis works by iterating over all possible starting indices in the list with range, and then extracting the sequence of length n using xs[i:i+n].\nIn the specific case of splitting text into sequences of words this is called w-shingling and can be done by splitting:\ndef shingle(text, w):\n    tokens = text.split(' ')\n    return [' '.join(xs) for xs in seq_ngrams(tokens, w)]\nThen to count the w-shingles in a corpus you can simply use the inbuilt Counter:\nfrom collections import Counter\ndef count_shingles(corpus, w):\n    return Counter(ngram for text in corpus for ngram in shingle(text, w))\nIf you’re dealing with very large collections you can drop in replace Counter with the approximate version bounter.\nThe rest of this article explores a slower way to do this with Pandas; I don’t advocate using it but it’s an interesting alternative.\n\nCounting n-grams with Pandas\nSuppose we have some text in a Pandas dataframe df column text and want to find the w-shingles.\n                                                         text\n0                                 Engineering Systems Analyst\n1                                     Stress Engineer Glasgow\n2                            Modelling and simulation analyst\nThis can be turned into an array using split and then unnested with explode.\nwords = (df\n.text\n.str.split(' ')\n.explode()\n)\nThis would result in one word per line. The index is preserved so you can realign it with the original series.\n0         Engineering \n0         Systems\n0         Analyst\n1         Stress\n1         Engineer\n1         Glasgow\n2         Modelling\n2         and\n2         simulation\n2         analyst\nTo get sequences of words you can use the shift operator which is like lead and lag in SQL.\nnext_word = words.groupby(level=0).shift(-1)\nResulting in:\n0         Systems\n0         Analyst\n0         NaN \n1         Engineer\n1         Glasgow\n1         NaN\n2         and\n2         simulation\n2         analyst\n2         NaN\nand these can be recombined with (words + next_word).dropna():\n0         Engineering Systems\n0         Systems Analyst\n1         Stress Engineer\n1         Engineer Glasgow\n2         Modelling and\n2         and simulation\n2         simulation analyst\nFinally you can find the total with value_counts.\nWhile this is a bit messier and slower than the pure Python method, it may be useful if you needed to realign it with the original dataframe. This can be abstracted to arbitrary n-grams:\nimport pandas as pd\ndef count_ngrams(series: pd.Series, n: int) -> pd.Series:\n    ngrams = series.copy().str.split(' ').explode()\n    for i in range(1, n):\n        ngrams += ' ' + ngrams.groupby(level=0).shift(-i)\n        ngrams = ngrams.dropna()\n    return ngrams.value_counts()    \nThis is similar to the approach of the R tidytext library for extracting n-grams which has the function unnest_tokens that can produce ngrams of arbitrary length."
  },
  {
    "objectID": "finding-book-heuristics/index.html",
    "href": "finding-book-heuristics/index.html",
    "title": "Bootstrapping a book classifier",
    "section": "",
    "text": "I’m working on a project to extract books from Hacker News. Most HackerNews posts aren’t about books, and it would be extremely tedious to manually annotate examples when most of them are negative. Instead I used different heuristics to determine whether a book contains a title, that can then be used for weak labelling.\nMy main takeaway is that zero shot classification seems like a great starting point for building a classifier. It doesn’t do as well as domain specific rules but it does well enough to be useful and has large coverage."
  },
  {
    "objectID": "finding-book-heuristics/index.html#methods",
    "href": "finding-book-heuristics/index.html#methods",
    "title": "Bootstrapping a book classifier",
    "section": "Methods",
    "text": "Methods\nThe methods I tried are:\n\nUsing a list of “Seed Books” as a gazetteer. This is a general approach in NER but the difficulty is selecting the items.\nFinding comments in Hacker News Threads asking for book recommendations.\nUsing the Work of Art entity using the SpaCy transformer model.\nUsing an NLI based Zero Shot Classification with the prompt “This comment mentions a book by title.” (with a cutoff of 80% probability).\nDirect child comments of the “Seed Books” comments\nDirect parent comments of the “Seed Books” comments\nSiblings of the “Seed Books” comments (excluding “Seed Books” comments themselves)"
  },
  {
    "objectID": "finding-book-heuristics/index.html#results",
    "href": "finding-book-heuristics/index.html#results",
    "title": "Bootstrapping a book classifier",
    "section": "Results",
    "text": "Results\nFor each of the heuristics I calculated the positive label rate and estimated the precision. The positive label rate is the number of examples labelled as positive; this tells us something about the number of labels. To estimate precision I annotated 20 random examples and calculated the number of comments that contain a book title.\n\n\n\nHeuristic\nPositive Label Rate\nPrecision\nGeneral?\nInference Speed\n\n\n\n\nSeed Book (Gazetteer)\n0.06%\n18/20\nSort of\nMedium\n\n\nAsk HN Book Comment\n0.04%\n11/20\nNo\nFast\n\n\nWork of Art\n2.9%\n7/20\nA little\nSlow\n\n\nZero Shot Classifier\n2.6%\n6/20\nYes\nSlow\n\n\nChild of Seed Book\n0.06%\n6/20\nNo\nFast\n\n\nParent of Seed Book\n0.05%\n5/20\nNo\nFast\n\n\nSibling of Seed Book\n0.7%\n2/20\nNo\nFast\n\n\n\nThe zero shot classifier comes out as a good comprimise of being a general approach that gets reasonable precision with very high coverage. The gazetteer has the highest precision, but has low coverage and is likely to have very low diversity of examples, and requires a lot of effort to maintain; nearby comments may contain more examples but they all have a lower precision than the other methods. The Ask HN Book Comment threads have a good precision and high diversity but are a very small sample - this could be good for a weak labeller. The Work of Art approach is comparable to the zero shot classifier (in precision because we’re having to exclude titles of movies, books, and songs), but requires the specific labelled data."
  },
  {
    "objectID": "stanford-ner-python/index.html",
    "href": "stanford-ner-python/index.html",
    "title": "Training a Stanford NER Model in Python",
    "section": "",
    "text": "I replicated the benchmark in A Named Entity Based Approach to Model Recipes, by Diwan, Batra, and Bagler using Stanford NER, and check it using seqeval. Evaluating NER is surprisingly tricky, as David Batista explains, and I want to check that the results in the paper are the same as what seqeval gives, so that I’m giving a fair comparison to other models. Thanks to the authors sharing the training data on github I was able to do this, as you can see in the Jupyter Notebook.\nThe rest of this article goes through how to train and evaluate a Stanford NER model using Python, and that the scores output by Stanford NLP on the test set match those produced by seqeval.\n\nSetting up Stanford NLP\nThe stanza library has both great neural network based models for linguistic analysis (see my previous writeup), but also an interface to Stanford Core NLP. Unfortunately it doesn’t provide a direct way of training an NER model using Core NLP, however we can do it ourselves using the stanford-corenlp JAR it installs. The first step is to install the models and find the path to the Core NLP JAR.\nimport os\nfrom pathlib import Path\n\nimport stanza\n\nstanza.install_corenlp()\n# Reimplement the logic to find the path where stanza_corenlp is installed.\ncore_nlp_path = os.getenv('CORENLP_HOME', str(Path.home() / 'stanza_corenlp'))\n\n# A heuristic to find the right jar file\nclasspath = [str(p) for p in Path(core_nlp_path).iterdir()\n             if re.match(r\"stanford-corenlp-[0-9.]+\\.jar\", p.name)][0]\n\n\nTrain NER Model\nThe Stanford NER model requires data where each line is a token, followed by a tab, followed by the NER tag. A blank line represents a sentence break. In this case I could get the relevant training and test repository in this format already.\nThe NER model has to be configured; but there’s no information on the paper on what features and hyperparameters are used. I copied the template configuration out of the FAQ, which happened to work well. The template can be saved to a file and then referred to when training.\ndef ner_prop_str(train_files: List[str],\n                 test_files: List[str],\n                 output: str) -> str:\n    \"\"\"Returns configuration string to train NER model\"\"\"\n    train_file_str = ','.join(train_files)\n    test_file_str = ','.join(test_files)\n    return f\"\"\"\ntrainFileList = {train_file_str}\ntestFiles = {test_file_str}\nserializeTo = {output}\nmap = word=0,answer=1\n\nuseClassFeature=true\nuseWord=true\nuseNGrams=true\nnoMidNGrams=true\nmaxNGramLeng=6\nusePrev=true\nuseNext=true\nuseSequences=true\nusePrevSequences=true\nmaxLeft=1\nuseTypeSeqs=true\nuseTypeSeqs2=true\nuseTypeySequences=true\nwordShape=chris2useLC\nuseDisjunctive=true\n\"\"\"\nFor more information on the parameters you can check the NERFeatureFactory documentation or the source. We need to write this to a file so we can write a wrapper to do this\ndef write_ner_prop_file(ner_prop_file: str,\n                        train_files: List[str],\n                        test_files: List[str],\n                        output_file: str) -> None:\n    with open(ner_prop_file, 'wt') as f:\n        props = ner_prop_str(train_files, test_files, output_file)\n        f.write(props)\nThe actual NER Training process is in Java, so we’ll run a Java process to train a model and return the path to the model file. We’ll also print out the report from stderr summarising the training.\nimport subprocess\nfrom typing import List\n\ndef train_model(model_name: str,\n                train_files: List[str],\n                test_files: List[str],\n                print_report: bool = True,\n                classpath: str = classpath) -> str:\n    \"\"\"Trains CRF NER Model using StanfordNLP\"\"\"\n    model_file = f'{model_name}.model.ser.gz'\n    ner_prop_filename = f'{model_name}.model.props'\n    write_ner_prop_file(ner_prop_filename, train_files, test_files, model_file)\n\n    result = subprocess.run(\n                ['java',\n                 '-Xmx2g',\n                 '-cp', classpath,\n                 'edu.stanford.nlp.ie.crf.CRFClassifier',\n                 '-prop', ner_prop_filename],\n                capture_output=True,\n                check=True)\n\n    if print_report:\n        print(*result.stderr.decode('utf-8').split('\\n')[-11:], sep='\\n')\n\n    return model_file\nRunning training on the AllRecipes.com train and test set produced an output like this.\nThe summary report shows for each model and entity type:\n\nTrue Positives (TP): The number of times that entity was predicted correctly\nFalse Positives (FP): The number of times that entity in the text but not predicted correctly\nFalse Negative (FN): The number of times that entity was not in the text and predicted\nPrecision (P): Probability a predicted entity is correct, TP/(TP+FP)\nRecall (R): Probability a correct entity is predicted, TP/(TP+FN)\nF1 Score (F1): Harmonic mean of precision and recall, 2/(1/P + 1/R).\n\nCRFClassifier tagged 2788 words in 483 documents at 9992.83 words per second.\n         Entity P   R   F1  TP  FP  FN\n             DF 1.0000  0.9608  0.9800  49  0   2\n           NAME 0.9297  0.9279  0.9288  463 35  36\n       QUANTITY 1.0000  0.9962  0.9981  522 0   2\n           SIZE 1.0000  1.0000  1.0000  20  0   0\n          STATE 0.9601  0.9633  0.9617  289 12  11\n           TEMP 0.8750  0.7000  0.7778  7   1   3\n           UNIT 0.9819  0.9841  0.9830  434 8   7\n         Totals 0.9696  0.9669  0.9682  1784    56  61\nThe Totals F1 score of 0.9682 exactly matched what was reported in the paper. Now let’s try to manually evaluate the test set using seqeval.\n\n\nRunning the model\nStanza has a robust way of running CoreNLP and annotating texts, as per the documentation. We can configure the NER model used to the one that we just trained. Because the text we’ve used is pre-tokenized I’m just going to join them with a space and tokenize on whitespace; for ingredients we want quantities like 1/2 to be treated as a single token but the default tokenizer will split them. When I first ran the annotations it would sometime output NUMBER, which wasn’t an input entity; it turns out this is hardcoded and we have to diable the numeric classifiers.\nfrom stanza.server import CoreNLPClient\n\ndef annotate_ner(ner_model_file: str,\n                 texts: List[str],\n                 tokenize_whitespace: bool = True):\n    properties = {\"ner.model\": ner_model_file,\n                  \"tokenize.whitespace\": tokenize_whitespace,\n                  \"ner.applyNumericClassifiers\": False}\n\n    annotated = []\n    with CoreNLPClient(\n         annotators=['tokenize','ssplit','ner'],\n         properties=properties,\n         timeout=30000,\n         be_quiet=True,\n        memory='6G') as client:\n\n        for text in texts:\n            annotated.append(client.annotate(text))\n    return annotated\nThe annotated data will have many attributes, but we’re just interested in the input words and named entities so we’ll extract them into a dictionary. Note that we extract the coarseNER; sometimes another default NER model predicts a fine grained NER (like NATIONALITY) which writes into the ner attribute if it’s empty. Using coarseNER means we only get tags from our training set.\ndef extract_ner_data(annotation) -> Dict[str, List[str]]:\n    tokens = [token for sentence in annotation.sentence\n                    for token in sentence.token]\n    return {'tokens': [t.word for t in tokens],\n            'ner': [t.coarseNER for t in tokens]}\n\ndef ner_extract(ner_model_file: str,\n                texts: List[str],\n                tokenize_whitespace: bool = True) -> List[Dict[str, List[str]]]:\n    annotations = annotate_ner(ner_model_file, texts, tokenize_whitespace)\n    return [extract_ner_data(ann) for ann in annotations]\nNow if we’ve got the test tokens as a list containing lists of words, and the test labels as a list containing corresponding lists of NER tags we can run them through the model.\ntest_texts = [' '.join(text) for text in test_tokens]\npred_labels = [text['ner'] for text in ner_extract(modelfile, texts)]\n\n\nEvaluating with seqeval\nThe library seqeval provides robust sequence labelling metrics. In particular scores should be at an entity level; you don’t get it right unless you predict exactly the tokens in an entity. I wanted to check seqeval gave similar results to the sumamary report above.\nSeqeval expects the tags to be in one of the standard tagging formats, but the data I had just had labels (like NAME, QUANTITY, and UNIT). It is impossible to disambiguate adjacent tags of the same entity type, but the annotations mostly assume there can only be one of each kind of entity in an ingredient. The simplest way to convert it is into IOB-1, which only adds a B tag when there are two adjacent tags of the same entity type. Since we’re assuming this doesn’t happen we just need to prepend I- to all tags other than O.\ndef convert_to_iob1(tokens):\n    return ['I-' + label if label != 'O' else 'O' for label in tokens]\n\nresult = convert_to_iob1(['QUANTITY', 'SIZE', 'NAME', 'NAME', 'O', 'STATE'])\nexpected = ['I-QUANTITY', 'I-SIZE', 'I-NAME', 'I-NAME', 'O', 'I-STATE']\nassert result == expected\nThen we can get the classification report using seqeval\nfrom seqeval.metrics import classification_report\n\nactual_labels = [convert_to_iob1(text) for text in actual_labels]\npred_labels = [convert_to_iob1(text) for text in pred_labels]\n\nprint(classification_report(actual_labels, pred_labels, digits=4))\nThe output report matches the report from Stanford NLP precisely. Note that it uses the support (total number of actual entities) instead of the True Positives, False Positives, and False Negatives, but actually they are equivalent.\n\nsupport = TP + FN\nTP = R * support\nFP = TP (1/P - 1)\nFN = support - TP\n\n              precision    recall  f1-score   support\n\n          DF     1.0000    0.9608    0.9800        51\n        NAME     0.9297    0.9279    0.9288       499\n    QUANTITY     1.0000    0.9962    0.9981       524\n        SIZE     1.0000    1.0000    1.0000        20\n       STATE     0.9601    0.9633    0.9617       300\n        TEMP     0.8750    0.7000    0.7778        10\n        UNIT     0.9819    0.9841    0.9830       441\n\n   micro avg     0.9696    0.9669    0.9682      1845\n   macro avg     0.9638    0.9332    0.9471      1845\nweighted avg     0.9695    0.9669    0.9682      1845\nUsing seqeval on all the training and test sets in the paper I could reproduce their f1-scores within 0.01. Below shows my results (left) and the results on the paper (right).\n\n\n\nResults\n\n\nThe Stanford NER library is a bit under-documented and has some surprising features, but with some work we can get it to run in Python. The metrics it produces line up with those from seqeval."
  },
  {
    "objectID": "regular-expressions-automata-monoids/index.html",
    "href": "regular-expressions-automata-monoids/index.html",
    "title": "Regular expressions, automata and monoids",
    "section": "",
    "text": "An automoton is (roughly) a set of symbols, and a set of states, along with transitions for each state that take a symbol and return another state. They can be used to model (and verify) simple processes.\nAutomata can be brought into correspondence with formal languages in a very natural way; given an initial state s, and a sequence of symbols (a1, a2, …, an) the automata has a naturally assigned state (… ((s a1) a2) … an) (where “(state symbol)” represents the state obtained from the transition on symbol using state). Then if we nominate an initial state, and a set of “accepting” valid states, we say a string is in the language of the automata if and only if when applied to the initial state it ends in a final state.\nThis gives a very useful pairing in computer science; formal languages are useful tools, and automata (often) give an efficient way to implement them on a computer.\n\nTo get a little more mathematical a semigroup is a a closed associative binary operation; if we add a two sided identity it is called a monoid, and if we additionally add inverses it becomes a group. For instance under addition the set of positive numbers (1, 2, …) is a semigroup, the set of non-negative numbers is a monoid with identity 0, and the set of integers is a group with -a being the inverse of a. Clearly every group is a monoid (forgetting about the inverses) and every monoid is a semigroup (forgetting about the identity).\nIn the same way a group often arises as a set of invertible transformations (isomorphisms), a monoid often arises as a set of transformations (morphisms). Another useful example of a monoid is sets under union, with 0 corresponding to the empty set.\nThe free monoid generated by a set S, denoted S*, is the set of all (finite or infinite) sequences of elements of S with multiplication defined as concatenation.\nFor example {x}* has set \\(\\{\\epsilon, x, xx, xxx, xxxx, \\ldots\\} = \\{x^0, x^1, x^2, x^3, x^4, \\ldots \\}\\) (where \\(\\epsilon\\) represents the sequence with no elements), and multiplication is given by \\(x^n x^m = x^{n+m}\\) . As a further example some elements of {x, y}* are \\(\\epsilon\\) , x, y, xx, xy, yx, yy, xxx, xxy, xyx, yxx, xyy, yxy, yyx, yyy, …\nIn computer science terms the free monoid generated by the set S is precisely the set of all strings (or words) in the alphabet S, with the monoid product corresponding to string concatenation.\nUsing this notation a language over a finite set S is a subset of S*; that is an element of the power set \\(2^{S*}\\) . More generally we can define a language over a monoid, M, as an element of the power set of M.\nThere is a natural product on the power set of a monoid; \\(AB \\equiv \\{ab | a \\in A, b \\in B\\}\\) , and so it too is a monoid with identity \\(\\{\\epsilon\\}\\) . There is another natural monoidal structure on any collection of subsets; union with the additive identity of the empty set. Notice that \\(A (B \\cup C) = (AB) \\cup (AC)\\) and similarly \\((A \\cup B) C = (AC) \\cup (BC)\\) , and \\(\\emptyset A = \\emptyset = A \\emptyset\\) . Consequently the power set of a monoid naturally has the structure of a semiring.\nGiven a subset S of a monoid M, denote S* (the Kleene star) to be its monoidal closure; the smallest submonoid of M containing S. The regular expressions over a set (alphabet) Σ is defined to be the set generated by the elements \\(\\{\\epsilon\\}, \\{a\\}, a \\in \\Sigma\\) using the Kleene algebra formed by the semiring \\(2^M\\) with the Kleene star.\nOn the other hand a deterministic finite automoton (DFA) over a set (alphabet) Σ is a finite set of states S, an initial state s in S, a subset F of accepting states of S, and a transition map \\(t : S \\times \\Sigma \\to S\\) . The language of a DFA is the set of all strings (a b … x) of symbols in Σ such that \\(t( \\cdots t( t(s, a), b) \\cdots , x) \\in F\\) . Often a DFA is represented diagrammatically using circles to represent states, and labelled arrows to represent the transitions between states [this looks rather like a category theory diagram]. The initial state is denoted by a horizontal arrow pointing to it, and the final states are represented by a double circle.\n\n\n\nAn example DFA Diagram\n\n\nIn the example above the alphabet is {0, 1} the states are {S1, S2}, the initial state is S1, and the final states are {S1}, the transitions are t(S1, 0) = S2, t(S1, 1) = S1, t(S2, 1) = S2, t(S2, 0) = S1.\nOften the transitions are represented as a table with states listed vertically and transitions listed horizontally e.g.\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nS1\n\n\nS2\n\n\nS1\n\n\n\n\nS2\n\n\nS1\n\n\nS2\n\n\n\n\n\nMore algebraically we can consider the transition to be the monoidal action; since the elements of Σ generate Σ* freely, the transition extends uniquely to a function \\(T : S \\times \\Sigma* \\to S\\) such that T(t, xy) = T(T(t, x), y) for any state t and elements of Σ* x and y.\nRephrasing and generalising slightly, a DFA over a monoid M is a set of states S, a (contravariant) monoid homomorphism \\(\\rho : M \\to \\hbox{Map}(S)\\) (where Map(S) represents all functions from S to S; i.e. \\(\\rho(xy) = \\rho(y) \\rho(x)\\) ), an initial state s from S, and a subset F of accepting states in S. Then the language of a DFA is precisely \\(L = \\{ x \\in M | \\rho(x) \\circ s \\in F\\}\\) .\nTheorem: The regular languages are equivalent to the languages representable by a DFA.\nThis theorem can be proved as follows: a DFA is inductively transformed into a regular expression by transforming the DFA that can only pass through an increasingly large subset of states. A regular expression is transformed into a nondeterministic finite automoton, which is in turn transformed into a DFA.\nA nondeterministic finite automaton (NFA) over a set Σ is a set S, an initial state s, a set of accepting states F and a transition map t \\(S \\times \\Sigma \\to 2^S\\) . Then a string (a b … c) is in the language of the NFA if and only if there is some \\(s_1 \\in t(s, a), s_2 \\in t(s_1, b), ... s_n \\in t(s_{n-1}, c)\\) such that \\(s_n \\in F\\) .\nSometimes epsilon transitions are allowed; that is transitions that take no input so t \\(S \\times \\Sigma \\cup \\{\\epsilon\\} \\to 2^S\\) , and we allow arbitrary epsilon insertions in the string. As with DFAs, NFAs can be represented diagrammatically and implemented efficiently as a table (though in this case we need to trace every possible path of a transition).\nThese can be extended more algebraically as follows: a NFA over a monoid M is a set S, a set of initial states I, a set of accepting states F, and a monoid/semigroup homomorphism \\(\\rho : M \\to 2^S\\) . The language of an NFA is \\(\\{x \\in M | \\rho(x) \\circ I \\cap F \\neq \\emptyset \\}\\) . (The previous definition is simply M = Σ*, I = {s} and \\(\\rho(x) \\circ A = \\bigcup_{a \\in A} t(x, a)\\) ).\nThe monoidal case corresponds to no epsilon transitions, and the semigroup case allows monoidal transitions (for then \\(\\rho(\\epsilon)\\) need not be the identity). To promote a semigroup homomorphism ρ to a monoidal homomorphism η we simply define \\(\\eta(x) = \\rho(x) \\rho(\\epsilon)^*\\) (considering \\(2^S\\) as a Kleene algebra in the obvious way).\nIt is almost trivial to represent a given NFA as a DFA; we take the set of the DFA to be \\(2^S\\) , the initial state to be S, the final states to be any state intersecting F, and the transition function to be the same. This is the so called power-set construction.\nSo DFA=NFA=Regular Languages."
  },
  {
    "objectID": "tree-diagram-suitcase-bills/index.html",
    "href": "tree-diagram-suitcase-bills/index.html",
    "title": "Tree Diagram Bills",
    "section": "",
    "text": "Make a tree diagram for your estimate in Problem 1.3. Do it in three steps: (1) Draw the tree without any leaf estimates, (2) estimate the leaf values, and (3) propagate the leaf values upward to the root.\n\nThis is referring to the suitcase of money.\n\nStep 1: Tree\n\n\n\n\ngraph LR;\n   VolBankNote[Volume of Bank Note]\n   VolSuitcase[Volume of Suitcase]\n   ValueBankNote[Value of Bank Notes in Suitcase]\n   NumBankNote[Number of Bank Notes in Suitcase]\n   ValueSuitcase[Value of Suitcase]\n   \n   \n   NumBankNote --> ValueSuitcase\n   ValueBankNote --> ValueSuitcase\n\n   VolBankNote -->|-1| NumBankNote\n   VolSuitcase --> NumBankNote\n\n   SuitWidth[Width of Suitcase] --> VolSuitcase\n   SuitHeight[Height of Suitcase] --> VolSuitcase\n   SuitDepth[Depth of Suitcase] --> VolSuitcase\n   \n   BankHeight[Height of Bank Note] --> VolBankNote\n   BankWidth[Width of Bank Note] --> VolBankNote\n   BankDepth[Thickness of Bank Note] --> VolBankNote\n\n\n\n\n\n\n\n\n\n\n\nStep 2: Annotated Leaves\n\n\n\n\ngraph LR;\n   VolBankNote[Volume of Bank Note]\n   VolSuitcase[Volume of Suitcase]\n   ValueBankNote[Value of Bank Notes in Suitcase<br>$100]\n   NumBankNote[Number of Bank Notes in Suitcase]\n   ValueSuitcase[Value of Suitcase]\n   \n   \n   NumBankNote --> ValueSuitcase\n   ValueBankNote --> ValueSuitcase\n\n   VolBankNote -->|-1| NumBankNote\n   VolSuitcase --> NumBankNote\n\n   SuitWidth[Width of Suitcase<br/>30cm] --> VolSuitcase\n   SuitHeight[Height of Suitcase<br/>50cm] --> VolSuitcase\n   SuitDepth[Depth of Suitcase<br/>30cm] --> VolSuitcase\n   \n   BankHeight[Height of Bank Note<br/>15cm] --> VolBankNote\n   BankWidth[Width of Bank Note<br/>6cm] --> VolBankNote\n   BankDepth[Thickness of Bank Note<br/>0.01cm] --> VolBankNote\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Propagate\n\n\n\n\ngraph LR;\n   VolBankNote[Volume of Bank Note<br>1 cm<sup>3</sup>]\n   VolSuitcase[Volume of Suitcase<br>50L]\n   ValueBankNote[Value of Bank Notes in Suitcase<br>$100]\n   NumBankNote[Number of Bank Notes in Suitcase<br>50,000]\n   ValueSuitcase[Value of Suitcase<br>$5 Million]\n   \n   \n   NumBankNote --> ValueSuitcase\n   ValueBankNote --> ValueSuitcase\n\n   VolBankNote -->|-1| NumBankNote\n   VolSuitcase --> NumBankNote\n\n   SuitWidth[Width of Suitcase<br/>30cm] --> VolSuitcase\n   SuitHeight[Height of Suitcase<br/>50cm] --> VolSuitcase\n   SuitDepth[Depth of Suitcase<br/>30cm] --> VolSuitcase\n   \n   BankHeight[Height of Bank Note<br/>15cm] --> VolBankNote\n   BankWidth[Width of Bank Note<br/>6cm] --> VolBankNote\n   BankDepth[Thickness of Bank Note<br/>0.01cm] --> VolBankNote"
  },
  {
    "objectID": "common-crawl-legality/index.html",
    "href": "common-crawl-legality/index.html",
    "title": "Legality of Publishing Web Crawls",
    "section": "",
    "text": "As a data analyst I rely on open code and open data to inform decisions. There’s a lot of data available on the web which would be great to transform and make openly available to the community. However it’s not my data to give, and I’m concerned whether it would violate copyright.\nAn interesting aspect is there are companies that scrape data from all over the web to use for analysis. A few of them are Google, Microsoft, Apple and Amazon. The scraped data is used for keyword analysis and powers their search products, among other things. They don’t verify terms and conditions of every website they visit, at most checking the robots.txt to see what parts of the site they’ve been able to index. However they don’t share this data.\nCommon Crawl is an organisation that does the same thing as the tech giants (although on a smaller scale), but releases their data in the open (the Internet Archive Wayback Machine is another example). The scraped websites HTML and metadata is available, they have a useful columnar index of the pages they’ve captured, and I’ve used it to build a reasonable sample of job ads. This dataset has been immensely beneficial in Natural Language Processing, and is often used in Information Extraction and large Language Models; for example it’s a large part of RoBERTa. Larger datasets would be even more useful; OpenAI curated their own web crawl for the powerful GPT-2.\nHow do they do this without breaching copyright? An excellent article from Forbes covers some of this from the director Sara Crouse. She raises three main points:\nThese points are useful in considering whether you should publish your own web crawl data."
  },
  {
    "objectID": "common-crawl-legality/index.html#availability-of-lawyers",
    "href": "common-crawl-legality/index.html#availability-of-lawyers",
    "title": "Legality of Publishing Web Crawls",
    "section": "Availability of Lawyers",
    "text": "Availability of Lawyers\nThe reality of laws is that they need to be enforced. Copyright infringement is only becomes a practical issue when you receive a letter from a lawyer. If you need to defend yourself in court, even if you’re in the right, it could be a huge cost in legal fees.\nCommon Crawl is a non-profit organisation trying to archive the web. They’ve got a bit of money and a lot of goodwill. If they get sued the litigator won’t always come off in a good light, and there are lots of lawyers willing to work with them to test the law on copyright.\nIf you’re an individual releasing some data scraped from a company’s website for research purposes, they are likely to have more at stake and more resources than you. For a small project you’re not likely to get a Pro Bono lawyer and would have to pay the legal costs yourself. If they decide to pursue legal action the time and cost would not be worth it for you, and it would likely be wise to take down the material if asked to do so.\nIf the data is user submitted content (for example a Reddit Dataset) the actual owner of copyright will typically be the users. This means, depending on the terms of the publishing site, they may not own the copyright and may not be able to legally pursue you because of it. But I don’t know whether that’s true, and the only way to find out is to pay some legal professionals for advice, and maybe even resolve it in court.\nSo the pragmatic answer to can I publish this is, will the source of the data be interested in legally pursuing you over it? If you’re putting their business model at risk the answer is almost certainly yes."
  },
  {
    "objectID": "common-crawl-legality/index.html#ease-of-access",
    "href": "common-crawl-legality/index.html#ease-of-access",
    "title": "Legality of Publishing Web Crawls",
    "section": "Ease of Access",
    "text": "Ease of Access\nI suspect the argument that the data is difficult to consume wouldn’t hold up in court. The difficulty to access a resource doesn’t seem to have any bearing on whether you have the right to republish that information. Besides it not as difficult as they make it sound in the article; a software engineer could make it easy with some straightforward work.\nHowever because it’s not aimed at human consumption it doesn’t touch the business model for many websites; attention and advertising revenue. This means they have little reason to pursue legal action; many website owners are probably not even aware their data is in Common Crawl.\nIf a published dataset has low visibility and low impact on the business then in practice it may be harmless to publish.\nIdeally you would contact the websites you are sourcing the data from directly and ask them the right to publish the data. But in general unless there is a large benefit to them they are likely to decline your request and potentially order you to remove the material."
  },
  {
    "objectID": "common-crawl-legality/index.html#fair-use",
    "href": "common-crawl-legality/index.html#fair-use",
    "title": "Legality of Publishing Web Crawls",
    "section": "Fair Use",
    "text": "Fair Use\nThe most interesting part of the argument for Common Crawl is that they just have a small sample of web pages from each domain. This resonates with the idea of Fair Use where reproducing a small number of pages from a book is reasonable (which is how Google Books exists).\nThis won’t be true for a lot of data analysis crawls. Typically you want structured data, which requires targeting a few websites in depth.\nIn any case you would need lawyers to resolve whether it is fair use, but if you’re only taking a small sample of the site it’s more likely to be viewed that way."
  },
  {
    "objectID": "common-crawl-legality/index.html#ethics",
    "href": "common-crawl-legality/index.html#ethics",
    "title": "Legality of Publishing Web Crawls",
    "section": "Ethics",
    "text": "Ethics\nA separate question to “can I share this dataset” is “should I share this dataset”? I would say that if it is from a publicly available datasource and won’t lead to significant harms then it is a reasonable thing to do.\nI consider data published on the open web (that is not behind a paywall or login) to be freely available. This doesn’t mean you can claim the content is your own and modify it. But it seems strange to say you can’t keep a copy of that web page and share it, since it was at one time made available to everyone.\nOf course you don’t want to do any harm because of it.\nPublic Transport Victoria publicly released data that can be used to identify individual commuters personal information. Even though this was a public release I would carefully consider republishing it because it could do harm to individuals, and is likely to bring marginal benefits. The NSW Opal Dataset, while less interesting as it is in aggregate, is still useful while likely to do minimal harm. It would be great to see this sort of data released when it is unlikely to do harm, say in 50 years.\nSimilarly if I extracted a large amount of information I wouldn’t want to release email address as a column. Even if emails were in the original data, having done the work of extracting them it makes for an easy target for spammers. It seems unlikely there’s much benefit for publishing email addresses, so better to redact them.\nThese decisions are rarely clear cut; but even if it is legal it is worth considering whether republishing data will bring any harm."
  },
  {
    "objectID": "open-library/index.html",
    "href": "open-library/index.html",
    "title": "Open Library: A Book Knowledge Base",
    "section": "",
    "text": "Book titles can be ambiguous and so we need some way to link it to a unique entity. For example “The Stranger” could refer to one of many books; such as that by Richard Wright, Stephen King, or Colin Wilson. Books can also have multiple names; “L’Étranger” by Albert Camus may also be known as “The Stranger” or “The Outsider” from different translations. While we’re at it are different translations the same “book”?\nOpen Library describes itself as “a universal catalog for book metadata”. It’s open for the public to edit, they make the data public, and it has over 25 million works (books), including most I can think of. A work which can have multiple editions; from their editing FAQ:\n\nWork and edition are both bibliographic terms for referring to a book. A “work” is the top-level of a book’s Open Library record. An “edition” is each different version of that book over time. So, Huckleberry Finn is a work, but a Spanish translation of it published in 1934 is one edition of that work. We aspire to have one work record for every book and then, listed under it, many edition records of that work.\n\nThis makes a good fit for our purpose of identifying books. They also release bulk data dumps which allow batch processing of our extracts, and include useful metadata such as the ISBN of editions.\nFor usage in Hacker News we may need to make our own extensions to this. There are specific references like SICP for the Structure and Interpretation of Computer Programming, or The Dragon Book for Compilers: Principles, Techniques, and Tools which may not fit into Open Library, but are important for this usecase. But it’s a very good starting point.\nI’ll need to do some more work to process the data dumps to allow efficient linking. They’re about 10GB of gzipped TSV, including a JSON column. One approach is the Libraries Hacked repository which imports it into a PostgreSQL database."
  },
  {
    "objectID": "pain-gain/index.html",
    "href": "pain-gain/index.html",
    "title": "Pain gain matrix for discussing approaches",
    "section": "",
    "text": "The primary risk of this approach is getting too precise about it. It’s good to compare options but it’s easy to get into analysis paralysis or endless debates about where something should be placed that are resolved by resignment rather than agreement. The most valuable part is identifying the most valuable options and ruling out the least valuable quickly. One technique is to use a large oval shape when there’s a lot of uncertainty as to where it should be placed.\nOnce you’ve found the most promising few solutions you can test them to validate their viability. The exact one you choose doesn’t matter that much if it’s a two way door decision and you can test it cheaply.\n\n\n\nPain gain matrix"
  },
  {
    "objectID": "firefox-screenshot/index.html",
    "href": "firefox-screenshot/index.html",
    "title": "Taking Screenshots in Firefox",
    "section": "",
    "text": "Taking Screenshots in Firefox\n\n\nYou can then choose to download it, or copy it to the clipboard. If you copy it to the clipboard you can actually save it to a file using xclip:\nxclip -selection clipboard -t image/png > screenshot.png\nTo take this one step further my most common use for this is pasting images into my this website. I created a hacky function in Emacs to make this easy (baking in the fact that for this hugo site images are stored in ../../static/images/ relative to the posts). It prompts for a name and saves the file in the right location as {name}.png and creates a Markdown image link in the current buffer. It doesn’t handle all the way things could go wrong (special characters in the name, clipboard is empty, file exists, etc.), but is a useful starting point.\n(defun er/hugo-save-clipboard-image (name)\n  \"Saves image from clipboard to NAME.\"\n  (interactive \"sFile name: \")\n  (let ((outfile (concat \"/images/\" name \".png\")))\n        (call-process-shell-command\n         (concat \"xclip -selection clipboard -t image/png -o >../../static\" outfile)\n         \"/dev/null\"\n         0)\n        (save-excursion\n          (insert (concat \"![](\" outfile \")\")))\n        (forward-char)\n        ))"
  },
  {
    "objectID": "hypothesis-test-pandas-apply/index.html",
    "href": "hypothesis-test-pandas-apply/index.html",
    "title": "Testing Pandas transformations with Hypothesis",
    "section": "",
    "text": "For example I’ve got some code where I’ve got a salary, but I don’t know whether the rate is hourly, daily or annual. I want to infer it from the code from some rules and return the number of hours it refers to. I can compare a version that works on Pandas series series_infer_salary_period_hours with one that works on individual salaries infer_salary_period_hours as follows:\nfrom hypothesis import given\nfrom hypothesis.strategies import floats\nfrom hypothesis.extra.pandas import series\nfrom pandas.testing import assert_series_equal\n\n@given(series(elements=(floats(0, 500_000))))\ndef test_infer_salary_period_hours_apply(s):\n    assert_series_equal(series_infer_salary_period_hours(s),\n                        s.apply(infer_salary_period_hours).astype('Int64'))\nThings to note are that we restrict the elements to a reasonable range, and have to be careful with the types.\nWe can also check that it works the same on a one-element series. In this case it’s easy to check special edge cases at the boundaries using the @example decorator.\nfrom hypothesis import example, given\nfrom hypothesis.strategies import floats\nimport pandas as pd\n\n@given(floats(0, 500_000))\n@example(15)\n@example(100)\n@example(300)\n@example(1000)\n@example(20_000)\ndef test_infer_salary_period_hours_element(s):\n    s_series = pd.Series([s])\n    series_ans = series_infer_salary_period_hours(s_series).iloc[0]\n    ans = infer_salary_period_hours(s)\n    assert ans == series_ans or (ans is None and pd.isna(series_ans))\nNote that we have to be a bit careful about how we check None which is converted to nan by Pandas, which is not equal to any other nan.\nIn general it can be useful to check a Numpy or Pandas row level function against a scalar function written in vanilla Python.\nHere’s a full extract of this example:\nfrom hypothesis import example, given\nfrom hypothesis.strategies import floats\nfrom hypothesis.extra.pandas import series\nfrom typing import Optional\nimport pandas as pd\nfrom pandas.testing import assert_series_equal\n\ndef infer_salary_period_hours(salary: float) -> Optional[int]:\n    \"\"\"Infer salary period from a salary.\n\n    Returns None if can't infer a period.\n    \"\"\"\n    if 15 <= salary <= 100:\n        # Likely hourly rate\n        return 1\n    elif 300 <= salary <= 1000:\n        # Likely daily rate\n        return 40\n    elif salary >= 20_000:\n        # Likely annual\n        return 2_000\n\ndef series_infer_salary_period_hours(s: pd.Series) -> pd.Series:\n    ans = pd.Series(None, s.index, dtype='Int64')\n    ans[s.between(15, 100)] = 1\n    ans[s.between(300, 1000)] = 40\n    ans[s >= 20_000] = 2_000\n    return ans\n\n@given(series(elements=(floats(0, 500_000))))\ndef test_infer_salary_period_hours_apply(s):\n    assert_series_equal(series_infer_salary_period_hours(s),\n                        s.apply(infer_salary_period_hours).astype('Int64'))\n\n@given(floats(0, 500_000))\n@example(15)\n@example(100)\n@example(300)\n@example(1000)\n@example(20_000)\ndef test_infer_salary_period_hours_element(s):\n    s_series = pd.Series([s])\n    series_ans = series_infer_salary_period_hours(s_series).iloc[0]\n    ans = infer_salary_period_hours(s)\n    assert ans == series_ans or (ans is None and pd.isna(series_ans))"
  },
  {
    "objectID": "structuring-python-analytics/index.html",
    "href": "structuring-python-analytics/index.html",
    "title": "Structuing Python Analytics Codebases",
    "section": "",
    "text": "Many analytics codebases consist of a pipeline of steps, doing things like getting data, extracting features, training models and evaluating results and diagnostics. The best way to structure the code isn’t obvious and if you’re having trouble importing files, getting module not found errors or are tinkering with PYTHONPATH it’s likely you haven’t got it right.\nA way I’ve seen many data analytics processing pipelines structured is with a series of numbered steps:\nThe good thing about this structure is that the steps involved are clear with their order. The bad thing about this is that you can’t easily import these because they start with a number (but it can be done with importlib), and they will only work when called from the right directory (otherwise paths will be messed up without some magic).\nInstead I recommend having each of the scripts in a directory with the name of the submodule (e.g. my_pipeline). Then there are a few options on how to orchestrate it:"
  },
  {
    "objectID": "structuring-python-analytics/index.html#making-the-module-executable",
    "href": "structuring-python-analytics/index.html#making-the-module-executable",
    "title": "Structuing Python Analytics Codebases",
    "section": "Making the module executable",
    "text": "Making the module executable\nIf you add a __main__.py (as well as the __init__.py) then you can execute the code in that file by running python -m my_pipeline. You can then import the downstream steps there and set up a CLI library such as Invoke, Click, or Typer.\n my_pipeline\n ├── __init__.py\n ├── __main__.py\n ├── fetch_data.py\n ├── extract_data.py\n ├── normalise_data.py\nThe nice thing about this is you don’t need to worry about paths and imports. It also lets you do more complex orchestration in the __main__.py file, and add other useful utilities that don’t fit into the pipeline metaphor.\nAnother advantage of this method is you can store paths to intermediate steps in __main__.py. If the pipeline caches intermediate results in some sort of storage then each step needs to know where the last step put it. This leads to duplication between the steps, or a large configuration file containing all the intermediate locations. Instead we can make the input and output locations arguments in the scripts, and set them all in __main__.py.\nNote that all your imports will need to be absolute to the package name, e.g import my_pipeline.fetch."
  },
  {
    "objectID": "structuring-python-analytics/index.html#creating-individual-step-scrtips",
    "href": "structuring-python-analytics/index.html#creating-individual-step-scrtips",
    "title": "Structuing Python Analytics Codebases",
    "section": "Creating individual step scrtips",
    "text": "Creating individual step scrtips\nIf you really like the idea of numbered scripts another way to handle it while having the right module structure is with symlinks. The structure we end up with is like this:\n my_pipeline\n ├── __init__.py\n ├── fetch_data.py\n ├── extract_data.py\n ├── normalise_data.py\n scripts\n ├── 01_fetch_data\n ├── 02_extract_data\n ├── 03_normalise_data\nThe module level at the top level and removing the numbers from the scripts fixes the import and path problems. Each of the scripts is a small shell script that invokes the corresponding step with python -m my_pipeline.{step}.\nFor example 01_fetch_data might look like:\n#!/bin/sh\n/usr/bin/env python -m my_pipeline.fetch_data\nThis script is then invoked using ./scripts/01_fetch_data.\nYou can invoke any of the individual scripts by passing the path with -m, e.g. python -m my_pipeline.fetch_data. We then put all the scripts to run in a scripts subfolder with the numbers like this:\nThe drawback with this method is you need a way to deal with the intermediate state."
  },
  {
    "objectID": "structuring-python-analytics/index.html#creating-a-makefile-or-shell-script",
    "href": "structuring-python-analytics/index.html#creating-a-makefile-or-shell-script",
    "title": "Structuing Python Analytics Codebases",
    "section": "Creating a Makefile or shell script",
    "text": "Creating a Makefile or shell script\nAn alternative to using the __main__.py for orchestration is creating a Makefile or shell script. The benefit of that is it can include functionality like installing a virtual environment or pulling a Docker image to run the Python scripts in (whereas the Python scripts won’t be able to run without the environment). The downside is these languages are a bit more brittle and less flexible than Python, and take more work to be nice to use than something like Typer.\n my_pipeline\n ├── __init__.py\n ├── fetch_data.py\n ├── extract_data.py\n ├── normalise_data.py\n Makefile\nIn this case I’d recommend making the input and output directories command line arguments that can be passed by the Makefile or shell script. It can then run, for example the extract_data, using\n/usr/bin/env python -m my_pipeline.extract_data ./data/01_raw ./data/02_primary\nIt’s critical you have a good command line interface with help (e.g. like this for Make) and good parsing. A Makefile has the advantage that it can see if the intermediate dependencies have been built and automatically resolve what needs to be done. However it requires some effort to get the sources and targets right so it works smoothly, and some targets might need to be generated for steps without output (e.g. setting up the environment)."
  },
  {
    "objectID": "evaluating-hn-book/index.html",
    "href": "evaluating-hn-book/index.html",
    "title": "Evaluating Book Retrieval from Hacker News",
    "section": "",
    "text": "However annotating a random dataset for evaluating finding book titles in Hacker News posts would be a terrible experience. I would be surprised if one in one thousand Hacker News posts contained a reference to a book. So we’d need to annotate heaps of examples, almost all of them negative, to get a few examples for evaluation. We need quite a few positive examples to get reliable estimates of accuracy, since we need to find the entity, extract it and link it to a record which could be much easier for some kinds of mentions than others.\nWe actually don’t want to measure this kind of accuracy for this use case. To be useful our system has to have high precision; if we say a book is mentioned in a comment we really want that book to have been mentioned. This enables us to say these books are really what are being talked about, allows meaningful link backs to comments, and gives trust in the system. Recall is somewhat important; we don’t want to be too biased towards certain kinds of books, and we want to get as many books as we can to give better insight. But at the end of the day if we miss some examples it’s fine.\nIn this case I’m thinking of implementing a two-pronged strategy for evaluation; one for precision and another for recall. We start with a separate unlabelled test split on the root post of the thread; for example randomly splitting these by id. For precision whenever we evaluate a model we pick completely at random subset of positive predictions and label the unlabelled ones; this gives a reasonable estimate of precision. Over time this set also gives a rough estimate of recall as we get more labelled likely positive predictions. We can supplement this with a random sample of likely positives using heuristics like links to Amazon or top level comments in Hacker News Book Recommendation threads. These will give a better measure of recall, especially if we don’t make this information available to the model (e.g. removing Amazon links, and not giving the parent information). If we want to label entities this can be sped up by correcting a pretrained model (like an Work of Art NER model or a Question Answering Model), rather than annotating from scratch.\nThis may give a completely fair comparison between models, since I’m building up the evaluation set over time and using different evaluations. But I’m not trying to micro-optimise a model; I’m trying get to a useful result with a minimum of human time. In this case getting fast feedback on what makes precision increase and roughly how recall is increasing is more valuable than annotating large quantities of data to get more precise estimates. There’s a time cost to evaluating more examples to increase the precision of evaluation, an information gain. It’s only worth investing in that when it’s going to change a decision on what approach to use. At the start of a project when there’s a lot of low-hanging fruit a rough evaluation is often good enough."
  },
  {
    "objectID": "blogdown/index.html",
    "href": "blogdown/index.html",
    "title": "From Hugo to R Blogdown",
    "section": "",
    "text": "R blogdown gives a really easy way to post blogs containing evaluated R (and Python!) code chunks and plots. This is really handy and can drop right into an existing Hugo blog with little modification. While there are some risks about the growing dependencies on building the blog, blogdown is well design to mitigate the issues and is very easy to set up.\nAbout 10 months ago I migrated my blog from Hakyll to Hugo (after already migrating once from Wordpress). There were three reasons, I wasn’t using Haskell anymore, I wanted a prettier website through Hugo themes and I wanted to use R blogdown to write posts. I’ve finally migrated to using Blogdown (and am writing this post in it) and it was much easier than I thought.\nAfter reading a tutorial of using blogdown with an existing Hugo site I thought it sounded too easy. But all I needed to do was install the package in R:\nAnd then I could immediately change building my website from hugo to:\nI saw a really well written warning:\nand simply adding that to the top of my config.toml fixed the warning.\nI could locally generate a preview on port 4321 with:"
  },
  {
    "objectID": "blogdown/index.html#examples",
    "href": "blogdown/index.html#examples",
    "title": "From Hugo to R Blogdown",
    "section": "Examples",
    "text": "Examples\nUsing R markdown I can show R snippets with their results:\n\nsqrt(2)\n\n[1] 1.414214\n\n\nEven better I can generate whole plots:\n\nplot(cars, pch = 18)\n\n\n\n\nOr even embed HTML Widgets:\n\nlibrary(leaflet)\n\nm <- leaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=174.768, lat=-36.852, popup=\"The birthplace of R\")\nm\n\n\n\n\n\nIn fact RMarkdown supports many difference language engines including Python:\n\nx = 'hello, python world!'\nprint(x.split(' '))\n\n['hello,', 'python', 'world!']\n\n\nI can even include Python plots:\n\nlibrary(reticulate)\nmatplotlib <- import(\"matplotlib\")\nmatplotlib$use(\"Agg\", force = TRUE)\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf=pd.DataFrame([[1, 2], [3, 4], [4, 3], [2, 3]])\nfig = plt.figure(figsize=(14,8))\nfor i in df.columns:\n    ax=plt.subplot(2,1,i+1)\n    df[[i]].plot(ax=ax)\n    print(i)\n\nplt.show()"
  },
  {
    "objectID": "stein-paradox/index.html",
    "href": "stein-paradox/index.html",
    "title": "A Reading Guide to Stein’s Paradox",
    "section": "",
    "text": "\\[\\hat\\theta^{JS}(X) = \\left(1 - \\frac{p-2}{\\lVert X\\rVert^2}\\right)X\\]\nA lot of the details here can be weakened substantially.\nThis article will give a guide on how to understand this phenomenon a bit better.\n\nWhat is Stein’s Paradox?\nThe best introductory resource is this Statslab Cambridge article by Richard Samsworth, which gives a clear explanation and a very simple proof. If the notation is a bit hard to follow I recommend the book All of Statistics which covers Decision Theory in Chapter 10 (and touches on the James-Stein Estimator).\nOne thing to note from this article is the improvement in risk over the Maximum Likelihood Estimator \\(X\\) is \\((p-2){\\mathbb E}\\left(\\frac{1}{\\lVert X \\rVert^2}\\right)\\). So the closer the points are to the origin the more the improvement (although there is always some improvement). And since the choice of origin is arbitrary (through a change in coordinates) having a good guess of where to shrink the estimates to will give a much better result like in the baseball example.\n\n\nWhy is it important?\nBradley Efron has done a lot of writing connecting it to empirical Bayes methods, showing it as a striking example of how on large datasets blending Bayesian methods with frequentist estimates can lead to striking solutions. The 1977 Scientific American Article Stein’s Paradox in Statistics by Efron and Morris gives a good flavour of what it means an why it’s important. They state a slightly different estimate (where \\(X_i \\sim N(\\theta_i, \\sigma)\\)):\n\\[\\hat\\theta^{JS} = \\bar X + \\left(1 - \\frac{(p-3) \\sigma^2}{\\lVert X - \\bar X \\rVert^2} \\right) (X-\\bar X)\\]\nhere instead of picking the origin they’re estimating it from the data as the grand mean \\(\\bar X = \\frac{\\sum_{i=1}^{p} X_i}{p}\\), which will give a better than random risk but at the cost of 1 degree of freedom (the p-3 in the numerator instead of p-2). The other thing to note is the larger the standard deviation the more we shrink the estimate (which makes sense since we are less certain about it). I suspect if they had different standard deviations you would shrink more in directions with larger standard deviation.\nTo understand this connection Chapter 1 of Efron’s Large-Scale Inference gives a very good introduction. It walks through how starting with the model \\(\\theta \\sim N(0, A)\\) and \\(X \\vert \\theta \\sim N(\\theta, 1)\\) the James-Stein estimator can be recovered, and then how it can be extended to estimate the mean or standard deviation. It also explains limited translation estimators where we shrink less, which gives a higher risk but less biased estimator. A similar (but briefer) explanation is in Chapter 7 of Computer Age of Statistical Inference which also covers the connection with ridge regression. A great more general tutorial is Casella’s An Introduction to Empirical Bayes Data Analysis.\n\n\nBut why does it work?\nThe connection to Empirical Bayesian methods gives useful applications, but it doesn’t indicate why it works. The best heuristic explanation I’ve seen is in A Geometrical Explanation of Stein Shrinkage by Brown and Zhao (2012), which shows how shrinking allows reducing the variance in the other dimensions (Joe Antognini has a good web article summarising this). Naftali Harris has a great visualisation of the shrinkage, and the argument reminds me of a how volume increases quickly with dimension.\n\n\nBut really, why does it work?\nThe paper Admissable Estimators, Recurrent Diffusions and Insoluble Boundary Value Problems by Brown (1971) connects the admissibility of estimators to recurrence of Brownian motion. I haven’t dug deep enough into the paper to understand it but it sounds mathematically deep and gives an idea as to why 3 is the critical dimension.\n\n\nHow did it all start?\nThe original paper showing inadmissibility is Inadmissibility of the usual estimator for the mean of a multivariate normal distribution by Stein (1956). This was followed with the explicit estimator in Estimation with Quadratic Loss by James and Stein (1961). The papers are certainly readable, but I found the earlier papers got to the point more succinctly. There was a lot of follow up papers at the time on improving the estimate, connecting it with Bayesian estimators and the like but they don’t strike me as deeply.\n\n\nWhere next?\nStein’s result really is still surprising to me; the best estimator in high dimensions are biased estimators no matter where you bias it, but it seems to have to do with removing some of the variance inherent in estimating multiple points in higher dimensional spaces. However for practical applications the biggest difference is when they are (unsurprisingly) biased towards their true values, which brings us back to things like hierarchical models and Empirical Bayesian methods.\nHowever this seems like a far reaching result that should change the practice of analysts; in a sense it’s another kind of regression to the (grand) mean. Whenever I’m calculating averages for lots of groups to maximise predictive accuracy I should shrink the estimates, and the shrinkage should increase with the variance. I think this gives a better solution to Evan Miller’s How not to sort by average rating; instead of using (arbitrary) confidence intervals we shrink towards the grand mean with more shrinkage the more ratings an item has. However I wouldn’t use the James-Stein estimator directly, but instead use an empirical Bayes method. It seems strange it’s not more widely advocated, especially as a step towards machine learning based methods.\nI find it interesting that even though the theorem is for normally distributed variables, the most common example are binomials from baseball (after a suitable transformation). I wonder what an empirical Bayes method would look like, and whether it would have a lower risk than the proportion of true results."
  },
  {
    "objectID": "pdb/index.html",
    "href": "pdb/index.html",
    "title": "Getting Started Debugging with pdb",
    "section": "",
    "text": "I’ll cover some basic techniques I use, but check out the manual for all the detail.\n\nGetting into the debugger\nThe easiest way to get into the debugger is to invoke your python script with pdb. So instead of python mymodule.py you run python -m pdb mymodule.py. You then will drop straight into a (pdb) prompt. If the script is going to raise an error then just type c (for continue), and you’ll drop back into the debugger as soon as the issue arises.\nIf you want to break somewhere an error isn’t raised you can set a breakpoint where you want to inspect. You can do this from within pdb with b (for break), for example to break at line 92 just type b 92 or to break when myfunction is called type b myfunction. Once you’ve set your breakpoints you can continue until you hit one.\nAnother way to set a breakpoint is by editing the source file and adding breakpoint() (for Python versions before 3.7 you will need to use import pdb; pdb.set_trace(). Now when the script is invoked normally (e.g python mymodeule.py) it will start a pdb debugger at that line.\nYou can even use pdb in Jupyter notebooks. After an error write %debug in a new cell and you’ll have a debugger. Just be careful because you can’t access stop the debugger from Jupyter, you’ll need to exit it (and if you delete the cell you may get into an unrecoverable state).\n\n\nUsing the debugger\nOnce you’ve hit the breakpoint you’ll want to have a look around. The first thing I generally do is run:\nlocals().keys()\nThis will show the variables available in your local environment. I use .keys() because sometimes a variable will be a giant list that pdb spends screens printing out. If you just want the arguments of the function that you’re in type a (for args).\nYou can then run normal Python commands to inspect what is happening, but there are a few caveats. You can’t type multiline commands (sometimes you may want to use a ; as a statement separator in a function definition). Many letters are used by the debugger (like n, c, etc), so if you need to print a variable of this name you can use p to print it. Also lambdas just don’t work in pdb. If you need some of these features you can go into a python interpreter by typing interact.\n\n\nNavigating with the debugger\nThe debugger lets you know the location of the file you are in and the line. Often the error will be deep within some called function instead of the code you’ve been working on. The debugger let’s you navigate around to see what is happening.\nThe most useful commands are u (up) for going higher in the stacktrace. I will often start by running u until I’m in some code I am familiar with. You can go back down again if you’re not sure what’s happening with down.\nIf you ever forget where you are you can type w (for where), and it will tell you the file, line and function you are in. If you want more context for the file you can type l to show the source code around the point. If you need to see more you can type l <start_line>,<end_line> to get the source code for those lines.\nYou can step down into the current function being called, or you move to the next line. When you’re finished you can continue. After an error it will restart the program (reloading the source), so you can see the effect of any changes you’ve made to the source code.\nNext time you need to handle an error in your Python code run python -m pdb or set a breakpoint() and open up the pdb manual. It’s a great way to find out what’s going on by uncovering your wrong assumptions through interactive querying."
  },
  {
    "objectID": "job-ad-title-salary/index.html",
    "href": "job-ad-title-salary/index.html",
    "title": "Understaning Job Ad Titles with Salary",
    "section": "",
    "text": "Using the jobs from Adzuna Job Salary Predictions Kaggle Competition I’ve found common job titles and can use the advertised salary to help understand them. Note that since the data is from the UK from several years ago a lot of the details aren’t really applicable, but the techniques are.\n\nChefs and cooks\nI don’t know much about different types of chefs but looking at the distributions of salary I can immediately see that a Commis Chef is lower than a Chef de Partie, which is below a Sous Chef, which is below a Head Chef. Reading about the roles on Wikipedia confirms this makes sense.\nNotice that the general chef role has quite a broad salary distribution, but it’s generally lines up with roles between a Commis Chef and Chef de Partie; not for a Sous Chef and above.\n\n\n\nAdvertised Salary distribution of types of chefs and cooks\n\n\n\n\nHuman Resources\nHuman resources has its own terminology for seniority. A quick glance at the salary chart shows an assistant or administrator are about the same level, the bottom rung. Then the next level is an officer or advisor. Finally the top level is a manager or business partner.\nThere may be differences between an advisor and officer that can’t be seen from advertised salary alone, but it’s easy to see the broad pecking order from the salaries.\n\n\n\nAdvertised Salary distribution of HR professionals\n\n\n\n\nSoftware Developers\nSoftware developers and engineers have a broad range of salaries that doesn’t depend much on their specialisation. To understand the different specialities would require extracting more information from the job ads.\nOne interesting thing is that web developers tend to be at the bottom of salary and backend and app developers are at the top of salary. This is an evolving space and I imagine has changed a lot even in the types of role titles since 2013.\n\n\n\nAdvertised Salary distribution of developers\n\n\n\n\nSales Professionals\nTitles containing the word “sales” really cover a broad range of jobs. However you can see that “internal sales executive” has a lower value than a general “sales executive” and move into more senior roles like “area sales manager”, “sales manager” or “regional sales manager” up to the “sales director”.\nAnother interesting observational is technical sales roles like “sales engineer” and “medical sales executive” as well as “new business sales executive” comman a much higher salary than generalist sales roles.\n\n\n\nAdvertised Salary distribution of sales professionals\n\n\n\n\nCleaner roles\nLooking into the specific role of cleaner they have a very tightly defined salary range around £10,000-13,000. However there are some strange outliers.\n\n\n\nAdvertised Salary distribution of cleaners\n\n\nIt turns out these are anomalies in the underlying dataset. The job with an apparent salary of £30,000 actually has a salary of “6.25 per hour 25 hours per week”. They have parsed this as “6.25-25 per hour”, and averaged and annualised it to 30,000 when it is actually 12,500.\nThe very low salaries are all for part time work, generally 15 or 20 hours per week, which is why they are much lower. If they were scaled up to full time they would be in the same general range.\n\n\nConclusions\nHaving extracted role titles made this easy to compare over a wide range of roles. The salary can then say something about how roles within an industry relate. One issue is that the developer roles don’t include similar software engineering roles; sometimes similar roles are written very differently. And some roles have a very broad salary where it’s uninformative from the title alone.\nIt’s also very useful for finding outliers in the data, like in the cleaner example. There’s another example of a “sales manager” role with double the typical salary because the ad is in Australia and the Australian dollar is worth about half a British Pound. This shows it’s actually pretty informative and would be a useful feature if we were actually trying to predict salary like in the competition.\nFor further details and code see the Jupyter Notebook."
  },
  {
    "objectID": "python-futures-exception/index.html",
    "href": "python-futures-exception/index.html",
    "title": "Raising Exceptions in Python Futures",
    "section": "",
    "text": "While you can perform concurrent downloads with multiprocessing it means starting up multiple processes and sending data between them as pickles. One problem with this is that you can’t pickle some kinds of objects and often have to refactor your code to use multiprocessing. It’s also just unsatisfying having to spin up a bunch of processes when you’re not really utilising them.\nAn alternative method is to use futures to perform the tasks asynchronously. Since most of the time is spent waiting for file I/O this has a similar speed up:\nfrom concurrent import futures\nwith futures.ThreadPoolExecutor() as executor:\n  results = futures.wait([executor.submit(download, filename) for filename in filenames])\nHowever if there’s some problem with one of the downloads we’ll never actually see the error. This is because asynchronous programming is hard; we may not want to fail if one file fails. This gives us the choice of how to resolve it, but it means we have to be diligent to resolve it.\nIf we just want to raise the exception we can do it by evaluating the results:\nfrom concurrent import futures\nwith futures.ThreadPoolExecutor() as executor:\n  results = futures.wait([executor.submit(download, filename) for filename in filenames])\nfor result in results.done:\n  result.result()\nThis works because when we evaluate .result() it brings it out of the async world back into normal programming and raises the exception. However this isn’t at all obvious, so we can be a little more explicit:\nfrom concurrent import futures\nwith futures.ThreadPoolExecutor() as executor:\n  results = futures.wait([download(filename) for filename in filenames])\nfor result in results.done:\n  if result.exception() is not None:\n    raise result.exception()\nThat works but it’s a bit painful to remember every time. I wonder if there are other asynchronous frameworks that make harder to overlook errors. I’ve heard that Erlang/Elixir force you to specify error handling in every sent message; I’d be interested in how that works at scale."
  },
  {
    "objectID": "sir/index.html",
    "href": "sir/index.html",
    "title": "Modelling the Spread of Infectious Disease",
    "section": "",
    "text": "Understanding the spread of infectious disease is very important for policies around public health. Whether it’s the seasonal flu, HIV or a novel pandemic the health implications of infectious diseases can be huge. A change in decision can mean saving thousands of lives and relieving massive suffering and related economic productivity losses. The SIR model is a model that is simple, but captures the underlying dynamics of how quickly infectious diseases spread. They can be used to understand many phenomena we see with the spread of real diseases."
  },
  {
    "objectID": "sir/index.html#change-in-susceptibles",
    "href": "sir/index.html#change-in-susceptibles",
    "title": "Modelling the Spread of Infectious Disease",
    "section": "Change in susceptibles",
    "text": "Change in susceptibles\nThe number of susceptible people is decreasing. This is because all a susceptible person can do is become infectious. When there are no infectious people left then the number of susceptible people doesn’t change; which makes sense since there’s no way they can get infected (obviously the initial source of infection is beyond this model!).\nAlso note the more susceptible people there are, the faster they decrease. If the number of infectious people is constant then susceptibles drop off exponentially like \\(e^{-bi}\\). If the number of infectious people is growing then it will drop off even faster. This is why infectious diseases are so scary; they can spread exponentially through a population."
  },
  {
    "objectID": "sir/index.html#change-in-infectious",
    "href": "sir/index.html#change-in-infectious",
    "title": "Modelling the Spread of Infectious Disease",
    "section": "Change in infectious",
    "text": "Change in infectious\nThe rate at which the number of infectious people changes is really crucial; this is how quickly people are getting sick. It’s useful to rewrite the equation \\(\\Delta i = bis - \\frac{i}{t}\\), as \\(\\Delta i = (bts - 1) \\frac{i}{t}\\). If the product b ✕ t ✕ s is greater than 1 then the number of infectious people is increasing. If it’s less than 1 then the number of infectious people is decreasing.\nThe product b ✕ t is called the basic reproduction number \\(R_0\\). Recall that b is the number of people an infectious person will come into infectious contact with per day. And t is the average number of days a person will be infectious. So their product is the average number of people an infectious person will infect.\nWhen \\(R_0 s \\ll 1\\) then \\(\\Delta i \\approx - \\frac{i}{t}\\) and the number of infectious people drops off exponentially like \\(e^{-1/t}\\). In this case the infection dies out, and approximately \\(s e^{-i R_0}\\) more of the population becomes infected.\nWhen \\(R_0 s \\gg 1\\) then the number of infectious people grows exponentially like \\(e^{R_0 s}\\). Since s is decreasing this rate of increase will slow down.\nThe critical point is when \\(R_0 s = 1\\), or equivalently \\(R_0 = \\frac{1}{s}\\). Here the rate of change of infectious is zero; we’ve hit the maximum of infectious cases.\nThe maximum number of infectious cases is very relevant for hospitals. Some fraction of infectious individuals will need to be hospitalised or they could have severe repercussions or even die. So to minimise harm hospitals should have enough staff and beds to cover this fraction of the infectious individuals at peak.\nSo if initially \\(R_0 s \\ll 1\\) then the infection will die out quickly. However if \\(R_0 s > 1\\) then the infection will grow exponentially (and susceptibles will decrease exponentially) until \\(R_0 s = 1\\) and then it will start to die out. In the second case we have an epidemic and a large fraction of the population will have contracted the disease."
  },
  {
    "objectID": "sir/index.html#basic-reproduction-number",
    "href": "sir/index.html#basic-reproduction-number",
    "title": "Modelling the Spread of Infectious Disease",
    "section": "Basic Reproduction Number",
    "text": "Basic Reproduction Number\nIt’s clear that the dynamics are really strongly governed by the product \\(R_0 s\\). For a new kind of disease, s will be close to 1; almost everyone is susceptible. Then all you can do is to reduce the basic reproduction number.\nReducing the basic reproduction number is all about reducing the number of infectious contacts. The simplest method is reducing the number of people contacted; quarantining infected individuals so that they can’t come into contact with susceptibles. Another tactic would be to try to isolate subpopulations (each of which has an independent SIR model), to try to contain spread between groups. Generally methods to decrease the density and variety of contacts within a population will slow the spread.\nAnother method is to reduce the likelihood of an infection on contact; for example using physical barriers such as masks and gloves for airborne diseases and condoms for sexually transmitted diseases. Many diseases can spread indirectly through contact with a surface, so regular cleaning is important too. Similarly any treatment to reduce the infectious period, or reduce the amount of pathogens excreted would help reduce this number.\nIt’s notable that it’s the product \\(R_0 s\\) that matters. If the basic reproduction number is 4, but much less than a quarter of the population is susceptible then the disease won’t spread. This is the phenomenon of Herd immunity. Essentially the removed individuals act as a buffer that extinguishes the disease before it can move too far. This is why immunisation can be so effective; if you can just get most of the community you can eradicate the disease."
  },
  {
    "objectID": "listening/index.html",
    "href": "listening/index.html",
    "title": "Listening",
    "section": "",
    "text": "There’s lots of reasons to spend more time listening than talking. When you get a greater diversity of ideas you generally get to a better solution, and often the quieter people in the room have a valuable perspective. When people come up with ideas on their own they feel more ownership of the problem and are more willing to work on it. When you understand all the stakeholders concerns you are aware what is needed in a solution that will satisfy all of them, rather than running into them when the project is delivered.\nComing into a meeting with the mindset of trying to understand people’s perspective and getting their buy-in rather than trying to explain my view of the problem and proposing a solution leads to much better outcomes. It’s like in sales; you want to understand the customer’s problems and concerns before you try to push a product, because then you can fit a product to the customer’s needs (or even learn that the customer doesn’t need your product, and you’re better off looking for new prospects). This is true even for people who you’re delegating to; if you listen you understand whether their view of the problem and key concerns is aligned to yours. Coming in with this mindset and understanding what you’re trying to achieve, there are two times when it makes sense to talk; framing the conversation and educating the audience.\nFraming the conversation is about making sure the conversation stays on topic and progresses towards the goals. You can do this by setting a clear agenda, stopping distracting discussions to be “taken off-line” and discussed outside the meeting, and ending the meeting with clear next actions for responsible people. Knowing what to put in an agenda and knowing when to stop a discussion requires a lot of understanding of your audience and sometimes it can help in unfamiliar situations to get them to contribute to these. Another aspect of framing the conversation is calling out people to join the conversation. You might notice one stakeholder who seems uncomfortable or distracted with the conversation, or maybe someone with a lot of expertise whose opinion you value; it can be helpful to call on this person directly to give their expert opinion on the subject.\nEducating the audience is about using your expertise to drive the conversation in a productive direction. There may be times when the conversation goes down a path that won’t be fruitful, or you know that a particular issue is addressable. This should only be used where necessary to help keep the conversation moving towards a workable solution. In particular you should spend more time on getting on the same page about what a good solution looks like, than selling any particular solution.\nI personally find this hard but putting my ego aside and listening more I often get a lot more out of conversations than when I talk a lot. Trying to focus on being a facilitator, making sure the conversation is moving in the right direction and that key concerns are being raised, helps me do this and leads to more productive meetings. If someone else is on the same page as you just let them talk it through and get an understanding in their own words. They’ll be more agreeable if you use their words in said of your own, which increases the chance of a successful outcome."
  },
  {
    "objectID": "interpretable-parameterisations/index.html",
    "href": "interpretable-parameterisations/index.html",
    "title": "Interpretable Parameterisations",
    "section": "",
    "text": "A simple example of this is in Regression and Other Stories by Gelman, Hill and Vehtari. With linear models the intercept term only makes sense when it makes sense to have all the predictors zero, otherwise it’s hard to interpret. A more useful parameterisation is to replace the intercept with the value at the average value of the other predictors.\nFor example suppose you’re modelling the weight of adult humans in Australia. The first model you build is a constant model, which gives the intercept as the average value weight = 80kg. Then based on Body Mass Index you add height^2 as a predictor and get weight = 40kg + (23kg/m^2) height^2. The intercept went down a lot when you added the extra term, what happened? The intercept represents the weight of a 0m high adult, which is meaningless - it’s out of the valid range of the model. However if we reparameterise it relative to average height weight = 80kg + (23kg/m^2) * (height - 1.7m)^2, it’s much easier to see not much has changed with the constant term. As we add more predictors we can make the model more interpretable by framing it in these terms. For example if we add gender, we could have the average difference weight between the genders represented explicitly as (86kg - 72kg) * is_male, rather than the more opaque 6kg * is_male. Even better would be to present the models separately for each gender (even if they share parameters), which makes the differences by gender directly stand out.\nThis doesn’t just apply to linear regression, but to more complex models with a handful of parameters too. In Regression and Other Stories they talk about the divide-by-4 rule for logistic regression. In the model \\(logit^{-1}(\\alpha + \\beta x)\\) a unit difference in x makes at most a difference in \\(\\beta / 4\\) in the probability (at a probability of 50%, and smaller near the extremes). When communicating a logistic regression to non-statisticians, especially one where most predictions are in the middle, the coefficient \\(\\beta / 4\\) is a much better parameter to explain than \\(\\beta\\) itself.\nAnother useful trick in generalised linear models is to divide the coefficients by the standard deviation, especially when the predictors are on different scale. If you’ve got both age and income as predictors, even if they have equal weight, in natural units the income coefficient will be much lower because it has a huge variation in tens of thousands of dollars, but age only varies by scores of years. To the untrained eye it makes income seem much less important, and the relative importance of factors is harder to reason about. Dividing by the standard deviation puts them on the same scale, in terms of the impact they can have on the outcome variable (assuming they are similarly distributed), which makes it much easier to compare models.\nA more complex example is the Generalised Logistic Function, which is an S-shaped curve that has the useful property of quickly switching between two extreme values. One way to write this is \\(Y(t) = \\frac{A \\left(C + e^{-Bt}\\right)^{1/\\nu} + J}{\\left(C + e^{-Bt}\\right)^{1/\\nu}}\\), but it’s not clear what all these things mean.\n\n\n\nExample Logistic Function\n\n\nWikipedia puts it in a much more interpretable form: \\(Y(t) = A + { K-A \\over (1 + Q e^{-Bt}) ^ {1 / \\nu}\\), where A is the lowest value it can reach and K is the highest value, and Q is the initial value at t=0. The other values; the growth rate B, the shape factor \\(\\nu\\) are harder to interpret. However you could further reparameterise these for a use case; maybe there’s two other points on the curve that mean something and we could use these instead of B and \\(\\nu\\). Or suppose this represents the number of people that would churn from a product at a given price, and when multiplying by the price we get the profit. Then the price at which profit is optimised, and the optimal profit (perhaps relative to the current profit) are great, meaningful parameters.\nIn these more general cases you may need to calculate the parameterisation functions numerically and the parameterisations may break down outside of some region; what happens when there’s no optimal profit? However it’s worth the effort if it helps communicate how the model works, helps understand changes to the model, and builds trust and adoption."
  },
  {
    "objectID": "sicp-1_2/index.html",
    "href": "sicp-1_2/index.html",
    "title": "SICP Exercise 1.2",
    "section": "",
    "text": "Exercise 1.2.\nTranslate the following expression into prefix form\n\\[\\frac{5 + 4 + (2 - (3 - (6 + \\frac{1}{5})))}{3(6-2)(2-7)}\\]\n\nSolution\nOne way to do this is to read it from the outside in and translate it into a tree (for example the first thing we extract is the division).\n\n\n\n\ngraph BT;\n\nDIV[`/`] --> ANS\nTOP[.] --> ANS[.]\nBOTTOM[.] --> ANS\n\nTOPSUM[+] --> TOP\nS1[5] --> TOP\nS2[4] --> TOP\nS3[.] --> TOP\n\nS3A[-] --> S3\nS3B[2] --> S3\nS3C[.] --> S3\n\nS3C1[+] --> S3C\nS3C2[.] --> S3C\nS3C3[.] --> S3C\n\nS3C2A[-] --> S3C2\nS3C2B[3] --> S3C2\nS3C2C[6] --> S3C2\n\nRATIODIV[`/`] --> S3C3\nRATIONUM[4] --> S3C3\nRATIODEN[5] --> S3C3\n\n\n\nPROD[*] --> BOTTOM\nP1[3] --> BOTTOM\nP2[.] --> BOTTOM\nP3[.] --> BOTTOM\n\nP2a[-] --> P2\nP2b[6] --> P2\nP2c[2] --> P2\n\nP3a[-] --> P3\nP3b[2] -->  P3\nP3c[7] --> P3\n\n\n\n\n\n\n\n\n\n\nWe can then read this diagram from the top down to get prefix notation:\n(/ (+ 5 4 (- 2 (+ (- 3 6) (/ 4 5))))\n   (* 3 (- 6 2) (- 2 7)))"
  },
  {
    "objectID": "weight-book-box/index.html",
    "href": "weight-book-box/index.html",
    "title": "How Much Does a Box of Books Weigh?",
    "section": "",
    "text": "How heavy is a small moving-box filled with books?\n\n\nGuesstimating weight\nI’ve moved small boxes of books a few times, it’s light enough for me to carry. It’s much heavier than a couple of 2kg bag of onions, but probably more similar to a 20kg bag of pool salt. I’d guess it’s in the range 10-20kg, so I’d guess around 15kg.\nLet’s think about what else we can relate it to as a check. The weight is the volume / density so we will try to estimate each of them.\n\n\nSize of a box\nA book box is a little wider than I am, since I can hold it with my arms straight. I would guess it’s a little less wide than two ruler lengths; let’s say it’s 50cm with a square base. They’re a little less high than they are tall; perhaps around 30cm. It’s convenient to estimate the lengths in decimeters (dm) which is 10cm, or equivalently 0.1m. That means the volume would be 5 dm × 5 dm × 3 dm = 75 dm³ = 75L. I remember seeing something like 60L on a moving box so this seems reasonable.\nA quick check online shows I overestimated the width (it’s more like 30-40cm), but underestimated the height. It looks like the actual volume is closer to 50L; but I’m in the right ballpark.\n\n\nDensity of paper\nI’m more familiar with printer paper than book paper, so I’ll use that as a place to start. Printer paper is typically around 100gsm, which means grams per square meter.\nA ream of 500 pages is around 5cm thick; so a paper is about 0.01cm thick. So the density of paper is 100g per (10⁴ cm² × 0.01 cm) which is 1g per cm³, or equivalently 1kg per L.\nAs a sanity check our 500 page ream is 5cm thick by 20cm wide by 30cm high. So it has a volume of 3000 cm³, and so a weight of around 3kg. This sounds about reasonable, and a quick check online confirms this.\n\n\nPutting it together\nIf a 50L box was filled to the brim with printer paper it would weigh 50kg.\nIn practice book paper is a bit lighter than printer paper, but a hardcover will add significant weight. The density should be in the right ballpark.\nOn the other hand we’re not going to be able to fill the box completely. It’s probably fair to say you could get it about 60% full, maybe a bit more, because of the gaps.\nSo the final estimate is: weight of box = volume of box × proportion of box with books × density of books. This is 50L × 0.6 × 1kg/L which is 30kg. This is actually a bit more than my initial guesstimate; which I would now revise upward to 20-35kg."
  },
  {
    "objectID": "finite-groups/index.html",
    "href": "finite-groups/index.html",
    "title": "Classifying Finite Groups",
    "section": "",
    "text": "One way to tackle this is to try to decompose them. One way of doing this is a decomposition series of normal subgroups.\n\\[1 = H_0\\triangleleft H_1\\triangleleft \\cdots \\triangleleft H_n = G\\]\nBy the Jordan-Hölder theorem the induced simple quotient groups \\(H_j / H_{j-1}\\) are unique up to permutation of order. The Classification of Finite Simple Groups lists all the possible groups. These can all be seen as sorts of symmetries:\n\nCyclic groups of prime order are analogouus to prime numbers\nAlternating group of even permutations\nGroups of Lie Type, which can be seen as symmetries of Buildings\n27 Exceptional groups which correspond to special types of symmetries\n\nThis is just like how any positive integer can be decomposed into prime factors; in fact the abelian finite groups capture exactly this phenomenon. However in the general case it’s not always trivial to multiply two groups together, and there are some options. This is called the extension problem. It turns out this problem is really hard to solve in general.\nHowever we do have all the material we need to generate all groups. In particular with the universal embedding theorem says that all groups are subgroups of a wreath product.\nI don’t know of any concrete applications of these kinds of classification. That being said prime numbers (which are isomorphic to the cyclic groups) have applications in cryptography (such as RSA) because they are hard to solve and easy to modify.\nHowver overall it’s better to focus on the groups as they appear. In your application are you expecting some strange symmetry related to an unusual group."
  },
  {
    "objectID": "common-crawl-time-ranges/index.html",
    "href": "common-crawl-time-ranges/index.html",
    "title": "Common Crawl Time Ranges",
    "section": "",
    "text": "Methodology\nThe best way to do this would be to use the Athena Columnar Index to search the dates, but I didn’t have Athena set up and I’d have to be a little careful about the costs. As a proxy I used the CDX Server and for each index searched ’en.wikipedia.org/*’ and got the oldest and newest timestamps across the first and last page of results. This won’t exactly get the whole range, but because Wikipedia is frequently crawled this would be a reasonable proxy.\nHere are the results showing the earliest and latest days found, as well as the ISO Week Start (so e.g. 2021-43 starts on 2021-10-31). Most crawls span a couple of weeks or less.\n\n\n\n\n\n\n\n\n\n\nIndex Name\nCrawl Days\nEarliest Date\nLatest Date\nISO Week Start\n\n\n\n\nCC-MAIN-2021-43\n13\n2021-10-15\n2021-10-28\n2021-10-31\n\n\nCC-MAIN-2021-39\n13\n2021-09-16\n2021-09-29\n2021-10-03\n\n\nCC-MAIN-2021-31\n14\n2021-07-23\n2021-08-06\n2021-08-08\n\n\nCC-MAIN-2021-25\n13\n2021-06-12\n2021-06-25\n2021-06-27\n\n\nCC-MAIN-2021-21\n14\n2021-05-05\n2021-05-19\n2021-05-30\n\n\nCC-MAIN-2021-17\n13\n2021-04-10\n2021-04-23\n2021-05-02\n\n\nCC-MAIN-2021-10\n13\n2021-02-24\n2021-03-09\n2021-03-14\n\n\nCC-MAIN-2021-04\n13\n2021-01-15\n2021-01-28\n2021-01-31\n\n\nCC-MAIN-2020-50\n13\n2020-11-23\n2020-12-06\n2020-12-20\n\n\nCC-MAIN-2020-45\n13\n2020-10-19\n2020-11-01\n2020-11-15\n\n\nCC-MAIN-2020-40\n14\n2020-09-18\n2020-10-02\n2020-10-11\n\n\nCC-MAIN-2020-34\n12\n2020-08-03\n2020-08-15\n2020-08-30\n\n\nCC-MAIN-2020-29\n14\n2020-07-02\n2020-07-16\n2020-07-26\n\n\nCC-MAIN-2020-24\n14\n2020-05-24\n2020-06-07\n2020-06-21\n\n\nCC-MAIN-2020-16\n13\n2020-03-28\n2020-04-10\n2020-04-26\n\n\nCC-MAIN-2020-10\n13\n2020-02-16\n2020-02-29\n2020-03-15\n\n\nCC-MAIN-2020-05\n12\n2020-01-17\n2020-01-29\n2020-02-09\n\n\nCC-MAIN-2019-51\n11\n2019-12-05\n2019-12-16\n2019-12-29\n\n\nCC-MAIN-2019-47\n12\n2019-11-11\n2019-11-23\n2019-12-01\n\n\nCC-MAIN-2019-43\n11\n2019-10-13\n2019-10-24\n2019-11-03\n\n\nCC-MAIN-2019-39\n9\n2019-09-15\n2019-09-24\n2019-10-06\n\n\nCC-MAIN-2019-35\n9\n2019-08-17\n2019-08-26\n2019-09-08\n\n\nCC-MAIN-2019-30\n9\n2019-07-15\n2019-07-24\n2019-08-04\n\n\nCC-MAIN-2019-26\n12\n2019-06-15\n2019-06-27\n2019-07-07\n\n\nCC-MAIN-2019-22\n8\n2019-05-19\n2019-05-27\n2019-06-09\n\n\nCC-MAIN-2019-18\n8\n2019-04-18\n2019-04-26\n2019-05-12\n\n\nCC-MAIN-2019-13\n9\n2019-03-18\n2019-03-27\n2019-04-07\n\n\nCC-MAIN-2019-09\n9\n2019-02-15\n2019-02-24\n2019-03-10\n\n\nCC-MAIN-2019-04\n9\n2019-01-15\n2019-01-24\n2019-02-03\n\n\nCC-MAIN-2018-51\n10\n2018-12-09\n2018-12-19\n2018-12-23\n\n\nCC-MAIN-2018-47\n10\n2018-11-12\n2018-11-22\n2018-11-25\n\n\nCC-MAIN-2018-43\n9\n2018-10-15\n2018-10-24\n2018-10-28\n\n\nCC-MAIN-2018-39\n8\n2018-09-18\n2018-09-26\n2018-09-30\n\n\nCC-MAIN-2018-34\n8\n2018-08-14\n2018-08-22\n2018-08-26\n\n\nCC-MAIN-2018-30\n8\n2018-07-15\n2018-07-23\n2018-07-29\n\n\nCC-MAIN-2018-26\n8\n2018-06-17\n2018-06-25\n2018-07-01\n\n\nCC-MAIN-2018-22\n8\n2018-05-20\n2018-05-28\n2018-06-03\n\n\nCC-MAIN-2018-17\n8\n2018-04-19\n2018-04-27\n2018-04-29\n\n\nCC-MAIN-2018-13\n8\n2018-03-17\n2018-03-25\n2018-04-01\n\n\nCC-MAIN-2018-09\n9\n2018-02-17\n2018-02-26\n2018-03-04\n\n\nCC-MAIN-2018-05\n8\n2018-01-16\n2018-01-24\n2018-02-04\n\n\nCC-MAIN-2017-51\n9\n2017-12-10\n2017-12-19\n2017-12-24\n\n\nCC-MAIN-2017-47\n8\n2017-11-17\n2017-11-25\n2017-11-26\n\n\nCC-MAIN-2017-43\n8\n2017-10-16\n2017-10-24\n2017-10-29\n\n\nCC-MAIN-2017-39\n7\n2017-09-19\n2017-09-26\n2017-10-01\n\n\nCC-MAIN-2017-34\n8\n2017-08-16\n2017-08-24\n2017-08-27\n\n\nCC-MAIN-2017-30\n9\n2017-07-20\n2017-07-29\n2017-07-30\n\n\nCC-MAIN-2017-26\n7\n2017-06-22\n2017-06-29\n2017-07-02\n\n\nCC-MAIN-2017-22\n8\n2017-05-22\n2017-05-30\n2017-06-04\n\n\nCC-MAIN-2017-17\n8\n2017-04-23\n2017-05-01\n2017-04-30\n\n\nCC-MAIN-2017-13\n9\n2017-03-22\n2017-03-31\n2017-04-02\n\n\nCC-MAIN-2017-09\n10\n2017-02-19\n2017-03-01\n2017-03-05\n\n\nCC-MAIN-2017-04\n9\n2017-01-16\n2017-01-25\n2017-01-29\n\n\nCC-MAIN-2016-50\n9\n2016-12-02\n2016-12-11\n2016-12-18\n\n\nCC-MAIN-2016-44\n9\n2016-10-20\n2016-10-29\n2016-11-06\n\n\nCC-MAIN-2016-40\n8\n2016-09-24\n2016-10-02\n2016-10-09\n\n\nCC-MAIN-2016-36\n9\n2016-08-23\n2016-09-01\n2016-09-11\n\n\nCC-MAIN-2016-30\n8\n2016-07-23\n2016-07-31\n2016-07-31\n\n\nCC-MAIN-2016-26\n8\n2016-06-24\n2016-07-02\n2016-07-03\n\n\nCC-MAIN-2016-22\n8\n2016-05-24\n2016-06-01\n2016-06-05\n\n\nCC-MAIN-2016-18\n9\n2016-04-28\n2016-05-07\n2016-05-08\n\n\nCC-MAIN-2016-07\n10\n2016-02-05\n2016-02-15\n2016-02-21\n\n\nCC-MAIN-2015-48\n8\n2015-11-24\n2015-12-02\n2015-12-06\n\n\nCC-MAIN-2015-40\n10\n2015-10-04\n2015-10-14\n2015-10-11\n\n\nCC-MAIN-2015-35\n9\n2015-08-27\n2015-09-05\n2015-09-06\n\n\nCC-MAIN-2015-32\n8\n2015-07-28\n2015-08-05\n2015-08-16\n\n\nCC-MAIN-2015-27\n9\n2015-06-29\n2015-07-08\n2015-07-12\n\n\nCC-MAIN-2015-22\n13\n2015-05-22\n2015-06-04\n2015-06-07\n\n\nCC-MAIN-2015-18\n19\n2015-04-18\n2015-05-07\n2015-05-10\n\n\nCC-MAIN-2015-14\n7\n2015-03-26\n2015-04-02\n2015-04-12\n\n\nCC-MAIN-2015-11\n9\n2015-02-26\n2015-03-07\n2015-03-22\n\n\nCC-MAIN-2015-06\n8\n2015-01-25\n2015-02-02\n2015-02-15\n\n\nCC-MAIN-2014-52\n12\n2014-12-17\n2014-12-29\n2015-01-04\n\n\nCC-MAIN-2014-49\n9\n2014-11-20\n2014-11-29\n2014-12-14\n\n\nCC-MAIN-2014-42\n12\n2014-10-20\n2014-11-01\n2014-10-26\n\n\nCC-MAIN-2014-41\n17\n2014-09-15\n2014-10-02\n2014-10-19\n\n\nCC-MAIN-2014-35\n14\n2014-08-20\n2014-09-03\n2014-09-07\n\n\nCC-MAIN-2014-23\n24\n2014-07-09\n2014-08-02\n2014-06-15\n\n\nCC-MAIN-2014-15\n9\n2014-04-16\n2014-04-25\n2014-04-20\n\n\nCC-MAIN-2014-10\n10\n2014-03-07\n2014-03-17\n2014-03-16\n\n\nCC-MAIN-2013-48\n18\n2013-12-04\n2013-12-22\n2013-12-08\n\n\nCC-MAIN-2013-20\n33\n2013-05-18\n2013-06-20\n2013-05-26\n\n\nCC-MAIN-2012\n130\n2012-01-27\n2012-06-05\n\n\n\nCC-MAIN-2009-2010\n435\n2009-07-02\n2010-09-10\n\n\n\nCC-MAIN-2008-2009\n245\n2008-05-09\n2009-01-09\n\n\n\n\n\n\nIsn’t it written somewhere?\nThis same problem is solved in CDX Toolkit where they hardcode timestamps for early dates:\n    table = {  # times provided by Sebastian\n        '2012': timestamp_to_time('201206'),  # end 20120605, start was 20120127\n        '2009-2010': timestamp_to_time('201009'),  # end 20100910, start was 20100910\n        '2008-2009': timestamp_to_time('200901'),  # end 20090109, start was 20080509\n    }\nFor more recent data which is in the form CRAWL-NAME-YYYY-WW they assume the first day of the week is at the end of the crawl. Looking at recent crawls this is true; for example the October 2021 Crawl was crawled Oct 15-28, and has label CC-MAIN-2021-43; which starts on 2021-10-31. I’ve validated this by looking through the Common Crawl blog posts back to December 2020.\nHowever it’s not always true; looking at the earliest crawl in this format which describes the format as:\n\nCRAWL-NAME-YYYY-WW – The name of the crawl and year + week# initiated on\nThe 2013 wide web crawl data is located at /crawl-data/CC-MAIN-2013-20/ which represents the main CC crawl initiated during the 20th week of 2013.\n\nUnfortunately I couldn’t easily find blog releases with capture dates for every dataset, so I had to run the query to find out when this changed. In fact it looks like it’s all over the place before around mid 2014, and the ISO week start is pretty close but not always after the crawl.\n\n\n\n\n\n\n\n\n\n\nIndex Name\nEarliest Date\nLatest Date\nISO Week Start\nDays After Week Start\n\n\n\n\nCC-MAIN-2017-17\n2017-04-23\n2017-05-01\n2017-04-30\n-1\n\n\nCC-MAIN-2015-40\n2015-10-04\n2015-10-14\n2015-10-11\n-3\n\n\nCC-MAIN-2014-42\n2014-10-20\n2014-11-01\n2014-10-26\n-6\n\n\nCC-MAIN-2014-23\n2014-07-09\n2014-08-02\n2014-06-15\n-48\n\n\nCC-MAIN-2014-15\n2014-04-16\n2014-04-25\n2014-04-20\n-5\n\n\nCC-MAIN-2014-10\n2014-03-07\n2014-03-17\n2014-03-16\n-1\n\n\nCC-MAIN-2013-48\n2013-12-04\n2013-12-22\n2013-12-08\n-14\n\n\nCC-MAIN-2013-20\n2013-05-18\n2013-06-20\n2013-05-26\n-25\n\n\n\n\n\nMissing index metadata\nWhen running the queries I filtered to those with a 200 OK status, but got an error for exactly two indexes:\nERROR:root:404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2015-11-index?url=en.wikipedia.org%2F%2A&output=json&filter=%3Dstatus%3A200\nERROR:root:404 Client Error: Not Found for url: https://index.commoncrawl.org/CC-MAIN-2015-06-index?url=en.wikipedia.org%2F%2A&output=json&filter=%3Dstatus%3A200\nRunning it without the filter showed just those two indexes seem to be missing status, and they are missing mimetype as well as other features. For example here’s the first two lines of a response from 2015-06:\n[{'urlkey': 'org,wikipedia,en)/?banner=blackout',\n  'timestamp': '20150131071342',\n  'url': 'http://en.wikipedia.org/?banner=blackout',\n  'digest': 'YZPQWVBHLGVUKY2DAZAVNZTMPOVOGV5P',\n  'length': '17573',\n  'offset': '107669292',\n  'filename': 'crawl-data/CC-MAIN-2015-06/segments/1422122108378.68/warc/CC-MAIN-20150124175508-00190-ip-10-180-212-252.ec2.internal.warc.gz'},\n {'urlkey': 'org,wikipedia,en)/robots.txt',\n  'timestamp': '20150129073755',\n  'url': 'http://en.wikipedia.org/robots.txt',\n  'digest': 'IXB3SAHXDS54OVWAQIIND22TH7HNXR7W',\n  'length': '6017',\n  'offset': '114218356',\n  'filename': 'crawl-data/CC-MAIN-2015-06/segments/1422115855845.27/warc/CC-MAIN-20150124161055-00146-ip-10-180-212-252.ec2.internal.warc.gz'},\nCompared with the previous index 2014-52:\n{'urlkey': 'org,wikipedia,en)/?banner=blackout',\n  'timestamp': '20141226070722',\n  'url': 'http://en.wikipedia.org/?banner=blackout',\n  'mime': 'text/html',\n  'status': '200',\n  'digest': 'CQ66DSSKOUHSJXUWYW3HI45BEG22ZTYI',\n  'length': '18811',\n  'offset': '33342669',\n  'filename': 'crawl-data/CC-MAIN-2014-52/segments/1419447548645.134/warc/CC-MAIN-20141224185908-00083-ip-10-231-17-201.ec2.internal.warc.gz'},\n {'urlkey': 'org,wikipedia,en)/?title=pet_scanner',\n  'timestamp': '20141227154942',\n  'url': 'http://en.wikipedia.org/?title=PET_scanner',\n  'mime': 'text/html',\n  'status': '200',\n  'digest': '52CNIQXTQ6DBUCSL7JQ7AUWP6CN3BU3L',\n  'length': '49966',\n  'offset': '30581136',\n  'filename': 'crawl-data/CC-MAIN-2014-52/segments/1419447552326.50/warc/CC-MAIN-20141224185912-00056-ip-10-231-17-201.ec2.internal.warc.gz'},"
  },
  {
    "objectID": "population-density-australia/index.html",
    "href": "population-density-australia/index.html",
    "title": "Population Density Australia",
    "section": "",
    "text": "I know it’s about 10 hours driving from Melbourne to Sydney, and about the same again to Brisbane. Brisbane is about halfway between Melbourne in the south and Cairns in the far north. The whole trip is about 40 hours, at say 80 km/h so around 3000 km. So Australia is roughly 3000 km high.\nAustrlaia is a roughly as high as it is long so let’s treat it as a square. That makes Australia about 9 million square kilometres.\nAustralia has a population of about 25 million. So the population density is about 3 people per square kilometre.\n\nChecking\nThe actual area of Australia is about 8 million square kilometres, pretty close to my estimate. The distance from Melbourne to Cairns is only about 2,300 km, and Melbourne to Perth is about 2,700 km. This rectangle slightly underestimates the area because Australia bulges out more than these cities. The density is 3.3 people per square kilometre, which puts is right near the lowest of inhabited countries."
  },
  {
    "objectID": "select-fetch-extract-watch/index.html",
    "href": "select-fetch-extract-watch/index.html",
    "title": "Select, Fetch, Extract, Watch: Web Scraping Architecture",
    "section": "",
    "text": "When you’re getting started put all your work into making sure you can get the information you need. But before you scale it up operationally think about trying to disentangle the four steps to make it more robust and flexible.\nThe rest of this article is going to go into detail on each of the four steps, through the lens of three types of use cases:\n\nmonitoring a single page for changes over time like VisualPing\nextracting information from all the product pages on a website\ncollecting a wide crawl of the web like Common Crawl\n\n\nSelect\nSelect is about choosing the data to extract, where to get it from, and when to retrieve it.\nFor example if you’re monitoring a single page for changes over time then you just need to keep track of a single URL to retrieve. The only choice you need to make is how often you retrieve it; this could be handled with a stateless scheduler (e.g. run on the first minute of every hour) or by tracking when it was last retrieved (e.g. if it was more than an hour ago then refetch).\nIf you’re extracting all the product pages on a website you’ll need a way to find a list of all those pages. The best way to do this will depend on the website; maybe there’s a sitemap.xml with all the pages listed, if they can be retrieved from a sequential numeric id you could search the space of allowed ids, but otherwise you may have to scrape category pages or search results to find all the product page URLs. The order doesn’t normally matter too much if you can scrape all the required pages in the allotted time. You also need to decide how often to retrieve the information; if it’s unlikely to change you may just want to fetch it once, but you may want to check back every month to see what’s changed. Generally you’ll get more product pages over time so it’s important to keep track of what you’ve already retrieved and when you retrieved it.\nFor collecting a wide crawl of the web you start with a small set of seed pages and collect all the URLs in already scraped pages as future sites to crawl. Here there’s too much to scrape every possible page, and you have to decide how to prioritise the crawl (going deep in a few websites, covering as many websites as possible) and when to retrieve a page again. You will also want to limit the rate at which you request from any given domain in the fetch step.\nSelect is the most robust phase to failure; the most common reasons it would fail is if a page is removed (for example because a product is no longer offered, or a dead link in a crawl), or more rarely because the URL structure is completely changed.\n\n\nFetch\nFetch is about actually retrieving the data over the network. There can be all sorts of issues in doing this, and sometimes you only get one shot - especially if you’re monitoring changes over time. Because of this I recommend serialising everything in the Web Archive format so you keep your data; this is easy to do with Python’s requests library or with Scrapy.\nThe issues here are fairly common across use cases, and well written about elsewhere. You need to make sure you’re being respectful; understanding the robots.txt, setting an appropriate User Agent, making requests at a rate that adds little load to the server and slowing the rate when there are server issues or the response time increases. Sometimes you need work around issues, like retrying failed requests, managing sessions and even using proxies. Overall you want to hit the remote server as few times as possible, so save all your data as WARC so you only need to do it once.\nIf you’ve got a huge scraping project keeping all the bulk of HTML and headers in WARC for the few fields you’re extracting (often 10x-100x larger after compression) may get burdensome. But I think of it as a backup; you can delete old ones over time as you’re more sure you don’t need them, but it’s always worth keeping so you can recover from issues in extraction.\n\n\nExtract\nExtracting is getting the data out of your responses. If you’re doing a wide crawl then the WARC is probably as much extraction as you can do, if you’re hitting a well structured API the extraction may already be done in the JSON response. But it’s very common to have to parse data out of HTML, or at least transform some JSON.\nFor monitoring part of a single page this could be a CSS selector that selects the content to be monitored, and potentially normalising irrelevant changes (e.g. whitespace in the markup). This could fail if the structure of the page changes so much the section disappears.\nA product page is more complicated and the best way to extract the required fields depends on how the page is structured. But it’s a good bet that your heuristics will eventually fail on unusual pages, or as the website changes its structure over time. A more subtle failure mode is you may discover some pages have a field that would add a lot of value that you weren’t previously extracting. For these reasons it makes sense to work off of WARC files you can retry on later, and skip over errors to extract the data that you can. Then we can watch these issues and improve the extraction iteratively.\n\n\nWatch\nWatch is monitoring your pipeline to make sure when it’s broken you know, before you end up with weeks of lost data.\nFor visiting product pages or a web crawl you could watch the select step to make sure you’re getting, and fetching, new items at an expected rate.\nFor all use cases it’s useful monitoring the response status codes; if you’re getting too many bad codes something is going wrong and you want to raise an alert. It’s also worth doing some sanity checks on the data; the response size should be variable within a certain range (a common failure case is getting empty or error pages).\nFor extract you can monitor the percentage of fetches that don’t extract cleanly and pass validations; they can alert you to a change in the page structure, or even errors with fetching.\n\n\nPutting them together\nIn reality you can’t always disentangle these steps. For a web crawl your selection will depend on the website you just extracted. Some websites will force you to perform a series of actions to fetch that may impact your choice of selection. For product information you may need to tie information from a category or search page with data from a product page.\nBut in the whole the cleaner the interface you can have between the steps, the easier time you’ll have maintaining your scraper and recovering from errors. Notably the Scrapy architecture follows a similar pattern; the scheduler selects, the downloader fetches, and the spiders extract into the pipeline."
  },
  {
    "objectID": "github-actions/index.html",
    "href": "github-actions/index.html",
    "title": "Using Github Actions with Hugo",
    "section": "",
    "text": "My workflow for publishing this website used to be commit and push the changes and run a deploy script. When I used a different laptop with a more recent version of Hugo this resulted in some issues, and it wasn’t easy for me to use the earlier version from my package manager. While I could use something like Docker this was a good excuse to automatically get a properly configured server to publish automatically.\nYou could long do this with a git post-receive hook on a self-hosted server or Gitlab CI or other CI tools. Recently Github released Actions which can do this (and also trigger on other Github events).\nThe documentation for Github Actions is pretty dense, describing each part in detail with no high-level howtos. After reading it for a couple of hours I understood enough to string together some marketplace actions into a workflow.\nIn my Github repository, in the “Actions” section, it has some prompts to get started. It suggested a Jekyll workflow (which is odd for a Hugo blog), so I searched the Marketplace for Hugo. It’s really hard to decide from the search results what I should to use, and strangely it’s not quite ranked by number of stars.\n\n\n\nHugo Marketplace Search has Many Results\n\n\nI read a the detail pages of a few, and Hugo Setup which had the most stars seemed to do what I needed. It has good documentation including a sample on how to publish to Github pages. A great feature is that I can specify the Hugo version, so my website won’t be broken immediately by changes to Hugo.\nHowever I don’t use Github Pages and I’ve been syncing the output with rclone. I couldn’t figure out how to do this directly, so I went back to the marketplace to find github action for rclone. I could store my rclone configuration as a Github secret called RCLONE_CONF (under Settings > Secrets) and then inject it into the Github workflow file using ${{ secrets.RCLONE_CONF }}.\nThis is where I stopped to think; can the action author steal my secrets? So I read the source code, which was thankfully straightforward, and saw that if I assume https://rclone.org is secure and that the code the action executes is the same as the source then it should be secure. I don’t know if there’s any way to guarantee that I continue to use this version, or if there’s any vetting of marketplace code.\nTogether this gave me a Github workflow to build and publish this website (except rebuilding blogdown posts). Overall the experience seems with Github actions seems to push the Marketplace, which is not really easy to understand and it feels clunky to compose actions when compared to composing scripts on Linux. For something this simple where actions existed it was straightforward, but for something more complex I would have to write my own action. The rclone action shows this could be straightforward with Docker, but the hugo action shows it can get quite involved.\nFrom a strategic position I can understand why Github is pushing the marketplace so hard. Gitlab has had a much better CI story for a long time (and as far as I can tell still does today), but Github has a far larger community. By leveraging the community to build useful actions and encouraging developers to use them for free will help get developers started with it. By encouraging chaining together lots of actions into workflows (which seems to be the fundamental design) will make it really difficult to migrate to another CI tool once you have complex workflows. Then they can monetise on projects that exceed the free quotas.\nFor me the next step would be to try to write a Docker action which I should then be able to use on any linux server. The specific benefits of the Github action ecosystem currently seem low, and it is less mature than other CI tools. But it’s a useful free tool to have access to if you’re using Github for hosting code."
  },
  {
    "objectID": "restoring-wayback-html/index.html",
    "href": "restoring-wayback-html/index.html",
    "title": "Restoring Wayback Machine HTML",
    "section": "",
    "text": "I originally came across this when fetching resources from the Internet Archive through its CDX Server. The server response includes a SHA-1 digest, but when I tried to recalculate it on the content I got a different value. When I searched for why I came across an Internet Archive post explaining the digest has the SHA-1 of the original content, not what’s in the Wayback Machine.\n\nAs you may have guessed, downloading all instances of a webpage, and hashing them yourself, would be worse than relying on the CDX digest. That is because all the instances of the webpage are guaranteed to be different, because the Wayback Machine replaces all links by internal hyperlinks. These urls contain timestamps, and the timestamps obviously differ.\n\nHowever it turns out to be trivial to get the original content; if the Wayback version is at http://web.archive.org/web/<timestamp>/<url> then the original capture is at http://web.archive.org/web/<timestamp>id_/<url>.\n\nThe Internet Archive allows us to retrieve the raw version of web pages. For example, if you have this URL (https://web.archive.org/web/20170204063743/http://john.smith@example.org/), replace the timestamp 20170204063743 with 20170204063743id_ (so the modified URL will look like https://web.archive.org/web/20170204063743id_/http://john.smith@example.org/) then you will get the original HTML without any additional comments added by the Internet Archive.\n\nBut I only learned that after spending time trying to reverse engineer the Wayback HTML, and the rest of the article covers what the changes are.\n\nAbout a test case\nTo work out what was happening I needed a small page and so I used my about page page.\nSearching the Internet Archive CDX I get a recent capture:\nimport requests\nr = requests.get('http://web.archive.org/cdx/search/cdx',\n                 params={'url': 'skeptric.com/about/', 'output': 'json'})\ncaptures = r.json()\n\nimport pandas as pd\ndf = pd.DataFrame(captures[1:], columns=captures[0])\nThis gives a capture of the page from 2020-11-12:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nurlkey\ntimestamp\noriginal\nmimetype\nstatuscode\ndigest\nlength\n\n\n\n\n0\ncom,skeptric)/about\n20211120235913\nhttps://skeptric.com/about/\ntext/html\n200\nZ5NRUTRW3XTKZDCJFDKGPJ5BWIBNQCG7\n3266\n\n\n\nWe can check the base 32 encoded SHA-1 digest against a current snapshot:\nfrom hashlib import sha1\nfrom base64 import b32encode\n\ndef sha1_digest(content: bytes) -> str:\n    return b32encode(sha1(content).digest()).decode('ascii')\n\noriginal_url = f'http://web.archive.org/web/{record.timestamp}id_/{record.original}'\noriginal_content = requests.get(original_url).content\nsha1_digest(original_content)\nThis gives Z5NRUTRW3XTKZDCJFDKGPJ5BWIBNQCG7 which matches the record.\nNow we can get the Wayback Machine version of the content by inserting the timestamp and original URL\nrecord = df.iloc[0]\nwayback_url = f'http://web.archive.org/web/{record.timestamp}/{record.original}'\nwayback_content = requests.get(wayback_url).content\n\nsha1_digest(wayback_content)\nThis gives us a different digest: DEXQJ2HFM7EYGOWJ6W6FPKIJC4V3VXEE.\n\n\nHeaders and footers\nLooking at the start of wayback_content there’s a bunch of Internet Archive Javascript and CSS:\n<!DOCTYPE html>\n<html lang=\"en-us\">\n<head><script src=\"//archive.org/includes/analytics.js?v=cf34f82\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app213.us.archive.org';v.server_ms=279;archive_analytics.send_pageview({});});</script>\n<script type=\"text/javascript\" src=\"/_static/js/bundle-playback.js?v=UfTkgsKx\" charset=\"utf-8\"></script>\n<script type=\"text/javascript\" src=\"/_static/js/wombat.js?v=UHAOicsW\" charset=\"utf-8\"></script>\n<script type=\"text/javascript\">\n  __wm.init(\"http://web.archive.org/web\");\n  __wm.wombat(\"https://skeptric.com/about/\",\"20211120235913\",\"http://web.archive.org/\",\"web\",\"/_static/\",\n          \"1637452753\");\n</script>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/_static/css/banner-styles.css?v=omkqRugM\" />\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/_static/css/iconochive.css?v=qtvMKcIJ\" />\n<!-- End Wayback Rewrite JS Include -->\n\n    <meta charset=\"utf-8\"/>\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"/>\n\n\n\n    <title>About Skeptric · </title>\n\n    <meta name=\"HandheldFriendly\" content=\"True\"/>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n\n\n    <link rel=\"stylesheet\" href=\"http://web.archive.org/web/20211120235913cs_/https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css\"/>\nTo get our original content you’d have to strip everything from the first <script tag through to the helpful End Wayback Rewrite JS Include. Here’s a very rough script to do it:\ndef remove_wayback_header(content):\n    _start = b'<script src=\"//archive.org/includes/analytics.js'\n    _end = b'<!-- End Wayback Rewrite JS Include -->\\n'\n    start_idx = content.find(_start)\n    end_idx = content.find(_end)\n    if start_idx < 0 or end_idx < 0:\n        raise ValueError(\"Could not find\")\n    return content[:start_idx] + content[end_idx+len(_end):]\nSimilarly if you look at the end there’s more boilerplate about the archival:\n</footer>\n\n    </div>\n\n</body>\n</html>\n<!--\n     FILE ARCHIVED ON 23:59:13 Nov 20, 2021 AND RETRIEVED FROM THE\n     INTERNET ARCHIVE ON 00:41:42 Dec 01, 2021.\n     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.\n\n     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.\n     SECTION 108(a)(3)).\n-->\n<!--\nplayback timings (ms):\n  captures_list: 198.782\n  exclusion.robots: 0.079\n  exclusion.robots.policy: 0.072\n  RedisCDXSource: 2.673\n  esindex: 0.007\n  LoadShardBlock: 177.421 (3)\n  PetaboxLoader3.datanode: 81.052 (4)\n  CDXLines.iter: 16.33 (3)\n  load_resource: 76.041\n  PetaboxLoader3.resolve: 25.907\n-->\nWe can similarly strip out everything from the file archival comment:\ndef remove_wayback_footer(content):\n    _prefix = b'</html>\\n'\n    _start = _prefix + b'<!--\\n     FILE ARCHIVED ON '\n    start_idx = content.find(_start)\n    if start_idx < 0:\n        raise ValueError(\"Could not find\")\n    return content[:start_idx + len(_prefix)]\n\n\nRestoring Links\nThe links in the Wayback Machine versino of the webpage are prefixed with http://web.archive.org/web/, with an extra cs_ for CSS, and js_ for Javascript, and im_ for images. For example some of the links can be found with:\nre.findall(b'(?:href|src)=\"([^\"]*)\"', wayback_content)\nThis gives results including:\nhttp://web.archive.org/web/20211120235913cs_/https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css\nhttp://web.archive.org/web/20211120235913/https://skeptric.com/\n/web/20211120235913/https://skeptric.com/about/\n/web/20211120235913/https://skeptric.com/\nhttp://web.archive.org/web/20211120235913/https://www.whatcar.xyz/\nhttp://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\nSo we can remove the prefixes:\ndef remove_wayback_links(content: bytes, timestamp: str) -> bytes:\n    # Remove web links\n    timestamp = timestamp.encode('ascii')\n    content = content.replace(b'http://web.archive.org', b'')\n    for prefix in [b'', b'im_', b'js_', b'cs_']:\n        content = content.replace(b'/web/' + timestamp + prefix + b'/', b'')\n    return content\n\n\nAnd the rest\ndef remove_wayback_changes(content, timestamp):\n    content = remove_wayback_header(content)\n    content = remove_wayback_footer(content)\n    content = remove_wayback_links(content, timestamp)\n    return content\nWe can then compare the cleaned wayback content with the original using seqmatcher (see side-by-side diffs in Jypyter for a fancier solution). For every area where the two are different we print the original and then the cleaned wayback version, with an additional 20 tokens of context on either side:\nfrom difflib import SequenceMatcher\nseqmatcher = SequenceMatcher(isjunk=None,\n                             a=original_content,\n                             b=clean_wayback_content,\n                             autojunk=False)\n\ncontext_before = context_after = 20\n\nfor tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        if tag == 'equal':\n            continue\n\n        a_min = max(a0 - context_before, 0)\n        a_max = min(a1 + context_after, len(seqmatcher.a))\n        print(seqmatcher.a[a_min:a_max])\n\n        b_min = max(b0 - context_before, 0)\n        b_max = min(b1 + context_after, len(seqmatcher.b))\n        print(seqmatcher.b[b_min:b_max])\n        print()\nThis yields a set of very small changes; here they are:\n\nRemoved trailing whitespace in tags\nMade relative links absolute\nAdded a trailing / to the domain URL\n\nmeta charset=\"utf-8\" />\\n    <meta http-eq\nmeta charset=\"utf-8\"/>\\n    <meta http-eq\n\ne\" content=\"IE=edge\" />\\n\\n    \\n    \\n    <t\ne\" content=\"IE=edge\"/>\\n\\n    \\n    \\n    <t\n\nndly\" content=\"True\" />\\n    <meta name=\"v\nndly\" content=\"True\"/>\\n    <meta name=\"v\n\n, initial-scale=1.0\" />\\n\\n    \\n    <link r\n, initial-scale=1.0\"/>\\n\\n    \\n    <link r\n\n015bf2d95d914e5.css\" />\\n<script async src\n015bf2d95d914e5.css\"/>\\n<script async src\n\n\"menuitem\"><a href=\"/about/\">About</a></\n\"menuitem\"><a href=\"https://skeptric.com/about/\">About</a></\n\n\"menuitem\"><a href=\"/\">Home</a></li>\\n\n\"menuitem\"><a href=\"https://skeptric.com/\">Home</a></li>\\n\n\nhttps://skeptric.com\">skeptric.com</a>.<\nhttps://skeptric.com/\">skeptric.com</a>.<\nWhat’s interesting about this is there’s no way to recover this information without the original; there’s no way of knowing for sure where the trailing whitespace is (you could search for it by matching against the SHA-1, but it would be expensive). It’s good that the Internet Archive provide an original version of the HTML as well!\nFor this case I wrote a little script that would munge the original content into something closer to what the Wayback Machine emits, but it wouldn’t be robust enough to work for other captures:\nimport re\ndef wayback_normalise_content(content, base_url):\n    url = base_url.encode('ascii')\n    content = re.sub(b' */>', b'/>', content)\n    content = content.replace(b'href=\"/', b'href=\"' + url + b'/')\n    content = re.sub(b'href=\"' + url + b'\"', b'href=\"' + url + b'/\"', content)\n    return content\n\nassert wayback_normalise_content(original_content, 'https://skeptric.com') == clean_wayback_content\nIf you want to try this at home there’s a Jupyter Notebook (or you can view it in your browser)."
  },
  {
    "objectID": "formatting-sql/index.html",
    "href": "formatting-sql/index.html",
    "title": "Offline SQL Formatting with sqlformat",
    "section": "",
    "text": "You can install sqlformat in Debian derivatives such as Ubuntu with sudo apt install sqlformat. Alternatively with any system with Python you can install it via pip install sqlparse, just make sure you have the binary in your path (e.g. if using pip install --user then ~/.local/bin/ should be in your path).\nThe formatting is provided by the Python library sqlparse, and you can see an example of how it works at sqlformat.org. The instructions are at sqlformat --help and there’s lots of arguments to choose how to format keywords, identifiers, alignment, comma placement and wrapping. Here’s a simple command that takes the contents of query.sql, converts the keywords to upper case, reindents the statements and puts space around operators, and outputs to formatted.sql:\nsqlformat -k upper -r -s -o formatted.sql query.sql\nThe command line is useful for batch changing SQL files, but often I want to change an SQL query inside a Python or R file. This works great, even if you’re using parameters, f-strings in Python or glue in R, it seems to do a good job. It’s straightforward to run on a region of SQL in Vim (or Emacs Evil) using a filter command, or in Emacs by executing a shell command, just make sure you pass - as the file so it reads from stdin.\nFor Emacs I’ve written a small function to SQL format a region; being careful not to clobber the final newline of the region. A nice improvement would be to customise the arguments.\n(defun remove-trailing-newline (point)\n  (if (= (char-before point) ?\\n)\n      (- point 1)\n    point))\n\n(defun sql-format (start end)\n  \"Formats the selected sql `sqlformat'\"\n  (interactive \"r\")\n  (shell-command-on-region\n   ;; beginning and end of buffer\n   start\n   (remove-trailing-newline end)\n   ;; command and parameters\n   \"sqlformat -k upper -r -s -\"\n   ;; output buffer\n   (current-buffer)\n   ;; replace?\n   t\n   ;; name of the error buffer\n   \"*Sqlformat Error Buffer*\"\n   ;; show error buffer?\n   t))"
  },
  {
    "objectID": "bernoulli-mixing/index.html",
    "href": "bernoulli-mixing/index.html",
    "title": "A Mixture of Bernoullis is Bernoulli",
    "section": "",
    "text": "Concretely suppose that 10% of our email list is new members and 90% is existing members. New members have an 80% conversion rate, and existing members have a 40% conversion rate. Intuitively the overall conversion rate is \\(0.1 \\times 0.8 + 0.9 \\times 0.4\\) or 44%.\nThis situation is have a mixture model. When we send an email we can think of the group we send it to being a draw from a Bernoulli distribution, which then determines which Bernoulli distribution we sample from. Formally we pick our groups with a random variable \\(G \\in \\mathrm{Bernoulli}(\\mu)\\) which picks between \\(X \\in \\mathrm{Bernoulli}(p_1)\\) and \\(Y \\in \\mathrm{Bernoulli}(p_2)\\). In our example \\(\\mu = 0.1,\\ p_1 = 0.8,\\ p_2 = 0.4\\). The overall distribution is then \\(Z = G X + (1 - G) Y\\).\nEnumerating the possible values of the variables and their likelihood gives \\(Z = 1\\) if and only if \\(G = 1,\\ X = 1\\) or \\(G = 0,\\ Y = 1\\), otherwise it is zero. The probability of this is \\(\\mu p_1 + (1 - \\mu) p_2\\), which is exactly the intuitive calculation we used above. So we see that \\(Z \\in \\mathrm{Bernoulli}(\\mu p_1 + (1 - \\mu) p_2)\\).\nAs you’d expect you can ignore the subgroups and treat the combined group together. This would make sense in an experiment where you expect similar behaviour accross groups, or you’re just interested in the overall effect. Because there’s only one response variable it’s not possible to distinguish the groups by the conversion rate alone. Note that this effect isn’t true for other kinds of models, a mixture of binomials is generally different to a binomial."
  },
  {
    "objectID": "jupyter-hugo-blog/index.html",
    "href": "jupyter-hugo-blog/index.html",
    "title": "Writing Blog Posts with Jupyter and Hugo",
    "section": "",
    "text": "It can be convenient to directly publish a mixture of prose, source code and graphs. It ensures the published code actually runs and makes it much easier to rerun at a later point. I’ve done this before in Hugo with R Blogdown, and now I’m experimenting with Jupyter notebooks.\nThe best available option seems to be nb2hugo which converts the notebook to markdown, keeping the front matter exporting the images.\nHow to use it; based loosely on the demo:\nThe nb2hugo script will create a new file with the same name in {site-dir}/content/{section}/{name}.md and extracts all your images as separate files in {site-dir}/static/{section}/{name}/."
  },
  {
    "objectID": "jupyter-hugo-blog/index.html#example-of-a-generated-image",
    "href": "jupyter-hugo-blog/index.html#example-of-a-generated-image",
    "title": "Writing Blog Posts with Jupyter and Hugo",
    "section": "Example of a generated image",
    "text": "Example of a generated image\nx = np.arange(-10, 10, 0.01)\ny = np.sin(x) / x\nplt.plot(x, y)\n[<matplotlib.lines.Line2D at 0x7fd22924c460>]\n\n\n\npng\n\n\nI had to work around a few issues to get this to work."
  },
  {
    "objectID": "jupyter-hugo-blog/index.html#fixing-broken-front-matter",
    "href": "jupyter-hugo-blog/index.html#fixing-broken-front-matter",
    "title": "Writing Blog Posts with Jupyter and Hugo",
    "section": "Fixing broken front matter",
    "text": "Fixing broken front matter\nIf the frontmatter had any quotes around it, or = signs instead of colons, then the generated frontmatter would be very broken. It seems that the method to extract front matter is quite fragile.\n# Writing Blog Posts with Jupyter and Hugo\n\nTags: writing, python\n\ndate: 2020-08-02T08:00:00+10:00\n\nfeature_image: /images/jupyter-blog.png\n\n<!--eofm-->"
  },
  {
    "objectID": "jupyter-hugo-blog/index.html#fixing-path-routing-issues",
    "href": "jupyter-hugo-blog/index.html#fixing-path-routing-issues",
    "title": "Writing Blog Posts with Jupyter and Hugo",
    "section": "Fixing path routing issues",
    "text": "Fixing path routing issues\nIn my case I use rewriting rules to remove the /post/ from the URL. In my Hugo config.toml I do that with the following configuration\n[permalinks]\n  post = \"/:filename/\"\nUnfortunately the image is referenced as a local file like output.png. This means the file is being looked at in /:filename/output.png when it’s been saved ad /post/:filename/output.png which means it’s not rendered. For this post I just moved the folder.\nUnforutnately this doesn’t seem configurable, and the nb2hugo repository seems stale. So either I’ll need to script up moving the files after, get Hugo to move the file, or develop my own fork of nb2hugo to handle these issues.\nNevertheless nb2hugo does a lot of the heavy lifting and makes it possible to publish reasonable looking blogs from a Jupyter notebook like this one. Blogdown is definitely a nicer experience, but even for R I sometimes use Jupyter notebook, and so it’s good to have it as an option."
  },
  {
    "objectID": "distribution-between-mean-median/index.html",
    "href": "distribution-between-mean-median/index.html",
    "title": "Probability Distributions Between the Mean and the Median",
    "section": "",
    "text": "The normal distribution is used throughout statistics, because of the Central Limit Theorem it occurs in many applications, but also because it’s computationally convenient. The expectation value of the normal distribution is the mean, which has many nice arithmetic properties, but the drawback of being sensitive to outliers. When discussing constant models I noted that the minimiser of the Lᵖ error is a generalisation of the mean; for \\(p = 2\\) it’s the mean, for \\(p = 1\\) it’s the median, and for \\(p = \\infty\\) it’s the midrange (half way betwen the maximum and minimum points). We can similarly generalise the normal distribution to a family of exponential distributions, \\(\\frac{A}{\\sigma} e^{- \\alpha \\left\\vert\\frac{x - \\mu}{\\sigma}\\right\\vert^p}\\) where the expectation value \\(\\mu\\) is the minimiser of the Lᵖ error. This distribution can then be used for estimates less sensitive to outliers, for p between 1 and 2."
  },
  {
    "objectID": "distribution-between-mean-median/index.html#calculating-the-moments",
    "href": "distribution-between-mean-median/index.html#calculating-the-moments",
    "title": "Probability Distributions Between the Mean and the Median",
    "section": "Calculating the moments",
    "text": "Calculating the moments\nConsider the probability distributions of the form above:\n\\[f(x) = \\frac{A}{\\sigma} e^{- \\alpha \\left\\vert\\frac{x - \\mu}{\\sigma}\\right\\vert^p}\\]\nThe constant A is determined by normalisation \\(\\int_{-\\infty}^{\\infty} f(x) = 1\\). Using the power exponential formulas gives the normalising constant as \\(A = \\frac{p \\alpha^{\\frac{1}{p}}}{2 \\Gamma\\left(\\frac{1}{p}\\right)}\\). Using normalisation and symmetry the mean is \\(\\int_{-\\infty}^{\\infty} x f(x) = \\mu\\). With a little more caluclation using the power exponential formulas it can be shown the variance is \\(\\int_{-\\infty}^{\\infty} x^2 f(x) = \\sigma^2\\) if \\(\\alpha = \\left(\\frac{\\Gamma\\left(3/p\\right)}{\\Gamma\\left(1/p\\right)}\\right)^{p/2}\\). So for example in the case \\(p = 2\\) we get the familiar constant of 1/2 in the normal distribution exponent.\nGiven data how do we estimate the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) for the distribution \\(\\frac{A}{\\sigma} e^{- \\alpha \\left\\vert\\frac{x - \\mu}{\\sigma}\\right\\vert^p}\\)? With the data \\(X_1, X_2, \\ldots, X_n\\) the log-likelihood is:\n\\[l(\\mu, \\sigma) = N \\log(A) - N \\log(\\sigma) - \\alpha \\sum_{i=1}^{N} \\left\\vert \\frac{X_i - \\mu}{\\sigma} \\right\\vert ^ p\\]\nThe Maxmium Likelihood Estimator for the expectation value of the distribution \\(\\mu\\) is the minimiser of \\(\\left\\vert X_i - \\mu \\right\\vert^ p\\), which is precisely the minimiser of the Lᵖ error. So for \\(p = 2\\) the expectation value is estimated by the mean, for \\(p = 1\\) by the median and as \\(p \\rightarrow \\infty\\) by the midrange. The Maximum Likelihood Estimator for \\(\\sigma\\) is\n\\[\\hat{\\sigma} = \\left(\\frac{p \\alpha}{N} \\sum_{i=1}^{N} \\left\\vert X_i - \\mu \\right\\vert^p\\right)^{1/p}\\]\nwhich is, up to a multiplicative constant, the Lᵖ error.\nSo by modelling data under the probability distribution \\(\\frac{A}{\\sigma} e^{- \\alpha \\left\\vert\\frac{x - \\mu}{\\sigma}\\right\\vert^p}\\) the Maximum Likelihood Estimate for \\(\\mu\\) is the minimiser of Lᵖ error, and for \\(\\sigma\\) is the Lᵖ error up to a constant factor. This could be used as a model in a regression allowing different kinds of estimates; in particular for \\(p = 1\\), with distribution \\(\\sqrt{\\frac{3}{2}} e^{-\\sqrt{6}\\left\\vert\\frac{x - \\mu}{\\sigma}\\right\\vert}\\) the MLE estimate is the median and the MAD is \\(\\frac{\\hat{\\sigma}}{\\sqrt{6}}\\). A reparameterisation of the exponent variable could ensure the MLE is the Lᵖ error, but then the variable would no longer correspond with the square root of the variance of the distribution."
  },
  {
    "objectID": "python-file-server/index.html",
    "href": "python-file-server/index.html",
    "title": "Serving Static Assets with Python Simple Server",
    "section": "",
    "text": "I had a JSON file I wanted to load into Javascript in a HTML page. Looking at StackOverflow I found I found fetch could do this\nfetch(\"test.json\")\n    .then(response => response.json())\n    .then(json => process(json))\nWhere process is some function that acts on the data; console.log is good for testing. Reading about using fetch shows that it works similarly to XMLHttpRequest, or jQuery’s getJson, with slightly worse browser coverage. See the hospodarets article on fetch for more on using it.\nWhen I tried to do this I opened the HTML file locally in Firefox and in the console got an error message:\nCross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at file:///...\nTrying to request a file from HTTP gives this error because it has security implications.\nThe simplest workaround is to serve “test.json” from a server. This is trivial with Python’s inbuilt HTTP server. Just go to the directory contianing the HTML and Json files in a shell and type python3 -m http.server to start a server. Then you can open the HTML file at http://localhost:8080/<filename.html> (or if you call it index.html you don’t need the extension) and it will be able to load in the JSON file.\nIn production you would be running this behind a robust HTTP server like Nginx or Apache, but for quickly testing locally Python’s inbuilt server is useful."
  },
  {
    "objectID": "symmetry-lie-alebras-qde-3/index.html",
    "href": "symmetry-lie-alebras-qde-3/index.html",
    "title": "Symmetry, Lie Algebras and Differential Equations Part 3",
    "section": "",
    "text": "As a motivating example consider the three-dimensional quantum Kepler problem \\(H \\psi = E \\psi\\) for \\(H=p^2 + \\frac{1}{\\sqrt{r^2}}\\) , where r is the 3 dimensional position operators and p is the 3 dimensional momentum vector in the previous article (that is the components of r and p satisfy the algebraic relation \\(x_ip_j-p_jx_i = i \\delta_{ij} \\mbox{Id}\\) ).\nThe Kepler problem clearly has a rotational symmetry about any axis, and so the angular momentum is conserved and the operators \\(H, L_z, L^2\\) form a mutually commuting set. In fact there is another conserved quantity, unique to the 1/r potential, the Laplace-Runge-Lenz vector, \\(A = \\frac{1}{2} (p \\times L - L \\times p) - \\hat{r}\\) where \\(\\hat{r}=\\frac{r}{\\sqrt{r^2}}\\) is the unit vector – in classical mechanics the conservation of this vector corresponds to the fact orbits are conic sections. As it turns out angular momentum ladder operators can be built from \\(L_z\\) and \\(A_z\\) for the negative eigenspace of \\(H\\) and can be used to find the spectrum and eigenfunctions as we did for angular momentum and the simple harmonic oscillator; for the details see e.g. Thaller’s Advanced Visual Quantum Mechanics.\nThe corresponding symmetry is quite subtle: in momentum space the Kepler problem can be viewed via stereographic projection as a free particle on a three sphere. With respect to \\(H,L^2,L_z\\) this symmetry is related to the fact the eigenvalues of \\(H\\) are independent of those of \\(L_z\\) .\nIt is interesting that according to Cordani’s monograph on the Kepler Problem the Kepler problem is separable in exactly 4 coordinate systems, and the corresponding “first integrals” (separation constants) are given by combinations of the operators above.\n\nSpherical Coordinates: \\(H,L_z,L^2\\)\nParabolic Coordinates: \\(H,A_z,L_z\\)\nConfocal Elliptic Coordinates: \\(H,L^2-a A_z,L_z\\)\nConical Coordinates: \\(H,L^2, L_x^2 - a^2(L_x^2+L_y^2)\\)\n\nthe eigenfunctions in these coordinates are some sort of hypergeometric functions. From a physical perspective it is also interesting that these mean the Kepler problem remains separable under certain perturbations (but the spectrum can no longer be derived algebraically): spherical coordinates are invariant under spherical perturbations and if the particle is charged interaction with a constant magnetic field; parabolic coordinates are invariant under perturbation by a constant force along the z-axis, and if the particle is charged interaction with a constant electric field; confocal elliptic coordinates are invariant under perturbation by another central force at the other focus, this corresponds to Euler’s restricted 3-body problem; I’m not sure about conical coordinates.\nIn fact as shown in Morse and Feschbach – Methods of Theoretical Physics the Helmholtz/Diffusion equation \\(\\nabla^2 \\psi = \\lambda \\psi\\) (which for \\(\\lambda < 0\\) is the Schrodinger equation for a free particle) in 3 dimensions is separable in 11 coordinate systems. There is some relationship between the separability, symmetry, conserved quantities and special functions, I will outline some special cases:\n\nCartesian coordinates. Translational symmetry. \\(p_x,p_y,p_z\\) . Coordinate functions \\(x,y,z\\) .\nCircular cylindrical coordinates. 1D translational + 1D rotational . \\(H=p^2,p_z,L_z\\) . Bessel functions.\nSpherical coordinates. Rotational symmetry. \\(H=p^2,L_z,L^2\\) . Spherical harmonics.\n\nI will leave open some questions I would like to answer:\nWhat is this relationship between the integrability of a dynamical system, its symmetries, representations of its Lie algebra and the special functions?\nAre there other physically important systems that are separable – breaking some but not all of the symmetries?\nThere are many hints to these questions in the literature; for example Miller’s expositionson the relation between special function and symmetry. There are derivations of possible coordinate systems an equation separates in using something called a Stäckel transformation (this is what both Morse and Feschbach and Cordani use). Lie algebras were in fact invented to understand the solubility of differential equations in the same way Galois groups use symmetry to understand the solubility of algebraic equations (but unfortunately algebraists removed it from its roots).\nAs to the second question there is Bruns’ theorem: the only conserved quantities in the classical three body problem are the centre of mass, momentum, angular momentum and energy. So in non-relativistic quantum mechanics we would expect no extra conserved quantities for the three body problem (and consequently the many body problem) – any separable systems must be effectively one or two particle (like Euler’s restricted 3-body problem)."
  },
  {
    "objectID": "truly-independent-thinkers/index.html",
    "href": "truly-independent-thinkers/index.html",
    "title": "Truly Independent Thinkers",
    "section": "",
    "text": "To succeed in science you need to study something people are interested in. Trying to start a new field is a tremendous amount of work and it cuts against the grain of existing institutions. If you want to study a non-string theory approach to Quantum Gravity it’s much harder to get funding and publications than to follow the mainstream. To get reviewed and published you need to work on the edifice of existing research, and the current fashions of that research. Most scientific papers are small pieces of work adding to a field.\nStartup founders, in the Silicon Valley sense, need to convince venture capitalists that they’re doing something valuable. When you have a collective noun for a group it’s hard to consider them independent minded, and venture capitalists are such a group. There’s a whole field on how to get startup funding and a process to doing it, and who you know is crucial to this. If you don’t fit the conventions of a venture capitalist business they’re not interested.\nEssayists have to write for an audience. If their notions are too peculiar no one would read them. To be successful they have to be widely known, which means either conformists read them, or independent thinkers who can persuade conformists read them. While they need to be original they are stuck to the interests of their readerships.\nI’m being fairly strict on the notion of independent-mindedness, but the division of people this way strikes me as wrong. Definitely scientists, startup founders and essayists all have a need to be inventive, to be novel and creative. But they all have to persuade an audience and act within the constraints of their field. In their own groups most of them are conventional (almost by definition of group membership), but they’re not the same as everyone else.\nThis is important because if you’re independent minded without being socially connected and persuasive then you’re not likely to be considered successful. Most people fitting this description would be diagnosed with a psychological condition. It makes me think about Terry Davis, inventor of TempleOS. He achieved the immense feat of creating an operating system, from the HolyC programming language, and many applications from scratch by himself. He claimed to be in direct communication with God, and was commanded by God to build it. He was also diagnosed with schizophrenia, and was struck by a train at 48. It’s clear he was independent minded, and built something very technically complex, but not something influential on the community. To get an idea of what it was like, see this video of an application.\nThere are many less extreme examples in mathematics of independent minded people who have been deeply influential, but they are still in the extreme minority. There’s Grigory Perelman, who solved the a 100-year old mathematical puzzle (building on work of Thurston and Hamilton) but turned down the prestigious Fields medal and the million dollar Millenium prize (something no one has done before). This was because he was independent minded, in the New Yorker article Manifold Destiny, he was quoted as saying “Everybody understood that if the proof is correct then no other recognition is needed.” Talking about ethics in mathematics he said “It is not people who break ethical standards who are regarded as aliens, it is people like me who are isolated.” Or there’s Alexander Grothendieck who was deeply influential in mathematics, but left the field to pursue anti-militarist political objectives and suppressed publications of his material. A tribute from Ricardo Nirenberg gives one perspective on Grothendieck; which I can’t summarise but it’s fair to say that being independent minded wasn’t easy for him. Another example of originality of thought is Georg Cantor whose work on transfinite numbers, now considered brilliant, was ridiculed and suffered extreme depression.\nHaving a broad mind view, being creative and doing novel things are great, but being really independent minded requires courage. It can alienate from the people around and lead to great personal suffering. Most of the time it would go unnoticed. Building on other people’s ideas, and integrating ideas from different communities is a much easier way to be inventive (but not revolutionary)."
  },
  {
    "objectID": "vmemm-using-all-ram/index.html",
    "href": "vmemm-using-all-ram/index.html",
    "title": "Why is Vmemm Using All My Memory?",
    "section": "",
    "text": "Vmemm is the process associated with virtual machines on Windows. I’m using WSL2 and Docker (through WSL2), and so all their memory appears on Vmemm. But I had no Docker containers running, and free in WSL2 said almost all the memory was available.\nIt seems that when Linux under WSL2 frees memory the cached memory isn’t made available back to Windows. So whenever I ran heavy docker containers the memory wouldn’t be available back in Windows anymore.\nThere’s a long WSL issue on Github addressing this; there’s no solution but there’s a workaround. You can always free up the memory by restarting WSL2; using wsl --shutdown (e.g. using Command Prompt or Powershell) and then starting up the process again. But for a longer term solution you can limit the amount of memory available to WSL using .wslconfig.\nThe file at C:/Users/<user name>/.wslconfig gives configuration options for WSL, and in particular you can set a maximum memory and number of processors available to WSL. The full configuration syntax is in the documentation. In my case 4GB was a good tradeoff so I updated the file as follows:\n[wsl2]\nmemory=4GB # Limits VM memory in WSL 2 to 4 GB\nprocessors=2 # Makes the WSL 2 VM use two virtual processors\nLimiting the memory available to WSL2 in the wslconfig at least stops it from bringing my whole machine to a crawl."
  },
  {
    "objectID": "linux-find-installed-file/index.html",
    "href": "linux-find-installed-file/index.html",
    "title": "Finding Files Installed in Ubuntu and Debian",
    "section": "",
    "text": "dpkg -L git | grep prompt\nDebian and its derivatives such as Ubuntu you can use apt to manage packages (e.g. apt upgrade, apt install). However apt is just a thin layer over dpkg that does useful things like resolving dependencies and downloading files. If you want to get low level with packages in Debian derivative distriubutions, look into dpkg.\nFrom the man page --listfiles or equivalently -L lists all files installed from a package. So dpkg -L git shows all files installed in the git package, including the prompt. We can then use grep to filter the results.\nIn Arch Linux it’s pacman -Ql git. For systems such as Fedora using yum it’s repoquery -l git."
  },
  {
    "objectID": "collecting-training-data-whatcar/index.html",
    "href": "collecting-training-data-whatcar/index.html",
    "title": "Importance of Collecting You Own Training Data",
    "section": "",
    "text": "External data sources are extremely convenient for training a model as they can often be obtained much more cheaply than curating your own data. But the data will almost always be different to what you are actually performing inference on, and so you’re relying on a certain amount of generalisation. While techniques like data augmentation can reduce the gap ultimately the best way to train a very good model is to also add your own labelled data.\nUsing external data gets a good start, but it doesn’t work as well on out of domain data. For Whatcar I used a Resnet50 model that was pretrained on Imagenet, and then added a custom classification head and fine tuned it on a few thousand example images of Australian car makes and models (for the top 400 makes and models). This definitely works reasonably well; if I’m careful to frame the car properly in a landscape image it correctly predicts the model about half the time. However if there’s some objects in the background, or I take the photo in portrait then it does pretty poorly. This is to be expected since most labelled images of cars will be well framed in landscape; these photos are out of domain.\nFurther fine tuning the model on real images I’ve taken should get better results. In the Don’t Stop Pretraining paper it was shown for NLP models that the best outcomes came from training on a massive corpus of language data, then further training it on a large amount of in domain data and then further training again on the specific data to be classified. What I’m proposing here is a very close analogy; with Imagenet being the massive dataset, external data like VMMRdb being the in domain data and finally using the images taken directly. The difference with NLP is they use language models all the way through; here we’re using different classification models. It would be interesting to try self supervised computer vision methods to see if they could make it even more data efficient.\nSo to this end I needed to make it easy to collect data for further use. I made a simple endpoint to my server where an image can be taken and annotated with a make and a model from dropdowns and then uploaded. The image is uploaded to a blob store and the annotation and metadata, such as image blob store location and uploader information, is stored in a database. I can then manually collect and label data on my phone; it’s ideal to do this at the same time because I can manually spot the badge which may not be visible from the photograph.\nI’m going to continue collecting training data and test whether it can make the model better. One limitation of collecting all the data on my mobile phone is I may overfit to the lens of my camera; it may not generalise well to other cameras. It may also overfit to cars in my survey area, I may inadvertently capture those cars or types of cars, many times over. Ideally I’d get lots of annotations from different people on different devices in different locations, and if I had a commercial application I would pay for these.\nThis model definitely works in cases other than car classification. The benefit of having a server is you can get domain experts in the field to annotate the data on the spot.\nThere could be more time efficient methods of obtaining lots of training data. For example taking a video walking around each car would get a lot of different angles. But it would require a lot more storage, transfer and processing."
  },
  {
    "objectID": "australian-deathographics/index.html",
    "href": "australian-deathographics/index.html",
    "title": "Australian Deathographics",
    "section": "",
    "text": "The Australian Bureau of Statistics has population by age, and the Australian Institute of Health and Welfare have Mortality Over Time and Regions (MORT) which summarises the current probability of death by age range. Here is a super summarised version of this data:\n\n\n\n\n\n\n\n\n\n\nAge\nPopulation\nDeath Rate\nPopulation Deaths\nFraction of Deaths\n\n\n\n\n0-19\n25%\n0%\n0%\n0%\n\n\n20-39\n29%\n0%\n0%\n0%\n\n\n40-59\n25%\n0%\n0%\n0%\n\n\n60-79\n17%\n1.5%\n0.25%\n42%\n\n\n80-84\n2%\n5%\n0.1%\n17%\n\n\n85+\n2%\n13%\n0.25%\n42%\n\n\n\nNotice that population deaths is the product of the previous two columns and fraction of deaths is the normalised population deaths. The life expectancy is the sum of the product of fraction of deaths with age; it’s hard to estimate because of the 85+ band but it’s likely in the 80-84 range. It’s hard to estimate the average age of death from bands (life expectancy), it’s the product of The median age of death is in the range 80-84 as well. These are consistent with an Australian life expectancy of 82 years.\nMultiplying and summing population with death rate gives an overall probability of death of 0.6%.\n\nUnderstanding the errors\nSo why did a rate of 1/life expectancy, give 1.2% and not 0.6%? And why are 300,000 babies born and the total fertility rate is less than 2 per woman, yet there are only 150,000 deaths so it seems like the population is increasing?\nIt comes down to demographics; if we assume a constant birth rate and use the current survival rates we get a very different picture of population. The 80-84 age band and 85+ age band are both 4%, about twice as big as we actually see, and the 60-79 age band is 20% of the population, a little bigger than the 17% we see. This would get us closer to a death rate of 1% in the ballpark of 1/life expectancy.\nSo why are there so few people above 80? It doesn’t quite make sense to be World War II since that started about 81 years ago, and couldn’t account for fewer people 82 and older. Another possibility is that the survival rates have changed a lot in the last couple of decades and so we have fewer over 80 year olds. Yet another is we have more migrants in the 20-39 age range outpacing older age brackets. I don’t know if any of these explanations hold water, or if it is something else entirely.\nI think I understand now why I estimated the death rate so poorly, but I still don’t have a simple model I could take to other countries to estimate their deaths. This would require better lumping and ways of estimating elderly populations (relative to age of death in that place and time)."
  },
  {
    "objectID": "people-patience/index.html",
    "href": "people-patience/index.html",
    "title": "Being Patient with People",
    "section": "",
    "text": "This isn’t a helpful reaction; getting short tempered won’t help resolve the problem. I haven’t taken the time to understand the speaker and their perspective. Why do they think this is the right thing to focus on? Are they seeing something that I’m missing?\nMoving from frustration to curiosity is the first step towards progress. Asking the right questions can help align our viewpoints. Why do we think the focus area is different? Do we have the same understanding of the objectives? What do they know about my biggest concerns? What are their biggest concerns?\nOnce the differences have been identified we can then work through the issue. Maybe we need to agree on the objectives, or find a stakeholder to clarify. Sometimes I need to point to the data about what the real concerns are, sometimes I need to be told what the data shows the concerns are. Depending on how different our viewpoints are this can take some time, but once the difference is understood we can work towards it.\nI’ve also often been on the other side of the meeting. Trying to give an update while knowing I don’t really understand the viewpoint of the stakeholders. In these cases I need to be clear about what I think the objective are, the approach I’m taking and why, and avoid the detail. By picking a clear direction on the areas I’m not sure about it makes it easier to have the conversation on what we should be focusing on. It’s hard because I feel stupid when I know I take a risk like this, but it leads to the best outcomes."
  },
  {
    "objectID": "plotting-bayesian-parameters-tidyverse/index.html",
    "href": "plotting-bayesian-parameters-tidyverse/index.html",
    "title": "Plotting Bayesian Parameter Distributions with R Tidyverse",
    "section": "",
    "text": "Suppose we’ve got an rstanarm model like this:\n\nmodel <- rstanarm::stan_glm(Petal.Width ~ Sepal.Length + Sepal.Width + Species, \n                            data=iris,\n                            refresh=0)\n\nWe can access all the coefficients from the posterior draws using as.matrix. With a few standard transformations we can plot the distribution of each of the coefficients.\n\nlibrary(magrittr)\n\nmodel %>%\n# Convert it to a data frame of coefficients\nas.data.frame() %>% \n# Remove the intercept and standard deviation estimates\ndplyr::select(-`(Intercept)`, -`sigma`) %>% \n# Pivot into long form, one row per variable and estimate\ntidyr::pivot_longer(dplyr::everything()) %>% \n# Reorder the variables for plotting in order of descending (median) value\ndplyr::mutate(name = forcats::fct_reorder(name, dplyr::desc(value))) %>% #\n# Draw a violin plot, with 20th and 80th percentiles marked\nggformula::gf_violin(value ~ name, draw_quantiles=c(0.2, 0.8)) +\n# Draw the variables on the vertical axis\nggplot2::coord_flip() +\n# Chang the theme\nggplot2::theme_minimal()\n\n\n\n\nWhile this is simpler and prettier with Bayesplot, I think it really shows the flexibility of the tidyverse.\n\nbayesplot::mcmc_areas(model, \n                      regex_pars=c(\"Sepal.Length\", \"Sepal.Width\", \"Species.*\"))\n\n\n\n\nThe least flexible piece in this is ggplot2. If I want to use a density plot rather than a violin plot, the only way I can work out how to do this is with faceting which makes the plots too small and hard to compare. I need to write custom code to only plot the top halves of the violin. And whenever I use a bar graph in ggplot2 I always have to reread how to use stat. Nevertheless there’s sufficient existing plots to plot almost anything, and the environment is remarkably flexible for solving data problems like this."
  },
  {
    "objectID": "polynomials-to-transcendental-numbers/index.html",
    "href": "polynomials-to-transcendental-numbers/index.html",
    "title": "From polynomials to transcendental numbers",
    "section": "",
    "text": "As we were taught in high school the roots of the quadratic equation \\(a x^2 + b x + c=0\\) can be found by completing the square, giving \\(x = \\frac{-b \\pm (b^2-4ac)^{1/2}}{2a}\\) .\nThere are some problems with implementing it: firstly we need to be able to take square roots (that is solve \\(x^2=a\\) ). This isn’t too bad there are lots of algorithms, geometrically we can do it with a compass and a straightedge (indeed this is where the first examples of irrational numbers came from) and recently it’s been done using DNA. A more serious problem is round-off error: if \\(b^2 \\gg ac\\) then \\(|b| \\approx |\\sqrt{b^2-4ac}|\\) and so if you only calculate this using a few decimal places there will be significant roundoff error in \\(b - \\mbox{sgn}(b) \\sqrt{b^2-4ac}\\) (whether this is important depends on the application). A simple workaround is to notice that at most one of the roots will have this roundoff error and the product of the roots is c so we can use this to find the other one.\nA much more detailed analysis of algorithms for solving the quadratic are in a pairof articles by James Blinn. In fact he also has a series of articles on solving the cubic using the analagous cubic formula. One important thing to draw from this is the amount of work involved in recasting the classical quadratic and cubic formulae in a numerically stable way. Also as Blinn points out the utility of these methods depends heavily on your tools and application: depending on your computer (whether it be a pen and paper, an old fashioned calculator, a GPU or a molecular computer) it may be faster to iterate a solution than solve using a formula, and whether it makes a difference depends on how many polynomial equations you have to solve and how long each takes to solve. [ There are specific zero finding methods for polynomials, for example the Jenkins-Traub algorithm that will converge much faster than generic methods such as Newton’s or gradient methods]. To solve a single cubic (or even a couple hundred) on a modern PC you wouldn’t notice a difference, but in some graphical applications you may need to solve thousands a second.\nIncidentally Felix Klein found the quintic equation was tied up with the geometry of the icosahedron. In 1989 it was shown further that the quintic could be solved by an iterative algorithm, which (I think) means that to each quintic is assigned a rational function and the roots can be found by repeatedly applying the rational function. The whole kit – from the insolubility by radicals to this algorithm – is explained in detail in this excellent set of notes. (I would love an excuse to implement this algorithm in a stable manner).\nWhy limit ourselves to radicals? Why not consider \\(\\tan(\\pi/15)\\) a perfectly good solution (it’s in a familiar ‘nice’ form). This is the type of question Timothy Chow asks. More precisely he looks at the ‘EL numbers’, the smallest subfield of the complex numbers closed under exponentiation and its compositional inverse, taking logarithms and asks what sorts of equations can you solve with it. The answer isn’t known, it lies in transcendental theory.\nIt’s possible, once you know the trick to show \\(e\\) and \\(\\pi\\) are transcendental. In fact the Lindemann-Weierstrass theorem states that given linearly independent algebraic numbers over the rationals their exponentials are linearly independent over the rationals. The Gelfond-Schneider theorem states that all values of \\(\\alpha^\\beta\\) are transcendental for \\(\\alpha \\neq 0,1\\) and \\(\\beta\\) irrational. There are some more theorems and a handful of other transcendental numbers known but a great deal is still unknown, for example are \\(e + \\pi\\) and \\(e \\pi\\) transcendental. The constant problem of determining when a given transcendental function is zero (useful for computer algebra) has only been solved or proven algorithmically undecidable in certain cases.\nA huge conjecture in transcendence theory is Schnaul’s conjecture: given n complex numbers linearly independent over the rationals, then some collection of n terms taken from these numbers and their exponentials are algebraically independent. It would have strong implications: the Lindeman-Weirstrass and Gelfond-Schneider theorems are special cases, it would imply that Euler’s identity \\(e^{i \\pi} + 1 = 0\\) is (in an appropriate sense) essentially the only algebraic relationship between \\(\\pi\\) and \\(e\\) , and would bring us closer to understanding which algebraic and transcendental equations are solvable in the ‘EL numbers’ and their closure (the elementary numbers).\nOf course one often talks as well about elementary functions (functions generated by constant functions, identity function and exponentiation under addition, multiplication, composition and their inverse operations) and it’s often said that \\(\\int_0^x e^{-t^2} \\mathrm{d}x\\) isn’t elementary. This apparently can be proved using Picard-Vessiot theory and differential galois theory. This is very closely related to my previous posts on integrable systems, Lie algebras and symmetries.\nOne last trick to leave you with. Solving one linear equation is easy, and in first year mathematics courses we are taught how to solve systems of linear equations. Since we’ve discussed solving one polynomial equation it’s natural to ask how would one solve a system of linear equations… one approach is a Gröbner basis"
  },
  {
    "objectID": "australian-births/index.html",
    "href": "australian-births/index.html",
    "title": "Australian Births",
    "section": "",
    "text": "Australia has 25 million people. I would estimate the birth rate is 0.8 children per person; I think it’s slightly less than one. These children are born across life, which is about 80 years.\nSo a really crude estimate for annual births is 25 million people times (0.8 children per person lifetime) divided by 80 years per lifetime. This is 20 million divided by 80 which is 250 thousand. So I would estimate 250,000 babies are born in Australia each year.\nIt would be interesting to use this to estimate number of birthing suites."
  },
  {
    "objectID": "reference-set/index.html",
    "href": "reference-set/index.html",
    "title": "Reference Sets as Pervasive Models",
    "section": "",
    "text": "Do you want complications related to just the specific procedure treating your condition, or for all procedures on that area of the heart for a variety of conditions? And do you want the complications for all procedures worldwide, or just in your country, or perhaps just in this hospital, or under this surgeon? Do you want to segment it by sex, age group and BMI category?\nThe more specific you make the comparison set the more related the data is to your specific outcome, but the less data you have left. Just looking at all surgeries from this particular surgeon on this particular condition for someone with a similar medical history like you may only leave 10 or 20 examples, and you can’t judge the probabilities of a serious complications that happen only in 3% of cases. On the other hand including surgeries for people much older or less healthy than you, or from hospitals using older, and more risky, procedures could make the incidence of bad outcomes appear more likely.\nIn the end you need to use judgement in picking a good reference set. You should identify the factors that are most strongly associated with a poor outcome, and only filter on those. That requires an understanding of the underlying mechanisms of the risk.\nIn the end this is just a constant model for classification, where we’re trying to pick the cases to include carefully. If you want to use the information more efficiently you need to build a model on the data. What does the incidence of complications on elderly people tell you about the likelihood on younger people; it almost certainly tells you something, but excluding it treats it as if it tells you nothing."
  },
  {
    "objectID": "spectra-of-atoms/index.html",
    "href": "spectra-of-atoms/index.html",
    "title": "Spectra of atoms",
    "section": "",
    "text": "To some degree all of these questions require knowing the spectra of atoms, which can in theory be calculated by Quantum mechanics. However the calculations of these spectra for arbitrary systems from first principles is prohibitively difficult and computationally intensive (which is why techniques such as Density Functional Theory are used).\nThis post will roughly outline to calculate the spectrum of the smaller atoms by explicitly diagonalising a matrix, whose elements are simple combinatorial quantities.\nThe non-relativistic Hamiltonian in the Born-Oppenheimer approximation of an n-electron atom in SI units is given by \\(H = \\sum_{i=1}^{n} \\left( \\frac{p_i^2}{2m} - \\frac{Ze^2}{4 \\pi \\epsilon_0} \\frac{1}{r_i} + \\frac{e^2}{4 \\pi \\epsilon_0} \\sum_{j > i} r_{ij} \\right)\\) where \\(p_i\\) is the momentum of the ith electron, \\(r_i\\) is its distance from the nucleus, \\(r_{ij}\\) is the distance between the ith and jth electron, m is the mass of an electron, Z is the charge of the nucleus, and e is the charge of an electron.\nTo simplify matters choose units such that \\(\\frac{e^2}{4 \\pi \\epsilon_0} = 1\\) , \\(m = \\frac{1}{2}\\) and \\(\\hbar = 1\\) , these will be used in the rest of this article. Then \\(H = \\sum_{i=1}^{n} p_i^2 - \\sum_{i=1}^{n} \\frac{Z}{r_i} +\\sum_{i=1}^{n}\\sum_{j>i}^{n} \\frac{1}{r_{ij}}\\) . The terms correspond to the kinetic energy of the electrons, the electron-atom interaction, and the electron-electron interactions respectively. If we neglect the third term we recover the equation of a Hydrogenic atom which can be solved algebraically.\nOur approach is to calculate the the elements of the Hamiltonian matrix in the Hydrogenic basis. We can then explicitly diagonalise the matrix in this basis; if the electron-electron term is small the matrix will be almost-diagonal. I will only cover the case of the bound states; the unbound states do need to be considered at a future point, but at least near the ground state their contribution should be negligible.\nThe Hydrogenic atom can be simultaneously diagonalised in a number of different basis sets (corresponding to different coordinate systems); we need a basis whose symmetry is preserved by the perturbation. Since the perturbed Hamiltonian is spherically symmetric, we choose the basis H, L, Lz where the bound states are characterised by the quantum numbers n, l, m, s ( \\(n \\geq 1\\), \\(0 \\leq l < n\\), \\(|m| \\leq l\\), \\(s = \\pm \\frac{1}{2}\\) with corresponding eigenvalues \\(\\frac{Z^2}{2n^2}\\), l(l+1), m. The electronic states of an n-electron atom, neglecting electron-electron interactions, are then the antisymmetric tensor products of these states.\nWe now proceed to calculate the matrix elements of the full Hamiltonian in this basis. The only non-trivial part of the calculation are the terms \\(\\wedge_{i=1}^{N} \\langle n_i, l_i, m_i, s_i | \\frac{1}{r_{st}} \\wedge_{j=1}^{N} | n_j, l_j, m_j, s_j \\rangle\\) . The spins and terms i, j not equal to s, t factor through, giving Kronecker deltas. The remaining calculation is \\(\\langle n_1, l_1, m_1, n_2, l_2, m_2 | \\frac{1}{r_{ij}} | n'_1, l'_1, m'_1, n'_2, l'_2, m'_2 \\rangle\\) . Since the term commutes with \\(L_i\\) and \\(L_j\\) , it is also proportional to \\(\\delta_{l_1}^{l'_1} \\delta_{l_2}^{l'_2} \\delta_{m_1}^{m'_1} \\delta_{m_2}^{m'_2}\\). This term commutes with \\(L_i + L_j\\), but not with \\(L_i\\) and \\(L_j\\) separately (intuitively rotating just one of the two electrons will change the distance between them).\nIntegrate in spherical coordinates over first \\(r_1\\) , then \\(r_2\\) setting the z-axis of the second coordinate system along the vector \\(r_1\\) . Then \\(\\theta_2\\) is the angle between the two electrons at the nucleus, and the integrand is \\(\\frac{1}{r_{12}} = \\frac{1}{\\sqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \\cos(\\theta_2)}}\\) , and consequently the first solid angle integral is trivial.\nThus we just need to evaluate \\(\\int d\\Omega Y_{l_2}^{m_2}(\\theta, \\phi) {Y_{l_2}^{m_2}}^*(\\theta, \\phi) \\int_{0}^{\\infty} dr_1 r_1^2 R_{n_1}^{l_1}(r_1) {R_{n'_1}^{l_1}}^*(r_1) \\int_{0}^{\\infty} dr_2 r_2^2 R_{n_2}^{l_2}(r_2) {R_{n'_2}^{l_2}}^*(r_2) \\frac{1}{\\sqrt{r_1^2 + r_2^2 - 2 r_1 r_2 \\cos(\\theta)}}\\) (notice that the integral must be invariant under interchange of all 1 labels with 2 labels; in practice we make the choice that makes the integral easiest).\nWe now separate the \\(r_2\\) integral into two regions; where it is less than \\(r_1\\) the \\(\\frac{1}{r_{12}}\\) term can be expanded as \\(\\sum_{t=0}^{\\infty} \\frac{1}{r_1} \\left( \\frac{r_2}{r_1} \\right)^t P_t(\\cos(\\theta))\\) where \\(P_t\\) is a Legendre Polynomial, and in the other region we switch \\(r_1\\) with \\(r_2\\) .\nThe integral then becomes \\(\\sum_{t=0}^{\\infty} \\int d\\Omega Y^{m_1}_{l_1} (\\Omega) Y^{-m_1}_{l_1}(\\Omega) \\sqrt{\\frac{4 \\pi}{2t+1}} Y^0_t(\\Omega) \\int_0^{\\infty} dr_1 R_{n_1}^{l_1}(r_1) {R^{l_1}_{n'_1}}^*(r_1) r_1^{2+t} \\int_0^\\infty dr_2 r_2^{1-t} R_{n_2}^{l_2}(r_2) {R_{n'_2}^{l_2}}^*(r_2)\\) plus the integral switching r1 with r2 (after a fiddling change of coordinates).\nThe angular integral is a combinatorial quantity, which can be expressed in terms of the Clebsch-Gordan coefficients. It can be expressed using recurrence relations which can be used to compute this part of the integral. [In fact there is an explicit combinatorial representation, although in practice it would be quicker to compute it using recurrence.]\nThe inner radial part of the integral can be calculated by expanding the Legendre polynomials as a power series and using the relation \\(\\int_R^\\infty e^{- \\alpha r} r^k = \\frac{e^{- \\alpha R}}{\\alpha^{k+1}} \\sum_{j=0}^{k} (R \\alpha)^j \\frac{k!}{j!}\\) , and the outer part of the integral can then be calculated using this relation again with R=0. This is simply a combinatorial factor than needs to be determined.\nThus once we have evaluated these combinatorial quantities, and combined them all to get an expression for the matrix elements of the total Hamiltonian H we can truncate it to a finite basis, and then diagonalise it computationally. It is a very interesting question as to how the truncation affects the eigenvalues."
  },
  {
    "objectID": "python-inequality-chaining/index.html",
    "href": "python-inequality-chaining/index.html",
    "title": "Python Inequality Chaining",
    "section": "",
    "text": "This wasn’t obvious to me because a lot of programming languages treat these associatively, so that a <= b < c may resolve to (a <= b) < c. This is very dangerous if boolean (True or False) are coerced to integers (1 or 0) because it may look like it works but give the wrong results.\nHowever Python’s documentation explains that a chained comparison like a <= b < c is translated to (a <= b) and (b < c), which is exactly what you expect (with b only evaluated once, in case it has side effects). This is a neat trick that can make code a bit easier to read (though you have to be careful if you’re switching between languages!)"
  },
  {
    "objectID": "recurring-article-ideas/index.html",
    "href": "recurring-article-ideas/index.html",
    "title": "Some Ideas for Recurring Articles",
    "section": "",
    "text": "Using a structured recurring segment with a familiar pattern and style gives a structured environment to be creative in. It’s really hard to be creative in a completely unstructured and original way, like Monty Python was, since there are a so many options. While done badly recurring segments can feel like canned time fillers, done well they allow improving the material.\nI’ve been writing daily articles for over 6 months, but I’m about to go through a period where I’m going to have much less time and energy for writing. So I want to think of some possible recurring articles that I can write to try to reduce the writing time from hours to under 15 minutes.\n\nTextbook exercises\nI’ve got handwritten solutions to exercises in the Structure and Interpretation of Computer Programs (SICP), All of Statistics and The Art of Insight in Science and Engineering lying around. I could write many of these up pretty quickly, and they’re easy to extend on if I’ve got more time with diagrams or examples.\nThere’s also the possibility of trying to very slowly work through new books. I could just write a short summary of a page, or an idea each day. Easier ones I have on my radar are How to Design Programs (similar to SICP) and Street Fighting Mathematics (similar to Art of Insight), or the Emacs Lisp book. Some harder ones are Jurafsky’s Speech and Language Processing, Structure and Interpretation of Classical Mechanics, Elements of Statistical Learning and Krushke’s Doing Bayesian Data Analysis. I’d have to experiment to find if any of these can be done in bit sized chunks.\n\n\nEnvelope Estimates\nI really enjoyed the sort of back of the envelope estimates in the Art of Insight and would really like to practice more of these. It’s sometimes hard to think of good problems, but they can be fairly simple to solve and are generally interesting.\n\n\nWriting Summaries\nAnother idea is summarising an insight from an article, book or podcast. If I’m consuming that material anyway writing up a note makes the consumption more meaningful and memorable. However to keep it short I have to just summarise it and fight the urge to do deeper research.\n\n\nEditing existing articles\nThis won’t help me write new articles, but I could spend time editing and compiling existing articles. I currently don’t edit the articles, but now I have enough material that I could begin to compose them into something quite useful.\n\n\nSmall Problems\nProject Euler has many little problems; some of them are very hard but a first step towards a solution could be written up pretty quickly. There are similar problems on /r/dailyprogrammer, or Project Rosalind.\n\n\nBuilding a project\nI’ve really enjoyed exploring building a jobs pipeline. Building some codebase, an analysis, or building a model is a really good way to write an article. However I can’t think of a project that’s small enough but still interesting. Maybe building off something like Project Euler I could build a reasonable mathematics library from scratch (for fun)."
  },
  {
    "objectID": "gridded-population-world/index.html",
    "href": "gridded-population-world/index.html",
    "title": "Gridded Population of the World",
    "section": "",
    "text": "You can immediately see a strip through the north of India, Pakistan and Bangladesh that is incredibly dense. The north-east of China and the island of Java in Indonesia are also very dense. There are smaller areas; in Africa high density in the small countries of Rwanda and Brunei, much smaller than neighbouring countries. There’s a high concentration in Cairo and then following down the Nile, in an otherwise sparse country.\nEqually interesting are the sparse areas, which seem to correspond to deserts and extremely cold areas. Most of Australia is empty, save a few small dots primarily along the coast. Canada is similar, with some population pockets in the south. There’s a huge empty strip through the north of Africa, and in the south west. Similarly the majority of Russia in the east, down through Mongolia and north eastern China are sparse.\nThe extreme high end of density is 1000 persons per square kilometre, or around 1000 square meters per person. This gives around 30 meters by 30 meters per person, including housing, streets and shopping areas. I personally find that difficult to understand.\nIt would be great to gradually upsample this data to get a better idea of larger regions for estimates. There are tutorials and videos on working with this data in QGIS."
  },
  {
    "objectID": "python-html-parser/index.html",
    "href": "python-html-parser/index.html",
    "title": "Python HTML Parser",
    "section": "",
    "text": "A lot of information is embedded in HTML pages, which contain both human text and markup. If you ever want to extract this information, don’t use regex use a parser. Python has an inbuilt library html.parser library to do just that.\nThe excellent html2text library uses it to parse HTML into markdown, which you can use for removing formatting. However for your own purposes you can use a similar approach to build a custom parser by subclassing HTMLParser.\nHere’s a simple example of a parser that tries to convert HTML to plain text. You would use it like this:\nWhen you feed HTML to a HTMLParser it executed handle_starttag whenever it encounters a new open tag, handle_endtag whenever it encounters a new close tag, and handle_data whenever it encounters data between tags.\nTo insert newlines whenever we hit a block level tag we can implement a custom handle_starttag, that adds a newline to an output method.\nIn this case we don’t need to do anything special with endtags, but we do need to output all data. We will strip off newlines, because they won’t be shown in HTML output.\nThe output method is one we need to add ourselves; we can append the output to internal state in a list called outdata. We add to a list rather than append to a string because Python strings are immutable which means we’d need to create a whole new string object when we append a single character which is very inefficient if the string gets large.\nOf course we need to initialise self.outdata to an empty list.\nFinally we can provide a nice interface that does all the work when we call converter(html) by implementing the __call__ magic method.\nThat’s all there is to implementing a simple HTML transformation in Python. If you wanted more complex transformations you would need to track more pieces of state; the html2text code is a good example of how this can work."
  },
  {
    "objectID": "python-html-parser/index.html#full-example-listing",
    "href": "python-html-parser/index.html#full-example-listing",
    "title": "Python HTML Parser",
    "section": "Full example listing",
    "text": "Full example listing\nHere’s an example listing of the HTML Parser. The functionality is very basic; it’s likely to produce way too much whitespace in certain cases, and fail on many HTML documents. However it’s a reasonable starting point for building a customer HTML transformation function.\nBLOCK_TAGS = (\n  'html', 'p', 'br',\n  'li', 'ul', 'ol',\n  'blockquote',\n  'table', 'tbody', 'tr',\n  )\nINLINE_TAGS = (\n  'strong', 'ul', 'em', 'i', 'b',\n  'a', 'figure', 'img',\n  'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n  'td',\n  )\n\nfrom html.parser import HTMLParser\nclass HTMLTextConverter(HTMLParser):\n    def __init__(self) -> None:\n        self.outdata = []\n        super().__init__()\n\n    def __call__(self, html):\n        self.feed(html)\n        output = ''.join(self.outdata).strip()\n        self.reset()\n        return output\n\n    def reset(self):\n        super().reset()\n        self.outdata = []\n\n    def output(self, data):\n        self.outdata.append(data)\n\n    def handle_starttag(self, tag, attrs):\n        if tag in BLOCK_TAGS:\n            self.output('\\n')\n        elif tag in INLINE_TAGS:\n            pass\n        else:\n            raise ValueError('Unexpected tag %s', tag)\n\n    def handle_endtag(self, tag):\n        pass\n\n    def handle_data(self, data):\n        self.output(data.strip('\\n'))"
  },
  {
    "objectID": "dnn-block/index.html",
    "href": "dnn-block/index.html",
    "title": "Deep Neural Networks as a Building Block",
    "section": "",
    "text": "Josh Tenenbaum gave an excellent keynote at ACL 2020 titled Cognitive and computational building blocks for more human-like language in machines. He talked about understanding how humans grasp meaning through language by combining statistical language models (currently deep neural networks), grounding in perception and action (through physical and social models) and semantic parsing (to extract meaning from the sentences).\nA series of papers he talked about is on CLEVR, a dataset with images of objects and questions like “What size is the cylinder that is on the left of the brown metal thing that is left of the big sphere”. To solve these kinds of problems in a paper Neural Symbolic VQA they extract the objects from an image with CNNs and parse the question with LSTMs and bring them together to form a program that they execute to answer the question. The Neuro-Symbolic Question Learner creates a variant of this approach that doesn’t require explicit supervision (building the intermediate representations), but still extracts features from the image and parses a semantic representation of the question text.\n\n\n\nImage from CLEVR paper\n\n\nIn general neural networks do amazingly well at detecting and identifying objects in images (e.g. YoLo) and at syntactic parsing of sentences (e.g. as in stanza). But while neural networks are powerful their failure modes can be hard to understand and debug, even with all the work on attention and explainability toolkits. Putting a classical linear model or decision tree on top of features extracted with a neural network could give a simple model with more control and interpretability.\nAnother issue is if you’re dealing with a very different sort of task (so you can’t just apply transfer learning) the amount of data for an end-to-end neural model is huge. However you could use a more tailored model, such as a Bayesian model, on top of features extracted by a neural network to make better use of the training data available.\nThere are definitely some questions about what the effective ways to extract this information are for a usecase; whether lists of categorical objects or a vector embedding. And how to determine when you’re better off writing custom components rather than trying to train a high parameter model like a neural network. But I think there’s a lot of potential in the space of building more traditional machine learning models on features extracted from neural networks. An example of this is extracting text from the dependency parse."
  },
  {
    "objectID": "changing-python-analytics-code/index.html",
    "href": "changing-python-analytics-code/index.html",
    "title": "Changing Python Analytics Code",
    "section": "",
    "text": "You’ve got a Python analytics process and have to make a change to how it works. You trace the dataflow and work out how you need to restructure the code to make the change. But how do you only make the change required without breaking something else?\nA common technique I’ve used is to make the changes, run the pipeline, then compare diagnostics before and after. Often these processes produce a report, or test set metrics or a dataset that can be compared. If the process runs and the outputs aren’t too different then I probably haven’t broken anything.\nHowever this process is typically very slow. Analytics pipelines can take a long time to run, comparing the outputs can be very time consuming, and deciding whether the changes are too severe can be error prone. Because it takes a long time to evaluate changes, it’s not worth making small changes; the overhead in checking them is too large. So a large change-set is made, evaluated, and then when there are errors a long time is spent tracing and debugging them. This is a slow and frustrating process.\nHow can we shorten the feedback loop and make things faster?\n\nUnit Tests\nUnit tests are the most effective tool to make changes faster. If the code you’re changing is under test, you can make a small change and run the tests in a few seconds. If the tests pass you’ll get a lot of confidence that you haven’t broken it, and can go on to the next change. If they fail you know exactly where the error occurred and can quickly isolate it.\nHowever what if the code isn’t tested? Automatic refactoring tools can help to safely change the code. In particular extract method is very useful for turning part of a large sequence of imperative statements into a separate function that can be tested. You can then write the test before you make the change, and then make the change.\nYou may need to change and restructure tests as you change the code, but you can separate out making the change from checking it acts as expected.\nExplicit unit tests also make for great documentation. Property based tests, using Hypothesis, can test the code a lot more thoroughly, but tend to be a bit slower. Bigger tests such as integration tests (which may do things like connect to the database, or hit APIs) will tend to be too slow to run on this cycle, but can be run intermittently. Finally regression tests, seeing how the output has changed after a full run can give a lot more confidence, but it much slower. It may be worth running this after all the small changes, that have passed the unit tests, to further confirm it acts as expected.\n\n\nLinters\nHave you ever run a pipeline for a long time, only for it to fail halfway through because of a typo, or a missing import? Pylint and pyflakes are tools that can quickly check your code and catch many of these kinds of bugs. Many Integrated Development Environments have these kinds of tools built into them, but they’re fast enough it can be worth adding in a pre-commit hook or as a check in Continuous Integration.\nThe easiest way to run pylint to just catch errors is pylint -E <path-to-module-or-script>. The warnings are often useful too, which can be added using pylint --disable=R,C <path-to-module-or-script> It can also do style checking, but requires quite a bit of configuration because the defaults are way too strict.\nPyflakes is much faster than pylint, but generally seems to catch less errors. If you’re interested in checking style at the same time you can use it in flake8.\nThese tools don’t check a lot, but they can very quickly find issues so are worth having in the toolbox.\n\n\nType Checking\nType checking is a useful tool to make sure your code fits together; that the right kinds of arguments are being passed. It’s easy when refactoring to make mistakes here that type checking can fix.\nEven though Python has type hints, and ways to check them like mypy but they are of limited usefulness in code that used pandas and numpy heavily. Unfortunately Pandas doesn’t support annotating dtypes which means a lot of real issues will be missed that have to be caught in tests. You could wrap types like thinc to check things like dimensions in numpy, but it’s still easy to make errors.\nWhile more Python libraries are incorporating types, you may still need to write type stubs to fill in missing types, which makes it even harder to get started with types.\nType checking is potentially worthwhile, and type annotations make good validation, once you’ve got a reasonable test suite and a linter in place.\n\n\nContract validation\nOne of the challenges with data analytics code is that both the code and the data can change. Even if you’ve got good tests to make sure the code works well, you can still get malformed data that you can only check at runtime to make sure you’ve got the right data. This is where runtime validations, or contracts, come into account.\nA contract is typically an assert statement in the code. You may want to make sure the data coming in is as expected, or the data coming out is as expected. It’s better to fail fast if something is seriously wrong than to produce the wrong data.\nOne tool in this space is pandera which lets you verify the schema of a dataframe. There’s also bulwark, and tdda that tries to discover the schema; more generally there is pydantic and marshmellow for checking data against a schema. For databases there’s great expectations but I’m not convinced that it’s better than just writing some checks by hand.\n\n\nGetting faster feedback cycles\nI’ve generally found the faster I find problems the faster I can change them. The more confidence I have that I will catch errors, the more aggressively I will change my code. Python by default doesn’t have much of a safety net, but with tests, linters, type checking and contract validation you can fail faster and be more confident the changes you’ve made haven’t broken anything."
  },
  {
    "objectID": "pybart/index.html",
    "href": "pybart/index.html",
    "title": "pyBART: Better Dependencies for Information Extraction",
    "section": "",
    "text": "I’ve seen that dependency based rules are useful for extracting skills from noun phrases and adpositions. But to get the long tail I tried to extract from conjugations and started writing some complex rules. An alternative would be to rewrite the dependency structure and then write simpler rules.\nThis is what pyBART does. Introduced by AllenAI in ACL2020 it comes with a paper, live demo and source code.\nWhen I was trying to extract job titles I wrote an ad hoc rule to rewrite e.g. “Director of Marketing” to “Marketing Director” This type of “Genitive Construction” is given an extra “compound” dependency (in green below) so the phrase can be extracted the same way.\n\n\n\nGenetive Construction\n\n\nThey try to make it easier to extract information by making the primary verb the root. For an indirect sentence like “Sam seems to like it” they move the root from “seems” to “like” and have “seems” as an event of “like”. To deal with copular sentences like “it is good” they add a phantom STATE node.\n\n\n\nEvents in pyBart\n\n\nThey also add “Uncertain” relations and “Alternate” relations to allow more recall in information extraction. This is really useful for common but not definite possibilities; and the rules could potentially learned with a machine learning model for specific corpora.\nThey’ve tried really hard to make it useful. It plugs into SpaCy, with Universal Dependendency Tagger, and so can be used with SpaCy’s dependency matcher.\nFor finding rules there is another useful AllenAI tool presented at ACL: Syntactic Search. There is a live demo where they show the new query language that is very easy to use and scalable. I tried to test my examples of searching for skills on Wikipedia, for example $experience :[t]in <>:[t]computing but the style of writing is so different on Wikipedia to job ads it’s not really useful. It’s not yet open source, as the code base is still a work in progress, so unless I send them a job corpora I can’t really test it out.\nI sat in on the Q&A session for pyBART at ACL2020. Most of it was linguistics discussion over my head, but I really enjoyed how friendly and open everyone was. This is a “practical” implementation, taking a small step away from pure syntax towards semantics. While all the rules are grounded in solid linguistics, some of the ideas like an “event” don’t exist for philosophical reasons in Universal Dependencies. This approach could be extended to other languages, but the rules (and some verb lists) would need to be appropriately changed to match that language.\nThere was also some interesting discussion about using higher trees rather than the “flat” projection of dependency triples for information extraction. I would have no idea how to do this though!\nThe rewriting rules are written in an ad hoc manner. It was suggested that a graph rewriting tool like OGRE (from a Phd Thesis) would allow the rules to be written by linguists, be more maintainable and less likely to interfere in weird ways.\nFinally there was some discussion of how it related to UDep Lambda (source code). The conclusion was UDep Lambda is much more theoretical, and harder for people to use in practice. But it sounds like an interesting (crazy?) idea to look more into later."
  },
  {
    "objectID": "extract-skills-1-noun-phrase/index.html",
    "href": "extract-skills-1-noun-phrase/index.html",
    "title": "Extracting Skills from Job Ads: Part 1 - Noun Phrases",
    "section": "",
    "text": "I’m trying to extract skills from job ads, using job ads in the Adzuna Job Salary Predictions Kaggle Competition. Using rules to extract noun phrases ending in experience (e.g. subsea cable engineering experience) we can extract many skills, but there’s a lot of false positives (e.g. previous experience)\nYou can see the Jupyter notebook for the full analysis."
  },
  {
    "objectID": "extract-skills-1-noun-phrase/index.html#analysing-the-results",
    "href": "extract-skills-1-noun-phrase/index.html#analysing-the-results",
    "title": "Extracting Skills from Job Ads: Part 1 - Noun Phrases",
    "section": "Analysing the results",
    "text": "Analysing the results\nLooking at the results from extracting the top fifty thousand job ads, the most common things it extracts aren’t skills but qualifiers like “previous experience”, “Proven experience”, “some experience”, and “demonstrable experience”.\nBy filtering with a blacklist of the most common qualifying words, and stop words (the, this, an) we get some kinds of fields of expertise:\n\nsales\nmanagement\nsupervisory\ncustomer service\ndevelopment\nsupervisory\ntechnical\nmanagement\ntelesales\nfinancial services\ndesign\nproject management\nretail\nbusiness sales\nSQL\nmarketing\npeople management\nSAP\nengineering\n\nWhile this is a good start, building a longer list requires. building a much longer blacklist of qualifier terms (e.g. proven, demonstrable, demonstrated, relevant, significant, practical, essential, desirable, …). The fact that these qualifier terms are so common is because job ads commonly contain phrases like “previous experience in …” or “some experience as …”.\nIn the next post in the series we look at extracting from these types of phrases, and get much better results."
  },
  {
    "objectID": "portable-custom-config/index.html",
    "href": "portable-custom-config/index.html",
    "title": "Customising Portable Dotfiles",
    "section": "",
    "text": "I keep my personal configuration files in a public dotfiles repository. This means that whenever I’m on a new machine it’s very easy to get comfortable in a new environment. However I find I often need machine specific configuration, so I provide ways to override them with local configuration.\nWhen I get to a new machine I’ll pretty quickly want some of my usual configuration (although I don’t need it). I can clone or download a zipfile of my dotfiles and then install it via some symlinks via a bootstrap bash script. There are better tools for managing dotfiles like rcm but they have dependencies that may not be easy to install (especially on oddball systems like Cygwin).\nBecause they’re symlinks I can edit the files, and push any changes back upstream. The problem is that sometimes I want private changes that I don’t want to push upstream. This is solved with the .local pattern; extend the configuration with a configuration file ending in .local if it exists. So for example a .bashrc is extended by a .bashrc.local, and a .gitconfig is extended by a .gitconfig.local. These local files are not in public version control, and I can add them as necessary to any machine."
  },
  {
    "objectID": "portable-custom-config/index.html#git",
    "href": "portable-custom-config/index.html#git",
    "title": "Customising Portable Dotfiles",
    "section": "Git",
    "text": "Git\nFor git configuration you can use the Include statement to insert configuration from a custom file, and nothing happens if the file doesn’t exist. So your gitconfig file looks like:\n...\n[include]\n  path = ~/.gitconfig.local"
  },
  {
    "objectID": "portable-custom-config/index.html#bash",
    "href": "portable-custom-config/index.html#bash",
    "title": "Customising Portable Dotfiles",
    "section": "Bash",
    "text": "Bash\nIn your bashrc you can add configuration with source, but you have to check the file exists to let it work when there is no local configuration.\nif [[ -e ~/.bashrc.local ]]; then\n    source ~/.bashrc.local\nfi"
  },
  {
    "objectID": "portable-custom-config/index.html#emacs",
    "href": "portable-custom-config/index.html#emacs",
    "title": "Customising Portable Dotfiles",
    "section": "Emacs",
    "text": "Emacs\nEmacs also tracks variables that have been customised through the easy customisation interface. Rather than having this litter the version controlled init file it makes sense to put these into a local file too; they can be migrated to the general configuration as desired. The .local convention is a little less clear here because I store the config in .emacs.d/init.el; so I call the custom file init.local.el.\n;; Put/save customisations through customize in a separate file\n(defconst custom-file (expand-file-name \"init.local.el\" user-emacs-directory))\n(unless (file-exists-p custom-file)\n  (write-region \"\" nil custom-file))\n(load custom-file)"
  },
  {
    "objectID": "portable-custom-config/index.html#everything-else",
    "href": "portable-custom-config/index.html#everything-else",
    "title": "Customising Portable Dotfiles",
    "section": "Everything else",
    "text": "Everything else\nThis pattern works across a large number of configuration files, but the syntax is slightly different each time. The thoughtbot dotfiles have great examples showing how to implement it for zshell, tmux, and psql .\nHappy configuring!"
  },
  {
    "objectID": "extracting-links-from-html/index.html",
    "href": "extracting-links-from-html/index.html",
    "title": "Extracting Links From HTML",
    "section": "",
    "text": "from bs4 import BeautifulSoup\ndef extract_links(html):\n    soup = BeautifulSoup(html, 'html.parser')\n    return [a.get('href') for a in soup.find_all('a') if a.get('href')]\nSome other methods would be to use regular expressions (which would be faster than parsing, but a little harder to get right), directly going through a parse tree or using lxml. These other solutions would likely be a bit faster, but I like the flexibility of BeautifulSoup (especially with it’s select method for CSS selectors)."
  },
  {
    "objectID": "common-substring/index.html",
    "href": "common-substring/index.html",
    "title": "Finding Common Substrings",
    "section": "",
    "text": "The most asymptotically efficient to find the longest common substring would be to build a suffix tree, but for experimentation the heuristics in Python’s DiffLib work well enough.\nI define a function that gets all common strings above a certain length. I look for all pairs difflib considers equal and print them out; this won’t get all common substrings but works well enough on the job ads I tried them on. A benefit of using difflib is that if we want to find the longest common string of tokens we can just pass in a and b as lists of tokens.\ndef common_substrings(a, b, min_length=7):\n    seqs = []\n    seqmatcher = difflib.SequenceMatcher(a=a, b=b, autojunk=False)\n    for tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        if tag == 'equal' and a1 - a0 >= min_length:\n            seqs.append(a[a0:a1])\n    return seqs\nOnce we have a way to find common substrings of pairs it’s straightforward to extend it to a list of substrings. In particular we just look to see for each existing common substrings whether it has any common substrings with the next text.\ndef all_common_substrings(args, min_length=7):\n    seqs = None\n    for arg in args:\n        if seqs is None:\n            seqs = [arg]\n            continue\n        new_seqs = []\n        for seq in seqs:\n            new_seqs += common_substrings(arg, seq, min_length)\n        seqs = new_seqs\n    return seqs\nThis could easily be extended to allow a few mismatches by collecting across difflib tags other than equal up to some length of tokens.\n\nExact brute force approach\nIt’s good to compare this with an exact solution to make sure the difflib heuristics are actually working. I always find it’s good to start with a simple slow obviously correct solution before trying to build a more complex efficient algorithm.\nIn particular we could start by producing all substrings of a string by iterating over each possible starting point and length:\ndef all_substrings(s):\n    for i in range(len(s)):\n        for j in range(len(s)-i):\n            yield s[i:i+j+1]\nTo know whether one string is a substring of another we can just check whether it matches a any position:\ndef contains_substring(a, b):\n    \"\"\"Does a constrin substring b\"\"\"\n    for i in range(len(a) - len(b) + 1):\n        if a[i:i+len(b)] == b:\n            return True\nThen we could find the common substrings by just checking if any of the substrings of one are in the other:\ndef naive_common_substrings(a, b):\n    for substring in all_substrings(a):\n        if contains_substring(b, substring):\n            yield substring\nThis will output a lot of substrings because any substring of a common substring is also a common substring. For example if “the” is in common, then so is “t”, “th”, “h”, “he” and “e”. We can filter this down to the “proper substrings”, those that aren’t contained in a larger substring.\ndef proper_substrings(a):\n    proper = []\n    for s in a:\n        if any(contains_substring(p, s) for p in proper):\n            continue\n        supersequence = [contains_substring(s, p) for p in proper]\n        if any(supersequence):\n            val = s\n            for idx, value in enumerate(supersequence):\n                if value:\n                    proper[idx] = val\n                    val = None\n            proper = [p for p in proper if p]\n        else:\n            proper.append(s)\n    return proper\nNote that this implementation is awfully slow; the operations in calcualting proper_substrings(naive_common_substrings(a, b)) is quadratic in the length of a and roughly linear in the length of b. But it’s good for a sanity check on some simple strings, and using it I find the difflib captures most of the common substrings on the job ads I tried."
  },
  {
    "objectID": "au-oil-imports/index.html",
    "href": "au-oil-imports/index.html",
    "title": "Australian Oil Imports",
    "section": "",
    "text": "How much oil does Australia import (in Barrels per Day)?\n\nAs in the text we approach this by estimating car consumption.\n\n\n\n\ngraph BT;\n   Import[Oil imports Barrel/Day]\n\n   ImportL[Oil imports L/Day] --> Import\n   Barrel[Size of Barrel L] -->|-1| Import\n   \n   Consumption[Oil consumed L/Day] --> ImportL\n   ImportRatio[Oil Imported / Consumed] --> ImportL\n   \n   CarConsumption[Oil Consumed by Cars L/Day] --> Consumption\n   CarFraction[Oil Consumed in Total  / Oil Consumed by Cars] --> Consumption\n   \n   Cars[Number of Cars] --> CarConsumption\n   ConsumptionCar[Oil Consumed by Car L/Day] --> CarConsumption\n\n   People[Number of People] --> Cars\n   CarPeople[Number of Cars per Person] --> Cars\n\n\n\n\n\n\n\n\nTo estimate imports we estimate demand, since that is estimable. For oil, if we ignore reserves, then we have imports + production = consumption + exports. I don’t know that Australia is a major oil exporter so I’ll assume production and exports can be ignored. So then production is approximately consumption.\n\nEstimating Car Consumption\nA major use of oil is at petrol for consumer cars. Let’s assume, as in the book, that car use is roughly equal to non-car use. So All Use/Car Use is 2.\nThere are 25 million people in Australia. It’s a very car heavy nation; most people have a car. I’d guess there’s around 0.8 cars per person (since this includes people who can’t drive). This gives about 20 million cars.\nA consumer probably fills up their car tank about once per week, for say a 40L tank. This means they use around 5L per day.\nSo the car overall consumption is about 100ML/day, or about 40 GL/year. This means the estimated overall consumption is about 200 ML/day.\n\n\nPutting it together\nA barrel is 160L, so 200 ML/day consumption is about 1.3 million barrels of oil per day. So our final estimate of imports is 1.3 million barrels per day."
  },
  {
    "objectID": "minhash-lsh/index.html",
    "href": "minhash-lsh/index.html",
    "title": "Searching for Near Duplicates with Minhash",
    "section": "",
    "text": "I’m trying to find near duplicates texts in the Adzuna Job Salary Predictions Kaggle Competition. In the last article I built a collection of MinHashes of the 400,000 job ads in half an hour in a 200MB file. Now I need to efficiently search through these minhashes to find the near duplicates because brute force search through them would take a couple of days on my laptop.\nMinHash was designed to approach this problem as outlined in the original paper. Finding exact duplicates was easy because we were checking for equality. If we had a new duplicate document we could use a hash table to quickly see if we had already seen it. Similarly here we group our MinHashes into bands that we store in a hash table to find collisions. The details are covered well in Chapter 3 of Mining Massive Datasets by Ullman et al. and chapter 6 of Gakhov’s Probabilistic Data Structures and Algorithms for Big Data Applicatoins.\nThis approach of putting similar things in the same bucket is called Locality Sensitive Hashing (LSH). There are a whole family of these for different distances, such as bit-sampling for Hamming Distance and SimHash for Cosine Distance.\nLSH can work really well as an online algorithm to efficiently check for near-duplicates in a large corpus, by storing and adding to these band hash tables."
  },
  {
    "objectID": "minhash-lsh/index.html#analysis-of-the-s-curve",
    "href": "minhash-lsh/index.html#analysis-of-the-s-curve",
    "title": "Searching for Near Duplicates with Minhash",
    "section": "Analysis of the S-Curve",
    "text": "Analysis of the S-Curve\nA little calculus helps understand how the parameters affect the shape of the curve. The probability of emission from the LSH as a function of similarity s is \\(P(s) = 1 - (1 - s^r) ^ b\\). The slope of this curve is \\(P^\\prime(s) = r b (1 - s^r)^(b-1) * s ^ {r - 1}\\), which is always increasing. This means the curve is a good high pass filter.\nCalculating where the second derivative is zero gives the curve is steepest at \\(\\sqrt[r]{\\frac{1 - \\frac{1}{r}}{b - \\frac{1}{r}}}\\). We’ll call this point the threshold of the curve, because it’s close to where it starts letting in values. This is approximately \\(\\hat{s} = \\sqrt[r]{\\frac{1}{b}}\\), and the approximation is better for larger r. For b and r larger than about 4 the probability at the threshold is \\(1 - e ^ {-1} \\approx 0.6\\), so it is close to the midpoint between accepting and rejecting. The slope at this point is around \\(\\frac{r} {e \\hat{s}}\\), which gives a first-order width of the s.\nThe upshot of all this is that if you double the number of rows and square the number of bands you get the same threshold with approximately half the width. Because the total number of hash functions required is the product of the number of bands and rows, it gets expensive to decrease the width because the bands required is large. It’s also easier to make sharp cutoffs for relatively large thresholds."
  },
  {
    "objectID": "minhash-lsh/index.html#choosing-a-practical-cutoff",
    "href": "minhash-lsh/index.html#choosing-a-practical-cutoff",
    "title": "Searching for Near Duplicates with Minhash",
    "section": "Choosing a practical cutoff",
    "text": "Choosing a practical cutoff\nGiven that we have 128 permutations how do we choose the cutoff? We need to pick r and b with a product at most 128 (so for example 16 and 8 would do, as would 5 and 25).\nBecause we can always filter out false negatives but can’t retrieve false positives the target should be to remove the bulk of texts with a very small overlap. When we looked at the 4-Jaccard near duplicates they were similar above about 0.5, and very different below 0.05, so we want to capture this threshold. So ideally for ads with a similarity below 0.05 there should be a very small chance (say 0.1%) that they get included and for ads with similarity about 0.5 there should be a high chance (say 99%) that they are included.\nLooking some different values for b and r shows the tradeoffs:\n\n\n\nDifferent bands and r\n\n\nThe best solution seems to be 42 bands of 3 rows. At 0.5 it’s got a 99.6% probability of inclusion, and at 0.05 it has a .5% chance of inclusion."
  },
  {
    "objectID": "minhash-lsh/index.html#dont-use-threshold-in-datasketch",
    "href": "minhash-lsh/index.html#dont-use-threshold-in-datasketch",
    "title": "Searching for Near Duplicates with Minhash",
    "section": "Don’t use threshold in DataSketch",
    "text": "Don’t use threshold in DataSketch\nThe datasketch library provides an argument for threshold for the MinHashLSH. It uses a deeply flawed way to try to calculate this. It tries estimate the error by integrating the S-curve, which doesn’t make sense because it isn’t a probability denisty. If the integral was divided by the length of the interval it would be the average probability over the interval, but as it is I’m not sure how to interpret this quantity.\nI raised an issue about this, but I closed it because:\n\nExisting users have probably tuned the threshold and depend on the behaviour\nI can’t think of a way to implement the existing interface with weights (unless they’re deprecated)\nThere’s no clear choice for the threshold; the value (1/b)**(1/r) is a reasonable candidate, but it has a different meaning to the API\nYou can work around it by passing the number of bands and rows as a tuple into params\nI found it difficult to communicate my argument (but I’m pretty sure it doesn’t make sense)\n\nIn practice it makes sense to tune the curve based on the value where the S-curve goes below ~0.1% or above 99% which is straightforward with a root finder, and just pass in these parameters."
  },
  {
    "objectID": "dict-to-dataclass/index.html",
    "href": "dict-to-dataclass/index.html",
    "title": "Dictionary to Dataclass",
    "section": "",
    "text": "However it’s not completely trivial to turn a dictionary into a dataclass. Dictionaries are accessed by using square brackets, d['a'], but dataclass attributes are accessed with a dot, d.a. Changing all accesses to the dictionary at once can be quite a large task.\nLet’s illustrate this with a simple example of representing a person as a dictionary:\nperson = {'id': 1, 'name': 'Bob', 'age': 32}\n\ndef person_old_enough(person):\n    return person['age'] >= 18\n    \ndef person_increment_age(person):\n    person['age'] = person['age'] + 1\nWe could then try to represent the person as a dataclass. Then we’d end up with something like:\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n  id: int\n  name: str\n  age: int\n  \n  def old_enough(self) -> bool:\n    return self.age >= 18\n    \n  def increment_age(self) -> None:\n    self.age = self.age + 1\nHowever if this person structure is used in lots of places across lots of files this is a very large and difficult refactoring. Instead if we add the magic methods to get and set items to operate on the corresponding attribute we don’t need to change how the object is called (as long as del isn’t used on any fields).\nfrom dataclasses import dataclass\n\n@dataclass\nclass Person:\n  id: int\n  name: str\n  age: int\n  \n  def __getitem__(self, key):\n    return getattr(self, key)\n    \n  def __setitem__(self, key, value):\n    return setattr(self, key, value)\n\nperson = Person(id=1, name='Bob', age=32)\n\ndef person_old_enough(person):\n    return person['age'] >= 18\n    \ndef increment_age(person):\n    person['age'] = person['age'] + 1\nThis enables us to slowly refactor the methods over time; first updating the functions to use . notation and then moving them into the dataclass. Even if we don’t ever get to this end state new uses of the dataclass can use the . notation and be statically validated by mypy to protect against typos."
  },
  {
    "objectID": "pandas-to-dictionary/index.html",
    "href": "pandas-to-dictionary/index.html",
    "title": "Fast Pandas DataFrame to Dictionary",
    "section": "",
    "text": "df.set_index(keys)[value].to_dict()\nThe rest of this article will discuss how I used this to speed up a function by a factor of 20.\n\nDiagnosing and improving a slow implementation\nI had a function that performed a few transformations to extract information from a dataframe, but is was pretty slow taking around a second per thousand rows. I was experimenting in a Jupyter notebook, and came across a good article on profiling in Jupyter notebooks. To profile the function func with arguments args I could run %prun func(args); and the first few rows looked like this:\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n  6307956    1.465    0.000    2.390    0.000 {built-in method builtins.isinstance}\n   126008    1.447    0.000    1.802    0.000 {pandas._libs.lib.infer_dtype}\n   125987    1.075    0.000   19.294    0.000 series.py:238(__init__)\nI didn’t find this terribly illuminating, but it seemed to be spending a disproportionate time in Pandas and guessing datatypes, which really should not have been a difficult problem. Because I had a few lines of Python functions it wasn’t immediately obvious where this was occurring, so I ran the line profiler, installing it from a Jupyter notebook with pip install line_profiler and loading the Jupyter extension with %load_ext line_profiler. Then I could look at the lines taking the most time with the function using %lprun -f func func(args), to find 99% of the time was spent in the following line:\nmapping = {tuple(k): v for (_idx, k), v in zip(keys_df.iterrows(), values)}\nThis was my clumsy way to go from a DataFrame to a dictionary; keys_df = df[keys] and values = df[value]. The only place I can see that a series would come up is from iterrows which emits an index and a series, and the series needs to hold all of the keys. To do this it needs to work out the least common denominator type of the types of each of the keys (for example if some are integers and some are strings then the resulting series will have dtype object). And it seemed to do this calculation for every row which was taking a ton of time!\nWhen something like this happens in Pandas or Numpy the best first step is to look for an inbuilt way of doing this, which is likely to be an order of magnitude faster. A little searching showed that a Pandas Series has a to_dict method, mapping the index to the values. So I could replace the line above by the simple expression at the top of the article to make the function go from taking tens of seconds to under a second:\ndf.set_index(keys)[value].to_dict()\nI admit I haven’t used profiling much before in Python (mostly just manually profiling by typing functions), but it’s very easy and useful, especially with line_profiler."
  },
  {
    "objectID": "decorating-pandas-tables/index.html",
    "href": "decorating-pandas-tables/index.html",
    "title": "Decorating Pandas Tables",
    "section": "",
    "text": "The Pandas style documentation gives pretty clear examples of how to use it. When you have your final dataframe you can then call .style and chain styling functions.\nFor example you can colour cells by their value using style.background_gradient to get an effect like Excel’s Colour Scales Conditional Formatting. You can choose a colormap through the cmap argument, using the Matplotlib colormaps. One handy trick is to get the reverse of a colormap by appending _r to the name.\n(\n df\n .style\n .background_gradient(cmap=\"PuRd_r\")\n)\n\n\n\nHeatmap Dataframe\n\n\nYou can even make a data barchart inside the dataframe using style.bar. You can set the color, minimum and maximum values, axis and choose a subset of columns to show bars on.\n(\n df\n .style\n .bar(vmax=len(df), color='lightblue')\n)\n\n\n\nData bar chart in Dataframe\n\n\nTo make the data easier to read you can add a style.format. This can take a dictionary of columns to formatters which can be format strings or functions. Because the HTML is rendered you can actually use this to do things like put in decorations.\ndef format_arrow_text(value):\n    if value < 0:\n        indicator = '<span style=\"color:red;\">⮟</span> ' \n    elif value > 0:\n        indicator = '<span style=\"color:green;\">⮝</span> ' \n    else:\n        indicator = ''\n    return f'{indicator} {value:.1%}'\n    \ndf.style.format(format_arrow_text)\n\n\n\nExample of Format With Arrows\n\n\nThis is just scratching the surface, you can do a lot more by writing custom styles. It’s convenient for simple things but styling with just CSS attributes at a cell level is a bit clunky, and for complex things you’ll want to render your own HTML (potentially by subclassing).\nFor contrast R has the formattable package which can achieve many of the same things. In this case the syntax isn’t much better than Pandas."
  },
  {
    "objectID": "embeddings/index.html",
    "href": "embeddings/index.html",
    "title": "Embeddings for categories",
    "section": "",
    "text": "Categorical objects occur all the time in business settings; products, customers, groupings and of course words. For models like linear or logistic regression or nearest neighbours the way to deal with these is by one-hot encoding. This means you end up with as many parameters as there are categories, and this can quickly become unwieldy. You end up having to do a bunch of preprocessing to normalise similar things, maybe even building custom hierarchies, and throwing away infrequent items because you don’t have enough data. In some cases this can work quite well as a starting point; but you lose the long tail which often has a lot of interesting information.\nTree based methods like gradient boosting trees and random forests can directly handle categorical data. However because they split the categories at cut points the order you give the categories in makes a big difference; close together items will tend to be grouped together. If you can group similar categories together you can expect much better performance, but it’s not always obvious what similar means and how to do this. While you will get the tail data in your model you can’t rely on it.\nEmbeddings use information you have to map the categorical items in a vector space where similar items are close together. This is done by building a model based on the structure of the information you have about the items. You can think of it as a lookup array; each category is mapped to a vector. What similar means depends on the information you use.\nFor text a classic approach is Word2Vec where each word in the corpus is mapped originally to random vectors. Then a pair of shallow neural networks are trained to predict neighbouring words. The embeddings end up where words that could be used in the same context end up close by in the vector space. There are many variations on this method, but training a language model on a lot of text is still generally the best way to get good models for solving other language tasks.\nFor products and customers a typical approach is to use the interaction between products and customers. Customers who purchase the same products are in some sense similar; products that are purchased together are in some sense similar. You could then factor this interaction matrix to get an embedding of users and an embedding of customers. Another method would be to construct the product-product matrix and then consider this as a distance matrix which you embed with multidimensional scaling.\nThese techniques are used in industry, for example Pinterest and Instacart.\nOne thing to keep in mind is that similarity will be defined by the type of data that you model. For example if you tried to predict items that would be purchased in the same session then similar items would be complementary items; things that go well together. When you try to predict items that are purchased by similar customers then you are more likely to get items that are substitutes close together. If you predict items that are viewed you’re also going to pick up some aspirational items; a lot of people like window shopping for objects they won’t actually buy. Consider the kind of similarity that would be useful to capture for downstream models.\nIt would be interesting to get some open data and demonstrate how well some simple product embedding approaches work for improving related modelling tasks."
  },
  {
    "objectID": "parsel-cli/index.html",
    "href": "parsel-cli/index.html",
    "title": "A Command Line Interface for HTML With parsel-cli",
    "section": "",
    "text": "Parsel is a fantastic library that gives a simple and powerful interface for extracting data from HTML documents using CSS selectors, Xpath and regular expressions. Parsel-cli is a very small utility that lets you use parsel from the command line (and can be installed with pip install parsel-cli).\nFor example if you wanted to extract all links from a HTML document; you could use parsel-cli 'a::attr(href)' You could also use it to extract particular useful data from a website without an API; for example to get the headlines from Hacker News you can use curl -q https://news.ycombinator.com/ | parsel-cli '.storylink::text' While it’s limited compared to actually writing scripts with parsel (especially only being able to extract one field), it’s a useful companion for transforming data in shell."
  },
  {
    "objectID": "symmetry-lie-alebras-qde-2/index.html",
    "href": "symmetry-lie-alebras-qde-2/index.html",
    "title": "Symmetry, Lie Algebras and Quantum Differential Equations Part 2",
    "section": "",
    "text": "Consider the rotation of a function of space in the x-y plane (anticlockwise about the z-axis): \\(\\psi(x,y,z) \\to \\psi(x \\cos \\phi+ y \\sin \\phi, -x \\sin \\phi+ y \\cos \\phi, z)\\) . For an infinitesimal rotation we get \\(\\psi(x+y \\mathrm{d}\\phi,-x\\mathrm{d}\\phi+y,z)=\\psi(x,y,z)+ \\left(y \\frac{\\partial \\psi}{\\partial x} -x \\frac{\\partial \\psi}{\\partial y}\\right)\\mathrm{d}\\phi\\) , so infinitesimally the rotations are \\(\\text{Id} + i (r_x p_y - r_y p_x) \\mathrm{d}\\phi\\) where \\(p_i = -i \\frac{\\mathrm{d}}{\\mathrm{d}x_i}\\) is the ith component of momentum. The commutation relations are \\([x_i,x_j]=0, [p_i,p_j]=0, [x_i,p_j]=i \\delta_{ij} \\text{Id}\\) . Note that in spherical coordinates \\(x=r \\cos \\phi \\sin \\theta, y=r \\sin \\theta \\sin \\phi, z = r \\cos \\theta\\) , \\(r_x p_y - r_y p_x = -i \\frac{\\partial}{\\partial \\phi}\\) . With this choice of coordinates it is easy to see finite rotations can be achieved by exponentiation: \\(e^{t \\frac{\\partial}{\\partial \\phi}} f(r,\\theta,\\phi) = \\sum_{n=0}^{\\infty} t^n \\frac{\\partial^n f}{{\\partial \\phi}^n}(r,\\theta,\\phi) = f(r,\\theta,\\phi+t)\\) , where the final equality is the Taylor expansion.\nThere are similar equations for rotations about the other coordinates, we define the ith component of angular momentum by \\(L_i = \\epsilon_{ijk} r_j p_k\\) (Einstein summation convention will be used throughout this article). The three components of angular momentum generate rotation about an arbitrary axis; rotations about the axis through the unit vector (a,b,c) are generated by \\(a L_x + bL_y + c L_z\\) . Rotations don’t commute, infinitesimally this corresponds to the components of angular momentum not commuting \\([L_i,L_j] = i \\epsilon_{ijk} L_k\\) .\nThere is another, quadratic, invariant associated to these operators \\(L^2 = L_x^2 + L_y^2 +L_z^2\\) which commutes with each component of angular momentum \\([L,L^2]=0\\) . These sorts of conserved quantities that are polynomials in the generators are called Casimir invariants; they are not a property of a Lie algebra but rather of a representation of a Lie algebra (or the universal enveloping algebra).\nA maximally commuting set is given by \\(L_z \\text{ and } L^2\\) (the choice of z is arbitrary, but conventional). These operators are invariant under rotations in the \\(L_x-L_y\\) plane:\n\\(\\begin{bmatrix} L_x' \\\\ L_y' \\end{bmatrix} = \\begin{bmatrix} \\cos \\phi& \\sin \\phi \\\\ \\sin \\phi & \\cos \\phi\\end{bmatrix} \\begin{bmatrix} L_x \\\\ L_y \\end{bmatrix}\\) ,\nand in fact the rotations are infinitesimally generated by \\(L_z\\) . Just as in the case of the simple harmonic oscillator we take the eigenstates of this transformation \\(L_{\\pm} = L_x \\pm i L_y\\) (notice that these are Hermitian conjugates of each other).\nFrom algebra the commutation relations follow \\([L_{\\pm},L_{\\mp}]=\\pm 2 i L_z\\) , \\([L_z,L_{\\pm}] = \\pm L_{\\pm}\\) , and \\(L_{\\pm}L_{\\mp} = L^2 - L_z^2 \\pm L_z\\) , \\([L^2,L_{\\pm}]=0\\) . Again these commutation relations allow us to find the eigenvalues of the operator \\(L^2\\) as well as of the operator \\(L_z\\) (since they commute they are simultaneously diagonalisable).\nSuppose \\(\\psi\\) is a simultaneous, normalized, eigenvector of \\(L^2\\) and \\(L_z\\) ; \\(L^2 \\psi = \\lambda \\psi, L_z \\psi = m \\psi, \\psi^{\\dagger}\\psi=1\\) . Taking inner products over the Hilbert space gives \\(0 \\leq (L_{\\pm} \\psi)^{\\dagger} L_{\\pm} \\psi = \\psi^{\\dagger} L^2 \\psi - \\psi^{\\dagger} L_z^2 \\psi \\mp \\psi^{\\dagger} L_z \\psi = \\lambda - m^2 \\mp m\\) , which implies that \\(m(m \\pm 1) \\leq \\lambda\\) . This implies that we can’t keep raising or lowering the z-component of angular momentum forever, there must be states for which e.g. \\(L_{-} \\psi_{\\mbox{min}} = 0\\) and \\(L_{+} \\psi_{\\mbox{max}} = 0\\) . Then \\(L^2 \\psi_{\\mbox{min}} = (L_{+}L_{-} + L_z^2 - L_z) \\psi_{\\mbox{min}} = m_{\\min}(m_{\\min}-1) \\psi_{\\mbox{min}}\\) . Similarly \\(L^2 \\psi_{\\mbox{max}} = (L_{-}L_{+} + L_z^2 + L_z) \\psi_{\\mbox{max}} = m_{\\max}(m_{\\max}+1) \\psi_{\\mbox{max}}\\) .\nSuppose the maximum state is reached from the minimum state by raising n times, that is; substituting into the equations for \\(\\lambda\\) above implies \\(m_{\\text{min}} = -\\frac{n}{2}\\) , \\(m_{\\text{max}}=\\frac{n}{2}\\) , \\(\\lambda =\\frac{n}{2}(\\frac{n}{2}+1)\\) .\nIt is in fact possible to show that it must be an integer for \\(L_i = i \\epsilon_{ijk} r_j p_k\\) (following an argument from Ballentine’s quantum mechanics textbook page 127): set \\(q_{\\pm} = \\frac{1}{\\sqrt{2}}(x \\pm p_y)\\) , \\(p_{\\pm} = p_x mp y\\) . Then \\(0=[q_{\\pm},q_{\\mp}] = [p_{\\pm},p_{\\mp}] = [q_{\\pm},p_{\\mp}]\\) , and \\([q_{\\pm},p_{\\pm}]=i=[q_{\\mp},p_{\\mp}]\\) . These are the commutation relations for independent components of momentum and position. Then \\(L_z = \\frac{1}{2} (q_{+}^2 + p_{+}^2) - \\frac{1}{2} (q_{-}^2 + p_{-}^2)\\) which is the difference between two simple harmonic oscillators. Thus \\(L_z\\) must have eigenvalues of the form \\((n+1/2) - (m+1/2) = (n-m)\\) for suitable integers \\(n,m\\) ; consequently the eigenvalues of \\(L_z\\) must be integers. Similar to the simply harmonic oscillator by solving a first order differential equation in terms of the lowering operator we can find solutions to the second order differential operator \\(L^2\\) ; the solutions are the spherical harmonics (which are related to complex exponentials and Legendre polynomials), and as before we get Rodrigues’ type formulae.\nJust as for the Simple Harmonic Oscillator the algebra of the angular momentum operators allowed us to find all the eigenvalues of a quadratic operator, and determine all its eigenfunctions in a particular basis. Incidentally the half-integer solutions can be realised, but not as rotations in 3 dimensions, they correspond to rotations in spinor space – a rotation through 360 degrees in spinor space is only a rotation of 180 degrees in three dimensional space."
  },
  {
    "objectID": "constrained-gradient-descent/index.html",
    "href": "constrained-gradient-descent/index.html",
    "title": "Constrained Gradient Descent",
    "section": "",
    "text": "The idea is simple, we’ve got a function loss that we’re trying to maximise subject to some constraint function. With gradient descent we will take a step in the direction of greatest decrease of the loss function, along the gradient. The size of the step we take is called the learning rate, lr. In Pytorch:\ndef gradient_descent_step(x, loss, lr=1e-3):\n    # Set gradient to zero\n    if x.grad is not None:\n        x.grad.zero_()\n\n    # Calculate the derivative of the loss function\n    loss().backward()\n\n    # Step in direction of greatest local decrease\n    with torch.no_grad():\n        x -= lr * x.grad\nHowever this may take us off of our constraint curve. As long as we are at a point x on the constraint curve, constraint(x) = 0, we want to stay on that curve. That means we want to take a step in the direction where the derivative of the constraint is zero (so the value won’t change). This happens in the direction orthogonal to the gradient of the constraint, which can be done by removing the component parallel to the constraints gradient. In Pytorch:\ndef gradient_descent_step(x, loss, constraint, lr=1e-3):\n    # Set gradient to zero\n    if x.grad is not None:\n        x.grad.zero_()\n        \n    # Calculate gradient of the constraint\n    constraint(x).backward()\n    direction = x.grad.clone()\n    \n    # Calculate gradient of loss function\n    x.grad.zero_()\n    aloss = loss()\n    aloss.backward()\n    \n    # Remove the projection of the loss gradient onto the constraint gradient.\n    # The resulting vector will be perpendicular to the gradient of the constraint.\n    perp_proj = x.grad - (x.grad @ direction) / (direction @ direction) * direction\n    \n    # Step in this direction\n    with torch.no_grad():\n        x -= lr * perp_proj\nWe can put this into highfalutin differential geometry terminology. By the implicit function theorem the equality constraint defines a submanifold of the overall space (except in pathological regions, but there’s often an area where this is true). We want to optimise the loss function on this submanifold. This is done by projecting the derivative of the loss function on the manifold to the tangent space of the submanifold defined by the constraints.\nIn fact the method of Lagrange Multipliers solves this exact problem, however on my problem I had difficulty getting the point back to the constraint curve. However this method of projection worked really well. We could potentially use a similar approach for an inequality constraint; by first searching in the interior of the region and applying the gradient projection along the boundary (Boyd and Vandenberghe’s Convex Optimisation has the details)."
  },
  {
    "objectID": "multilevel-models-questions/index.html",
    "href": "multilevel-models-questions/index.html",
    "title": "Learning about Multilevel Models",
    "section": "",
    "text": "The concept of a multilevel model, also called a mixed effects model or a hierarchical model, is reasonably new to me. It’s not the kind of thing typically taught in physics (where there are very explicit models) or in machine learning, but is quite common in social science. I first came across it through Lauren Kennedy on the Learning Bayesian Statistics Podcast, through talking with a trained neuroscientist and a trained statistician who were talking about fixed and variable effects as I went cross-eyed, and through the excellent Regression and Other Stories textbook which makes many allusions to it (to be expounded on in their upcoming sequel Applied Regression and Multilevel Models). The exposition in Chapter 9 of Kruschke’s Doing Bayesian Data Analysis gives a good introduction in the binomial case, but I still don’t really understand multilevel models and this article will list some questions I’m trying to understand."
  },
  {
    "objectID": "multilevel-models-questions/index.html#how-do-we-estimate-the-models",
    "href": "multilevel-models-questions/index.html#how-do-we-estimate-the-models",
    "title": "Learning about Multilevel Models",
    "section": "How do we estimate the models?",
    "text": "How do we estimate the models?\nThese can be estimated using Bayesian methods, but for large datasets and complex heirarchies MCMC based approaches can be computationally intractable. I’m not sure whether they can be structured in a way to make them faster, or perhaps using good priors or approximate Bayesian approaches could help. Is there are way to estimate heirarchical models on large datasets using Bayesian methods in reasonable time?\nAnother approach is Maximum Likelihood Estimation where we find the most likely parameters (and Bayesian priors can be incorporated as regularisation). This boils down to function optimisation which can be done efficiently with blackbox methods. Given we have a large amount of data do we lose anything with these methods over Bayesian approaches (I would expect the modes to be quite sharp, depending on the complexity of the model)? I’ve also heard of Restricted Maximum Likelihood Estimation - what is that and how does it compare?"
  },
  {
    "objectID": "multilevel-models-questions/index.html#can-we-include-information-about-the-correlation",
    "href": "multilevel-models-questions/index.html#can-we-include-information-about-the-correlation",
    "title": "Learning about Multilevel Models",
    "section": "Can we include information about the correlation?",
    "text": "Can we include information about the correlation?\nIf we have lots of information about the categories is there a way to encode this in the models (perhaps as priors or as extra predictors)?"
  },
  {
    "objectID": "multilevel-models-questions/index.html#what-software-is-there-for-these-models",
    "href": "multilevel-models-questions/index.html#what-software-is-there-for-these-models",
    "title": "Learning about Multilevel Models",
    "section": "What software is there for these models?",
    "text": "What software is there for these models?\nFor a likelihood approach lme4 seems to be the staple; how do we use it? Is there an equivalent in Python; statsmodels MixedLinearModel looks a lot less flexible?\nFor Bayesian methods there’s Stan, PyMC3, JAGS among other software (along with wrappers like brms that provides a formula syntax close to lme4 in Stan). How does the performance compare between them, and how can they be implemented in an efficient way?"
  },
  {
    "objectID": "multilevel-models-questions/index.html#using-with-non-linear-models",
    "href": "multilevel-models-questions/index.html#using-with-non-linear-models",
    "title": "Learning about Multilevel Models",
    "section": "Using with non-linear models?",
    "text": "Using with non-linear models?\nI wonder whether heirarchical shrinkage would be useful in tree based models (and perhaps make their estimates more stable along with less variable selection like ctrees). In general heirarhcical shrinkage can be added to Bayesian models by having higher level distributions of the parameters, does it make sense to do this in practice?"
  },
  {
    "objectID": "multilevel-models-questions/index.html#how-does-shrinkage-vary-with-distribution",
    "href": "multilevel-models-questions/index.html#how-does-shrinkage-vary-with-distribution",
    "title": "Learning about Multilevel Models",
    "section": "How does shrinkage vary with distribution?",
    "text": "How does shrinkage vary with distribution?\nIt would make sense that the higher the interclass deviation and the smaller the intraclass deviation the more shrinkage there should be to the group parameters. Does this happen? Can we quantify this?"
  },
  {
    "objectID": "sunk-cost-pure-maths/index.html",
    "href": "sunk-cost-pure-maths/index.html",
    "title": "Sunk Cost of Pure Mathematics",
    "section": "",
    "text": "A large amount of the material is pure mathematics. Notes on differential geometry, topology, and measure theory. These are particularly vexing because I don’t believe they hold much real value. Yet I spent so long trying to understand the concepts it’s hard for me to let them go.\nAnalysis in the sense of pure mathematics gazes into the navel of mathematical structures themselves. By very carefully examining the axiomatic definitions of structures you end up with all sorts of exotic creatures. The Weirstrass function which has no holes, but you can’t draw any tangent lines to it. The Cantor function which has derivative zero almost everywhere, but is monotonically increasing. Space filling curves which wind in themselves so tightly they can fill a rectangle. The Banach Tarski construction of decomposing a ball into 5 pieces that can be rearranged into two copies of the original ball. All these constructions have one feature in common; they involve cutting space up into infinitesimal chunks.\nAny real measurement can only have a finite precision. You can’t measure whether an object has irrational length with a ruler. These kinds of concepts are not practical in any sense.\nHandling all these counterexamples requires a lot of careful definitions and theorems. You have to preface things with “differentiable in its interior and continuous on its boundary”, when real objects don’t have a distinct interior and boundary. Spending weeks to prove when you can switch the order of integrals.\nThe underlying notions of mathematical analysis are very useful; convergence and fixpoints, tangent lines, calculating areas and local approximations and cooridantes. However the epsilon-delta style of proof takes the focus away from the concepts into technical details.\nWhen I read the first chapters of Wasserman’s All of Statistics I translated them into measure theory. I could rigorously prove most of the theorems on the sound footing of a probability measure. However by focusing on rigour I lost a lot of the statistical insight. Understanding what the models mean for real situations is much more useful than proving theorems about abstract measures. Then you can use your intuition about the processes underlying your analysis to draw conclusions more easily.\nAnother problem of pure mathematics is focusing too much on being beautiful. I love how beautiful defining division rings with geometric axioms seems. However it’s an illusion; any small change to the axioms can look just as beautiful but yield a useless theory or exotic objects like Non-desarguesian planes. I had to spend a long time unlearning this notion of compact beauty as a programmer.\nThere are some useful areas of pure mathematics like abstract algebra. The underlying ideas of abstract algebra are very relevant in programming; of composing systems and matching types. Moreover symmetry groups often correspond to analytically solvable systems that form the useful toy-models of theoretical physics. However beyond toy problems it’s normally easier to resort to brute force computation to solve systems.\nDespite my doubts on the utility of pure mathematics I still find it hard to let go of my notes. I spent a long time trying to understand the Haar measure theorem, gauge integration and Donaldson’s theorem. While this was useful for learning how to think very rigorously and how to perform research, I’m very unlikely to ever need these things again. Yet I want to keep the notes, and have dreams of writing up some of the details\nAt the back of my mind I’m worried that I may be paying too much attention to utility. I think of Hardy’s A Mathematician’s Apology\n\nthe study of mathematics is, if an unprofitable, a perfectly harmless and innocent occupation\n\nWhen I studied pure mathematics, and no one deluded me into thinking it would be useful. I studied it because I enjoyed the intellectual challenge. But my world view has now shifted and I really enjoy working on problems that connect to the real world and influence real decisions. All that rigorous thinking has benefited how I solve problems, and I’ve learned deeply about the traps about taking a model too seriously. However that doesn’t mean I need to keep those notes, and I won’t benefit more from keeping them.\nEasier said than done."
  },
  {
    "objectID": "displaying-hn-book-comments-html/index.html",
    "href": "displaying-hn-book-comments-html/index.html",
    "title": "Displaying Hacker News Book Comments in HTML",
    "section": "",
    "text": "I ended up building a minimal prototype by manually curating some examples; this let me focus on the design rather than on the technical aspects. The landing page is a listing of all the books in decreasing number of comments on Hacker News (which here are just wrong estimates); I picked three books I’m familiar with. This helps find the most popular books; it doesn’t solve the problems of finding books on a topic it’s a skateboard prototype, a step in the right direction.\n\n\n\nListing view\n\n\nThe detail page then contains an extract from Open Library and I extracted the most recent 3 comments using the Hacker News search. The comment content varies wildly (the Structure and Interpretation of Computer Programs tends to end up in lists of recommendations, The Art of Computer Programming is talked about much more than read, and Pragmatic Programmer is often cited for useful tips), and they’re not always useful. However for these common books there are clearly lots of comments, so once we find a way to extract them we can start working out ways to help extract useful information.\nThe other finding was linking with Open Library was difficult. All these books have multiple records with different levels of completeness; some works have every edition and author but no covers or description, others have covers and description but are missing an author. There is going to need to be some level of human curation of the Open Library sources to get good detail pages (picking covers, checking authors, updating descriptions, maybe choosing the relevant editions to link).\nOverall this was a useful experience, and I’m glad I did it manually rather than programmatically filling out templates. The design is ugly and there’s only a few examples, but it’s enough to give me a feel of what I’m building towards. The ASIN approach only returned one result per book, so to build a useful example I’m going to need to extract more books."
  },
  {
    "objectID": "point-in-time-joins/index.html",
    "href": "point-in-time-joins/index.html",
    "title": "Point-in-time joins and real time feature stores",
    "section": "",
    "text": "When we’ve got an online feature store we need to be careful that we don’t use it to train and evaluate new models. We want the data as it would have been when the prediction was made, this means we need to record all the results in an offline analytics store. However if we change the features of the model we want to use the current version of the features, not the version at the time the prediction was made. Let’s illustrate this with an example.\nConsider a recommendation system for articles on this website. On each article we are going to recommend 3 other articles to suggest, to help the reader find other relevant content. Using our historical logs we see that looking at the previous page viewed can lead to substantially better recommendations than just using the current page viewed. In particular we build a model that extracts key terms from the previous page viewed and the current page viewed, then these are used to form a query to return relevant pages from a database. However our logs are only updated daily and people tend to view pages within minutes of each other, so we need a way of storing the information.\nWe create a PostgreSQL database to track all our state; we could use any other number of key-value stores but as it’s a standard database it may be more familiar to Data Scientists. We have a table articles for articles and their keywords which we update whenever a new article is published.\n\n\n\n\n\n\n\n\n\narticle_id\narticle_name\nupdate_time\nkeywords\n\n\n\n\n1\nPoint-in-time joins\n2022-04-01\n[“sql”,“python”,“machine-learning”]\n\n\n2\nRecipe NER\n2022-03-01\n[“python”,“nlp”]\n\n\n\nAnd another table activity for tracking user activity which collects events from the frontend:\n\n\n\nuser_id\nevent_time\narticle_id\n\n\n\n\n1\n2022-05-11T11:00:00Z\n1\n\n\n1\n2022-05-11T11:05:00Z\n2\n\n\n2\n2022-05-11T11:00:00Z\n1\n\n\n\nThen we can get get out the keywords with an SQL query, which we use to recommend articles:\nSELECT history.*,\n       articles.keywords || last_articles.keywords AS keywords\nFROM\n  (SELECT user_id,\n          event_time,\n          activity.article_id,\n          lag(article_id) OVER (PARTITION BY user_id\n                                ORDER BY event_time) AS last_article_id,\n                               row_number() OVER (PARTITION BY user_id\n                                                  ORDER BY event_time DESC) AS user_row_number\n   FROM activity) AS history\nLEFT JOIN articles ON history.article_id = articles.article_id\nLEFT JOIN articles AS last_articles ON history.last_article_id = last_articles.article_id\nWHERE user_row_number = 1;\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nevent_time\narticle_id\nlast_article_id\nuser_row_number\nkeywords\n\n\n\n\n1\n2022-05-11T11:07:00Z\n3\n2\n1\n[“python”,“nlp”,“sql”,“python”,“machine-learning”]\n\n\n2\n2022-05-11T11:00:00Z\n1\nnull\n1\n[“sql”,“python”,“machine-learning”]\n\n\n\nThis works, but we find as the activity table grows the queries are getting slow and the recommendations are taking minutes to load. Also when we are training a model on this database the recommendations get even slower again. Instead we create a current_activity table that only carries the current snapshot of the data needed to generate the recommendations. To keep it very fast we send and update the keywords on the fly:\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nevent_time\narticle_id\nlast_article_id\ncurrent_keywords\nlast_keywords\n\n\n\n\n1\n2022-05-11T11:07:00Z\n3\n2\n[“python”,“nlp”]\n[“sql”,“python”,“machine-learning”]\n\n\n2\n2022-05-11T11:00:00Z\n1\nnull\n[“sql”,“python”,“machine-learning”]\nnull\n\n\n\nBut from the current_activity table we can’t evaluate the historical recommendations. Whenever a new page is viewed it UPDATEs the row for that user and we lose the history. So we set up Change Data Capture to track all the changes and store it in a separate analytics database. We then have a historical_activity table that contains the state of the activity every time.\nWe think that a better keyword extraction strategy could produce better recommendations. When training and evaluating on historical data we want to use our new keyword feature, not the one that was available at the time. Otherwise we’re not testing the model on the same data we will have at inference time, we have training-serving skew. This means we can’t use the keywords from the historical_activity, but have to lookup the new keywords using article_id and last_article_id. So we add a new keywords_v2 column and use a variation of the SQL query above to get the new features. An alternative would be to add keywords_v2 to the live feature store and wait for historical data to accumulate, but this is a very slow way to iterate on features.\nHowever we actually rewrote the point-in-time joins article to use Julia instead of Python. When recommending to people who viewed the old version we should use the keywords based on the text of that article; presumably they were more interested in Python. To do this we need to do a point-in-time join; we join with the article table containing all versions and get the most recent version available at that time. We do that by joining on event_time >= update_time and then pick the row with the most recent update_time. It’s a bit more complex because we need to do this for the previous version as well; here’s an SQL query that may do it.\nSELECT *\nFROM\n  (SELECT history.*,\n          articles.keywords_v2 || last_articles.keywords_v2 AS keywords,\n          row_number() OVER (PARTITION BY user_id,\n                                          event_time,\n                                          history.article_id,\n                                          history.last_article_id\n                             ORDER BY articles.update_time DESC, last_articles.update_time DESC) AS rn\n   FROM\n     (SELECT user_id,\n             event_time,\n             activity.article_id,\n             lag(article_id) OVER (PARTITION BY user_id\n                                   ORDER BY event_time) AS last_article_id,\n                                  lag(event_time) OVER (PARTITION BY user_id\n                                                        ORDER BY event_time) AS last_article_event_time,\n                                                       row_number() OVER (PARTITION BY user_id\n                                                                          ORDER BY event_time DESC) AS user_row_number\n      FROM activity) AS history\n   LEFT JOIN articles ON history.article_id = articles.article_id\n   AND history.event_time >= articles.update_time\n   LEFT JOIN articles AS last_articles ON history.last_article_id = last_articles.article_id\n   AND history.last_article_event_time >= last_articles.update_time\n   WHERE user_row_number = 1 ) all_versions\nWHERE rn = 1;\nThis substantially improves our offline metrics. We take the latest version of keyword_v2 for all users, insert it into our online feature store, and run an A/B experiment to see whether it is really better.\nIn summary we want to get the data as it would have been if the new features were used at that time. For the raw data, such as the interaction events and article text, we want to get the data as it was at interaction time. If this involves combining data between historical tables we need to do a point-in-time join to get the correct data. For the features that are built on the raw data, such as keywords, we want to calculate them as they are now. This becomes very clear when you think about brand new features, such as using a the location of the user or a dense embedding of the articles instead of keywords. This lets us do offline evaluation of new features in a faithful way.\nIn practice it may make sense to use various approximations to make this process computationally tractable. We can truncate the update times (e.g. to the hour or the day) which can create fewer potential join points if there are multiple updates. We can also limit the updates to some window if articles become stale and expire. Any offline evaluation itself requries approximation, we don’t know how a user would really react to a different set of recommendations, so these are generally acceptable.\n\nHow do feature stores do it?\nThis hopefully illustrates the kinds of problems that feature stores try to solve. Here are a few examples of point-in-time joins in different feature stores:\n\nFeast looks like it snapshots features at regular intervals and only retains them over some window (in the diagram feature_hourly_stats only has aggregate stats on the hour they set a ttl=timedelta(hours=2))\nHopsworks looks like it generates SparkSQL to do the point-in-time join (join on a.key = b.key and a.time <= b.time), I think over a limited time window\nSagemaker makes you roll your own point-in-time join and windowing manually\nFeaturetools (not a feature store) has an informative discussion of the different approaches (windowing, approximating, windowing)\n\nDoing this manually, especially from SQL, can be very error prone (I wouldn’t be surprised if the query above is wrong). In general the features may need to represent different kinds of joins and aggregations (for example maybe we want the last 3 referral sources for a user). If you need to do this kind of point-in-time join you should try to automate it, and make the process of generating features for model training/evaluation and at inference time as similar as possible. This can be tricky because the performance characteristics are quite different, at training time you want to generate features on a very large amount of data with high throughput, but at inference time you want to generate features on a single datum with low latency. Having a strategy that lets you do both is important for making these kinds of features practicable."
  },
  {
    "objectID": "normalise-job-title-words/index.html",
    "href": "normalise-job-title-words/index.html",
    "title": "Normalise Job Title Words",
    "section": "",
    "text": "The process I use is simple:\n\nrewrite terms containing of, e.g. “Director of Sales” to “Sales Director”\nExpand punctuation with whitespace; e.g. “Receptionist/Administrator” to “Receptionist / Administrator”\nSingularize each word; e.g. “Cleaners” to “Cleaner”\nExpand known acronyms; e.g. “DBA” becomes “Database Administrator”\nLowercase the text; e.g. “iOS Developer” becomes “ios developer”\nReplace common variants and misspellings; e.g. “Adviser” becomes “Advisor”\nReplace multiple whitespace with a single space\n\nIn code this looks like:\ndef normalise_text(text, acronyms=None, variants=None):\n    text = rewrite_of(text)\n    text = expand_punctuation(text)\n    text = singularize(text)\n    text = expand_acronym(text, acronyms)\n    text = text.lower()\n    text = expand_acronym(text, variants)\n    text = compress_whitespace(text)\n    return text\nThis works reasonably well, but the order really matters and it’s a little bit fragile. We need to expand_punctuation before we singularize, because singularize tokenizes on spaces and so “Receptionists/Administrators” would be singularized to “Receptionists/Administrator”, but “Receptionists / Administrators” would correctly transform to “Receptionist / Administrator”. We need to singularize before we expand_acronym, so that for example RGNs can be transformed to RGN before expanding to Registered General Nurse. We need to lowercase before expanding variants, because they apply for any casing.\nThere are still some cases where this goes wrong, like ENGINEERS will not be singularised correctly unless we insert a second round of singularisation after lower casing. But it does a pretty reasonable job.\nWe could be a lot more aggressive with out normalisation, in particular we could use stemming instead of making words singular, drop stop words and remove all punctuation. However this would potentially lose some useful linguistic information, and I would rather gradually remove these as needed (by examining output data) rather than doing it all up front.\nThe rest of this article goes over each piece, except for rewrite_of and singularize which are covered in their own articles\n\nExpanding Punctuation\nThis is a simple process of putting extra space around each punctuation mark. This helps downstream processes that rely on processing separate tokens work. One caveat is we need to be careful with punctuation that can be part of an acronym (like A&E). Perhaps it would be safer to do this just at the boundaries of words; but in practice this works well enough on the Adzuna dataset.\nEXP_PUNC_RE = re.compile('([/()\\'\":,])+')\ndef expand_punctuation(text):\n    return EXP_PUNC_RE.sub(r' \\1 ', text)\n\n\nExpanding Acronyms\nThere are lots of common acronyms that need to be expanded to be matched. This is a simple process of substituting at word boundaries.\ndef expand_acronym(title, acronyms):\n    for source, target in acronyms.items():\n        title = re.sub(fr'\\b{source}\\b', target, title)\n    return title\nHere’s the list of acronyms I used:\nacronyms = {\n    'PA': 'Personal Assistant',\n    'DBA': 'Database Administrator',\n    'RGN': 'Registered General Nurse',\n    'RMN': 'Registered Mental Health Nurse',\n    'NQT': 'Newly Qualified Teacher',\n    'CEO': 'Chief Executive Officer',\n    'MD': 'Managing Director', # Medical doctor doesn't occur often here\n    'EA': 'Executive Assistant',\n    'GP': 'General Practitioner',\n    'ODP': 'Operating Department Practitioner',\n    'A&E': 'Accident and Emergency',\n}\n\n\nNormalising variants\nSpelling variants is a very similar problem to acronyms, picking a common target way of writing something. I reuse the expand_acronyms function on a list of variants; I do this after lowercasing to get common forms.\nvariants = {\n    'adviser': 'advisor',\n    'draughtsman': 'draughtsperson',\n    'registered mental nurse': 'registered mental health nurse',\n    'comm': 'communication'\n}\n\n\nCompressing whitespace\nThis is a simple procedure to remove any extra whitespace.\nWHITESPACE_RE = re.compile(r'\\s+')\ndef compress_whitespace(text):\n    return WHITESPACE_RE.sub(r' ', text)"
  },
  {
    "objectID": "rules-and-models/index.html",
    "href": "rules-and-models/index.html",
    "title": "Rules, Pipelines and Models",
    "section": "",
    "text": "Media coverage in natural language processing focuses on the new massive models like BERT, GPT-2 and Turing-NLG trained using tens of thousands of GPU hours on massive web crawls. This kind of hype made me think that training a single deep neural network, with the right architecture, is the right solution for almost any supervised learning problem. However there’s lots of good reasons to also use interpretable handcrafted rules, simpler models and pipelines.\n\nInforming decisions\nThe primary value in any model is to help make a decision, by better understanding the consequences of different decisions. A black box model used on unstructured data may make a prediction without guiding what action needs to be taken.\nConsider the task of writing a job advertisement; you want to write it in such a way that the people you want to hire will apply while maintaining a diverse candidate pool. Suppose you could track how many good candidates applied for a large number of different advertisements, then you could build a Transformer to predict the number of good candidates that will apply given the job ad text.\nThen to write a good job ad you could try many different variations of the ad and see what optimises the score. However this approach is very time consuming for the person writing the job ad, and you may not even think of the types of changes that will improve the job ad. Moreover it’s not clear how much of score is real signal versus random noise in the model.\nPerhaps you could implement lime to see which words in the text have the most impact on the score. Highlighting those words could help direct your attention to where the ad needs to be edited. Unfortunately improving a job advertisement is rarely as simple as deleting a few words in isolation, which are the kinds of suggestions lime makes by default.\nSo maybe you use the language model from your Transformer to rewrite the sentences and rescore it (like a more sophisticated lime), and make the suggestions that would improve the job the most. Even if we can generate text that is reasonable and the predictions don’t overfit to noise (making random suggestions), they may start suggesting things that you can’t or don’t want to change. Maybe it will suggest changing your company name to Google (their jobs get a lot of applicants), or increasing the salary significantly, or maybe even chaning the kind of role you’re hiring for! As the ad is changed the definition of a “good candidate” might change (maybe writing the ad a certain way indicates you have lower expectations) so it may no longer be meeting your objective.\nA different, more tractable, approach is to implement a bunch of rules like Taprecruit does:\n\n\n\nTaprecruit suggesting change to the job ad title\n\n\nIt seems like Taprecruit has a list of different rules of things that will strongly impact the number of candidates that apply and the gender balance. It then directly makes recommendations based on these rules like “add a programming language to a Software Engineer job title”. Under the hood there could be some sophisticated models to detect when the rules apply, what suggestions to make, and to give overall scores.\nThe rule based strategy lets you suggest the kinds of changes hirers will make, and those that you have strong evidence that will make an impact based on expertise, independent research, or modelling. It also lets you explain the suggestions in context which makes them more likely to be adopted. You may miss some things that a black box model will find, but the rules approach leads to a product that can actually help a hirer. The management of creating hundreds of these sorts of rules has a much better payoff than trying to build a complex neural network that adheres to many subtle constraints.\n\n\nAdding real world knowledge\nAnother benefit of rule based approaches is they let you add real world knowledge which can improve data efficiency, or even encode things that can’t be seen in the model. In the job advertisement example the model may not have known what changes a hirer would make when writing an ad (and obtaining this data might be very expensive), but a knowledgeable human might be able to hypothesise and collect qualitative data to support this. Adding this real world knowledge can make models much more effective and simple to implement.\nAs a simple example if you wanted to extract human readable dates (like “5th of April, 2020”) you could get dates annotated in thousands of documents and build a complex machine learning model to tag the spans of text. You could even go a step further and train it to output the year, month and day; although I’d worry whether it generalises well to years not in the training set! Or you could do what dateparser does and just list out the most common ways of writing dates. You could then do lots of different testing to capture most of the variants and implement a high precision date extractor that’s easy to maintain fairly quickly. Moreover this approach will operate orders of magnitude faster than the neural network, with higher reliability.\n\n\nExisting tools are far cheaper\nWhen there’s an existing tool that can do the job it will often be orders of magnitude cheaper to implement, operate, and maintain (which often makes more things worth doing) and may just perform better in general. To build a supervised model you need sufficiently many labelled data of a high enough quality, and it can be a challenge to choose the right model architecture, train it and debug any issues.\nIn the date parser example it was much cheaper and faster to build the rules, but it’s also very unlikely to give unexpected results. Even Google’s high profile language translator gives very unusual results that are hard to interpret with data far out of the training set.\n\n\n\nGoogle translates gibberish from Frisian to an unrelated sentence in English\n\n\nThese kinds of issues are notoriously hard to debug and fix in neural networks; whereas rules are much easier to trace and amend.\nWhen Dropbox created an application to extract text from uploaded documents they used a deep neural network to convert images of words to text. However they didn’t use the same neural network to detect the words in the image, instead using an algorithm called Maximally stable extremal regions (MSER). It wasn’t clear that training an image detector for words would work well with hundreds to thousands of words in a document. However MSER works very well, there’s an existing fast implementation in OpenCV, and is easy to debug and fix issues. The pipeline of MSER and a neural network image-to-text converter was right for their application.\n\n\nModularity\nInstead of having a single end-to-end model you can have multiple pieces in a pipeline that fit together and can be reused for different applications. For Dropbox’s text extractor they could use the off-the-shelf MSER algorithm, and if they found a better algorithm they could replace it with minimal impact on the image-to-text converter. This kind of technique is very common in natural language processing where there is a long history of using pipelines to get useful features out of text.\nThe spaCy library implements this kind of pipeline where it tokenizes the text (splitting the text into words), tags the parts of speech (like nouns and verbs), parses the dependencies (like saying which noun a verb refers to) and performs Named Entity Recognition (finding names of people, places, times).\n\n\n\nspaCy pipeline\n\n\nThere’s a great example from Ines Montani on how you could use this to extract sales figures from earning’s reports. You could use the entity recogniser to find company names and money, and text matching to find dates (like Q2 2018) then writing rules with the dependency parser to find the relations between money figures, dates and company names. Text classification could be used to help find the relevant sentences, or to tag whole paragraphs based on the header, to help make the process faster with less false positives. While perhaps you could build a deep neural network to do this, it would require a lot of annotation, experimentation with models, expensive equipment, and may just not work.\nHowever there is one big drawback to pipelines; they’re only as good as their weakest component. If the tokenizer fails (maybe because of some special character or stray HTML tag), then the part of speech tagger, dependency parser and named entity recogniser will all do poorly. This means you may have to do some analysis and improvements for each component to work really well on your data (maybe writing a custom tokeniser or retraining the dependency parser). The upside of this is it is normally easy to see where your pipeline breaks down and how to fix it; when a black box model performing a complex task doesn’t work it can be incredibly hard to understand why and how to fix it.\n\n\nPipelines of models and rules\nAdvances in deep learning have made whole new things feasible in fields like object detection and automatic translation and transcription. However deep learning models are expensive to annotate for, train, operate and debug, and it’s not clear upfront whether they will succeed at complex end-to-end tasks. A very promising approach is to combine these deep learning models with existing techniques like rules, algorithms and simpler machine learning models, to attach complex tasks. When Google put BERT into search results they used it to augment their results for complex search phrases, not to replace their existing well-functioning search algorithm.\nBreaking a complex task into steps makes it much easier to obtain reusable components that can be built or bought and optimised separately. This is as true in systems using machine learning as it is for general software engineering. While there are some cases where an end-to-end deep learning model is the right solution, it’s rarely the place to start."
  },
  {
    "objectID": "composition-over-inheritence/index.html",
    "href": "composition-over-inheritence/index.html",
    "title": "Composition Over Inheritence",
    "section": "",
    "text": "My current design started pragmatically; a data source needed to be able to fetch, extract and normalise. However I had two common data sources, CommonCrawl and Kaggle, and so it didn’t make sense to repeat the fetch logic everywhere and so I made these subclasses of the AbstractDatasource. Then a lot of the CommonCrawl datasources had data in JSONLD or Microdata format, and so I made these subclasses. And then each data source was a subclass of one of these.\n\n\n\nDatasource as a hierarchy\n\n\nExtensive use of inheritance is an architecture smell. Inheritence tightly binds things together and makes them hard to compose. This came up as I started to think about adding the Internet Archive or file storage datasources. Some of these would have data JSONLD or Microdata format, but that doesn’t fit in the hierarchy. Do I copy the logic around in different inheritance trees? Do I try to do something clever with multiple inheritance or mixins?\nLooking at the layers each step is an independent piece of logic. The CommonCrawlDatasource and KaggleDatasource are primarily about how to fetch the data. The Microdata and JSONLD classes are primarily about how to extract the data. Each source currently has its own way to normalise the data, but as I expand some of those could be shared. A data source should be composed of these pieces.\n\n\n\nDatasource as composition\n\n\nIn a language like Java this would be done by making Fetcher, Extractor and Normaliser classes, but in Python they can just be functions we pass at instantiation. This makes it easy to mix and match the components when they make sense. I’m sure this has a catchy design pattern name, but in effect a datasource is just a container of the 3 functions.\nInheritence is a very inflexible construct that tightly binds different code together. Wherever possible try to compose things out of components rather than use a hierarchy, because it allows a lot more flexibility and independence."
  },
  {
    "objectID": "commitment-burnout/index.html",
    "href": "commitment-burnout/index.html",
    "title": "Burnout from Creeping Commitments",
    "section": "",
    "text": "The way I was working was unsustainable, and it was taking a toll on my sleep, my health and my personal relationships. But it just crept up on me, I never consciously took on this responsibility, but I felt responsible for the teams delivery and with our deadlines it seemed the only way to make sure we deliver. However it also meant the team didn’t have a chance to work on some parts of the codebase I was using, or to contribute to the parts of the solution I was picking up. By not giving other team members the space to try, and potentially fail, they wouldn’t be able to improve and would feel disconnected from the outcomes.\nI took stock of my priorities and worked out what I could delegate, and focus more on where I could deliver value. I delegated some of the work I was doing to the new team member to help him get on board, and made sure there was enough time to give him feedback and let him iterate by setting expectations. I delegated the executive presentations to my manager and the product manager, who were more than capable of handling it without my involvement. I could then focus on the deep data science and software work I wanted to do, and the project management work I had to do because of my unique understanding of the systems.\nIn retrospect a lot of this happened because I didn’t have clear priorities, I ended up just saying yes to things until I got to burn out. One thing I need to work on more is having clear values and priorities that I communicate to the people around me. Then I can frame any new ask as a trade-off; I could spend time this week developing new rituals, but then I can’t work on this requirement and we won’t hit our delivery deadline. Offering options and trade-offs is a very powerful way to control the amount of work you do while empowering your stakeholders."
  },
  {
    "objectID": "fastai-callback-lisp-advise/index.html",
    "href": "fastai-callback-lisp-advise/index.html",
    "title": "Fastai Callbacks as Lisp Advice",
    "section": "",
    "text": "However there’s a similar concept from the early 90’s in Emacs Lisp called advising functions, and in the Common Lisp Object System as method combination. Emacs Lisp is very dynamic and is designed so that users can easily modify the functionality of core components without needing to directly edit that code. Advising a function lets you execute some code before or after the function is invoked letting you change its behaviour with side effects.\nLearning from how Lisp allows changes in functions can help design a better system for modifying machine learning training loops.\n\nfastai callbacks\nFastai callbacks allow for adding a wide variety of functionality; like recording metrics, clipping gradients or saving the best model.\n\n\n\nMany different functionalities can be included in callbacks\n\n\nThis is done by having places in the training loop code that can “call back” to external code.\n\n\n\nCallbacks hook in between different stages of model\n\n\nIn the training loop below from fastai v2 the callbacks corresponds to the calls to self; e.g. self('begin_batch').\ndef fit_epoch(self):\n    self.iters = len(self.dl)\n    try:\n        for i,(xb,yb) in enumerate(self.dl):\n            try:\n                self.iter, self.xb, self.yb = i, xb, yb;        self('begin_batch')\n                self.pred = self.model(self.xb);                self('after_pred')\n                self.loss = self.loss_func(self.pred, self.yb); self('after_loss')\n                if not self.in_train: continue\n                self.loss.backward();                           self('after_backward')\n                self.opt.step();                                self('after_step')\n                self.opt.zero_grad()\n            except CancelBatchException:                        self('after_cancel_batch')\n            finally:                                            self('after_batch')\n    except CancelEpochException:                                self('after_cancel_epoch')\nFor example to add a Callback that stops an epoch after 5 batches is straightforward.\nclass StopAfterNBatches():\n    def __init__(self, max_batches=5): self.max_batches = max_batches\n\n    def begin_batch(self):\n        if self.iter > self.max_batches:\n            raise CancelEpochException\nNotice that a Callback can access (and modify) the state of variables in the training loop, like iter. One drawback of this is if you’re not careful then different Callbacks may interfere and not be usable together.\nCallbacks can be added or removed by name:\ndef add_cbs(self, cbs):\n    for cb in list(cbs): self.add_cb(cb)\n\ndef add_cb(self, cb):\n    setattr(self, cb.name, cb)\n    self.cbs.append(cb)\n\ndef remove_cbs(self, cbs):\n    for cb in list(cbs): self.cbs.remove(cb)\nThe mechanism that is used to invoke the callbacks is fairly straightforward:\ndef __call__(self, cb_name):\n    res = False\n    assert cb_name in self.ALL_CBS\n    for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name, self) and res\n    return res\nFor a deeper understanding take a look at Sylvain’s Talk on Callbacks, or the fast.ai Deep Learning course (version 3 part 2).\n\n\nAdvice and Method Combinators\nCommon Lisp Object System and Emacs Advice lets you change the behaviour of a function by triggering custom code to be executed whenever the function is invoked. One frequent usecase is to add tracing or profiling around certain functions. There are examples of method combinators in Common Lisp, but here’s an example in Python.\nclass Foo(Advisable):\n    def bar(self, x): return x\n\nfoo = Foo()\ndef print_name(self, x): print(f'{self.name}: {x}')\n\nprint(foo.bar(2))\n# Output:\n# 2\n\nfoo.advise_before(foo.bar, print_name)\nprint(foo.bar(2))\n# Output:\n# Baz: 2\n# 2\n\nfoo.remove_advice(foo.bar, print_name)\n\nprint(foo.bar(2))\n# Output:\n# 2\nNotice this is similar to callbacks, and we can add and remove them, but we can define them on any function. This is implemented with a bit of metamethod magic in Python:\nfrom collections import defaultdict, OrderedDict\n\ndef apply_before(function, before_functions, extra_args=[]):\n    def out_function(*args, **kwargs):\n        for f in before_functions: f(*extra_args, *args, **kwargs)\n        return function(*args, **kwargs)\n    out_function.__name__ = function.__name__\n    return out_function\n\nclass Advisable(object):\n    def __init__(self): self.advice_before = defaultdict(OrderedDict)\n\n    def advise_before(self, name, advice):\n        if callable(name): name = name.__name__\n        if not callable(object.__getattribute__(self, name)):\n            raise ValueError('{} not a callable method'.format(name))\n\n        advice_name = advice.__name__\n        if advice_name is None: raise ValueError('The advice must have a name')\n\n        self.advice_before[name][advice_name] = advice\n\n    def remove_advice(self, name, advice):\n        if callable(name): name = name.__name__\n        if callable(advice): advice = advice.__name__\n        del self.advice_before[name][advice]\n\n    def __getattribute__(self, name):\n        value = object.__getattribute__(self, name)\n        if callable(value) and name not in ('__init__', 'advise_before'):\n            value = apply_before(value,\n                                 self.advice_before[name].values(),\n                                 [self])\n        return value\n\n\n\nFrom advice to callbacks\nWe could use this kind of method to refactor the fastai Callback by breaking it into separate functions:\n...\ndef fit_epoch(self, data):\n    try:\n        for xb, yb in data: self.fit_batch(xb, yb)\n    except CancelEpochException: self.cancel_epoch()\n\ndef fit_batch(self, xb, yb):\n    try:\n        pred = self.predict_batch(xb)\n        loss = self.loss_batch(pred, yb)\n        self.backward(loss)\n        self.optimise()\n    except CancelBatchException: self.cancel_batch()\n\ndef predict_batch(self, xb): return self.model(xb)\n\ndef loss_batch(self, pred, target): return self.loss(pred, target)\n\ndef backward(self, loss): loss.backward()\n\ndef optimise(self):\n    self.opt.step()\n    self.opt.zero_grad()\n\ndef cancel_epoch(self): pass\ndef cancel_batch(self): pass\n...\nWe could then add a callback as before:\nclass StopAfterNBatches():\n    def __init__(self, max_batches=5):\n        self.max_batches = max_batches\n\n    def __call__(self, model, xb):\n        if hasattr(model, 'iter'):\n            model.iter += 1\n        else:\n            model.iter = 0\n        if model.iter > self.max_batches: raise CancelEpochException\nHowever note that because we have to have a separate advice for each method, instead of fastai Callbacks, we would have to define many more classes for a general callback and communicate between phases by storing data on the model. This is a lot harder to maintain, and in this sense fastai callbacks are superior. It’s also going to be hard for someone reading the code to understand how it works (as they’ll need to dig into the __getattribute__ code). This barrier to entry is only worthwhile if this is something that’s going to be used all over a large codebase; otherwise it makes a small piece of code much harder to understand. However knowing that advice and callbacks are related means we can translate the idea of advice combinators to callbacks.\n\n\nDifferent ways of advising functions\nIn the example above we focused on advising before the function was called, so we get behaviour:\n# Before\nadvice(*args)\nfunction(*args)\nHowever there are a number of different advice combinators for combining functions.\nFor example there’s before-while pattern which was used in fastai v1\n# After While\nif advice(*args):\n    function(*args)\nBut the most interesting examples compose with the function being called.\n# Filter Args\nadvice(*function(*args))\n\n# Filter Return\nfunction(*advice(*args))\n\n# Around\nadvice(function, *args)\n\n\nA purer callback system\nThe fastai callback system involves communicating everything using state. This is flexible, and performant because we’re not copying large tensors around. However it increases the risk that different callbacks will interfere by modifying the same variable. (This is always a risk because that’s sometimes what they need to do to function properly!) It also increases the cognitive overhead of the programmer because they need to know about the entire internal state of the training process to use it.\nA better approach could be to pass the data as arguments; so instead of\nself.iter, self.xb, self.yb = i, xb, yb;        self('begin_batch')\nself.pred = self.model(self.xb);                self('after_pred')\nself.loss = self.loss_func(self.pred, self.yb); self('after_loss')\nself.loss.backward();                           self('after_backward')\nself.opt.step();                                self('after_step')\nwe could use something like:\nself('begin_batch')\npred = self.model(xb)\npred = self('after_pred', xb, pred) or pred\nloss = loss_func(pred, yb)\nloss = self('after_loss', loss, pred, yb) or loss\nloss.backward()\nself('after_backward', self.model) # Look at model weights\nself.opt.step()\nself('after_step, self.model, self.opt)\nself.opt.zero_grad()\nThis is still a work in progress, and I’ll need to experiment with it to see whether it’s any better in production.\nFull source:\nfrom collections import defaultdict, OrderedDict\n\ndef apply_before(function, before_functions, extra_args=[]):\n    def out_function(*args, **kwargs):\n        for f in before_functions: f(*extra_args, *args, **kwargs)\n        return function(*args, **kwargs)\n    out_function.__name__ = function.__name__\n    return out_function\n\nclass Advisable(object):\n    def __init__(self): self.advice_before = defaultdict(OrderedDict)\n\n    def advise_before(self, name, advice):\n        if callable(name): name = name.__name__\n        if not callable(object.__getattribute__(self, name)):\n            raise ValueError('{} not a callable method'.format(name))\n\n        advice_name = advice.__name__\n        if advice_name is None: raise ValueError('The advice must have a name')\n\n        self.advice_before[name][advice_name] = advice\n\n    def remove_advice(self, name, advice):\n        if callable(name): name = name.__name__\n        if callable(advice): advice = advice.__name__\n        del self.advice_before[name][advice]\n\n    def __getattribute__(self, name):\n        value = object.__getattribute__(self, name)\n        if callable(value) and name not in ('__init__', 'advise_before'):\n            value = apply_before(value,\n                                 self.advice_before[name].values(),\n                                 [self])\n        return value\n\nclass Foo(Advisable):\n    name = 'Baz'\n    def bar(self, x): return x\n\nfoo = Foo()\ndef print_name(self, x): print(f'{self.name}: {x}')\n\nprint(foo.bar(2))\n# 2\n\nfoo.advise_before(foo.bar, print_name)\nprint(foo.bar(2))\n# 4\n# 2\n\nfoo.remove_advice(foo.bar, print_square)\n\nprint(b.f(2))\n# 2"
  },
  {
    "objectID": "r-keeping-up/index.html",
    "href": "r-keeping-up/index.html",
    "title": "R: Keeping Up With Python",
    "section": "",
    "text": "Python has a wider audience than R, and keeps to its reputation as “not the best language for anything but the second best language for everything”. It may be the best language for Deep Learning; both Tensorflow and PyTorch are developed primarily for Python. The package ecosystem for Python is growing quickly.\nHowever R aims squarely at analysts and statisticians, who are not as close to the low level details but are typically closer to the problems to solve. A strength of this is there is a lot more focus in the R community on usability. Dplyr, and its derivatives such dbplyr, give a clear consistent way of manipulating data; compared with the hodge podge of Pandas (which is closer to base R), SQLAlchemy, and PySpark. RMarkdown and its derivatives blogdown and bookdown make it really easy to integrate text and executed code in a way that’s much more difficult than Sphinx and produces more reproducible and aesthetic output than Jupyter Notebooks. Ggplot2 makes it much easier to produce and iterate on bespoke graphics than Matplotlib and Seaborn (Altair is closer but can’t produce graphics directly since it builds on Vega-lite). Shiny makes it easy to build an interactive analytics application; I’m not sure whether Dash is up to scratch now but the alternative in Python is building your own Django application which requires much more expertise. I suspect for model fitting R is more versatile than Python’s scikit-learn (I’ll know after I read Tidy Modeling With R).\nR is also doing admirably filling in the gaps where Python is ahead. In particular reticulate lets you use Python from R, which can cover many gaps (and there’s rpy2 for calling R from Python). The renv package helps with the pain point of environment management, catching up with Python’s pip and venv (and it can manage Python dependencies too for cross-language projects). There’s an interface to Tensorflow, and now to Torch in R.\nWhile I’d still stick to Python for most production analytics workloads, R is doing an admirable job. As an analytics tool R is more powerful and much easier to use; if it keeps leaning into these strengths it will continue to be popular for a long time."
  },
  {
    "objectID": "social-flashcards/index.html",
    "href": "social-flashcards/index.html",
    "title": "Social Flashcards",
    "section": "",
    "text": "However remembering things about people are really important for building relationships. If you take an interest in other people’s lives they will be more receptive to you. So having a bad memory for these things puts me at a disadvantage, and I often feel awkward that I’ve forgotten someone’s partner’s name.\nSpaced repetition is a very effective tool to memorise facts, and there are great tools such as Anki for doing this. I’ve started building an Anki deck for relationships; storing important facts about people I know.\nIt feels a little conceited to consciously learn social facts. But it’s because I honestly care enough about people that I want to be able to talk to them about their lives without getting self conscious that I can’t remember the name of their eldest child."
  },
  {
    "objectID": "solving-solved/index.html",
    "href": "solving-solved/index.html",
    "title": "Solving Solved Problems",
    "section": "",
    "text": "I heard an interesting technique from Jeremy Howard in one of the fast.ai courses about how to read a paper. First read the abstract and introduction. Then spend a couple of days trying to implement what you think they’re talking about. Then go back and read the rest of the paper and see how it compares to what you did. Most of the time you’ll find that they did something you didn’t think of, once in a while you’ll have come up with something better than they did. But either way you’ll be a lot more invested in the paper and appreciate the results much more.\nThis idea of trying to solve problems, even ones that have already been solved, resonates with me. Really deeply understanding something means spending a lot of time with it, approaching it from different angles. This has to be an active process, but reading is mostly a passive process. Trying to do it yourself first, failing, and then reading the answer is much more impactful than reading it straight away.\nAn example of this technique is how the mathematician Michael Murray invented Bundle Gerbes. In his paper An Introduction to Bundle Gerbes he talks about how he was thinking about gerbes, and ordered a book that looked relevant. However it took a couple of months for that book to be shipped to Australia and he spent a lot of time trying to understand the advertising material for the book. When the book came it turned out not to be at all relevant to what he was interested, but from thinking about what he thought the book was about he invented Bundle Gerbes.\nOne downside of having so much information available at our fingertips is it’s easy to access a lot of information without really engaging in it. It’s worthwhile sometimes delaying your gratification and thinking deeply through how you would solve a problem before looking up the answer, even in a paper. This is a great way to get better at solving problems; if you are good at solving problems without worrying about whether they have been solved before, one day you’ll solve a problem no-one else has ever solved."
  },
  {
    "objectID": "binomial-power/index.html",
    "href": "binomial-power/index.html",
    "title": "How big a sample to measure conversion?",
    "section": "",
    "text": "So let’s say we want to measure the conversion rate within about 5%. To be conservative we’d want the standard error to be a bit less than that, say 3% Then we would need at least \\(\\frac{1}{4 (0.03)^2} \\approx 278\\) samples. Note that to double the precision we need to quadruple the sample.\nIf you want to run a null hypothesis test with 95% CI and 80% power then you need to multiply by the square of 2.8. That is \\(\\frac{(2.8)^2}{4 \\sigma^2}\\), where \\(\\sigma\\) is the detection size. For an A/B test we require double this (since the variances add); so to see an uplift of 5% would require \\(\\frac{2 (2.8)^2}{4 (0.05)^2} = 1570\\) in each group (actually a little more due to the continuity correction. If this seems too big maybe you don’t actually want a significant test; look into something like test and roll.\nFor the details on why this is read from Bernoulli to the Binomial"
  },
  {
    "objectID": "wsl2-start/index.html",
    "href": "wsl2-start/index.html",
    "title": "Getting Started with WSL2",
    "section": "",
    "text": "It took me a while to try WSL2 because, as per the introductory blog post I was waiting for Windows version 2004 to appear when running “Check for Updates”. But it has been 2 months since the May release and it still hadn’t appeared. So after checking there were no known issues with my hardware, I manually downloaded the update.\nAfter some time the upgrade installed without any issues. When I booted Docker for Windows prompted me to install WSL2, because it can now run through that. I followed the WSL2 installation instructions, and installed Ubuntu 20.04 LTS from the Microsoft Store as my Ubuntu version (I had Ubuntu 18.04 in WSL1). I then set it up with WSL2 using wsl --set-version Ubuntu-20.04 2.\nI was getting confused because wsl --set-default-version 2 wasn’t changing the default version when I checked with wsl -l -v. This is because I was using a different distribution, and had to set the default distribution with wsl -d Ubuntu-20.04.\nFinally because I’m using the excellent Windows Terminal I set the default profile to WSL2.\nUnfortunately there are a couple of things that don’t work out of the box. I can’t get Emacs working with VcXsrv, even after following these instructions for working with Emacs and WSL; it just defaults to -nw. Another issue is that I can’t just run things on localhost in WSL and expect it to work in Windows, and have to deal with some virtual networking.\nBut after a few years on WSL it’s amazing to have much faster speeds and have things like emacs magit and Python virtualenvs feel usable again."
  },
  {
    "objectID": "australian-deaths/index.html",
    "href": "australian-deaths/index.html",
    "title": "How many People in Australia Die?",
    "section": "",
    "text": "The life expectancy in Australia is about 80 years, and the population is 25 million. So each year the number of people that die would be about 25 million divided by 80, which is about 300,000.\nThe actual number of people that died in 2018 is 160,000. This is about half my estimate; what am I doing wrong?\nOne factor is life expectancy is at birth, the longer people live the longer they will be expected to live. The number of deaths increases sharply with age especially from age 55. The number of living people decreases sharply with age as well. However I can’t really put it all together - why does this add up to a factor of 2?\nI actually thought this would be a simple calculation; but it turns out I don’t understand the fundamental concepts! This is a great outcome from doing these kinds of calculations - now I know that I don’t know something, which is the first step towards learning it."
  },
  {
    "objectID": "cluster-exploration/index.html",
    "href": "cluster-exploration/index.html",
    "title": "Clustering for Exploration",
    "section": "",
    "text": "There are many techniques to cluster structured data or even detect them as communities in the graph of interactions with your users. The problem is evaluating the clusters is very difficult, and requires a lot of product expertise. There are in fact different useful ways to group products; for example a retailer could group together all books, or they could group the Harry Potter books with the Harry Potter movies. Depending on your application you could use a different grouping; in fact you could have overlapping hierarchies, or just a generic measure of difference, but they’re harder to use.\nTrying to group things manually even with thousands of products is hard; the number of possible groupings grows exponentially with the number of items. However if you have an example grouping, splitting the thousands of products into a couple dozen groups, it’s straightforward to manually check if it makes sense. This can be a great tool for better understanding the products and how they relate, and it’s relatively easy to move items between groups manually to improve the grouping. This means using any technique to get a cluster in the right ballpark will be very useful for exploring the data in ways that would be very difficult without it.\nThis kind of balance between using algorithms and domain expertise is very powerful; while it’s not totally data driven you can use knowledge not in the data (for example on new products) to improve the result."
  },
  {
    "objectID": "jupyter-download/index.html",
    "href": "jupyter-download/index.html",
    "title": "Downloading files from Jupyter Notebook",
    "section": "",
    "text": "You can download individual files from the file navigator (which you can get to by clicking on the Jupyter icon in the top left corner). You just need to click the checkbox next to the file you want to download and then click the “download” button at the top of the pane. The only drawback is it only works for individual files; not directories or multiple files.\n\n\n\nDownload interface in Jupter\n\n\nAn even easier way in a Python notebook is to use FileLink:\nfrom IPython.display import FileLink\nFileLink(filename)\nthen a link will display below the cell.\nIf you want links to multiple files in a single cell you need to use display to show them all (otherwise it only gets shown if it’s the last line of the cell):\nfrom IPython.display import FileLink\ndisplay(FileLink(filename1))\ndisplay(FileLink(filename2))\nWhat if you want to download multiple files or a whole directory? You don’t really want to have to click “download” a whole bunch of times. But you can easily put them all in a zipfile in Python, and then download the single zipfile as before. For example if you wanted to download all the csvs in the current directory:\nfrom zipfile import ZipFile\nfrom pathlib import Path\nzipname = 'sample.zip'\nfilenames = Path.glob('*.csv')\nwith ZipFile(zipname, 'w') as zipf:\n    for name in filenames:\n        zipf.write(name)\nIf you want to download a whole directory there’s an even easier way with shutil.make_archive.\nfrom shutil import make_archive\nmake_archive('sample.zip', 'zip', directory)\nThen you can download the files very quickly via FileLink without ever leaving the Jupyter notebook."
  },
  {
    "objectID": "duplicate-tfidf/index.html",
    "href": "duplicate-tfidf/index.html",
    "title": "Near Duplicates with TF-IDF and Jaccard",
    "section": "",
    "text": "When trying to find similar ads with the Jaccard index we looked at the proportion of n-grams they have in common relative to all the n-grams between them. However if both contain a common phrase like “please contact our office” then they could be spuriously thought to be common. So the idea is to weight them down by the inverse document frequency. Then we can use the weighted Jaccard Index \\(J_\\mathcal{W}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sum_i \\min(x_i, y_i)}{\\sum_i \\max(x_i, y_i)}\\).\nLooking at the frequencies of values for 1-grams and 4-grams for a sample of 2000 ads this gives a very similar result:\n\n\n\nHistograms for TF-IDF and regular Jaccard are similar\n\n\nInspecting the results it seems broadly similar to the unweighted version; it doesn’t give a major separation benefit. I also lowercased the words for the TF-IDF but not for the unweighted version, so that might make part of the difference between the two. Moreover in practice it’s harder to use because when you get new data you would need to reevaluate everything with the new TF-IDF.\n\nMethod\nWe use scikit-learn’s TfidfVectorizer, and use ngram_range=(4,4) to get just the 4-grams.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer_4 = TfidfVectorizer(lowercase=True, \n                        tokenizer=tokenize, \n                        stop_words=None, \n                        ngram_range=(4,4), \n                        norm='l2',\n                       use_idf=True,\n                       smooth_idf=True,\n                       sublinear_tf=False)\nWe then fit the data to our sample of 2000 ads to get the weights matrix:\nX4 = tfidf_vectorizer_4.fit_transform((ads[index] for index in sample_indices))\nWe can get the weighted Jaccard, using the inclusion-exclusion principle \\(\\sum_i \\min(x_i, y_i) + \\sum_i \\max(x_i, y_i) = \\sum_i x_i + \\sum_i y_i\\) to avoid calculating the maximum:\ndef weighted_jaccard(d1, d2):\n    n = d1.minimum(d2).sum()\n    return n / (d1.sum() + d1.sum() - n)\nWe can then calculate the weighted Jaccard for all distinct pairs.\nans = {}\nfor i in X4.shape[0]:\n    for j in range(X4.shape[0]):\n        if i < j:\n            ans[(i, j)] = weighted_jaccard(X4[i], X4[j])\nThis doesn’t seem to be dramatically better, so we’ll drop the complexity of the TF-IDF and go back to just calculating common n-grams."
  },
  {
    "objectID": "coarse-geocoding/index.html",
    "href": "coarse-geocoding/index.html",
    "title": "Coarse Geocoding",
    "section": "",
    "text": "For example one job post has an addressRegion of Wales and and addressLocality of Dyfed. If I put this in the Nominatim geocoder built on Open Street Map it gives a plausible location in the United Kingdom. Similarly it can tell me that a job with addressLocality of Regensburg is in Bavaria, Germany. It can also tell me the country メキシコ is Mexico.\nThis is great, but if I want to use it at scale to process my data they have a usage limit of 1 request per second. While I could process the data slowly, or go to a commercial provider, I should be able to do it myself if it’s build on Open Streetmap Data.\nNominatim can be self-hosted but it requires 64GB of memory for an installation and setup including PostGIS. This is because it’s designed to be a performant address search engine at an address level. Trying to batch normalise addresses to a country level with it is like using a sportscar to plough a field.\nHowever the placeholder library is a much better fit. It’s a component of the Pelias geocoder, but for geocoding at a regional level, and runs on a 2GB SQLlite database. They have a live demo and a guide to getting a coarse geocoder in (almost) one line. Another candidate is twofishes from foursquare, but they don’t make it as easy to use.\nIt’s actually really easy to get started and returns a list of JSON results which works accross multiple languages. The top result is normally really good; here’s an example of the output containing the ‘lineage’ of regions above it and a bounding box. The ids refer to Whos on First.\n{'id': 85676997,\n 'name': 'Dakar',\n 'abbr': 'DK',\n 'placetype': 'region',\n 'population': 3137196,\n 'lineage': [{'continent': {'id': 102191573,\n    'name': 'Africa',\n    'languageDefaulted': True},\n   'country': {'id': 85632365,\n    'name': 'Senegal',\n    'abbr': 'SEN',\n    'languageDefaulted': True},\n   'region': {'id': 85676997,\n    'name': 'Dakar',\n    'abbr': 'DK',\n    'languageDefaulted': True}}],\n 'geom': {'area': 0.045757,\n  'bbox': '-17.530915772,14.586814028,-17.1262208188,14.8863398389',\n  'lat': 14.772684,\n  'lon': -17.220068},\n 'languageDefaulted': True}\nOn my laptop running in a docker container I can fetch and parse a result in around 50ms. This is about 20 requests per second, so it would take around 14 days to get through all 1 million JobPosting URLs in the 2019 Web Data Commons extract. However many locations are repeated and this could be reduced by an order of magnitude with some caching."
  },
  {
    "objectID": "html2text_bi/index.html",
    "href": "html2text_bi/index.html",
    "title": "An edge bug in html2text",
    "section": "",
    "text": "I’ve been trying to find a way of converting HTML to something meaningful for NLP. The html2text library converts HTML to markdown, which strips away a lot of the meaningless markup. But I quickly hit an edge case where it fails, because parsing HTML is surprisingly difficult.\nI was parsing some HTML that looked like this:\nWhen I ran html2text it produced an output like this:\nThis isn’t correct markdown; there shouldn’t be a space after the underscore.\nI was pretty unlucky to hit this so quickly. Having only one type of emphasis or whitespace between the . and <i> tag would produce correct output. So once I found a minimum reproducible example I raised an issue."
  },
  {
    "objectID": "html2text_bi/index.html#understanding-the-issue",
    "href": "html2text_bi/index.html#understanding-the-issue",
    "title": "An edge bug in html2text",
    "section": "Understanding the issue",
    "text": "Understanding the issue\nThe html2text library was originally written by Aaron Swartz, the inventor of Markdown. The bulk of it is implemented as a very large Python html.parser object.\nThe issue comes in how whitespace is handled in the following code:\ndef no_preceding_space(self: HTML2Text) -> bool:\n    return bool(\n        self.preceding_data and re.match(r\"[^\\s]\", self.preceding_data[-1])\n    )\n\nif tag in [\"em\", \"i\", \"u\"] and not self.ignore_emphasis:\n    if start and no_preceding_space(self):\n        emphasis = \" \" + self.emphasis_mark\n    else:\n        emphasis = self.emphasis_mark\n\n    self.o(emphasis)\n    if start:\n        self.stressed = True\n\nif tag in [\"strong\", \"b\"] and not self.ignore_emphasis:\n    if start and no_preceding_space(self):\n        strong = \" \" + self.strong_mark\n    else:\n        strong = self.strong_mark\n\n    self.o(strong)\n    if start:\n        self.stressed = True\nWhen there’s not a preceding space one is added. However this inserted space in the output should count as preceding, but it doesn’t, and so another space is added.\nSince this is the only usage of preceding_data a safe workaround is to append a space to self.preceding_data. There are many other ways we could possibly preserve the state if altering this may break existing code. I’ve tried submitting this as a pull request.\nParsing HTML in a meaningful way is really hard! The html2text library is very mature and does very well, but even it has weird edge cases."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the personal writings of Edward Ross; Data Scientist, Writer and Learner."
  },
  {
    "objectID": "about.html#things-ive-built",
    "href": "about.html#things-ive-built",
    "title": "About",
    "section": "Things I’ve built",
    "text": "Things I’ve built\n\nWhatcar.xyz a Deep Learning classifier for Australian cars (inference source).\nA system for extracting and cleaning Australian job ads from Common Crawl."
  },
  {
    "objectID": "about.html#things-ive-written-about",
    "href": "about.html#things-ive-written-about",
    "title": "About",
    "section": "Things I’ve written about",
    "text": "Things I’ve written about\nI like to write lots of small articles on skeptric.com.\n\nCalculating Moving Averages in SQL\nSearching Common Crawl with CDX\nSearching common Crawl with Athena\nFinding near duplicates with Minhash\nCalculating the Centroid on a Spherical Polygon\nConstant models as baselines for predictive modelling"
  },
  {
    "objectID": "about.html#things-ive-worked-through",
    "href": "about.html#things-ive-worked-through",
    "title": "About",
    "section": "Things I’ve worked through",
    "text": "Things I’ve worked through\nI sometimes keep examples of tutorials and books I’ve worked through\n\nComputational Exercises from All of Statistics by Larry Wasserman\nExercises from Regression and Other Stories\nBaking Pi Tutorial for building a Baremetal OS on Raspberry Pi\nSome NLP algorithms inspired bu Jurefsky’s Speech and Language Processing, 3rd edition"
  },
  {
    "objectID": "two-point-eight/index.html",
    "href": "two-point-eight/index.html",
    "title": "Statistical Testing: 2.8 Standard Deviations",
    "section": "",
    "text": "The ratio \\(\\frac{\\sigma}{\\epsilon}\\) says the smaller the effect size is relative to the variability in the data the larger the sample size you will need. The quadratic dependence means that if you can double the effect size you only need 1/4 of the sample; wherever possible maximise your effect.\nBut why the factor 2.8? For a 95% confidence the measured value must be 1.96 standard deviations from the mean. But there’s variability in the measure itself - if the true effect size is \\(\\epsilon\\) the measurement will be drawn from a normal distribution centred at \\(\\epsilon\\) with standard deviation \\(\\sigma\\). So using 1.96 as the sample coefficient would only measure the effect half the time. We need to increase our sample size so that two standard deviations occurs at the 20th percentile of the distribution for \\(\\epsilon\\); namely 1.96 + 0.84 = 2.8 standard deviations. More generally the coefficient for \\(\\alpha\\) confidence at \\(\\beta\\) power is \\(z_{1 - \\alpha / 2} z_{1 - \\beta}\\), or in R qnorm(1-(1-alpha)/2) + qnorm(1-(1-beta)).\nAs a side note you shouldn’t skimp on power. If your study is under-powered you’re much more likely to overestimate the effect size or even get the sign wrong when you do find a significant result. Andrew Gelman and John Carlin’s paper Beyond Power Calculations go into the detail; essentially because a significant result has to be 2 standard deviations from the mean, if you’re unlucky enough to see a spurious “statistically significant” result, the apparent effect size will always be far too large.\nAnd do you really want to minimise type 1 errors anyway? When you’re making decisions you typically want to make the best decisions subject to constraints, and often less precise estimates of effects is good enough (it’s easy for the effect size to be below practical significance). An example of this approach is Test and Roll that first “learns” the best alternative with an A/B test and then “earns” by rolling it out to the rest of the population; but rather than picking sample sizes for 95% confidence, they pick sample sizes to maximise expected returns."
  },
  {
    "objectID": "getting-started-rstan/index.html",
    "href": "getting-started-rstan/index.html",
    "title": "Getting Started with RStan",
    "section": "",
    "text": "A simple place to start is a linear model \\(y \\sim N(\\alpha + X \\beta, \\sigma)\\) (often written, equivalently, as \\(y \\sim \\alpha + X \\beta + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma)\\). We can write out the model in Stan explicitly as follows and save it to linear_flat.stan:\n// linear_flat.stan Linear model\ndata {\n  int<lower=0> N;              // Number of data points\n  int<lower=0> K;              // Number of predictors\n  matrix[N, K] X;              // Predictor matrix\n  real y[N];                   // Observations \n}\nparameters {\n  real alpha;           // intercept\n  vector[K] beta;       // coefficients for predictors\n  real<lower=0> sigma;  // error scale\n}\nmodel {\n  y ~ normal(alpha + X * beta, sigma); // target density\n}\nNote that another way we could have written the model is in terms of how it increments the log probability target += normal_lpdf(y | alpha + X * beta, sigma), but for simple cases like this I find the sampling statement y ~ normal(alpha + X * beta, sigma) much clearer.\nWe can then construct some data in R and fit the model using rstan::stan (assuming “linear_flat.stan” is in the working directory):\npredictors <- mtcars[, 2:ncol(mtcars)]\n\nstan_data <- list(\n    N = nrow(mtcars),\n    K = ncol(predictors),\n    X = predictors,\n    y = mtcars$mpg\n  )\n\nfit <- rstan::stan(\"linear_flat.stan\", data=stan_data)\nThe fit is a stanfit object which contains the matrix of posterior draws, which is useful for all kinds of things. For example we could plot the posterior distributions of the coefficients (although note because we haven’t standardised the regression variables the coefficients aren’t comparable to each other - a small coefficient on a variable that changes a lot could have a big impact).\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggformula)\n\nfit %>% \nas.data.frame() %>% \npivot_longer(starts_with(\"beta\")) %>%\ngf_violin(name ~ value, scale=\"width\")\n\n\n\nDistribution of regression coefficients\n\n\nWe can also get out estimates of the regression coefficients and their variation, by taking their median and (scaled) Median Absolute Deviation. Note as well as the coefficients (we also get the lp__ which is the log density up to a constant, which I’m not interested in here).\nrbind(as.data.frame(fit) %>% summarise_all(median),\n      as.data.frame(fit) %>% summarise_all(mad)) %>%\nmutate_all(round, 1)\nThe first row is the estimate (median) and the second is the error (MAD SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalpha\nbeta[1]\nbeta[2]\nbeta[3]\nbeta[4]\nbeta[5]\nbeta[6]\nbeta[7]\nbeta[8]\nbeta[9]\nbeta[10]\nsigma\nlp__\n\n\n\n\n10.71\n-0.07\n0.01\n-0.02\n0.76\n-3.68\n0.84\n0.29\n2.50\n0.68\n-0.19\n2.77\n-46.93\n\n\n19.59\n1.07\n0.02\n0.02\n1.72\n1.98\n0.77\n2.15\n2.23\n1.50\n0.86\n0.45\n3.02\n\n\n\nThis gives similar estimates to lm(mpg ~ ., data=mtcars) or rstanarm::stan_glm(mpg ~ ., data=mtcars, prior=NULL, prior_intercept = NULL, prior_aux = NULL).\nThis works but it’s a bit of work to get all the right data for Stan in R, and the coefficients names like beta[2] are not very meaningful. Let’s look at making 3 improvments:\n\nUsing a formula interface to get the coefficients\nAdding names to the outputs\nWrapping it all in a function\n\n\nUsing formula interface to get the coefficients\nIf we have a formula like mpg ~ . and a data frame we need to get the vector of responses (e.g. mtcars$mpg) and the matrix of predictors.\nFor getting the responses I couldn’t find a simpler way than model.response(model.frame(formula, data)). The model.frame extracts all the columns from the formula, response and predictors, and the model.response just gets the column of responses.\nThe predictors matrix can be extracted using model.matrix. Unfortunately this also contains the intercept which we want to take out of our matrix (a more flexible solutions is to remove \\(\\alpha\\) from the model).\nHere’s a little piece of logic to remove the first column, which should be the intercept, from the model. This will fail for formula’s without an intercept (e.g. mpg ~ cyl - 1).\nremove_intercept_from_model <- function(X) {\n    if (colnames(X)[1] == '(Intercept)') {\n        X[,-1, drop=FALSE]\n    } else {\n        stop('Missing Intercept')\n    }\n}\nNote the drop=FALSE which prevents it converting X to a vector when there’s only one column left. This is important because Stan is expecting a matrix (as I learned the hard way).\nHere’s some examples of using this:\nX <- matrix(c(1,2,3,4,5,6), ncol=2)\ncolnames(X) <- c('(Intercept)', 'foo')\n\nremove_intercept_from_model(X)\n\n\n\nfoo\n\n\n\n\n4\n\n\n5\n\n\n6\n\n\n\nX <- matrix(c(1,2,3,4,5,6), ncol=3)\ncolnames(X) <- c('(Intercept)', 'foo', 'bar')\n\nremove_intercept_from_model(X)\n\n\n\nfoo\nbar\n\n\n\n\n3\n5\n\n\n4\n6\n\n\n\n\n\nAdding useful names to the model\nIt turns out we can just set the names of the stanfit object to the appropriate variable names, and it will propagate everywhere. To do this I needed some logic that would take the names from the stanfit object, and the coefficient names, and return the vector of names to set. I’ll assume that alpha and beta[...], and sigma are the first coefficients (as specified in the parameters of the Stan model), and just try to update alpha and beta[...].\nget_linear_names <- function(fit_names, coef_names) {\n    # Should check (Intercept) not in coef_names\n    if (fit_names[1] != 'alpha') {\n        stop(\"Unexpected name alpha\")\n    }\n    \n    K <- length(coef_names)\n    if (!all(fit_names[2:(1+K)] == paste0('beta[', 1:K, ']'))) {\n        stop(\"Unexpected name in beta\")\n    }\n    \n    # Check nothing is missing\n    if (fit_names[2+K] != \"sigma\") {\n        stop(\"Expected Sigma\")\n    }\n    \n    \n    c('(Intercept)', coef_names, fit_names[(K+2):length(fit_names)])\n}\nget_linear_names(c('alpha', 'beta[1]', 'beta[2]', 'sigma', 'cat'),\n                 c('a', 'b'))\nGives '(Intercept)' 'a' 'b' 'sigma' 'cat'\n\n\nWrapping it in a function\nNow we have all the pieces we can combine it all together into an easy to use function:\nfit_stan_linear_flat <- function(formula, data,\n                            ...) {\n    y <- model.response(model.frame(formula, data))\n    X <- remove_intercept_from_model(model.matrix(formula, data))\n    \n    K <- ncol(X)\n    N <- nrow(data)\n    \n    fit <- rstan::stan(\n        file = \"linear_flat.stan\", \n        data = list(\n            N = nrow(X),\n            K = ncol(X),\n            X = X,\n            y = y\n          ),\n        ...\n        )\n    \n    names(fit) <- get_linear_names(names(fit), colnames(X))\n        \n    fit\n}\nThen running our model is as easy as fit_stan_linear_flat(mpg ~ ., data=mtcars), giving the same model as before but with more helpful column names. This function should be pretty much equivalent to rstanarm::stan_glm(formula, data=data, prior=NULL, prior_intercept = NULL, prior_aux = NULL), but the latter runs much faster because of optimisation tricks (for many predictors setting QR=TRUE in stan_glm makes it run much faster again).\nSo we can now run our custom built Bayesian linear regression in Stan and R. But what priors did we set on the coefficients? From the Stan User Guide\n\nIn Stan, if you declare but don’t define a parameter it implicitly has a flat prior (on the scale in which the parameter is defined)\n\nWe could do better by setting a weakly informative prior, or an informative prior where we have one. We talk through how to that in the next article."
  },
  {
    "objectID": "similar-companies/index.html",
    "href": "similar-companies/index.html",
    "title": "Finding Duplicate Companies with Cliques",
    "section": "",
    "text": "Adzuna gets most of its ads by aggregating ads from other job ad websites. I have noticed that sometimes and advertiser has slightly different names when it is sourced from different websites. For example a company is called “360 Rockwool” on jobs sourced from totaljobs.com, and “Rockwool” on jobs from other sources. It would be useful to be able to identify jobs posted by the same company to better understand the job ad data.\nWhen a job ad is written by a company that regularly recruits people they usually use a standard template that contains things like a description of the company. So we’d expect the Jaccard similarity of these job ads to be high for a number of job ads. So the idea is to look for groups of 5 job ads that all have at least 20% 3-Jaccard similarity to each other. The cutoffs could be tuned but it’s a fine place to start.\nWhile this does find some similar companies it also does bring in some noise, like jobs sources to multiple recruiters. A bigger barrier is the algorithm to find all cliques is exponential and so for larger groups it takes longer than is practicable.\n\nFinding cliques\nFrom our pairs of near duplicate texts we have a list of pairs of job ads with 3-Jaccard similarity of at least 20% in a vector similar. We can then separate it into connected components with networkx\nimport networkx as nx\nG = nx.Graph(similar)\nsimilar_connected = sorted(nx.connected_components(G), key=len)\nWe can then extract from these the cases where the group contains at least 5 ads and they correspond to different companies in the source dataframe df.\nmulticompany = [list(idxs) for idxs in similar_connected\n                if len(idxs) >= 5\n                and (df\n                     .iloc[list(idxs)]\n                     .Company\n                     .dropna()\n                     .unique()\n                     .size) > 1]\nThen we can extract the cliques containing at least 5 members using find_clique:\nn = 0\nSG = G.subgraph(multicompany[n])\ncliques = [clique for clique in nx.find_cliques(SG) if len(clique) >= 5]\nWe can then examine the companies in the cliques and the job ad texts.\n\n\nExamining similar companies\nThis technique definitely extracts some similar companies. There are some interesting examples where a job board name has replaced the company name, or it has been completely removed. Sometimes the terms or contact details that identify the company have been removed making it difficult to directly extract from the text. In these cases it can be difficult to determine what the source company actually is.\nI found most of the examples I looked at were from recruitment companies sourced across multiple job boards. It’s common practice for recruiters to use software to post to multiple job boards, and how their name appears probably depends on how they set up the accounts and integrations, which is why this happened. Recruiter job ads on Adzuna tend to be fairly bland and not have much company detail, so are hard to identify.\nLooking at a bigger clique with 10 ads I found an example where it looks like the same job had been contracted out to two different recruiters. A substantial portion of the text was the same, with some slight changes, but they came from different recruitment companies. In this case just because the ads were very similar did not mean the companies were the same.\nBetween all this it seems it’s quite difficult to identify the true company that posted the ad on Adzuna. On a higher quality dataset the best approach would likely be to try to extract the company name from the ad text because most companies have a section about themselves in the ad.\n\n\nCliques are slow to find\nThis approach worked well on small graphs, but finding all the cliques in the graph can take exponential time (and there can be exponentially many cliques!). Once I got to graphs of around 400 nodes I saw this issue. I found on a graph with 369 nodes it took 3 seconds, with 392 nodes it took 12 seconds, with 427 nodes it took 20 seconds and I’m still waiting on a graph with 429 nodes.\nI’m sure there are other heuristic techniques that would work for finding dense groups on the graph that are more efficient, like using mincut to iteratively reduce the graph to a highly connected core."
  },
  {
    "objectID": "probability-square/index.html",
    "href": "probability-square/index.html",
    "title": "Probability Squares",
    "section": "",
    "text": "For example suppose we had a random process that generated 1, 2 or 3 with equal probability (for example half the value of a die, rounded up). If we want to calculate the probability distribution of adding the values of two results we can use the probability square. We can work out the relative areas just by counting the squares with the same value to create the distribution on the right.\n\n\n\nDistribution of 2 three sided dice\n\n\nIf you want to combine 3 (or more) such things conceptually it’s a cube (or hypercube) which you could flatten into slices. But it’s much easier to draw as 2 separate products; first combining two variables and then combining the result with the third. For example the distribution of adding three outcomes equally split between 1 and 3 can be obtained from the previous result.\n\n\n\nDistribution of 3 three sided dice\n\n\nConceptually you can think of taking a sample from the product distribution as throwing a dart at the product square. If the variables were not independent the lines for each segment wouldn’t be straight; they would vary by segment. For a continuous variable you could cut up the cumulative distribution function on values of probability to estimate the product square; this is the kind of idea used in measure theory.\nIt’s not a very useful computational device, but it’s handy for thinking through the basics of probability."
  },
  {
    "objectID": "job-extraction-pipeline/index.html",
    "href": "job-extraction-pipeline/index.html",
    "title": "Building a Job Extraction Pipeline",
    "section": "",
    "text": "I need a way to extract the job ads from heterogeneous sources that allows me to extract different kinds of data, such as the title, location and salary. I got stuck in code for a long time trying to do all this together and getting a bit confused about how to make changes. So I spent some time thinking through how to do it, and then it was straightforward to implement.\nThe architecture of download, extract and normalise makes it easy to inspect and incrementally add to the pipeline. The first step download gets the raw web archives from Common Crawl; this lets me inspect the source HTML directly in a browser of notebook. The next step extract takes the structured data from the HTML and outputs JSON Lines; this lets my inspect all the structured data I can extract from the webpages for a source. The final step normalise takes the extracted fields and applies normalisations to get them in a common format; this allows all the data to be aligned and merged. When adding a new data source I can first use the downloaded data to construct a parser, then extract with the parser to get all the raw data, then finally transform and align it in with a normaliser. If I want to add a new column to the output I just need to write the corresponding normaliser to every source, as long as the information is in the underlying extracted data.\nInitially I was trying to combine parts of the normalise and extract steps, then run the same normalisers to convert HTML to text, extract the salary and normalise the location. The problem is that not every source requires the same steps; one source emits plain text rather than HTML, some have the salary amount and period as separate fields rather than together as a string, and some have very different ways of normalising the location. I was trying to solve all this with the extract step but it got very confusing and it was mixing transformation with extraction.\nWriting separate normalisers for each source means I end up with more repeated code, although a lot of it uses common functions. But it makes it easier to make small adjustments per source, to debug the process and results in a much simpler codebase (simple in the Rich Hickey sense). It also allows me to start simply with one or two normalisers such as title and description, and then incrementally add more such as posted date, location, and salary."
  },
  {
    "objectID": "all-of-statistics/index.html",
    "href": "all-of-statistics/index.html",
    "title": "All of Statistics",
    "section": "",
    "text": "As I was working more in analytics I found that I needed to understand more about statistics. People around me were talking about ANOVA and I didn’t know what that meant (was it something to do with linear regression?) I also had a friend recommend me The Elements of Statistical Learning, but I didn’t have enough statistics to get through the first chapter.\nAfter researching online I bought Statistics in a Nutshell. This is a good practical reference, it covers many situations and discusses the techniques to use with worked examples. But I felt I got really stuck on why all these different tests existed. It seemed like you needed a reference guide to tell you what tests to perform - but what if you were doing something that wasn’t in the guide.\nThen I came across Larry Wasserman’s All of Statistics. I spent about 18 months working through it (on trains, in coffee shops before work and at home in the evenings). I have 4 notebooks working through the exercises and the code exercises. It enabled me to understand how to use different statistical tests, gave me the terminology to read The Elements of Statistical Learning and broadened my statistical horizons.\nHere are some highlights of things I have used since reading the book:\n\nAlgebraic tricks for expectations and variances like \\(V(X + Y) = V(X) + V(Y) + 2 \\rm{Cov}(X, Y)\\) (which are helpful when reading books/papers)\nBootstrap confidence intervals are very powerful for calculating statistics on large datasets where the central limit theorem doesn’t apply\nMaximum Likelihood Estimators as a tool for fitting parametric models\nHypothesis tests essentially cut a confidence interval; the various statistical tests are (estimates of) confidence intervals for different distributions\nWhen doing multiple tests you need to adjust your error rate using something like a Bonferroni Correction\nHow ggplot’s density plots use Kernel Density Estimation and to always set the bandwidth to ucv or SJ\nImplementing tree algorithms and understanding the order of factors of a categorical variable matters\nLearning about bagging and boosting\n\nThere’s still a lot of the book I haven’t used much, especially Statistical Decision Theory and Causal Inference, but I got a lot out of working through the book. More importantly it gave me the tools and language to be able to practice statistics and machine learning."
  },
  {
    "objectID": "non-desarguesian-projective-planes/index.html",
    "href": "non-desarguesian-projective-planes/index.html",
    "title": "Non-desarguesian projective planes",
    "section": "",
    "text": "Suppose we have an n-dimensional affine space over a skewfield k. We can construct an (n-1)-dimensional projective space by taking the pencils of lines as points (a pencil of lines is a complete set of parallel lines; that is the lines under the equivalence relation of parallelism), and 2-pencils of planes (complete sets of parallel planes) as lines, and so on. A less invariant way is to choose an origin, so we get a vector space; then each pencil is represented by a unique line through the origin, and each 2-pencil is represented by a unique plane through the origin; thus the points of projective space are lines through the origin and the lines are planes through the origin. Thus a projective space of dimension n can be constructed as the quotient of an affine or vector space of dimension n+1.\nNow given an n-dimesional projective space, consider an arbitrary (n-1)-dimensional projective subspace P. We can use this to form an affine subspace of dimension n: The points are the projective points not on P, the lines are the set of all lines not in P, and two lines are said to be parallel if they intersect at the same point of P. Given any line l and a point L not on that line the line through L and (the intersection of l with P) is the only line parallel to l through L. Thus a projective space of dimension n can be decomposed into an affine space of dimension n and a projective space of dimension (n-1).\nThis second construction can in fact be extended to all projective planes. Given a skewfield we have a natural way of representing two dimensional (left)-affine lines, as the set of points (x,y) satisfying the equation y = xa+b for some a and b. For general planes we extend the notion of this triple T(x,a,b)=xa+b. A planar ternary ring is a set R with at least two elements and a ternary operation \\(T: R \\times R \\times R \\to R\\) that satisfies:\n\nFor each a, b, c in R there exists a unique x in R such that T(a,b,x) = c\nFor each \\(a \\neq a'\\) , b, b’ in R there exists a unique x in R such that T(x, a, b) = T(x, a’, b’)\nFor each \\(a \\neq a'\\) , b, b’ in R there exists a unique pair (x, y) in \\(R \\times R\\) such that T(a, x, y) = b and T(a’, x, y) = b’.\n\nThen the following construction, roughly following Weibel(who’s following Hall), yields a projective plane by approximately the reverse of the construction above. The points are \\(R \\times R\\) (the ordinary points), R and \\(\\infty\\) (the projective line at infinity). The lines are \\(\\{(x,y) \\in R \\times R | y = T(x, a, b)\\} \\cup \\{a\\}\\) for each a in R and b in R, \\(\\{(c,y) | y \\in R\\} \\cup \\{\\infty\\}\\) for each c in R (these are the ordinary lines) and \\(R \\cup \\{\\infty\\}\\) (the line at infinity). One can check that this does in fact define a projective plane.\nTwo ternary rings (R, T) and (R, T’) are comparative if there exist permutations on R \\(\\alpha, \\beta, \\{\\phi_x\\}_{x \\in R}, \\{\\psi_x\\}_{x \\in R}\\) such that \\(\\phi_x \\circ T(x, a, b) = T'( \\alpha \\circ x, \\beta \\circ a, \\psi_a \\circ b)\\) for all a,b, x in R. It is easy to show comparative rings yield isomorphic projective planes.\nWe can conversely construct a ternary ring from a projective plane, but there are clearly a lot of choices to be made; we need to choose a line at infinity, a y-axis and some isomorphisms between the y-axis minus it’s intersection with infinity and other lines, and use the lines and these isomorphisms to define T(x, a, b). Different choices need not yield comparative ternary rings (see here for a necessary and sufficient condition).\nIf we add additional structure this is sometimes unique; for instance given a projective plane coordinitised by an alternative division ring, isomorphic projective planes yield isomorphic alternative division rings (see Bruck and Kleinfeld – The structure of alternative division rings, for a proof). I’m not sure if this is the best result one can obtain; in the finite case the coordinate rings are isomorphic (in the sense of Hall ternary rings; ternary rings with a 1 and a 0) if and only if they are a finite field.\nIt’s interesting to note a generalised construction of projective planes of the first type for every division algebra over the real numbers (including the octonions). The points and lines of the space are each the 3-dimensional Hermitian idempotents of unit trace (that is 3×3 matrices P satisfying \\(P = P^\\dagger\\) , \\(P^2=P\\) so P is a projection, and trace(P)=1). A point P lies on a line Q when PQ+QP=0. It can be shown this is a projective plane (see Conway and Smith, On Quaternions and Octionions, for the details)."
  },
  {
    "objectID": "local-lie-groups/index.html",
    "href": "local-lie-groups/index.html",
    "title": "Local Lie Groups and Hilbert’s Fifth Problem",
    "section": "",
    "text": "Hilbert’s Fifth Problem essentially asked how restrictive just looking at analytic actions is – what if we looked at continuous actions, how many more groups would we get?\nTheorem [Gleason, Montgomery, Zippin] For a locally compact group \\(G\\) the following are equivalent:\n\n\\(G\\) is locally Euclidean.\n\\(G\\) has no small subgroups; i.e. there exists a neighbourhood of the identity that contains no non-trivial subgroups of \\(G\\).\n\\(G\\) is a Lie Group.\n\n\nThis is very remarkable: Every topological group that is a manifold is a Lie group! That is if the group operation is continuous and forms a topological manifold then there is an analytic structure on the manifold such that the group operation is analytic.\n(Note that this is not true at all for non-group manifolds: In dimensions \\(\\geq4\\) there exist topological manifolds that admit no smooth structure, let alone an analytic one. Evidently these can not be given a compatible group structure.)\nNow if we take a neighbourhood of the identity we can paste an \\(n\\)-dimensional Lie group onto an \\(n\\)-dimensional vector space via the manifold structure – which is extremely convenient for calculations: we have explicit coordinates and everything is analytic so we can extend it (unless we hit a pole). Consequently we get the notion of a Local Lie Group: the restriction of a Lie group to some submanifold.\nEssentially the definition of a local group is a Hausdorff topological space with a “locally defined” group: the product and inverse are only defined near the identity and the product is associative (where it’s defined). (For a proper definition see the references later in the post).\nWe say a local group is globalisable if it is the restriction of some topological group.\nIf a locally Euclidean local group is globablisable then the solution to Hilbert’s Fifth Problem would imply that it is a local Lie group.\nIt turns out not every local group is globalisable even if we impose an analytic structure (making it a local Lie group) – a very elegant explicit example is given in a paper by Olver here.\nMal’cev (apparently) proved that a local topological group is globalisable if and only if it is globally associative; for a group it is well known that associativity implies that all products with any number of terms is well defined, however for a local group this is not the case (in this sense a local group is something like a higher categorical group). If all finite products of elements are independent of the order (bracketing) of operations we say it is globally associative. This is the trick in Olver’s paper: he explicitly constructs a local Lie group that has all 3-times products well defined (i.e. associative in the usual sense), but products of 4-terms depends on the bracketing.\nHowever close to the identity any local (Lie) group is globally associative: so some restriction of the group is globalisable.\nWith all this in mind we have to be a bit careful if we try to look at a local version of Hilbert’s fifth problem, but this is what Isaac Goldbring has done.\nHe proves that each locally Euclidean topological local group has a restriction that is a local Lie group.\nSo my question is what can local topological groups do that global ones can’t?\nThe papers I’ve referenced give some hints: The theory of local topological groups is applicable to the theory of cancellative topological semigroups – but I have trouble thinking of what these mean (if a Lie group is about locally symmetries, a cancellative Lie semigroup is about…? A semigroup would be about mappings (non-invertible transformations) but cancellative is a strong restriction) .\nAnother interesting construction is the compactification of a group (alluded to in Olver’s paper). As I understand it you can add an “infinite group element” to a non-compact group [that may not be associative, invertible or even cancellative] to make it a compact local group. Admittedly these seem like rather nasty structures though – I doubt analysis would be much easier on this compact structure.\nPostscript: There is a lot of excellent information on Hilbert’s Fifth Problem on Terry Tao’s website."
  },
  {
    "objectID": "sql-diff/index.html",
    "href": "sql-diff/index.html",
    "title": "Diffing in SQL",
    "section": "",
    "text": "For exact matching you can use union all to find the number of rows that don’t occur in both datasets. For approximate matching you can use a join to check whether the differences are within some bounds. These techniques work well together: for an approximate match you can first check the keys with union all, and then check the values with join.\n\nExact Matching: Union All\nA simple way of checking whether two tables are the same is to use UNION ALL. This query will return all the rows that occur in only one or the other table, and which table (‘old’ or ‘new’) that they occur in.\nSELECT col_1, col_2, ..., col_n,\n       count(*) as mult,\n       max(source) as source\nFROM\n(\n  SELECT col_1, col_2, ..., col_n,\n         'old' as source\n  FROM A\n\n  UNION ALL\n\n  SELECT col_1, col_2, ..., col_n,\n         'new' as source\n  FROM B\n)\nGROUP BY col_1, col_2, ..., col_n\nHAVING COUNT(DISTINCT source) <> 2\nThe only downside of this is it can be a lot of typing if the tables have a lot of columns to type in. You could always template this in a programming language; for example in the excellent dbplyr you can easily do a variation like this:\nunion_all(table_a %>% mutate(source='old'),\n          table_b %>% mutate(source='new')) %>%\ngroup_by(across(-c('source'))) %>%\nfilter(n_distinct(source) != 2)\n\n\nApproximate Matching: Join\nSometimes the results are allowed to vary a little bit because of slight changes to the data, random inputs and race conditions. In this case the union all approach will return much more than we want. The union all is still useful for checking both tables have the same keys; we can then use a join to check the values are similar.\nFor example to get all rows where the column val changes by more than 5% you could use something like:\nSELECT *\nFROM A\nJOIN B on a.key_1 = b.key_1 and a.key_2 = b.key_2 ...\nWHERE a.val NOT BETWEEN 0.95 * b.val and 1.05 * b.val\nThese kinds of approximate joins require a lot more thought; how much variation is expected? Maybe it’s ok for 1% of the rows to vary by more than 5%, but only 0.1% of the rows to vary by more than 10%. However when you’ve decided the rules they’re pretty straightforward to implement in SQL."
  },
  {
    "objectID": "jaccard-containment/index.html",
    "href": "jaccard-containment/index.html",
    "title": "The Problem with Jaccard for Clustering",
    "section": "",
    "text": "Suppose you have sets that you want to cluster together for analysis. For example each set could be a website and the elements are people who visit that website. We want to group together similar websites.\nThere’s a niche blog B, and every single person who visits it visits a very popular news aggregator A. The Jaccard similarity is simply the number of people who visit website B divided by the number of people who visit site A, which is a very small number. However B will be quite similar to another niche blog C that a few of it’s members visit.\n\\[J(A,B) = {{|A \\cap B|}\\over{|A \\cup B|}}\\]\n\n\n\nSets A, B and C\n\n\nDepending on your application this might be the wrong metric; you really do want to emphasise that B is similar to A. Moreover we’ll only end up with similar sized objects in clusters which makes aggregation less effective. The Overlap Coefficient is another metric that instead of using the union of sets in the denominator, it uses the size of the smallest set. So in the case A contains B the overlap coefficient will be its maximum value of 1.\n\\[\\operatorname{overlap}(A,B) = \\frac{| A \\cap B | }{\\min(|A|,|B|)}\\]\nHowever there’s another problem with this; almost every person uses the Google search engine. So every site will have an overlap close to 1 with Google, which will make the clustering awful.\n\nIdeas from Association Rule Learning\nI find the terminology from Association Rule Learning (a.k.a market basket analysis) very useful here. Association Rule Learning tries to find groups of items that often go together, but here we’re just interested in the simple case of pairs. A transaction in Association Rules corresponds to a person in our example, and an item corresponds to a website. We’re looking at the association rules between websites \\(A \\Rightarrow B\\) between websites. Note that our notation is dual to the usual for Association Rule Learning because it is in terms of itemsets and we are writing in terms of transactions.\nThe confidence of the rule \\(A \\Rightarrow B\\) is the containment \\(\\frac{\\lvert A \\cap B \\rvert}{\\lvert A \\rvert}\\). This is the probability that someone visits website B given that they visited website A. So in our examples above our niche blog will have a confidence of 1 for visiting both the news aggregator and google. Note that the overlap coefficient is just \\(\\operatorname{overlap}(A,B) = \\max ( A \\Rightarrow B, B \\Rightarrow A )\\).\nThe lift of the rule \\(A \\Rightarrow B\\) is how much more likely a user is to visit B given that they visited A. It’s \\(\\frac{ \\lvert A \\cap B \\rvert \\lvert U \\rvert }{\\lvert A \\rvert \\lvert B \\rvert }\\), where U is the total number of users. So suppose our website B has 100 users, the news aggregator A has 10,000 users and Google has 1 million users (which is all of them). Then the lift from B to A is 100, and the lift from B to Google is 1 (because it’s essentially independent). So the lift complements the confidence showing how specific the relationship is.\nIt seems at first glance lift could make a good measure of similarity. However consider the case when there’s only one person in the overlap; this might blow up the lift by pure chance. That’s why it’s often complemented by confidence and support (the size of the set relative to all users). Perhaps this instability could be resolved with a resampling approach; bootstrapping the lift (defaulting to 1 when there’s no overlap)?\nI’m still not sure the right way to cluster this kind of scenario, but I’m reasonably convinced Jaccard similarity is not it."
  },
  {
    "objectID": "on-applying-optimisation/index.html",
    "href": "on-applying-optimisation/index.html",
    "title": "On Applying Optimisation",
    "section": "",
    "text": "These techniques can solve the linear constrained problem\n\\[\\begin{align}\n& \\text{maximize}   && \\mathbf{c}^\\mathrm{T} \\mathbf{x}\\\\\n& \\text{subject to} && A \\mathbf{x} \\le \\mathbf{b}, \\\\\n&  && \\mathbf{x} \\ge \\mathbf{0}, \\\\\n\\end{align}\n\\]\nwhere some of the x must be integers (the quadratic version is solvable too). However it’s hard to immediately see how this helps with real problems.\nThe optimisation part is straightforward; we’re trying to maximise some objective (e.g. profit, negative cost, negative time). The constraints then form minimum requirements and encode contention between resources or limits. A classic example is the Stigler Diet, trying to minimise the cost of a diet (objective) from a certain list of foods (variables) that meets all nutritional needs (constraints).\nPutting something in this form can be difficult, but there are lots of benefits if you can. In the 60 years since there’s been improvements in the algorithms, computers are exponentially more powerful and there now exists very good software to run it. Even with lots of variables and constraints the optimal solutions can be found relatively quickly, and can be recomputed as the information changes.\nI haven’t used these techniques much but I would love to use them more. Part of the problem is I work mainly in digital spaces that have fewer constraints (although there are some, such as attention, screen space, computer time) and the problems I’ve dealt with often have only a few variables so can be solved with brute force methods.\nThe really hard thing seems like formulating the right objective and constraints (as in general defining the problem can harder than solving it). The book Model Building in Mathematical Programming by Williams looks like a promising work in framing problems in this way."
  },
  {
    "objectID": "typing-beautiful-soup/index.html",
    "href": "typing-beautiful-soup/index.html",
    "title": "Type Checking Beautiful Soup",
    "section": "",
    "text": "With some small changes to your code you can make it typecheck with BeautifulSoup. This helped me catch some places where I could have got an error when None was returned (the Billion Dollar Mistake). The naive way to do this requires peppering your code with assertions, but a much better way is to use CSS selectors.\nConsider something that extracts a the text from a h1 tag like this:\nimport bs4\n\nsoup = bs4.BeautifulSoup(html, \"html5lib\")\ndata = soup.h1.b.get_text()\nIn the source this is resolved with __getattr__:\n    def __getattr__(self, tag):\n        \"\"\"Calling tag.subtag is the same as calling tag.find(name=\"subtag\")\"\"\"\n        #print(\"Getattr %s.%s\" % (self.__class__, tag))\n        if len(tag) > 3 and tag.endswith('Tag'):\n            # BS3: soup.aTag -> \"soup.find(\"a\")\n            tag_name = tag[:-3]\n            warnings.warn(\n                '.%(name)sTag is deprecated, use .find(\"%(name)s\") instead. If you really were looking for a tag called %(name)sTag, use .find(\"%(name)sTag\")' % dict(\n                    name=tag_name\n                )\n            )\n            return self.find(tag_name)\n        # We special case contents to avoid recursion.\n        elif not tag.startswith(\"__\") and not tag == \"contents\":\n            return self.find(tag)\n        raise AttributeError(\n            \"'%s' object has no attribute '%s'\" % (self.__class__, tag))\nI could annotate this, but if I don’t annotate all the other functions and attributes this could lead to incorrectly inferred attributes, and that’s a lot of annotation. Instead I could just use my code above to use find directly.\ndata = soup.find(\"h1\").find(\"b\").get_text()\nThe BeautifulSoup code has some sort of type annotations in the docstring which makes it much easier to annotate. It tells us the return type of find is bs4.element.Tag | bs4.element.NavigableString. Looking at the implementation it just returns the first results of find_all, or None if there isn’t one. The docstring for find_all tells us that it’s a ResultSet (a subclass of list) of PageElements. It turns out that Tag and NavigableString are PageElements so this more or less lines up. So putting this together we have a first implementation of a type stub at bs4/element.pyi\nfrom typing import Optional\n\nclass PageElement:\n  pass\n\nclass Tag(PageElement):\n    def find(\n        self,\n        name: Optional[str] = None,\n        attrs={}, recursive=True, text=None, **kwargs\n    ) -> Optional[PageElement]: ...\n    \n    def get_text(\n        self, separator: str = \"\",\n        strip: bool = False\n    ) -> str: ...\nTrying to typecheck now comes up with an error:\nerror: Item \"None\" of \"Optional[PageElement]\" has no attribute \"find\"\nThis makes a good point; what if the page doesn’t have an h1? I have to do something, perhaps in this case I know my pages will contain a h1 and so this isn’t a problem. The same is true for the b inside the h1, but this is more common, and maybe we have a default action here. I can either suppress the type error with an explicit assertion (for the h1) or explicitly handle the None case (for the b):\nh1_tag = soup.find(\"h1\")\nassert h1_tag is not None\nh1b_tag = h1_tag.find(\"b\")\nif h1b_tag:\n    data = h1b_tag.get_text()\nelse:\n    data = None\nBut now I get a different error:\nerror: \"PageElement\" has no attribute \"find\"\nerror: \"PageElement\" has no attribute \"get_text\"\nNeither PageElement nor NavigableString have a find or get_text method. But I can’t actually see with this usage how I can get a NavigableString (maybe I can with a certain set of arguments to find). So I could explicitly declare this with an ugly bunch more assertions:\nh1_tag = soup.find(\"h1\")\nassert h1_tag is not None\nassert isinstance(h1_tag, bs4.Tag)\nh1b_tag = h1_tag.find(\"b\")\nif h1b_tag:\n    assert isinstance(h1b_tag, bs4.Tag)\n    data = h1b_tag.get_text()\nelse:\n    data = None\nThis is pretty horrible. But there’s a better way with CSS selectors. They can represent all sorts of complex relationships with combinators, in this case to find a bold element in a h1 the selector is h1 b (using the descendent combinator). By constructing some path with CSS Selectors we only need to check whether the whole path is null, not every step of the way. CSS Selectors are a similar power to XPath, but you can test them in a browser console with document.querySelectorAll\nBeautifulSoup has CSS selector methods; select for multiple results, and select_one for the first result (or None if there aren’t any). According to the types in the code they always return Tag so there’s no need to worry about NavigableString or other types of PageElement. We can add these to our type stub:\nclass Tag(PageElement):\n    def select_one(\n        self, selector: str, namespaces: Optional[Dict[str, str]] = None, **kwargs: str\n    ) -> Optional[Tag]: ...\n    def select(\n        self, selector: str, namespaces: Optional[Dict[str, str]] = None, **kwargs: str\n    ) -> List[Tag]: ...\nThen we have a much simpler method, that gets a good tradeoff between simplicity and capturing the cases where the tag is absent.\nh1b_tag = soup.select_one(\"h1 b\")\nif h1b_tag:\n  data = h1b_tag.get_text()\nelse:\n  data = None\nThat’s enough to get a long way with type checking BeautifulSoup. For more check out my tips on using BeautifulSoup."
  },
  {
    "objectID": "book-review-human-loop-machine-learning/index.html",
    "href": "book-review-human-loop-machine-learning/index.html",
    "title": "Human-in-the-Loop Machine Learning: Book review",
    "section": "",
    "text": "Human-in-the Loop Machine Learning by Robert (Munro) Monarch is an excellent book on annotating data for Deep Learning practitioners in industry. It covers the whole process from selecting data with active learning, to quality control for teams of annotators, to the annotation interface. These are explained comprehensively focused on applying them in practice, and illustrated with common tasks such as text or image classification, image segmentation, object detection, named entity recognition, and translation.\nI haven’t been able to find many good resources on the holistic process of annotating data for machine learning, and this stands out as a gem. Other books focus too much on the technical annotation model, representation, and tooling like Natural Language Annotation for Machine Learning and Training Data for Machine Learning or dense academic tomes (like Language Corpora Annotation and Processing or the Handbook of Linguistic Annotation). In contrast Human-in-the Loop Machine Learning is a readable, focused work on efficiently getting humans to annotate data for real applications, full of useful insights from the authors real world experience. While at times it goes into great depth, for example in advanced active learning strategies, it’s mostly material that’s not well covered elsewhere, and the structure of the book makes it easy to only read what’s relevant. Given the multitude of resources available on machine learning it’s really surprising there aren’t more practical resources on data annotation.\nThe book consists of 5 parts; first steps, active learning, annotation, human-computer interface, and bringing it all together. The first steps has a detailed hands-on example of annotating news headlines referring to a disaster illustrating many of the concepts of the book, and which I’ve previously written about. Active learning explores ways of using models to effectively select data to annotate in a way that leads to better models with less human cost. Annotation goes in depth in how to work with annotation teams from how to work with people to annotate data through to quality control. Human Computer Interaction explores how to represent annotation tasks in a way that leads to fast, meaningful, and high quality annotation as well as exploring human-in-the loop use cases. The final chapter brings everything together with 3 case studies of different human-in-the-loop machine learning models, with their source code.\n\nActive learning\n\nYou should use active learning when you can annotate only a small fraction of your data and when random sampling will not cover the diversity of the data.\n\nThe active learning section discusses different ways to pick examples that are representative of a population, that the model is uncertain about, or that are diverse. The recommended method is interleaving all three methods; the representative samples form a conservative baseline, the uncertain items help improve the model by identifying borderline cases, and the diversity sampling finds items the model is unlikely to have seen. This also helps with human factors; uncertainty sampling and diversity sampling help pick interesting samples that keep the annotator engaged.\nRepresentative sampling is the simplest method; most generally picking items completely at random from those available. This helps the model get slowly better, but in many cases its inefficient because a lot of similar items end up being annotated that don’t help the model. It’s always a safe fallback if the other methods fail. There is some choice in terms of the population, and it can be extended by stratifying across different factors such as time or demographics to make it representative across these factors.\nChoosing the examples that the model is uncertain about is a good way of getting the most model improvement per annotation. It turns out there’s a lot of ways of doing this and subtlety in the method. In particular this book goes in depth into how the base (or equivalently temperature) of the softmax can change what ranks as “most uncertain”; this exemplifies what this book is great at - going into the details of things you won’t find elsewhere. It goes through increasingly sophisticated ways of active learning, such as using other models to predict the model uncertainty.\nI hadn’t previously considered the importance of diversity sampling; finding the “unknown unknowns” of the model. However this is important in real world scenarios; we normally want the model to work in unusual cases. It’s not obvious how to find these; some methods suggested are using clustering, LDA, or samples where the last layer of the neural network has low activation. Each of these methods can find different kinds of outliers, but the text gives no clear way to evaluate them.\n\n\nAnnotation and accuracy\n\nThe more respect you show the people who label your data, the better your data becomes.\n\nThe book has chapters on how to work with annotation teams, and how to evaluate their accuracy. I haven’t worked much in this area but the suggestions on annotation teams make a lot of sense, and seem to be drawn from the authors experience. Treat the people who label data well, make sure they understand their impact, work closely with them. There are good intuitive outlines of different commonly used statistics.\nOne concept I struggled with was the idea of “ground truth” and treating annotators as independent. How do you get “ground truth” examples for data labelled by humans, when even experts will make mistakes? Treating annotators as independent is statistically convenient for quality control, but might you get better results if you give feedback and let annotators collaborate? While these ideas are useful for modelling quality control, some careful thought needs to go into any particular usecase.\n\nA real-time chat allows annotators to collaborate on hard-to-annotate items, the only downside being that quality control becomes harder when annotators aren’t independent.\n\nIndeed what “ground truth” is depends a lot on the application. Designing an appropriate annotation model and guidelines is slippery business, and there are lots of edge cases. The book could have gone into more detail here about choosing and defining tasks appropriate to a business goal and dealing with ambiguity.\n\nwe spent more time on refining the definiiton of what goes into a span than on any other part of the task\n\nThere are some good references from the book that go into more of the difficulty of annotating data:\n\nTruth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation\nAre We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets\nRevolt: Collaborative Crowdsourcing for Labeling Machine Learning Datasets\n\n\n\nInterfaces\nThe user interface can make a huge impact on efficiency, accuracy, and the agency in annotation. There are lots of good examples in the text about designing user interfaces, such as using predictive typing to speed up translation (which is much more efficient than correcting suggestions), or instead of highlighting sentiment words asking to edit the words to express the opposite sentiment (changing the task to a more informative one), or Extreme clicking for efficient object annotation where instead of drawing a bounding box you click on each of the highest, lowest, left-most and right-most point (which is 6x more efficient, and captures more information).\nThere’s lots of general user experience tips in here too. Making sure the interface can capture all the information annotators feel are important (general features include being able to add notes, or flag examples). If end users are annotating an optional field, a suggestion is to pre-fill it because “people are more averse to wrong data than to missing data” and are more likely to correct it.\n\nThe biggest shortcoming of training data provided by your users is that the users essentially drive your sampling strategy.\n\n\n\nConclusion\n\nUnderstanding the human task that you are solving will help every aspect of your prdoct design: interface, annotations, and machine learning architecture\n\nThe last chapter brings everything together in three fully coded Human-in-the Loop examples on Bicycle Detection, Food Safety and News Headlines. I’ll go into more detail on them in following posts, but these are very interesting and exciting examples of how these techniques can be used.\nThese kinds of tools could completely change the face of analytics. Working as an analyst machine translation (e.g. including via OpenNMT and Marian MT) has greatly increased my ability to find information in documents in other languages. However this just scratches the surface; these kinds of tools can help extract concepts via example and quickly scale annotation. I’ve previously found analytics has focused on things easy to extract; things that match regular expressions (such as phone numbers or email), or keyword matching with stemming and splitting. Anything that requires custom development is too expensive for a lot of one-off analytics use cases, but these techniques could greatly reduce the cost.\nThey have already changed the face of digital products. Things like auto-completion, pre-filling fields, and scrolling feeds are increasingly powered by machine learning with tight feedback loops. They involve a tight coupling between a computer interface and a machine learning model, that helps people achieve a task and allows appropriate annotation (often implicitly or explicitly by the end user). Some good resources referenced in the book for these are Microsoft’s Guidelines for Human-AI Interaction (see also their Human-AI eXperience (HAX) Toolkit) and Building Machine Learning Powered Applications by Emmanuel Ameisen.\nThis book fills a major gap in all the machine learning resources I can find, in a practical guide of how to efficiently collect accurate labelled data with worked examples of Human-in-the Loop processes. It focuses much more on annotation being a human task to help human ends than other machine learning books, which either take it as a given or focus on the technical process. It’s surprising there’s not more of this; when I studied Physics we learned about our measurement equipment, the basics of electronics, the kinds of systematic and random errors that occur, and did laboratory work of running experiments. I haven’t seen another machine learning book that talks about human factors in annotation, the basics of psychology, the kinds of systematic errors (such as priming) that occur, and actually labelled some data. The closest I’ve seen is in survey methodology, which tends to be focused more in marketing and user experience departments, and is different to many of the annotation tasks in machine learning. Hopefully as transfer learning and better tooling makes machine learning more accessible, there will be more resources like this book to ensure we get the most out of them.\n\nRather than spending a month figuring out an unsupervised machine learning problem, just label some data for a week and train a classifier.\nRichard Socher"
  },
  {
    "objectID": "diff-tests/index.html",
    "href": "diff-tests/index.html",
    "title": "Diff Tests",
    "section": "",
    "text": "For batch model training or ETL pipeline there’s typically a natural way to do this. You can take a sample of data and run it through the pipeline and inspect the output. If you run the data pipeline twice you should get (roughly) the same result.\nThe basic process is to create a test harness that can tell if two outputs are the same. For a file this can be as simple as running diff across the outputs. Sometimes there’s state you need to remove from the output or the tests; for example gzip contains a timestamp but zdiff ignores this in the calculations.\nAnother issue is if the output is non-deterministic due to randomness or race conditions. Sometimes this can be configured away by setting random seeds and running on a single thread on a single machine; but sometimes not. When not then you’ll have to run stochastic tests that check whether things are approximately the same; this is more difficult to implement and more difficult to determine bounds (you can do things like run the existing pipeline many times to get some bounds).\nWherever possible you should freeze inputs by using immutable snapshots from a database, letting time be a configuration parameter (rather than reading the system clock directly throughout the code), and caching any resources fetched from external APIs. These typically make the system easier to test, to inspect, recover from errors and run in small pieces so are generally beneficial.\nIt’s important for this workflow to get the code running as quickly as possible to allow fast iteration of changes. One way to do this is to isolate parts of the dataflow to the part you’re modifying and serialise the output somewhere; this reduces the amount of processing you need to run. Another is take a smaller sample of the data (which can be a bottleneck); however the smaller the sample the less edge cases you’re likely to find (it’s probably good to run a larger sample through once a day). Finally there are often ways to speed up the bottlenecks like paralellising (using multiprocessing or futures in Python), through caching frequent costly operations, or optimising the code for example by vectorising.\nThis methodology doesn’t replace tests. Tests can much more efficient at verifying the code works on a range of specified cases; and lets you specify all the edge cases that have given you trouble before (which may not occur in your sample dataset). Although the idea has some resonance with property based testing where you specify a relation; in this case that the new implementation should give the same result as the old implementation (within some bounds). However it lets you work on a codebase with no tests, and allows you get it into a workable state when you can start implementing tests."
  },
  {
    "objectID": "indirectness/index.html",
    "href": "indirectness/index.html",
    "title": "Don’t Not Avoid Being Indirect",
    "section": "",
    "text": "Instead of saying “I don’t want the soup”, I say something like “It’s not that I don’t like your soup; I just don’t feel like it right now. I mean it’s not my favourite soup and I wouldn’t be unhappy having it.” In response I’ll just get a confused stare. I still have to deliver the hurtful message that I don’t like the soup, and often have to explain what I meant in more detail. It takes courage to be direct, but delivers the painful message swiftly and concisely and we can move on from there.\nDescribing what something is not is a more moderate way of saying something in English, however it’s often more confusing than helpful. A typical response to “How’s it going” is “Not bad”, which is positive, but not as positive as “good”. Negations put a statement between the extreme polarities; saying “it’s not that I don’t like your soup” I’m stating that I don’t not like the soup, which is a weaker statement than that I like the soup. This is a peculiarly English thing; in Slavic languages more negations just emphasises the no more strongly, there’s no notion of a double negative. The Slavic way is easier to understand (and incidentally their spelling is logical, unlike English, too).\nI was recently reading A Founder’s Guide to Writing Well which emphasised the standard rule of using short, simple words. If you want to be understood you should make your writing and speech as easy to interpret as possible. I’m going to strive to be more direct and simple to be more easily understood, and show respect to my audience."
  },
  {
    "objectID": "rdf-to-dict/index.html",
    "href": "rdf-to-dict/index.html",
    "title": "Converting RDF to Dictionary",
    "section": "",
    "text": "This turns something like this:\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://schema.org/JobPosting> <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://schema.org/identifier> _:genid2d8020c9b7d2294a778072a41d6d59640a2db2 <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\n_:genid2d8020c9b7d2294a778072a41d6d59640a2db0 <http://schema.org/title> \"Category Manager - Prof. Audio Visual Solutions\" <http://jobs.anixter.com/jobs/inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719?lang=en_us> .\n...\nInto something like this (which is still not ideal but much closer to usable):\n{\n 'http://schema.org/employmentType': ['FULL_TIME'],\n 'http://schema.org/datePosted': [datetime.datetime(2019, 8, 1, 17, 48, 55)],\n 'http://schema.org/description': [...]\n 'http://schema.org/jobLocation': [{\n   'http://www.w3.org/1999/02/22-rdf-syntax-ns#type': ['http://schema.org/Place'],\n   'http://schema.org/address': [\n     'http://schema.org/addressCountry': ['United States'],\n     'http://www.w3.org/1999/02/22-rdf-syntax-ns#type': ['http://schema.org/PostalAddress'],\n     'http://schema.org/addressLocality': ['Glenview'],\n     'http://schema.org/postalCode': ['60026'],\n     'http://schema.org/addressRegion': ['IL']}]}],\n 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type': ['http://schema.org/JobPosting'],\n 'http://schema.org/validThrough': [datetime.datetime(2019, 11, 11, 0, 0)],\n 'http://schema.org/title': ['Category Manager - Prof. Audio Visual Solutions'],\n 'http://schema.org/identifier': [\n   'http://schema.org/name': ['Anixter International'],\n   'http://www.w3.org/1999/02/22-rdf-syntax-ns#type': ['http://schema.org/PropertyValue'],\n   'http://schema.org/value': ['inventory-management/glenview-il-60026-/category-manager-prof-audio-visual-solutions/153414552962719']}],\n 'http://schema.org/hiringOrganization': [\n   'http://schema.org/name': ['Anixter International'],\n   'http://www.w3.org/1999/02/22-rdf-syntax-ns#type': ['http://schema.org/Organization']}]}\nThe idea is pretty simple. We’re going to take each job posting object, and map each of its properties to the list of objects with which it’s associated. If one of those objects is a blank node we will recursively transform it into a dictionary of properties to the list of objects with which it’s associated. The resulting code is quite simple:\nclass CycleError(Exception): pass\n    \ndef graph_to_dict(graph, root, seen=frozenset()):\n    result = {}\n    for predicate, obj in graph.predicate_objects(root):\n        predicate = predicate.toPython()\n        if obj in seen:\n            raise CycleError(\n                f\"Cyclic reference to {obj} in {graph.identifier}\")\n        elif type(obj) == rdflib.term.BNode:\n            obj = graph_to_dict(graph, obj, seen.union([obj]))\n        else:\n            obj = obj.toPython()\n        result[predicate] = result.get(predicate, []) + [obj]\n    return result\nWe also need a way of extracting the nodes we want at the root of the document. One simple way to do this is to extract all nodes of a given schema.org type:\nfrom rdflib.namespace import Namespace\nSDO_NAMESPACES = [Namespace('http://schema.org/'), Namespace('https://schema.org/')]\ndef get_blanks_of_sdo_type(graph, sdo_type):\n    for namespace in SDO_NAMESPACES:\n        rdf_type = namespace[sdo_type]\n        for subject in graph.subjects(rdflib.namespace.RDF.type, rdf_type):\n            if type(subject) == rdflib.term.BNode:\n                yield subject\nWe could then use this, for example, to get all job postings:\ndef get_job_postings(graph):\n    return get_blanks_of_sdo_type(graph, 'JobPosting')\nThese can be stitched together to take in an nquads file and a type and output all graphs rooted of that type.\ndef extract_nquads_of_type(lines, sdo_type):\n    for graph in parse_nquads(lines):\n        for node in get_blanks_of_sdo_type(graph, sdo_type):\n            yield graph_to_dict(graph, node)\n\nRDF Graph to dictionary\nAn RDF consists of a set of triples (subject, predicate, object). Subject is typically a blank node, which you can think of like a variable. A predicate is typically a URI which describes the kind of relation, like <http://schema.org/Organization/name>. An object can either be another blank node, a URI or a Literal (which can be a string in some language, or a value like an integer or a date).\nThe idea is given a blank subject node we can represent it by all of its relations to other predicates and objects. Because a predicate can appear multiple times (e.g. a job could have two locations that you’re working at) in general we can represent it as mapping from a predicate to a list of objects. For a URI or Literal we can represent them directly as Python objects, and for a blank node we can transform that into a dictionary mapping predicates to lists of objects.\nThe code to do this is fairly simple. Given a blank node root we can get all the corresponding predicates and objects with graph.predicate_objects(root). It the object is a blank node then we run graph_to_dict on that node to expand it into a dictionary, otherwise we convert it to a Python object with toPython. Finnally append each object to the list of predicates.\ndef graph_to_dict(graph, root)):\n    result = {}\n    for predicate, obj in graph.predicate_objects(root):\n        if type(obj) == rdflib.term.BNode:\n            obj = graph_to_dict(graph, obj, seen.union([obj]))\n        else:\n            obj = obj.toPython()\n        predicate = predicate.toPython()\n        result[predicate] = result.get(predicate, []) + [obj]\n    return result\nThere’s one thing I worry about: will this terminate? What if there’s some nasty data with a cycle in it. Then it will recurse until we exceed the maximum stack depth. Here’s some example data.\nfrom rdflib import URIRef, BNode, Literal\n\nisa = rdflib.term.URIRef(\"http://example.org/is_a\")\nnota = rdflib.term.URIRef(\"http://example.org/not_a\")\n\nthing = BNode()  # a GUID is generated\nother_thing = BNode()\n\ntautology_graph = rdflib.Graph()\n\ntautology_graph.add((thing, isa, thing))\n\ncycle_graph = rdflib.Graph()\ncycle_graph.add((thing, nota, other_thing))\ncycle_graph.add((other_thing, nota, thing))\nThe easiest way to avoid this is to track the blank nodes on the path we’ve seen; if we see one of those nodes again then we’ve hit a cycle and should raise an error.\nThis gives the original code. Note that undefined objects will be represented as an empty dictionary.\nclass CycleError(Exception): pass\n    \ndef graph_to_dict(graph, root, seen=frozenset()):\n    result = {}\n    for predicate, obj in graph.predicate_objects(root):\n        predicate = predicate.toPython()\n        if obj in seen:\n            raise CycleError(\n                f\"Cyclic reference to {obj} in {graph.identifier}\")\n        elif type(obj) == rdflib.term.BNode:\n            obj = graph_to_dict(graph, obj, seen.union([obj]))\n        else:\n            obj = obj.toPython()\n        result[predicate] = result.get(predicate, []) + [obj]\n    return result\n\n\nFinding root nodes\nNow that we can convert a blank node to a dictionary, we need a way to find the relevant blank nodes. One strategy is just to get all the blank node subjects:\ndef get_blank_subjects(graph):\n    \"\"\"Returns all blank subjects in graph\"\"\"\n    return frozenset(s for s in graph.subjects() if type(s) == rdflib.term.BNode)\nBut some subjects are just used as objects of other subjects (e.g. an address in a job listing); so we could just use the ones that are not an object.\ndef get_blank_objects(graph):\n    \"\"\"Returns all blank objects in graph\"\"\"\n    return frozenset(o for o in graph.objects() if type(o) == rdflib.term.BNode)\n\n\ndef get_root_blanks(graph):\n    \"\"\"Returns all blank nodes that are not objects of any triple\"\"\"\n    return get_blank_subjects(graph) - get_blank_objects(graph)\nA different strategy is to get all the items of a certain RDF type. Many objects have some RDF type which is defined in schema.org:\nfrom rdflib.namespaces import RDF, SDO\ndef get_blanks_of_sdo_type(graph, sdo_type):\n    rdf_type = SDO[sdo_type]\n    for subject in graph.subjects(RDF.type, rdf_type):\n        if isinstance(subject, rdflib.term.BNode):\n            yield subject\nThere’s one problem with this: SDO refers to the namespace https://schema.org/, whereas in practice most of the nodes use http://schema.org. This means we lose most of our nodes using this approach! According to the schema.org FAQ either is fine, and I’ve raised an issue in RDFLib to discuss it. In the meantime we have to manually try both:\nfrom rdflib.namespace import Namespace\nSDO_NAMESPACES = [Namespace('http://schema.org/'), Namespace('https://schema.org/')]\ndef get_blanks_of_sdo_type(graph, sdo_type):\n    for namespace in SDO_NAMESPACES:\n        rdf_type = namespace[sdo_type]\n        for subject in graph.subjects(rdflib.namespace.RDF.type, rdf_type):\n            if type(subject) == rdflib.term.BNode:\n                yield subject\n\n\nStitching it all together\nWe can stick these together with our nquad streaming solution to get dictionaries of lists of a given type.\ndef extract_nquads_of_type(lines, sdo_type):\n    for graph in parse_nquads(lines):\n        for node in get_blanks_of_sdo_type(graph, sdo_type):\n            yield graph_to_dict(graph, node)\n            \nwith gzip.open('nquads.gz', 'rt') as f:\n  # Extract all job postings\n  for d in extract_nquads_of_type(f, 'JobPosting'):\n    ...\nThere’s still a few things that need to be done to make this usable. There are many properties that almost always occur only once (although this doesn’t seem to be in the specification at all), and so it would be much nicer to represent it as a single object and not a list. Also the property names are typically very long, being URIs and it would be useful to shorten them. But now we have the data in a form where we can analyse them and build better transformations for each schema."
  },
  {
    "objectID": "experiment-generalisability/index.html",
    "href": "experiment-generalisability/index.html",
    "title": "Experimental Generalisability",
    "section": "",
    "text": "Suppose you’re testing two alternate designs for a website. One has a red and green button with a santa hat and bauble, and the other has a blue button. You run an A/B test over Christmas and the first design has a significantly better conversion rate (p < 0.001). Should you have a button with a santa hat and bauble all year long?\nThe problem is that this condition won’t generalise. You ran the experiment at a certain time of year (Christmas) where the relation could be impacted by that time of year. Any relation you find may not hold at any other time.\nThis over-generalisation occurs in social science studies all the time. They will run some specific experiment on a group of undergraduates at their own university and then claim that it is sufficient evidence for a broad theory. But really they’ve just shown that the particular subgroup they’ve chosen has acted in that way.\nThere’s a misconception that null hypothesis testing means you don’t make any (or many) assumptions about your experiment. But when you use the results you need some theory about how it generalises to other samples in different places and conditions. And for this you need some theory of mechanism of how it works, or what factors could contribute to it."
  },
  {
    "objectID": "integrating-power-exponential/index.html",
    "href": "integrating-power-exponential/index.html",
    "title": "Integrating Powers of Exponentials",
    "section": "",
    "text": "\\[\\int_{0}^{\\infty} x^m e^{-x^p}\\, \\rm{d}x = \\frac{\\Gamma\\left(\\frac{m+1}{p}\\right)}{p}\\]\nThis is useful for calculating moments of powers of exponentials, namely for positive p and k:\n\\[\\int_{-\\infty}^{\\infty} (x - c)^m e^{-\\left\\vert\\frac{x-c}{k}\\right\\vert^p} \\, \\rm{d}x = \\left\\{\\begin{array}{rl} 0, & \\text{if }  m = 1,3,5,7,9,\\ldots \\\\ \\frac{2 k^{m+1} \\Gamma\\left(\\frac{m+1}{p}\\right)}{p}, & \\text{if } m = 0, 2, 4, 6, 8, \\ldots \\end{array}\\right.\\]\nThis can all be proved with simple integration by substitution. First starting with the moment integral \\(\\int_{-\\infty}^{\\infty} (x - c)^m e^{-\\left\\vert\\frac{x-c}{k}\\right\\vert^p} \\, \\rm{d}x\\), where m is a non-negative integer and p and k are positive. We can shift the integral so it centred on the midpoint c with the substitution \\(u = x - c\\), which gives the integral \\(\\int_{-\\infty}^{\\infty} x^m e^{-\\left\\vert\\frac{x}{k}\\right\\vert^p} \\, \\rm{d}x\\).\nNow the term \\(e^{-\\left\\vert\\frac{x}{k}\\right\\vert^p}\\) is symmetric about the origin, and so when m is odd the integrand is anti-symmetric about the origin, and so the contributions for positive and negative x cancel out, resulting in a zero integral. When m is even the integral is symmetric about the origin and so the contributions for positive and negative x are equal; so we can calculate the integral as \\(2 \\int_{0}^{\\infty} x^m e^{-\\frac{x}{k}^p} \\, \\rm{d}x\\), where we have dropped the absolute value because x/k is positive.\nFinally we can remove k from the exponent by rescaling the distribution with the substitution \\(u = \\frac{x}{k}\\), giving the integral as \\(2 k^{m+1} \\int_{0}^{\\infty} x^m e^{-x^p} \\, \\rm{d}x\\). So we have reduced the problem to calculating the integral of a power of x multiplied by a power of the exponential.\nThe integral \\(\\int_{0}^{\\infty} x^m e^{-x^p} \\,\\rm{d}x\\) can be transformed to remove the power from the exponent with the transformation \\(u = x^p\\); this gives\n\\[\\int_{0}^{\\infty} x^m e^{-x^p} \\,\\rm{d}x  = \\frac{1}{p} \\int_{0}^{\\infty} x ^{\\frac{m + 1}{p} - 1} e^{-x} \\,\\rm{d}x\\]\nand by the definition of the Gamma function this is just \\(\\frac{\\Gamma\\left(\\frac{m+1}{p}\\right)}{p}\\).\nIt’s easy to verify this in simple cases; when \\(p = 1\\) we get the integral being \\(m!\\) which can be proved by integration by parts. For \\(m = 0\\) and \\(p = 2\\) this gives \\(\\frac{\\sqrt{\\pi}}{2}\\) which is the Gaussian integral.\nThese integrals are useful for calculating statistics of different exponential type distributions."
  },
  {
    "objectID": "book-ner-work-of-art/index.html",
    "href": "book-ner-work-of-art/index.html",
    "title": "Book NER as a Work of Art",
    "section": "",
    "text": "I quickly tried 3 well known systems all trained on this corpus; SpaCy, Stanza, and Flair NLP. SpaCy (en_core_web_trf; the smaller models don’t work) and Flair NLP both performed quite well. They both got a lot of the titles and persons, although struggled on punctuation. The lack of periods seemed to hurt both the models in hyphen lists, and things like quotations seemed to leak into results. Stanza performed much worse than the other two models and isn’t worth considering.\nThese aren’t good enough to solve the problem well by themselves, but are a good starting point for training a better NER model for getting book titles.\nIf you want the quick qualitative analyses see the notebooks for SpaCy, Flair, and Stanza."
  },
  {
    "objectID": "tensor-notation/index.html",
    "href": "tensor-notation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "I know three types of notations for tensors and each seem to be useful in different situations and gives you a different perspective on how tensors “work”. [Technical Note: I will assume all vector spaces are finite dimensional so \\(V\\) is naturally isomorphic to \\(V^{**}\\)]\n\nThe first is the “functional” notation. An \\((r,s)\\) tensor on a vector space \\(V\\) over a field \\(F\\) is a function \\(T: \\overbrace{V^* \\times \\cdots \\times V^*}^{\\mbox{r terms}} \\times \\overbrace{V \\times \\cdots \\times V}^{\\mbox{s terms}} \\to F\\) that is linear in each variable separately. (\\(V^*\\) is the dual of \\(V\\), that is all linear functions from \\(V\\) to \\(F\\)).\nThat is a tensor is something that takes r covectors and s vectors (all ordered) and gives us a number, and does so in a linear way.\nThe second is the index notation. An \\((r,s)\\) tensor on an \\(n\\)-dimensional vector space on a field \\(F\\) is a symbol with \\(r\\) “upstairs” indices and \\(s\\) “downstairs” indices where the indices are in the range \\(\\{1,\\ldots,n\\}\\) : \\(T^{i_1,\\ldots,i_r}_{j_1,\\ldots,j_s}\\) . Substituting in any particular set of indices gives you an element of the field. It comes hand in hand with the Einstein summation convention: any repeated upstairs and downstairs indices are implicitly summed over, e.g. \\(T^{ij}_{k}v_{i} = \\sum_{i=1}^{n} T^{ij}v_{i}\\) .\nThe third notation I know of is Penrose’s Graphical tensor notation. An \\((r,s)\\) tensor over a field \\(F\\) is a “blob” with \\(r\\) arms and \\(s\\) legs.\n\n\n\nA blob with r arms and s legs\n\n\nIf there are no arms or legs sticking out it’s just an element of the field F.\nThese objects are all closely related. To go from the functional notation to the index notation choose a basis for the vector space \\(V\\), \\(\\{e_1,\\ldots,e_n\\}\\) . Then we can pick a basis on the dual space \\(V^*\\), \\(\\{e^1,\\ldots,e^n\\}\\), by requiring \\(e^i(e_j) =\\delta^i_j\\) (the Kronecker delta; \\(1\\) if \\(i\\) and \\(j\\) are equal, otherwise \\(0\\)). [Why write the dual basis as \\(e^{i}\\) ? It gives us the nice upstairs-downstairs relationship for index notation. It’s also important to distinguish the vector space from its dual.]\nThe \\((r,s)\\) tensors also form a vector space of dimension \\(n^{r+s}\\) and we can form a basis for tensors using the tensor product  \\(\\otimes\\) . The tensor product of an \\((r,s)\\) tensor and an \\((r’,s’)\\) tensor is an \\((r+r’,s+s’)\\) tensor, and its action on \\(r+r’\\) covectors and \\(s+s’\\) vectors is to put the first \\(r\\) covectors and \\(s\\) vectors in the first tensor, put the rest in the second tensor – this gives you two numbers and you multiply them together. Then a basis for \\((r,s)\\) tensors is \\(e_{i_1} \\otimes \\cdots e_{i_r} \\otimes e^{j_1} \\otimes \\cdots e^{j_s}\\) where each index runs from \\(1\\) to \\(n\\).\nGiven a basis we can talk about the components. For instance for a vector \\(v\\) its components are \\(v^i:=e^i(v)\\) and the vector can be written \\(\\sum_{i=1}^{n} v^i e_i\\) . Note that the \\(e_i\\) are vectors but the \\(v^i\\) are just numbers (though they still form a vector space dual to \\(V\\)); but having fixed our basis these components completely determine our vector by linearity. Also note that if the basis index is downstairs the component index is upstairs. [This almost surely explains why physicists write vectors with upstairs indices \\(v^i\\) when downstairs would seem more natural].\nSimilarly we can talk about the components of a tensor, a general \\((r,s)\\) tensor can be written \\(T=T^{i_1,\\ldots,i_r}_{j_1,\\ldots,j_s} e_{i_1} \\otimes \\cdots e_{i_r} \\otimes e^{j_1} \\otimes \\cdots e^{j_s}\\) . If we fix our basis there’s no need to write it out at the end every time – so let’s just drop it off and understand it’s implied. This gives us index notation. We can immediately see that the action of T on the vectors \\(v_a = v_a^{i} e_i\\) (the subscript “\\(a\\)” here is a label and not an index) and the covectors \\(w_b = w_{b,i} e^i\\) for \\(a\\) in \\(1\\) to \\(s\\) and \\(b\\) in \\(1\\) to \\(r\\) is\n\\(T(w_1,\\ldots,w_r,v_1,\\ldots,v_s) = T^{i_1,\\ldots,i_r}_{j_1,\\ldots,j_r} w_{1,i_1} \\cdots w_{r,i_r} v_{1}^{j_1} \\cdots v_s^{j_s}\\) .\nIt is important to note that under a change of basis the components of the tensor must transform in a certain way for them to be well defined (this is why such an emphasis is put on the indices transforming ‘covariantly’). [There is another, coordinate-free interpretationof index notation, but the one I have given is standard].\nTo see how the diagram notation fits with the functional notation, we see vectors look like\n\n\n\nPenrose vector\n\n\nand covectors look like\n\n\n\nPenrose covector\n\n\n(with different shaped/coloured blobs for different vectors), so the action of \\(T\\) on \\(r\\) vectors and \\(s\\) covectors is obtained just from ‘plugging’ together the legs and arms with those of\n.\nWe can also connect it to the index notation: if we label the arms (typically left to right) \\(i_1,i_2,\\ldots,i_r\\) and the legs (typically left to right) \\(j_1,j_2,\\ldots j_s\\) we can think of it as representing the components \\(T^{i_1,\\ldots,i_r}_{j_1,\\ldots,j_s}\\) .\nThese are just the basics, the devil is in the detail. I find it much easier to think in the diagrammatic notation (and to a lesser extent the index notation) – they are more powerful because they are less versatile. The only reason we can draw (or write) tensors in this specific way is because of some of their linear properties; this is hidden in the functional notation, we can write an arbitrary non-linear function in the functional notation.\nTo compare the languages here is an article outlining the hodge star in functional notation [watch out, there are some errors], here is an expression in index notation, and I think this is the diagram for it: .\nMost mathematics textbooks that cover tensors will primarily use the functional notation, but many will also mention the index notation (e.g. Marsden, Ratiu and Abraham). If you read an advanced Electromagnetism or Mechanics physics textbook (or a general relativity book) you will get a good working introduction to the index notation.\nDiagram notation is much harder to find: the original description I read was in Penrose’s Road to Reality where (like everything else he covers) it is described in just enough detail to suss out what he means if you think about it for long enough. The best description I’ve seen is in Cvitanović’s Group theory book where he uses variations on it to classify the semisimple Lie groups using nothing more than a few thousand pictures! (What I’ve read so far is excellent). The description is somewhat off the main tensor track but the major points seem to be covered in there. For some other references and other types of diagrammatic notations see this blog post."
  },
  {
    "objectID": "haar/index.html",
    "href": "haar/index.html",
    "title": "Symmetry in probability",
    "section": "",
    "text": "Similarly for a fair die there are 6 possible outcomes, that are all equally likely. This means they each have the probability 1/6.\nThe idea of symmetry is behind random sampling. If we want to understand a population we can take a number of random cases and it tells us something about the whole. However this is only true if the sample is random with respect to the properties we’re measuring. That is if we exchanged people randomly we would be equally likely to measure them.\nAnother example is a spinner, like a roulette wheel. The model is that a fair spin is equally likely to land anywhere on the circumference circle. So by symmetry the probability of an outcome is proportional to the length of the arc it subtends on the circle. This also happens to be proportional to the angle of the arc, which is proportional to the area of the arc.\nAnother more elaborate example would be a sphere with labelled regions that can spin on both axes, and a pin pointing to a point on the sphere (say the topmost). Then if it spins freely in any direction the probability of landing on any point is proportional to the surface area on the sphere.\nYou could cut the sphere in half and flatten it into two circles. This allows you to make the same reasoning about a dartboard; the probability of an uncontrolled throw landing in a region is proportional to its area. This kind of geometric reasoning is behind probability squares.\nThis idea can be generalised to any sort of symmetry. In practice all we really need is the finite examples like samples, dice and coins since real populations are finite. However the more general case in mathematics is an extension of the Haar measure theorem. It’s something like any free and transitive continuous group action on a compact Hausdorff space induces a unique probability measure. For the finite cases it’s the symmetric group, for the circle U(1) and for the sphere SO(3) (which isn’t covered by the Haar measure theorem since a sphere is not a group). But none of this will actually help you with probability and statistics, because again real populations are finite (we just sometimes pretend they’re infinite to make the maths easier). And even if you do need it, the geometric intuition about areas is likely to help you more than group theory.\nIn practice all the symmetry ideas break down. We have unfair outcomes and so we have to introduce a bias parameter. It’s really hard to get a truly random survey sample and so we have to try to model the bias.\nHowever the ideas are still useful for simplifying hard problems. In survey sampling we often assume that a measurement is symmetric within some demographic attributes and just try to weight the survey based on its demographic composition. This is an old tradition, but can go terribly wrong. Unfortunately for a very biased sample there are limits on the amount of population information it’s possible to extract."
  },
  {
    "objectID": "schema-jobposting/index.html",
    "href": "schema-jobposting/index.html",
    "title": "Schemas for JobPostings in Practice",
    "section": "",
    "text": "The website schema.org contains schemas for representing everything from Anatomical Structures, to a How to Tip to types of beds. While a lot of work goes into making these consistent and complete, they are only as useful as they are fit for purpose and adopted in practice. For example a Job Posting contains some very unusual items like SensoryRequirement.\n\nsensoryRequirement\nA description of any sensory requirements and levels necessary to function on the job, including hearing and vision. Defined terms such as those in O*net may be used, but note that there is no way to specify the level of ability as well as its nature when using a defined term.\n\nTo understand how people use the schema in practice I analysed the 2019 Web Data Commons Job Postings subsets. There’s two different datasets based on the source; JSON-LD and Microdata. To understand the breadth of common usage I took a singe JobPosting from a sample of domains (1840 JSON-LD and 2820 Microdata) and examined the fields. In practice there may be some domains with lots of JobPostings that have a specific term. Many times the data may be contained in the job description and good examples could be used as training data in a supervised learning model.\nIn general there is more Microdata but it’s less consistent and of lower quality than JSON-LD. There’s a lot of variation in consistency so even this “structured” data requires some processing to work with. There’s also many different ways to structure the same thing; for example a JobPosting can have a salaryCurrency but also a a baseSalary which is a MonetaryAmount and can have a currency.\nHere are the most common objects, their types, descriptions and some examples.\n\n\n\nSchema\nProperty\nJSON-LD Coverage\nMicrodata Coverage\nTypes\nDescription\nExample\nNotes\n\n\n\n\nJobPosting\ndatePosted\n99%\n64% (7% Date/57% Text)\nDate\nPublication date of listing\n11/20/2019 08:55:24 AM\nShould be an ISO-8601 date, often is another type of date(time)\n\n\nJobPosting\ntitle\n99%\n84%\nText\nTitle of the job\nPsychiatric Nurse Practitioner\n\n\n\nJobPosting\ndescription\n99%\n76%\nText (HTML)\nDescription of the job\n&lt;p&gt;&lt;strong&gt;Category Manager …\nSometimes HTML is double encoded, sometimes not\n\n\nJobPosting\nhiringOrganization\n98%\n59% (35% Organization/24% Text)\nOrganization\nOrganization offering the job\nNTT DATA Services (See also Organization)\n\n\n\nJobPosting\njobLocation\n98%\n63% (49% Place/14% Text)\nPlace\nLocation associated to job\nSee Place\n\n\n\nJobPosting\nemploymentType\n82%\n39%\nText\nType of employment\nFULL_TIME\nWhen it’s multiple it tends to be alternatives\n\n\nJobPosting\nvalidThrough\n60%\n23% (3% Date, 20% Text)\nDateTime (90%), Date (10%)\nDate after when offer is not valid\n2019-11-28\nShould be an ISO-8601 date/datetime, often is another type of date(time)\n\n\nJobPosting\nbaseSalary\n47%\n21% (12% MonetaryAmount/9% Text)\nMonetaryAmount, Text, PriceSpecification (Rarely)\nThe base salary of the job\n40 000 60 000 руб. (Also see MonetaryAmount)\n\n\n\nJobPosting\nidentifier\n41%\n7%\nPropertyValue, Text, URL\nAny kind of identifier\n39576074\nThe value is often a path or an id\n\n\nJobPosting\nindustry\n39%\n21%\nText\nIndustry associated with job\nEngineering\nDoesn’t seem to be standard, can be in different languages\n\n\nJobPosting\nurl\n23%\n20% (9% URL, 11% Text)\nURL\nURL of the item\nhttps://nmhc.selectleaders.com/job/90569/senior-debt-analyst/\n\n\n\nJobPosting\nsalaryCurrency\n15%\n4%\nText\nISO 4217 currency for salary\nGBP\nOccasionally € instead of EUR\n\n\nJobPosting\neducationRequirements\n10%\n7%\nEducationalOccupationalCredential (Rarely), Text\nEducational background needed\nMBO\nOften it’s “UNAVAILABLE” or “Not Applicable”, etc\n\n\nJobPosting\noccupationalCategory\n9%\n8%\nCategoryCode, Text\nA category describing the job\nInformation Technology\nVery little consistency\n\n\nJobPosting\nexperienceRequirements\n9%\n9%\nText\nSkills and experience required\n1 - 2 Year(s)\nSometimes years, sometimes level (senior), sometimes text description\n\n\nJobPosting\nworkHours\n8%\n8%\nText\nTypical working hours\n11:00~24:00 週2日\nVariable formats, sometimes specifying “various”\n\n\nJobPosting\njobBenefits\n8%\n2%\nText\nBenefits associated with job\nJob Security, HRA, TA, DA\nInconsistent, often comma separates list of text\n\n\nJobPosting\nskills\n8%\n4%\nDefinedTerm, Text\nCompetency needed to fill this role\nJavaScript, Apple iOS, Android\nOften comma separated list\n\n\nJobPosting\nqualifications\n7%\n6%\nEducationalOccupationalcredential (Rarely), Text\nQualifications required for this role\nSie müssen Personaler eines Unternehmens sein\nTypically long text, inconsistent\n\n\nJobPosting\nimage\n6%\n7%\nURL (mostly), ImageObject (sometimes)\nAn image of the item\nhttps://images.rigzone.com/images/rz-logo.jpg\nMostly logos\n\n\nJobPosting\njobLocationType\n3%\n<1%\nText\nA description of the job location\nTELECOMMUTE\nWhen it’s present it’s almost always TELECOMMUTE\n\n\nJobPosting\nincentiveCompensation\n3%\n<1%\nText\nBonuses and commissions\nProvides Equity\nHighly variable\n\n\nPlace (JobLocation)\naddress\n94%\n44%\nPostalAddress (mostly), Text\nPhysical Address\nAmsterdam (Also see PostalAddress)\n\n\n\nPlace\ngeo\n5%\n<1%\nGeoCoordinates (mostly), Text\nGeo coordinates of place\n54.727,55.955 (See GeoCoordinates)\nSometimes 0,0 (garbage)\n\n\nPlace\nname\n3%\n1%\nText\nName of the place\nSouthwark\n\n\n\nPostalAddress (address)\naddressLocality\n89%\n34%\nText\nLocation within Region\nPhiladelphia\n\n\n\nPostalAddress\naddressRegion\n81%\n30%\nText\nRegion within Country\nCalifornia\nSometimes country specific abbreviation\n\n\nPostalAddress\naddressCountry\n77%\n16%\nText (95%), Country (5%)\nCountry\nUnited States\nSometimes name, sometimes country code\n\n\nPostalAddress\npostalCode\n54%\n12%\nText, Int\nPostal Code\nJ3A1B6\n\n\n\nPostalAddress\nstreetAddress\n34%\n7%\nText\nStreet Address\n21 Lassell Gardens\nCan be junk like ‘-’, UNKNOWN\n\n\nGeoCoordinates (geo)\nlatitude\n5%\n<1%\nFloat,Text\nLatitude (WGS 84)\n54.727615356462\n\n\n\nGeoCoordinates\nlongitude\n5%\n<1%\nFloat,Text\nLongitude (WGS 84)\n55.955778063477\n\n\n\nCountry (addressCountry)\nname\n4%\n0%\nText\nName of the country\nItalia\nSometimes name, sometimes country code\n\n\nMonetaryAmount (baseSalary)\ncurrency\n38%\n11%\nText\nCurrency (e.g. ISO 2417 code)\nGBP\n\n\n\nMonetaryAmount\nvalue\n44%\n9%\nQuantitativeValue (95%), Text (5%)\nQuantity of salary\n25000 (See QuantitativeValue)\nWhen text contains odd things like ‘Hourly’\n\n\nMonetaryAmount\nminValue\n2%\n2%\nText, Int, Float\nLower value of salary\n9.94\n\n\n\nMonetaryAmount\nmaxValue\n2%\n2%\nText, Int, Float\nUpper Value of salary\n13,500,000\n\n\n\nQuantitativeValue (baseSalary value)\nunitText\n35%\n5%\nText\nUnit of measurement\nYEAR\nShould be 3 letter UN/CEFACT Common Code (e.g. HUR, DAY, WEE, MON, ANN)\n\n\nQuantitativeValue\nminValue\n7%\n4%\nText, Int, Float\nQuantity of salary\n400\nSometimes 0\n\n\nQuantitativeValue\nmaxValue\n6%\n3%\nText, Int, Float\nLower value of salary\n10000\nSometimes 0\n\n\nQuantitativeValue\nvalue\n29%\n2%\nText, Int, Float\nUpper Value of salary\n300\nSometimes NULL, sometimes text range\n\n\nOrganization (hiringOrganization)\nname\n94%\n32%\nText\nName of the company\nAnixter International\n\n\n\nOrganization (hiringOrganization)\nsameAs\n60%\n6%\nURL\nURL that identifies company\nhttps://www.socialdeal.nl\nAppear to be URLs\n\n\nOrganization (hiringOrganization)\nlogo\n54%\n10%\nURL (usually), ImageObject (sometimes)\nAssociated logo\nhttps://kaigoworker.jp/img/gfjimg_kaigo.png\n\n\n\nOrganization (hiringOrganization)\nurl\n5%\n8%\nURL\nURL of company\nhttp://www.lgcassociates.com\n\n\n\nImageObject (image, logo)\nurl\n\n\nURL\nURL of image\nhttps://www.hiq.se/globalassets/bilder/hiq_bg_bild_some.jpg\n\n\n\nImageObject\ncontentUrl\n\n\nURL\nURL of image\nhttps://media.rabota.ru/processor/logo/small/2010/04/08/silajjn.gif\n\n\n\nImageObject\nwidth\n\n\nInt\nWidth of image\n1043\n\n\n\nImageObject\nheight\n\n\nInt\nHeight of image\n1800\n\n\n\nImageObject\nname\n\n\nText\nName of image\nTRN Logo with Website\n\n\n\n\n\nGetting the data\nThe nquad data was turned into Graphs with parse_nquads.\nf = gzip.open('ndquads.gz', 'rt')\nall_graphs = parse_nquads(f)\n\nseen_domains = set()\ngraphs = []\nskipped = []\n\nfor _ in tqdm(range(100_000)):\n    graph = next(all_graphs)\n    dom = graph_domain(graph)\n    if dom in seen_domains:\n        continue\n    \n    try:\n        jp = list(get_job_postings(graph))[0]\n    except IndexError:\n        # This can happen because a disjoint graph from the\n        # page without a job posting is split\n        skipped.append((graph.identifier, dom))\n        continue\n    graphs.append((graph, jp))\n    seen_domains.update([dom])\nThe domain of the graph is extracted with a simple function:\ndef graph_domain(graph):\n    return urllib.parse.urlparse(graph.identifier).netloc \nYou can view the whole laborious analysis in Jupyter."
  },
  {
    "objectID": "tree-grid/index.html",
    "href": "tree-grid/index.html",
    "title": "Representing Decision Trees on a grid",
    "section": "",
    "text": "Decision trees are often represented as a hierarchy of splits. Here’s an example of a classification tree on Titanic survivors.\n\n\n\nCART Tree represented as a hierarchy\n\n\nThis is useful but if there are a large number of splits, even on a few dimensions the trees can get quite large. The tree can also be linearised into a series of if else rules (so each following rule applies only to cases not already classified).\n\nSurvived if sex not male\nDied if age > 9.5\nDied if sibsp > 2.5\nElse Survived\n\nThese can get quite confusing when they get quite large. Another way of looking at them is showing the cases on a grid, which is feasible as long as the number of dimensions split on is small (up to 4 or 5 depending on the number of cut points).\n\n\n\nCART Tree represented on a Grid\n\n\n\nSpecifying rules\nNote that the rules as above can get quite hard to interpret because you have to keep in mind all the previous rules. For a simple example suppose we have these rules on a chessboard.\n\nBlue if letter < D\nOrange if letter >= C\n\n\n\n\nBlue if < D, Orange if >=C\n\n\nThe order matters here, switching them around gives a different result:\n\nOrange if letter >= C\nBlue if letter < D\n\n\n\n\nOrange if letter >= C, Blue if Letter < D\n\n\nFirst one can be written more precisely with the rules:\n\nBlue if letter < D\nOrange if letter >= D\n\nin this form the order doesn’t matter.\nAny tree can be specified in this way where the order doesn’t matter; in the Titanic example above:\n\nSurvived if sex not male\nDied if male and age > 9.5\nDied if male and sibsp > 2.5\nSurvived if male and age <= 9.5 and sibsp <= 2.5\n\nThis removes the amount of context you have to remember but can still be hard to understand from the description with lots of splits.\n\n\nMore complex rules\nConsider this set of rules for colouring a chessboard:\n\nBlue if number >=7 and letter < C\nBlue if number < 2 and letter >= H\nGreen if number < 3 and letter >= G\nYellow if number < 7 and letter > F\nPink if number >= 5 and letter >=C and letter < H\nRed if letter < C\nGrey if letter < E\nGrey if number < 4\nElse yellow\n\nWe could specify them with exact rules but the description would get very long; for example yellow is the rules:\n\nYellow if letter >= F and number >= 3 and number < 7\nYellow if letter >= H and number >= 7\nYellow if letter >= F and letter < G and number < 2\nYellow if letter >= E and letter < F and number >= 4 and number < 5\n\nSimilarly any tree description would be quite large. However this is very clear on a grid:\n\n\n\nComplex rules on chessboard grid\n\n\nWhile this example might seem artificial, I’ve actually used this technique with great success on trees with 4 dimensions. With higher dimensions you have to make facets (like in the titanic example above), but if these groups don’t have too many cuts the visualisation is readable. You can also clearly see the boundaries between groups which can be informative in a classificaition (though with high dimensions you have to eyeball this across facets).\nThese diagrams are relatively simple to construct. First you need to extract all the cutpoints from the rules, and then create a grid of all cutpoints. Then you can evaluate the rules on each square of the grid to get the classification map. You could then augment this with extra information like the number of data points in each square, or the classification accuracy (like in the Titanic tree diagram above). A nice extension would be to combine information accross contiguous regions of the grid."
  },
  {
    "objectID": "hawthorne-myth/index.html",
    "href": "hawthorne-myth/index.html",
    "title": "Myth of the Hawthorne Effect",
    "section": "",
    "text": "The economists Steven D. Levitt (of Freakonomics fame) and John List have a paper Was there really a Hawthorne Effect at the Hawthorne Plant?. They chased the original data from the study and found that it doesn’t at all match the popular story that any change increased output. They found that there may have been a small increase in productivity during weeks they performed experimental changes (a few percent, on the order of difference in productivity between Mondays and Tuesdays). But there’s also an open question of what else changed in those weeks.\nBut the myth lives on; even though this paper was written in 2009 I’ve heard this effect referenced as late as last year. The New York Times article, Scientific Myths That Are Too Good to Die, talks about another aspect of the Hawthorne experiment where any changes in break time (longer, shorter, more, fewer) increased productivity. But in that study there were only five workers, and two were replaced during the experiment for low output. It explains other similar myths.\nHow do we avoid these myths and tell fact from fiction? There’s a whole heap of popular psychological effects that couldn’t be replicated. Things like ego depletion, that smiling makes people feel happier, or the Lady Macbeth effect (that shame makes people want to clean) failed to replicate. But you still hear these things around.\nDoing good research is difficult, especially when it comes to people. Communicating that research and pulling it apart from stories that sound good but have no evidence is actually really hard for people. It’s easy to just cherry pick studies that support your current point of view.\nThe Hawthorne effect is just one of many examples of stories that sound good, but have no factual basis."
  },
  {
    "objectID": "regex-property-testing/index.html",
    "href": "regex-property-testing/index.html",
    "title": "Property Based Testing with Regular Expressions",
    "section": "",
    "text": "While you could easily do this with hypothesis from_regex strategy you could roll your own with Hypothesis and the library rstr as follows:\nfrom hypothesis.strategies import composite, randoms\nfrom rstr import Rstr\n\n@composite\ndef text_from_regex(draw, regexp):\n    random = draw(randoms())\n    return Rstr(random).xeger(regexp)\nI came up with this when I had a tricky regex to try to parse salary expressions from strings. I found a particular case where the regex returned something that wasn’t a number and raised an error when it tried to get a float.\nThe first thing I tried was to generate random text to reproduce the error:\nfrom hypothesis import given, strategies as st\n\n@given(text())\ndef test_extract_salary_types(text):\n    salary_range = extract_salary(text)\nBut it couldn’t reproduce the error, even after tens of thousands of test cases because it wasn’t generating the right kind of text. I also tried Crosshair to find a failing case but it also failed to (maybe because it’s hard to parse the regex).\nThen I realised the regex I was using to parse would be a good hint on the kinds of things to try. I found two libraries that can generate text from regex rstr and exrex, and the former seemed easier to plug into Hypothesis (but exrex looks interesting because it can generate cases in increasing complexity, which would be good for shrinking).\nAs part of my expression I had a blacklist of things I tried to exclude from the regular expression (BLACKLIST_RE) and so tried this:\n@given(text_from_regex(regexp=rf\"([\\$£€\\d\\w\\s-]|{BLACKLIST_RE})*\"))\ndef test_extract_salary_types(text):\n    salary_range = extract_salary(text)\nThis immediately came up with a counterexample 0a0m.\nThis seems to be an interesting way to generate targeted examples to test your code against. You could take this even further by generating text from a grammar using the Hypothesis Lark extension. In another direction you could use a language model trained on examples of your data, such as an n-gram language model, Hidden Markov Models or deep language models like BERT, ULMFiT or ELMO (and this is close to the ideas in checklist and textattack)."
  },
  {
    "objectID": "demjson/index.html",
    "href": "demjson/index.html",
    "title": "Demjson for parsing tricky Javascript Objects",
    "section": "",
    "text": "The problem shows up when using json.loads as the following obscure error:\njson.decoder.JSONDecodeError: Expecing value: line N column M (char X)\nLooking at the character in my case looking near the character I see that it is a JavaScript undefined, which is not valid in JSON.\n{\"key\": undefined, ...\nHowever it turns out the demjson library handles this well using demjson.decode(text). It represents undefined with a special demjson.undefined class. Because this isn’t serializable I need to convert it to something else; I can walk the dictionary to turn it into a Python None.\ndef undefined_to_none(dj):\n    if isinstance(dj, dict):\n        return {k: undefined_to_none(v) for k, v in dj.items()}\n    if isinstance(dj, list):\n        return [undefined_to_none(k) for k in dj]\n    elif dj == demjson.undefined:\n        return None\n    else:\n        return dj\nUsing demjson and converting undefined to None works well, but it seems to run about 20 times slower than json.loads. So I’ll try a strategy of first using json.loads and falling back to demjson when necessary.\ntry:\n    data = json.loads(text)\nexcept json.decoder.JSONDecodeError:\n    logging.warning('Defaulting to demjson')\n    data = demjson.decode(text)\n    data = undefined_to_none(data)\nAn alternative approach would be to extend the automoton that extracts the object to replace undefined, and then just parse with json.loads. I’m not sure whether there are other types of non-JSON objects demjson can parse too."
  },
  {
    "objectID": "multiprocesing-future/index.html",
    "href": "multiprocesing-future/index.html",
    "title": "From Multiprocesing to Concurrent Futures in Python",
    "section": "",
    "text": "From the previous post suppose we have this multiprocessing code:\nfrom multiprocessing import Pool\nfrom urllib.request import urlretrieve\n\nurl_dests = [('http://example.com', 'example.html'), ...]\nwith Pool(8) as p:\n    p.starmap(urlretrieve, url_dests)\nTurning this into futures code is very easy; we replace a multiprocessing pool with a futures PoolExecutor, and starmap(f, xs) with map(f, *zip(*xs)).\nfrom concurrent.futures import ProcessPoolExecutor\nfrom urllib.request import urlretrieve\n\nurl_dests = [('http://example.com', 'example.html'), ...]\nwith ProcessPoolExecutor(max_workers=8) as p:\n    p.map(urlretrieve, *zip(*url_dests))\nThe main difference is map, like the inbuilt Python function, wants each argument as a separate list, where starmap wants a list of argument lists. We use the standard via *zip(*) trick to convert between the two forms.\nThen moving from processes to threads is as simple as replacing the ProcessPoolExecutor with a ThreadPoolExecutor. Interestingly when using this with the Pyathena library I found that the ThreadPoolExecutor didn’t seem to result in a concurrency speedup (it was as slow as executing the queries serially). I’m not yet sure why this is (something in the GIL?); but it makes me want to be more careful when switching to a ThreadPool.\nThis is a good starting point for more complex workflows. However if you start using a execute/wait paradigm make sure that you explicitly raise exceptions."
  },
  {
    "objectID": "pandas-pipe/index.html",
    "href": "pandas-pipe/index.html",
    "title": "Chaining with Pandas Pipe function",
    "section": "",
    "text": "For example to raise a function to the 3rd power with numpy you could use\nnp.power(df['x'], 3)\nBut another way with pipe is:\ndf['x'].pipe(np.power, 3)\nNote that you can pass any positional or keyword arguments and they’ll get passed along. So df.pipe(f, *args, **kwargs) is equivalent to f(df, *args, **kwargs).\nIf you build up a series of transforms on dataframes to dataframes you can then express this with a series of pipe statements. It even works with a groupby."
  },
  {
    "objectID": "excel-chart-templates/index.html",
    "href": "excel-chart-templates/index.html",
    "title": "Templates for Excel Charts",
    "section": "",
    "text": "Once you’ve created a chart that you’re happy with, right click and select “Save as Template…” and give it a clear name, so when you go to select it you immediately know what it’s for. Then next time you create a chart you can use “Insert > Recommended Charts”, then click on the pane titled “All Charts” and then select “Templates” on the left navigation bar and select your template. Or you can right click on an existing chart, and select “Change Chart Type…” and then select “Templates” and choose your template form the list.\n\n\n\nThe Excel Template Chart Selection\n\n\nNote that the templates are files so you could copy and save them somewhere central or share them. The same templates are available in Powerpoint. Where possible try to make the charts directly in Powerpoint because it’s easier to edit them than if you copy the image of the chart (especially for people who don’t have the workbook the charts were created in).\nA similar technique you can use in Office is themes. If you search for “Theme Colors” you can “Customize theme colors” (at the bottom) and select colors to have readily at hand in your default palette. Similarly you can search for “Theme Fonts” and select default fonts; unfortunately it just supports a heading font and a body font. You can then save the whole theme after searching “Themes”. One benefit of themes are you have the right fonts and colors available (I’ve spent way too much time trying to find font inconsistencies and looking up color codes). Another is if you stick to the theme palette and fonts you can easily change the look of your document by changing theme (a bit like a CSS stylesheet); this could be helpful if you wanted to customise the look of charts to suit your clients."
  },
  {
    "objectID": "2020-monitor/index.html",
    "href": "2020-monitor/index.html",
    "title": "My next monitor in 2020",
    "section": "",
    "text": "I only want to spend a few hundred dollars and to work with my Linux laptop, and be as large as practicable. I’m coming from a 21.5 inch monitor, and in this price range it looks like a 24 inch or 27 inch fit.\nI found the Cnet buying guide and Tom’s Hardware Guide useful for understanding what to look for.\nOne factor to consider is the pixel density; ideally it’s around 90-110 PPI (Pixels Per Inch). For a 24 inch monitor at Full HD (1080p) it’s 92 PPI which is just on the low side of the range. For a 27 inch monitor at Full HD it’s 82 PPI, which is on the low side - some people say it’s alright, some say it’s hard to read text. A 27 inch monitor at QHD (1440p) it’s at 108 PPI which is back in the manageable zone. I’d play it safe and stay above 90 PPI.\nThe low end of a 27 inch QHD monitor is a Panasonic 272B8QJEB which is a bit under $400. This is a pretty featureful monitor with lots of connectors (HDMI, DVI, VGA, Display Port), and a VESA mount. One slight concern I have is whether my old Linux laptop will handle this monitor properly; I think it will, but it’s a risk.\nFor a 24 inch HD monitor there are lots of options around the $200 mark. I know my laptop can handle 2 external monitors with xrandr, so it seems like 2 x 24 inch monitors would be a better choice for the same price. Otherwise I could get a single monitor and just work with 20% less screen real estate.\nHaving lots of options is hard; I end up with analysis paralysis - I have a maximising mindset. I thought a bit about connectors, but really ended up filtering by known brand and sorting by price."
  },
  {
    "objectID": "james-stein-bayes/index.html",
    "href": "james-stein-bayes/index.html",
    "title": "Estimating Group Means with Empirical Bayes",
    "section": "",
    "text": "When calculating the averages of lots of different groups it doesn’t make sense to treat the groups as independent, but to pool information across groups, especially on groups with little data. One way to do this is to build a model on covariates of the groups or on categorical embeddings to use information from other observations to inform that observation. Surprisingly Stein’s Paradox says if we’re trying to minimise the root mean square error, have only one observation per group, and they’re all normally distributed with the same standard deviation we’re always better off shrinking the means towards a common point than treating them separately (even if they’re completely unrelated quantities!). Many of these conditions can be weakened, and in practice when estimating averages of related quantities the gain from shrinking (even when we don’t have covariates) could be substantial.\nCalculating averages (or proportions) in groups is a very common procedure for analysts, which is why GROUP BY is an essential feature of SQL. Some examples are calculating the average sales by salesperson, the conversion by referrer, salary by job ad title, the success rate of a surgery by surgeon, hit-rates of baseball players, average runs of cricket players, or student test scores by classroom. The ordinary procedure is to treat each group independently, and sum and divide, but this can lead to very unstable estimates for small groups. An extreme example is calculating a proportion with only 1 observation which is necessarily 0% or 100% and so always will be at the top or the bottom. A common approach to these is to filter out groups with too few observations, and a less common approach is to use confidence intervals to represent the uncertainty. A less common approach is to use information from other groups to inform the estimate for that group; but is used and known by names like random effects models (or more generally mixed-effects models), multilevel models or hierarchical models (among others)."
  },
  {
    "objectID": "james-stein-bayes/index.html#equal-within-group-variance",
    "href": "james-stein-bayes/index.html#equal-within-group-variance",
    "title": "Estimating Group Means with Empirical Bayes",
    "section": "Equal within-group variance",
    "text": "Equal within-group variance\nSuppose all the points \\(X_i\\) have the same standard deviation, \\(\\sigma_i = \\sigma\\), and so we have \\(X_i \\sim {\\mathcal N}(\\theta_i, \\sigma)\\).\n\\[\\begin{align}\n\\hat M &= \\frac{1}{K} \\sum_{i=1}^{K} X_i =: \\bar X \\\\\n\\hat A + \\sigma^2 &= \\frac{1}{K} \\sum_{i=1}^{K} (X_i - \\bar X)^2 =: \\frac{S}{K} \\\\\n\\hat \\omega &= \\frac{K\\sigma^2}{S} \\\\\n\\hat \\theta_i &= \\omega \\bar X + (1 - \\omega) X_i\n\\end{align}\\]\nThese estimators are biased; since \\(X_i \\sim {\\mathcal N}(M, A + \\sigma^2)\\) then \\(S := \\sum_{i=1}^{K} (X_i - \\bar X)^2 \\sim (A + \\sigma^2) \\chi^2_{K - 1}\\). Given \\(Z \\sim \\chi^2_\\nu\\) then\n\\[\\begin{align}\n{\\mathbb E}(1/Z) &= \\int_0^{\\infty} \\frac{1}{x} \\frac{ x^{\\nu/2-1} e^{-x/2}}{2^{\\nu/2}\\Gamma(\\nu/2)} \\,{\\rm d}x\\\\\n&= \\frac{2^{\\nu/2-1}\\Gamma(\\nu/2-1)}{2^{\\nu/2}\\Gamma(\\nu/2)} \\int_0^{\\infty} \\frac{ x^{(\\nu-2)/2-1} e^{-x/2}}{2^{(\\nu-2)/2}\\Gamma((\\nu-2)/2)}  \\,{\\rm d}x\\\\\n&= \\frac{1}{2 (\\nu/2 - 1)} \\cdot 1 \\\\\n&= \\frac{1}{\\nu - 2}\n\\end{align}\\]\nso an unbiased estimator of the \\(\\theta_i\\) is\n\\[\\hat \\theta_i = \\frac{(K - 3)\\sigma^2}{S} \\bar X + \\left(1 - \\frac{(K - 3)\\sigma^2}{S} \\right) X_i\\]\nthis is precisely a James-Stein estimator (when M is known it replaces \\(\\bar X\\) and removes one degree of freedom from S, meaning we get for \\(\\sigma=1\\), \\(\\theta_i = \\frac{K-2}{S} M + \\left(1 - \\frac{K-2}{S}\\right) X_i\\), which is the original estimator proposed by James and Stein which has lower risk than \\(\\hat\\theta_i^{\\rm MLE} = X_i\\) under the sole assumption \\(X_i \\sim {\\mathcal N}(\\theta_i, 1)\\)).\nThe crucial factor here is the ratio between the variance within groups to that between groups \\(\\frac{\\sigma^2}{S/(K-3)}\\). When this is small the estimate is close to the actual points, and when this is large the estimate is close to the overall average \\(\\bar X\\). As we more precisely estimate the group mean the more we gain from biasing towards it, and the lower the within-group variation the less there is to benefist from biasing the estimate by shrinking."
  },
  {
    "objectID": "james-stein-bayes/index.html#lower-within-group-variance-than-between-group",
    "href": "james-stein-bayes/index.html#lower-within-group-variance-than-between-group",
    "title": "Estimating Group Means with Empirical Bayes",
    "section": "Lower within-group variance than between-group",
    "text": "Lower within-group variance than between-group\nSuppose \\(\\sigma_i^2 \\ll A\\) for all i. Then \\(\\hat \\theta_i \\approx X_i (1 - \\sigma_i^2/A) + M \\sigma_i^2 / A\\), so the estimates are closer to the data points and the Maximum Likelihood Estimate. In this case the benefit of this approach will be marginal. Also \\(\\hat M \\approx \\frac{1}{K} \\sum_{i=1}^{K} X_i\\) and \\(\\hat A \\approx \\frac{1}{K}\\sum_{i=1}^{K} (X_i - \\hat M)^2\\) as in the equal variance case."
  },
  {
    "objectID": "james-stein-bayes/index.html#lower-between-group-variance-than-within-group",
    "href": "james-stein-bayes/index.html#lower-between-group-variance-than-within-group",
    "title": "Estimating Group Means with Empirical Bayes",
    "section": "Lower between-group variance than within-group",
    "text": "Lower between-group variance than within-group\nSuppose \\(A \\ll \\sigma_i^2\\) for all \\(\\sigma_i\\). Then \\(\\hat M \\approx \\left.\\sum_{i=1}^{K} \\frac{X_i}{\\sigma_i^2} \\middle/ \\sum_{i=1}^{K} \\frac{1}{\\sigma_i^2} \\right.\\). In particular if the \\(X_i\\) are formed as group averages with equal variance, so \\(\\sigma_i^2 = \\sigma_0^2/N_i\\), then \\(\\hat M \\approx \\frac{\\sum_{i=1}^{K} N_i X_i}{\\sum_{i=1}^{K} N_i}\\), that is it’s the average of all the points themselves (instead of averages of the group centres). The means are approximately \\(\\hat \\theta_i \\approx M (1 - A/\\sigma_i^2) + X_i \\sigma_i^2/A\\) is much closer to the mean of the data than the points themselves."
  },
  {
    "objectID": "james-stein-bayes/index.html#estimating-the-variances",
    "href": "james-stein-bayes/index.html#estimating-the-variances",
    "title": "Estimating Group Means with Empirical Bayes",
    "section": "Estimating the variances",
    "text": "Estimating the variances\nWe have assumed the \\(\\sigma_i\\) are known; in some cases like a Binomial distribution they are part of the model, but in practice they may need to be estimated. When the \\(X_i\\) are formed from groups with more than one data point they can be estimated over a prior distribution in a similar way to the mean. More simply we can assume they are the same and estimate it from the mean of the sample standard deviations (correcting for sample size).\nTo solve the equations for \\(\\hat A\\) an extremely low starting value would be 0. An extremely high value would be when \\(\\hat A \\gg \\sigma_i^2\\), in which case we get estimates of \\(\\hat A\\) identical to the equal variance case. In general then I’d expect the solution to be between these and could be solved by bisection (though I’m not completely sure of the upper bound, or that it has a unique solution)."
  },
  {
    "objectID": "teaching-programming-by-editing/index.html",
    "href": "teaching-programming-by-editing/index.html",
    "title": "Teaching Programming by Editing Code",
    "section": "",
    "text": "When someone asks me for advice on what resources to recommend, I carefully consider their background and where they want to get to. Often analysts will have come across things like if statements through Excel or Tableau, and have an understanding of what they want to do to the data, and how to break down problems. This actually means they are quite capable of picking up many programming concepts, even if they’ve never studied programming.\nBut more important than where they are is where they want to get to. Generally people want to learn programming to do something, and I’ve seen a variety of examples:\n\nsummarising themes from free text survey responses\nperforming custom statistical tests on skewed data, that their testing tools can’t handle\nfitting more models to better inform the outcomes of a decision\nautomating regular reports that are currently painfully extracted from multiple sources\n\nDepending on what they want to do, the actual materials that are relevant will vary. Some people want to transition into a software engineering career, and that will often be a different path again.\nHowever I get frustrated that the vast majority of learning to program materials spend a lot of time on dry theory. There are typically chapters on every different type of programming construct (types, string, conditionals, loops, vectors, objects, …), with little motivation as to why they study them.\nThere’s a learning concept called the Whole Game which focuses on teaching by getting people to actually play the game rather than just learn the rules. By getting students to do a small version of what they’re trying to accomplish, and getting them to learn themselves, is extremely motivating. I’ve seen this myself in the fastai course where I learned this concept.\nFor example take the problem of summarising themes from free text survey responses. You could start by present a basic topic model to extract themes from a text. Then get them to point this topic model at their own text; this alone can often be quite a challenge to extract the text and put it in the right format. Then you could give them some options on how to customise it; to see how changes make the model better or worse. This can start as simply as learning to edit the code to change constant values. Then maybe they need to find ways to print out words and sentences of different topics by changing an index. A for loop would let them display all the topics at once, or maybe process multiple surveys. Maybe converting the text to lowercase before fitting the model would lead to better results?\nThis is a basic example, but I really think if someone is really motivated to build a good theme summary automatically they could learn a lot from doing this. And it would enable them to better learn how to solve related problems. When they start hitting some limits they might start wondering how the model actually works, and that could lead to much deeper learning.\nDepending on the level experience you may want to hide some of the complex logic (one of the wondrous things about programming is that you can do this through functions). How you hide the complexity, and how much you hide, depends on where you want the student to focus. This means thinking a bit about where they are, and how they can learn in small steps. Also it’s really hard to do this - I find sitting with someone as they work through the problem can be really helpful because they’ll hit issues you didn’t even think of. Some gentle encouragement, hints, or occasionally fixing weird problems can be very helpful in getting them through the process. But as much as possible you want them to drive their own learning.\nI haven’t seen this idea of teaching programming by editing code to be widely taught. But in practice it’s how most programmers actually work. I’m much more likely to be maintaining and extending some existing code than I am to be writing my own. Even when I’m writing my own, it’s not unusual for me to look at tutorials or Stack Overflow for how to solve particular problems. Similarly I think this would be a valuable way to learn how to code.\nThat’s not to say the fundamentals aren’t important. But you’re much better off learning why they’re useful first, before learning all the intricate details of what they are. I’ve learned a lot by taking two courses in algorithms and data structures and a course in compilers, by reading (most of) the Structure and Interpretation of Computer Programs (one day I’d like to look at How to Design Programs). But I started learning programming by trying to build games, first in various frameworks and then in Java. I then learned a bunch more trying to do Physics simulations and calculations.\nReally the biggest risk isn’t that a student never reads the specification; it’s that they lose the motivation and give up. As Peter Norvig points out, learning programming takes years, but it starts with small steps."
  },
  {
    "objectID": "git-folder-identities/index.html",
    "href": "git-folder-identities/index.html",
    "title": "Git Folder Identities",
    "section": "",
    "text": "The easiest way to do this consistently is with a includeIf statement. For example to have custom options for any git repository under a folder called apache add this to the bottom of your ~/.gitconfig.\n[includeIf \"gitdir:apache/]\n    path = .gitconfig_apache\nThen put any custom configuration options in ~/.gitconfig_apache. Then if you have a git folder in ~/src/apache/code/my-repo then any configuration in ~/.gitconfig_apache will be applied, but it won’t by if you’re in ~/src/random/my-repo (since apache/ is not in the path to the repository root).\nIf you just want the configuration to apply to a few specific repositories you can add local configuration in each repository in .git/config. Between these two methods you should be able to get the context dependent configuration you want.\n\nThe details of git configuration\nConfiguration is obtained by reading through various config files in order from top to bottom, where later assignments overwrite previous; so for example a variable in a local configuration will overwrite a variable in a global configuation.\nsystem configuration:    $(prefix)/etc/gitconfig\nglobal configuration:    ~/.gitconfig\nlocal configuration:     (repo)/.git/config\nEnvironment variables\nCommand line\nI’m skipping over some details including worktree configuration, but this captures most of it. One other handy thing to know is the local configuration can also be specified to be a different file with the GIT_CONFIG environment variable or the -f/--file command line argument.\nThis gives you plenty of ways to set your configuration; especially when you combine this with includeIf (and the unconditional include) which inserts the referenced configuration file at that point based on the location of the gitdir or the name of a branch with onbranch.\nYou could actually implement the includeIf behaviour by setting environment variables based on your working directory, but that’s a bit flaky and won’t work if you use some tool outside the shell you configured it in. The includeIf method is pretty common and seems to work well.\nThe gitdir (and onbranch) in includeIf uses a gitignore style syntax so you could even do funky things like gitdir:~/src/**/apache/*/.git to get any repositories under ~/src one folder below an apache directory. I don’t know why you would, but you certainly could.\nOne final gotcha; gitdir only works if you’re in a git directory. So you can sort-of think of gitdir:apache/ as equivalent to gitdir:**/apache/**/.git, and the configuration won’t apply in ~/src/apache/ if that’s not a git directory. It rarely comes up, but can be confusing when debugging."
  },
  {
    "objectID": "warm-water/index.html",
    "href": "warm-water/index.html",
    "title": "Mixing Warm Water",
    "section": "",
    "text": "When you mix together two volumes of water at different temperatures their volumes add and the resulting temperature is a volume weighted average of the temperatures. For example if you take 25mL of water at 10° C and 75mL of water at 40° C you will get 100mL of water at 32.5° C.\nA fridge is typically around 3° C, and so this is the temperature of well refrigerated water. Water boils at 100° C so this is close to the temperature you get out of a kettle. So if you mix x mL of refrigerated water with y mL of boiled water you will get (x+y) mL of water at \\(\\left(\\frac{3x + 100y}{x+y}\\right)\\)° C.\nWe can reparameterise this in terms of the ratio and total water. The total volume is V = (x + y) and the fraction of hot water is \\(h = \\frac{y}{x+y}\\) and so the final temperature is \\(\\left(3(1-h) + 100h \\right)° \\rm{C} = (3 + 97h)° \\rm{C}\\)\nIf we round down the temperature of a fridge to 0° C then this becomes a trivial equation to solve; to get water at t° C, between 0 and 100, then we require h = t. So to get water at 80° C we would want 80% boiling water and 20% refrigerated water. To get 50mL tepid water at 40° C we would want 40% boiling water and 60% refrigerated water, that is 20mL of boiling water to 30mL of refrigerated water.\nNote that water left out will equilibrium over time to room temperature, so you should overshoot slightly for above room temperature and undershoot slightly for below room temperature."
  },
  {
    "objectID": "job-extraction-first-cut/index.html",
    "href": "job-extraction-first-cut/index.html",
    "title": "A First Cut of Job Extraction",
    "section": "",
    "text": "I’ve already talked about how to extract jobs from Common Crawl and the architecture for extracting the data. I’ll just briefly summarise how it all fits together.\nThe data is taken from Common Crawl’s scans of the web. We have a set of predefined URL patterns to search on Common Crawl that correspond to different job ads, that were found through various means. These are searched for in the Common Crawl capture index to find where all the corresponding pages are. Then all the corresponding pages are downlaoded from S3 as WARC.\nThe pages are then converted to JSON Lines for each source, using a custom extractor for each source capturing the semi-structured data. These are then normalised (e.g. HTML to text, extract salary, normalise location) and output as a feather dataframe.\nWith just two crawls (which happen about once a month) I’ve got over 13,000 job ads. Now I’ve got all this together I’m going to start analysing the results; and probably finding some bugs in the pipeline!"
  },
  {
    "objectID": "tdd-salary/index.html",
    "href": "tdd-salary/index.html",
    "title": "Test Driven Salary Extraction",
    "section": "",
    "text": "Price parser has a very large set of tests covering different ways people write prices. The solution is a simple process involving a basic regular expression, but it solves all these different cases.\nWhen extracting the value and period of salaries from job ads I took the same approach. I looked through a bunch of example data to find different patterns, and especially things that could go wrong. This gave me over fifty test cases.\nIt was very easy to write these test cases and use them to work out how to write a parser. I started with the approach from price parser and modified it to deal with things in salaries, such as ranges like $50-70k.\nRegular expressions are finicky and a test based approach let me quickly see if I broke anything when I changed the expression (as I often did). However within an hour I could get all but a couple of very hard (and obscure) tests passing, which I removed.\nFor this problem starting with the tests was the best way forward. As I discover new issues I can add tests and make sure any extensions or changes don’t break existing examples."
  },
  {
    "objectID": "evil-yank-pop/index.html",
    "href": "evil-yank-pop/index.html",
    "title": "Pasting text from long ago in Emacs and Vim",
    "section": "",
    "text": "In Vim (and Evil mode) I can type :reg to see what’s in the registers, find the value I want to paste, commit the register to memory (for example 8), exit and then paste that register (\"8p from normal mode, C-r 8 from insert mode). This is a bit painful and breaks the flow. If I’m copying from in Vim/Emacs and thinking ahead I can copy to a certain register (say a \"y<motion>), and then paste back with that same register, but I don’t find that at all natural. To find out more about Vim registers either type :h registers or read Brian Storti’s article on it.\nEmacs has a notion of a kill-ring, which contains all the recently killed (Emacs-speak for cut) text. You can then yank (Emacs-speak for paste) the text from the top of the ring (bound to C-y). But then if you wanted something further back you can use yank-pop (bound to M-y) to go back through the kill ring until you find the text that you want. Evil supports this with evil-paste-pop (overwriting the M-y binding by default); you can then paste with p or P (evil-paste-after or evil-paste-before) and then continue pressing M-y until you get back to the text you want. Vim (but not Evil) also supports this through the redo-register; after you paste from the first delete register with \"1p you can scroll through the registers with u..\nI still find the Vim and Evil mechanisms a bit clunky because it requires switching keybindings. But the Counsel completions for Emacs contains counsel-yank-pop which opens the completions in an Ivy minibuffer. This is great because you can select a completion either by navigating with up and down arrows (or C-p and C-n keys), or by typing a part of the text you want and then selecting it to paste with Enter. I find this a much easier way to retrieve text from long ago. I’ve bound this to gp in normal mode and Ctrl-Shift-r in insert mode.\n(use-package counsel\n  :demand t\n  :bind (:map evil-normal-state-map\n         (\"gp\" . 'counsel-yank-pop)\n         :map evil-insert-state-map\n         (\"C-S-r\" . 'counsel-yank-pop))\n   ;... Rest of configuration\n  )"
  },
  {
    "objectID": "beautiful-soup-tips/index.html",
    "href": "beautiful-soup-tips/index.html",
    "title": "Tips for Extracting Data with Beautiful Soup",
    "section": "",
    "text": "The source data can be obtained at the end of the article.\n\nUse a good HTML parser\nPython has an inbuilt html.parser, but it often misparses HTML. You’re better off using html5lib (or maybe html5-parser).\nFor example with this document I tried to extract the description with html.parser.\nimport bs4\nsoup = bs4.BeautifulSoup(html, 'html.parser')\ndescription = soup.select_one('.txt-pre-line')\nHowever it gave me back an empty span.\n<div class=\"txt-pre-line\">\n<ul></ul></div>\nBut viewing it in the browser shows the text should be there.\n\n\n\nSample txt-pre-line\n\n\nThis was fixed by switching the “features” to html5lib.\nsoup = bs4.BeautifulSoup(html, 'html5lib')\ndescription = soup.select_one('.txt-pre-line')\nThis gave the full HTML; here’s a small snipped\n<div class=\"txt-pre-line\">\n<ul><br/>\n<li>An opportunity now exists to lodge your details for consideration for ...\n<p>We Barwon Health is proud to be Totally Smoke Free.</p>\n</div>\n\n\nExtracting Structured data with selectors\nThe webpage contained a lot of examples of key value pairs marked up like this:\n<p class=\"txt-info location txt-one-line\"><span class=\"txt-bold\">Location: </span>Geelong</p>\nThe pairs all occurred with a class of txt-info so could be extracted using the CSS selector soup.select('.txt-info') returning a list of these. The key is always in a txt-bold class and so can be retrieved with .select_one('.txt-bold'). To get everything the value to the right of the key we can use .next_siblings which generates the sequence ['Geelong'].\n\n\nConverting to text\nThe .next_siblings will generate a mixture of Beautiful Soup objects: Tag, NavigableString and Comment.\nTo turn a Tag into text you can use the .get_text() method which extracts all the text. Unfortunately it also includes whitespace padding, so in this example you would get all the whitespace.\n<p class=\"txt-info txt-one-line\">\n    <span class=\"refnum\">\n        <span class=\"txt-bold\">Reference: </span>\n        <span>160271</span>\n    </span>\n</p>\nYou can remove the excess whitespace by invoking .get_text(strip=True).\nHowever it doesn’t interpret tags in the text; for example a <p> or a <br/> should be turned into a line break, but they’re not. You can pass a separator argument which will join the tags, but setting this to \\n puts a newline between all tags, including <b> and <a> where it shouldn’t be. For good HTML to text conversion we’d need to look somewhere else.\nTo turn a NavigableString into text we can just wrap it in str. To convert all the siblings we can need to treat them separately like:\n''.join(\n  str(\n      sib.get_text() if isinstance(sib, bs4.element.Tag) else sib\n  ).strip() \n  for sib in key.next_siblings)\n\n\nSerializing\nTrying to pickle a Beautiful Soup object leads to an error like this:\nTraceback (most recent call last):\n  File \"./02_merge_data.py\", line 50, in <module>\n    pickle.dump(data, f)\n  File \"/home/eross/.virtualenvs/ds/lib/python3.6/site-packages/bs4/element.py\", line 731, in __getnewargs__\n    return (str(self),)\nRecursionError: maximum recursion depth exceeded while getting the str of an object\nThis is because everything is connected to everything else, which is what lets you navigate the soup. To save the HTML I just wrap a Tag in str.\nstr(soup.select_one('.txt-pre-line'))\n\n\nGetting the data\nIf you want to rerun any of this script you can download the data from Common Crawl like this:\ncurl -H \"Range: bytes=403742602-403762161\" https://data.commoncrawl.org/crawl-data/CC-MAIN-2020-16/segments/1585371656216.67/warc/CC-MAIN-20200406164846-20200406195346-00353.warc.gz |\\\n  gzip -dc |\\\n  tail -n +35 >\\\ndata.html"
  },
  {
    "objectID": "ner-prodigy/index.html",
    "href": "ner-prodigy/index.html",
    "title": "Training a job title NER with Prodigy",
    "section": "",
    "text": "After thinking through an annotation scheme for job titles I wanted to try annotating and training a model. I tried this with Prodigy 1.7.1 and found the experience really interesting and it does seem like a valuable product. There’s a great tutorial on how to train a NER from scratch in Prodigy that helped me do this.\nFirst I needed to extract the training data into json lines of text, like:\n{\"text\":\"Health & Safety Manager\"}\n{\"text\":\"Field Sales Executive  Central London\"}\n{\"text\":\"HR Manager Branded Company East Midlands\"}\n{\"text\":\"Account Executive\"}\n{\"text\":\"Front End JavaScript HTML5 CSS JQuery Developer\"}\nI had a CSV of training data so I transformed it with Pandas:\nimport pandas as pd\ndf = pd.read_csv('Train_rev1.csv')\n(df\n .sample(10000)\n [['Title']]\n .rename(columns={'Title': 'text'})\n .to_json('sample.jsonl', orient='records', lines=True)\n)\nI also needed to create a blank spacy model to tokenize the text for annotation:\npython -c 'import spacy; spacy.blank(\"en\").to_disk(\"./blank_model\")'\nThen I could start annotating the job_title label into a dataset called job_titles:\nprodigy ner.manual -l job_title job_titles ./blank_model sample.jsonl\n\n\n\nProdigy Annotation Interface\n\n\nIt was pretty quick to make annotations by click and dragging the mouse. If you dragged anywhere within the word it would select the whole word and join to contiguous regions, then press ‘a’ on the keyboard to accept and move on to the next annotation. Sometimes the text wouldn’t be tokenised properly so annotation wasn’t possible, I just skipped those examples. It took me a couple of rounds to decide on what I was annotating (it gets quite subtle what you include), but once I got there it took me about an hour to annotate 1000 examples.\nI could then see how the amount of data impacted the model size of an NER model using train-curve which trains a model on successively larger portions of the training set. Each 200 annotations after the first improved accuracy by around 6% so it seems like if I made more annotations it could get better. This trained pretty quickly on a laptop.\nprodigy ner.train-curve \\\n        -l job_title \\\n        -es 0.2 \\\n        job_titles_min en_vectors_web_lg \\\n\n\n\nProdigy Training Curve\n\n\nFinally I could train a model on the whole dataset, leaving a 20% holdout for evaluation.\nprodigy ner.batch-train \\\n        -l job_title \\\n        -es 0.2 \\\n        job_titles_min en_vectors_web_lg \\\n        -o job_title_model/\nThe output model was around 73% accuracy and took about 1.4G of disk space. But I wanted to see the kinds of mistakes it made:\nprodigy ner.print-stream job_title_model/ sample.jsonl | less\n\n\n\nProdigy NER Results\n\n\nOverall it was pretty good at picking out job titles. There were issues especially when there were extraneous punctuation, and sometimes it wouldn’t capture the whole title. This might be resolved by annotating more examples, and being more consistent in my annotation (I drifted a bit!), or making my scheme a bit simpler. But the results seem good enough to be a useful starting point.\nFinally I could export the annotations into jsonlines for reference, or if I wanted to train a model directly.\nprodigy db-out job_titles job_title_annotations/\nThe output contains the tokens, the spans tagged and the results.\n{\n  \"text\": \"Health & Safety Manager\",\n  \"_input_hash\": -1231507245,\n  \"_task_hash\": 2039059237,\n  \"tokens\": [\n    {\n      \"text\": \"Health\",\n      \"start\": 0,\n      \"end\": 6,\n      \"id\": 0\n    },\n    {\n      \"text\": \"&\",\n      \"start\": 7,\n      \"end\": 8,\n      \"id\": 1\n    },\n    {\n      \"text\": \"Safety\",\n      \"start\": 9,\n      \"end\": 15,\n      \"id\": 2\n    },\n    {\n      \"text\": \"Manager\",\n      \"start\": 16,\n      \"end\": 23,\n      \"id\": 3\n    }\n  ],\n  \"spans\": [\n    {\n      \"start\": 0,\n      \"end\": 23,\n      \"token_start\": 0,\n      \"token_end\": 3,\n      \"label\": \"job_title\"\n    }\n  ],\n  \"answer\": \"accept\"\n}\nThis is a good start, but even if I can extract the job titles I still need to be able to normalise and relate them."
  },
  {
    "objectID": "low-abstraction-software/index.html",
    "href": "low-abstraction-software/index.html",
    "title": "Low abstraction software",
    "section": "",
    "text": "But sometimes these abstractions leak and you are forced to understand the layers. This is especially true as a developer where you are exposed to a plethora of tools to do the simplest things; Integrated Development Environments, source control systems, build tools, package managers and containers, debuggers. Moreover you have to interface across multiple systems; if you’re running PySpark you’ve got JVMs interacting with Python and its libraries written in C distributed across many machines - it gets hairy. This in particular puts up a barrier for new developers, or part-time developers; to make a simple change to some code you need to understand so many things.\nIs there a better way to build these abstractions, with fewer layers and less code? This requires tradeoffs; with speed of execution, functionality, hardware support, or integration with other environments. But there are some systems that make these tradeoffs and show other ways of doing things.\n\nBuilding from the ground up\nBuilding up on baremetal systems lets you understand how everything works down the CPU. However connecting to the plethora of peripherals is difficult and so they often only work on certain hardware and have limited support. Alex Chadwick’s Baking Pi tutorial works on a Raspberry Pi v1 from baremetal and can control LEDs, drawing to a screen and taking keyboard input. Kartik Agaram’s mu is a low level computing environment for x86 machines with some apps and ability for input and output, written in a way to be understood. More ambitious is Project Oberon by Niklaus Wirth and Jürg Gutknecht which builds a whole operating system on RISC-V in a Pascal-like language including it’s own form of networking. Finally it’s worth mentioning nand2tetris which simulates layers of abstraction from a NAND gate up to a programming language.\n\n\nLanguage with leverage\nProgramming languages tend to be large complex things with many corners, but there are some small well-crafted languages with high leverage. Lisps are especially syntactically simple lanugages, and in particular scheme and racket are simple languages with powerful languages with macro facilities (code that writes code) and control flow with continuations. Emacs lisp and emacs are a particularly striking example of a malleable system that is highly adaptable (although emacs itself is quite complicated!) Lua is a very compact language built mainly around the table data structure, a hybrid between a hashmap and a list, which is relatively flexible and has good C interoperability; Kartik Agaram built the teliva interpreted environment on top of it. Then there are concatenative stack languages like Forth and Factor which can build from low level operations to high level functions with minimal syntax. Swift build from quite low level LLVM code, for example the builtin Math trigonometric and exponential functions up to a high level.\n\n\nPragmatism\nAt the end of the day it’s very useful to be able to interact with other systems. Writing drivers for every network cards or USB is non-trivial in any system because it comes down to a lot of details, and if you can leverage Linux or BSD kernel code that’s great (or maybe we should build our own hardware?). And languages with leverage often have too much freedom, making it harder to switch to other people’s code bases and integrate between systems. But maybe in all this there are simpler ways of making software from the low to the high level that can get more people building software."
  },
  {
    "objectID": "suitcase-of-money/index.html",
    "href": "suitcase-of-money/index.html",
    "title": "How much Money is in a Suitcase?",
    "section": "",
    "text": "In the movies, and perhaps in reality, cocaine and elections are bought with a suitcase of $100 bills. Estimate the dollar value in such a suitcase.\n\n\nSize of $100 note\nLet’s assume a banknote is about the same thickness as paper; Australian notes are probably a little bit thicker. A 500 page ream of paper is about 5cm tall, so each sheet is about 0.01cm thick.\nA note is around 15cm long and about 6cm wide.\nSo the total volume is about \\(15 \\rm{cm} \\times 6 \\rm{cm} \\times 0.01 \\rm{cm} \\approx 1 \\rm{cm}^3\\).\n\n\nVolume of a Suitcase\nSuitcases come in a lot of sizes; lets consider a small wheely travel suitcase. Standing up it’s around 50cm high, by 30cm wide and 30 cm deep. This gives a total depth of 45L, call it 50L.\nA quick search online shows this is in a reasonable range; typically they’re in the range of 30L to 110L.\n\n\nNumber of notes in a suitcase.\nA litre is \\((10 \\rm{cm})^3 = 1000 \\rm{cm}^3\\).\nSo the total value of money in the suitcase would be $100 per note × 1000 notes per L × 50L in the suitcase which is $5 million.\nAround $5 million sounds like a reasonable estimate.\n\n\nWeight of the suitcase\nTo go a little further how much does the suitcase weigh?\nAn Australian banknote is probably a little thicker than printer paper, which weighs about 100 grams per square meter. So an Australian banknote weighs around .15 m × .06 m × 100 grams per square meter which is about 1 gram.\nSo the 50,000 banknotes in $5 million weighs about 50kg. This is a reasonably heavy suitcase of money to wheel around, and bit too heavy to bring on a flight (which typically has limits of 20-30 kg per bag).\nChecking the data for Australian Banknotes shows that 1 gram of weight for a note is a reasonable estimate."
  },
  {
    "objectID": "delegation-directions/index.html",
    "href": "delegation-directions/index.html",
    "title": "Directions of Delegation",
    "section": "",
    "text": "Downwards delegation is the classic kind that comes to most people’s minds. A manager redirects work to one of their employees, or an assistant. This is very common for domestic tasks through services; I’ve previously delegated cleaning a house, moving furniture and completing a tax return.\nYou can delegate sideways by handing work over to a colleague, friend or family member. This kind of delegation typically relies on trust and mutual exchange rather than payment. Someone will help you because they trust you will help them in return; don’t abuse that trust. This can be very useful when working with people with diverse skills and interests. In my personal life I often take detail oriented tasks from my partner, and exchange tasks that require relationship building because these play to our strengths. In my work life I will often take tasks that will unblock colleagues or help meet a deadline, and ask for help when that’s the best way to move the team forward.\nFinally you can delegate upwards to a manager. Obviously this is not appropriate for work that has been given directly to you and falls in your remit. But sometimes when dealing with a difficult stakeholder, or a roadblock you don’t know how to navigate, or if you just need assistance to meet a committed deadline then asking a manager for help can be useful. If it helps them meet their objectives they’ll always be willing to help, and will have connections and authority that can accomplish things much more easily than you could on your own. I’ll often talk to my manager to help break down a big task to work out what we can push back on (that is delete), or where to delegate sideways because they have a broader perspective.\nFor tasks that don’t meet your personal goals, but that you can’t delete, consider whether you can delegate them to someone. Delegation comes with a cost and management overhead, but is can worthwhile for tasks that don’t meet your purpose."
  },
  {
    "objectID": "source-mapping-html/index.html",
    "href": "source-mapping-html/index.html",
    "title": "Source Mapping Text HTML in Python",
    "section": "",
    "text": "We’re going to start by making our own subclass of HTMLParser with some useful functionality. HTMLParser can give us the line and character number just before the data to be processed, we add a property current_index to get the current index. We’ll also make it so when you call it on data it gets parsed and whatever is in self.result gets returned.\nfrom html.parser import HTMLParser\nfrom itertools import accumulate\n\nclass MyHTMLParser(HTMLParser):\n    def reset(self):\n        super().reset()\n        self.result = None\n\n    @property\n    def current_index(self):\n        line, char = self.getpos()\n        return self.line_lengths[line - 1] + char\n\n    def __call__(self, data):\n        self.reset()\n        self.line_lengths = [0] + list(accumulate(len(line) for line in data.splitlines(keepends=True)))\n        self.feed(data)\n        self.close()\n        return self.result\nNow we want to design a Text Extractor that extracts each span of text along with the start and end indices in the original HTML. Something like this:\nparser = HTMLTextExtractor()\n\nassert parser(\"Hello world\") == [{'text': 'Hello world', 'start': 0, 'end': 11}]\n\nassert parser(\"Hello <b>world</b>\") == [{'text': 'Hello ', 'start': 0, 'end': 6},\n                                        {'text': 'world', 'start': 9, 'end': 14}]\nWe can do this by capturing the data and index whenever we get text, and then when we hit the next tag capture the end index and append it to our result.\nclass HTMLTextExtractor(MyHTMLParser):\n    def reset(self):\n        super().reset()\n        self.result = []\n        self.text = None\n\n    def handle_data(self, data):\n        self.text = {'text': data, 'start': self.current_index}\n\n    def handle_starttag(self, tag, attrs):\n        self.text_end()\n\n    def handle_endtag(self, tag):\n        self.text_end()\n\n    def handle_comment(self, data):\n        self.text_end()\n\n    def text_end(self):\n        if self.text:\n            self.text['end'] = self.current_index\n            self.result.append(self.text)\n            self.text = ''\n\n    def close(self):\n        super().close()\n        self.text_end()\nThis works but it’s then difficult to do the source lookup. We’ll add some objects to make this easier. Instead of directly capturing the text each text span can capture a HTMLDoc object containing the raw html.\nfrom dataclasses import dataclass, field\nfrom __future__ import annotations\n\n@dataclass\nclass HTMLTextSpan:\n    start: int\n    end: int\n    doc: 'HTMLDoc'\n\n    @property\n    def text(self) -> str:\n        return self.doc.html[self.start:self.end]\nOur HTMLDoc in turn holds all the text_spans as well as the html. We can append to our text_spans, find get the text, find the length of that text.\n@dataclass\nclass HTMLDoc:\n    text_spans: list[HTMLTextSpan] = field(default_factory=list)\n    html: str = ''\n\n    def append(self, text: HTMLTextSpan) -> None:\n        self.text_spans.append(text)\n\n    @property\n    def text(self) -> str:\n        return ''.join(span.text for span in self.text_spans)\n\n    def __len__(self) -> int:\n        return sum((span.end - span.start) for span in self.text_spans)\nFor finding the source of a text index we just need to iterate through the text spans to that index, and extract the offset.\nclass HTMLDoc\n    ...\n    def source_index(self, idx: int) -> int:\n        if idx < 0:\n            idx = len(self) + idx\n        if not 0 <= idx < len(self):\n            raise ValueError(f\"Index {idx} not in range of {len(self)}\")\n\n        for span in self.text_spans:\n            size = span.end - span.start\n            if idx <= size:\n                return span.start + idx\n            idx -= size\n        return span.end\nThen for a non-HTML text we’d get the same thing:\nparser = HTMLTextExtractor()\n\nassert parser(\"Hello world\").text == 'Hello world'\nassert parser(\"Hello world\").source_index(0) == 0\nassert parser(\"Hello world\").source_index(6) == 6\nassert parser(\"Hello world\").source_index(10) == 10\nWith HTML we have to calculate the offset from tags:\nassert parser(\"Hello <b>world</b>\").source_index(0) == 0\nassert parser(\"Hello <b>world</b>\").source_index(6) == 9\nassert parser(\"Hello <b>world</b>\").source_index(10) == 13\nThe key property of the source map is if we re-parse the source of a text we should get the same text back.\nhtml = \"Hello <b>world</b>\"\n\nstart_idx = 3\nend_idx = 10\n\ndoc = parser(html)\n\nhtml_source = html[doc.source_index(start_idx):doc.source_index(end_idx)]\nassert parser(html_source).text == doc.text[start_idx:end_idx]\nWe can do this with pretty much the almost the same code as before:\nclass HTMLTextExtractor(MyHTMLParser):\n    def reset(self):\n        super().reset()\n        self.result = HTMLDoc()\n        self.text_start = None\n\n    def feed(self, data):\n        super().feed(data)\n        self.result.html += data\n\n\n    def handle_starttag(self, tag, attrs):\n        self.text_end()\n\n    def handle_endtag(self, tag):\n        self.text_end()\n\n    def handle_data(self, data):\n        self.text_start = self.current_index\n\n    def text_end(self):\n        if self.text_start is not None:\n            text = HTMLTextSpan(start=self.text_start,\n                                end=self.current_index,\n                                doc=self.result)\n            self.result.append(text)\n            self.text_start = None\n\n    def close(self):\n        super().close()\n        self.text_end()\nThat’s all there is to a simple, bare bones HTML to text source map. For real usecases you’d need to think about removing extra whitespace from the HTML, breaking across paragraphs, and extracting relevant HTML tags. This could be then further processed using something like SpaCy and we could annotate the document with spans of relevant tags, such as emphasis."
  },
  {
    "objectID": "bridging-bipartite-graph/index.html",
    "href": "bridging-bipartite-graph/index.html",
    "title": "Bridging Bipartite Graph",
    "section": "",
    "text": "Mathematically if the adjacency matrix of the bipartite graph is A, then this joint matrix is \\(A A^{T}\\). However most of the time this matrix is very sparse, and calculating this matrix product directly is very slow. But by iterating over the sorted events you can calculate it efficiently.\nSuppose you have the adjacency matrix stored as a list edges of pairs of (actor, event). The naive way to combine them in Python would be:\nfrom collections import defaultdict\ncounts = defaultdict(int)\nfor actor_1, event_1 in edges:\n  for actor_2, event_2 in edges:\n    if event_1 == event_2:\n      counts[(actor_1, actor_2)] += 1\nHowever this is quadratic in the number of edges. Instead we could use something like this, which will be much more efficient.\nfrom itertools import groupby\ncounts = defaultdict(int)\n# Sort by the events\nsorted_edges = sorted(edges, key=lambda x: x[1])\nfor _event, group in groupby(sorted_edges, lambda x: x[1]):\n  actors = [x[0] for x in group]\n  for actor_1 in actors:\n    for actor_2 in actors:\n      counts[(actor_1, actor_2)] += 1\nAn extension of this would be to filter out any bad actors that participate in too many events (because they contribute very heavily to the counts).\nThe same thing can be done in Presto SQL; suppose that we have a table edges that contains columns actor, event. Then the naive solution is:\nselect a.actor as actor_a, b.actor as actor_b\nfrom edges a\njoin edges b on a.event = b.event\nWhereas we can do the more efficient version in Presto/Athena SQL using arrays, and even filter on bad actors:\nselect actor_a, actor_b, count(*) as count from (\nselect event, array_agg(actor) as actors\nfrom edges\ngroup by event\n-- Filter out any actors with more than 100 events\nhaving cardinality(array_agg(actor)) < 100\n)\ncross join unnest(actors) as ta(actor_a)\ncross join unnest(actors) as ta(actor_b)\ngroup by 1, 2\nSo now you have a relatively efficient way to bridge sparse bipartite graphs, that you can then use for example in community detection."
  },
  {
    "objectID": "presto-integer-division/index.html",
    "href": "presto-integer-division/index.html",
    "title": "Calculating percentages in Presto",
    "section": "",
    "text": "Suppose for example you want to calculate the percentage of a column that is not null; you might try something like\nSELECT count(col) / count(*) AS col_pct_nonnull\nFROM some_table\nHowever I was surprised when I got a 0; was the whole column null? It is because Presto uses integer division by default. So 1/6 gives 0 and 7/6 gives 1.\nOne way to work around this is by explicit casting to double:\nSELECT count(col) / cast(count(*) AS DOUBLE) AS col_pct_nonnull\nFROM some_table\nBut that’s a lot of typing very quickly. We can try to coerce the number by adding a decimal, but this changes it to fixed precision in Presto. So SELECT 1/6., 1/6.0, 1/6.00 returns 0, 0.2, 0.17. This can be useful if you want to show a truncated percentage, e.g for 3 decimal points. However in Athena these all coerce to floating point returning 0.1666…\n-- 3 decimal places in Presto\n-- Floating point in Athena\nSELECT count(col) / sum(1.000) AS col_pct_nonnull\nFROM some_table\nHowever I tend to prefer storing things in double precision; otherwise if you do something like calculate a cumulative sum the rounding errors can compound. You can force this in Presto by explicitly using scientific notation; SELECT 1/6e0 gives the result to double precision 0.16666… So we could change our query to:\nSELECT count(col) / sum(1e0) AS col_pct_nonnull\nFROM some_table\nIf you’re ever doing a division in Presto or Athena it’s good practice to throw in a 1e0 * to make sure you’re doing floating point arithmetic, otherwise you will often get misleading results."
  },
  {
    "objectID": "point-of-computer-algebra/index.html",
    "href": "point-of-computer-algebra/index.html",
    "title": "The point of computer algebra systems",
    "section": "",
    "text": "\\(\\int_0^{2\\pi} \\frac{i e^{it}}{e^{it}-a} dt\\)\nfor various values of \\(a\\) . By Cauchy’s integral formula we know it should be \\(2 \\pi i\\) if \\(|a|<1\\) and 0 if \\(|a|>1\\) . Interestingly both programs gave the right answer for \\(a=0\\) (I suppose the calculation is easy there) but gave totally wrong answers for \\(a \\neq 0\\) (Matlab gave \\(0\\) for \\(|a|<1\\) and \\(-2 \\pi i\\) if \\(|a|>1\\) and Maxima gave \\(0\\) everywhere).\nI have no idea why this happens! In Matlab if you expand the complex exponential it gives the right answer. In Maxima it then has trouble computing the integral – but if you perform operations to make the denominator real and perform some simplifications it gives the right answer.\nNow people sometimes ask Why learn arithmetic when we have calculators? I think this shows exactly why: you need to know when your calculator is giving you garbage.\n\nCalculators are pretty good – but you can still trick them. Taking an example out of Hjorth Jensens’ superb notes on Computational Physics [available online]\n\\(\\frac{\\sin x}{1+\\cos x} = \\frac{1-\\cos x}{\\sin x}\\)\nHowever if I put these into my pocket calculator for \\(x=7 \\times 10^{-7}\\) I get\n\\(3.5 \\times 10^{-7} = 3.514285714 \\times 10^{-7}\\).\n(This is of course just round off error due to the finite precision that a calculator works at). Understanding the ideas behind how your calculator does arithmetic lets you understand why it can screw up – another common error is the ordering of operations.\nComputer Algebra Systems (which are just more sophisticated calculators) are useful for the same reason pocket calculators are: doing work that you understand how to do (and hopefully can check) but would be laborious for you to do.\nFor instance if I want to calculate the square root of \\(170\\) to a given precision manually. E.g. I could use\n\\(\\sqrt{170}=13\\sqrt{1+\\frac{1}{169}}\\)\nand use a Taylor expansion, or use a recursive linearisation: for \\(a \\ll x\\) we have \\((x+a)^2 \\approx x^2 + 2ax\\) , so we start with \\(x=13\\) (which is close to a solution), solve for \\(a\\), which gives us a new \\(x=13+a\\) which we feed back into the algorithm. (Or one of the other methods). In any case it would require a large number of divisions and multiplications with an increasing number of digits, and it gets very laborious as the precision grows. If I’m calculating lots of square roots to high precision this could take weeks (unless I bought a good slide rule). Calculators are fantastic time savers.\nSimilarly Computer Algebra Systems should only be used on things you know how to calculate but which would take considerable effort to do so. (Modern CAS will often not give answers or even give incorrect answers if you don’t set up the problem in the right way, so you have to be very careful). They can’t supplant understanding, but they can save mounds of time.\nPostscript: Since I wrote this article I have found an error in Mathematica 8.0. Typing\nSeries[HarmonicNumber[n-1, 2], {n, Infinity, 5}]\ngives the result\n\\(\\frac{\\pi ^2}{6}-\\frac{1}{n}-\\frac{1}{2 n^2}-\\frac{1}{6 n^3}+\\frac{1}{2 n^4}+\\frac{31}{30 n^5}+\\textrm{O}\\left(\\frac{1}{n^6}\\right)\\)\nbut a bit of messing around with the Euler-Maclaurin formula gives\n\\(\\sum_{k=1}^{n-1} \\frac{1}{k^2}= \\frac{\\pi^2}{6} - \\frac{1}{n} - \\frac{1}{2n^2} -\\frac{1}{6n^3} + \\frac{1}{30n^5}+ \\textrm{O}\\left(\\frac{1}{n^7}\\right)\\)\nThese are clearly in disagreement, and not in an obvious way! The only ways you could find out the Mathematica result is wrong is to derive the formula independently by hand or check the results numerically (for this particular case it’s useful to note that the error in the approximation is less than the first discarded term – see Concrete Mathematics by Graham, Knuth and Patashnik).\nThis reinforces the moral of the story: Computer Algebra Systems can be very useful in making your mathematics more efficient (it can be tedious deriving Euler-Maclaurin formula by hand), but they are currently far from perfect and you have to be sceptical about its output. This means you really have to understand the mathematics behind what you’re doing and check the results that are important to you using more reliable tools (checking special cases, testing numerically, …). Ideally you understand a little bit about how your CAS works.\nThis is all of course equally true of complicated numerical solvers – e.g. differential equation solvers. You need to check it’s giving you sensible output, because you may just have chosen parameters that your particular algorithm is terrible at."
  },
  {
    "objectID": "righteous-mind/index.html",
    "href": "righteous-mind/index.html",
    "title": "The Righteous Mind: Book Review",
    "section": "",
    "text": "The book is broken into three parts; how we justify our morals, what morality is, and how it formed.\n\n1. Intuitions come first, strategic reasoning second\nThe first part argues that intuition, not rationality, leads moral decisions. This resonates with the work of Kahneman and Tversky in behavioural economics on intuitions (System 1) leading rational thought (System 2) in many important decisions. Haidt uses the vivid metaphor of rational thought being a small rider on top of the large elephant of intuition, which largely leads the way.\nThis also resonates with my experiences that it’s very hard to convince someone who thinks something is morally wrong with logical argument. We can easily poke rational holes in other people’s arguments, but can’t see the faults in our own.\n\nIt is easy to see the faults of others, but difficult to see one’s own faults. One shows the faults of others like chaff winnowed in the wind, but one conleals one’s own faults as a cunning gambler conceals his dice.\nBuddha\n\nHe talks about Tetlock’s work (Lerner and Tetlock 2003), where even when decision makers are accountable they only engage in exploratory thought challenging their initial intuition (in contrast to confirmatory thought) under extraordinary circumstances:\n\nAccountability increases exploratory thought only when three conditions apply: (1) decision makers learn before forming any opinion they will be accountable to an audience, (2) the audience’s views are unknown, and (3) they believe the audience is well informed and interested in accuracy.\n\n\n\n2. There’s more to morality than harm and fairness\nThe second part argues there are multiple factors involved in morality, and it can’t be a simple fact of harm and fairness. Haidt talks about morality in the observed sense; something is immoral based on what people intuitively say are “wrong”. He has some interesting example stories of people crossing taboos (such as incest and necrophilia) without causing harm that many people say are wrong.\nThe people in Western Educated Industrialised Rich Democracies (known as WEIRD countries) are much more individualistic than the majority of other societies. Haidt uses this to argue that we see individual harm and fairness as important because of this, and recounts his own experience of working in part of India of seeing different cultural values.\nHe proposes a set of 6 factors (“moral tastebuds”) that each culture has to different degrees that characterise their values; Care, Fairness, Loyalty, Authority, Sanctity and Liberty. His arguments for there being 6 factors is incredibly weak; I would want evidence that the factors are mutually exclusive or completely exhaustive. The only evidence he gives is surveys of American “Liberals” and “Conservatives” on questions that he has labelled as pertaining to each of the 6 values above, and showing that Conservatives have a broader range of these morals. However Americans are definitely WEIRD, and with only 2 groups it’s impossible to observe whether, for example Loyalty and Authority are independent or always correlated. A convincing argument would require evidence from a large number of different societies that these 6 factors all occur in different amounts, and explain most of what we consider as moral. In this area Haidt needs to do more exploratory thought, and less confirmatory thinking.\nHaving made this unsupported assertion he goes on long tangents about how he could have gotten John Kerry to win the American election by using more moral factors. But he never really showed any strong evidence that using more of these factors would lead to better election results in the US, and that it’s more important than other aspects of the campaign. I found these parts of the book quite tiresome.\n\n\n3. Morality binds and blinds\nIn the final part of the book Haidt constructs and elaborate evolutionary theory of where morality comes from. I feel like he has decided that morality is so important it needs an extraordinary origin story, and he goes to great lengths to give a story about how morality arises from natural selection occurring at the group level (as opposed to the individual level) over the last 10,000 years, that humans are the only mammal that can act as a superorganism like bees or ants (claiming “we’re 10% wasp”) and that religion was an evolutionary byproduct. However the list of extraordinary claims doesn’t have the requisite extraordinary evidence; just plausible stories lacking exploratory thinking.\n\nOur righteous minds were shaped by kin selection plus reciprocal altruism augmented by gossip and reputation management\n\nDespite the unscientific aggrandising he does reference some interesting research and makes some valid points about us as a society. It’s just a challenge unpicking the wild claims from the research.\n\nIt is religious belongingness that matters for neighborliness, not religious believing.\nPutnam and Campbell, American Grace: How Religion Divides and Unites Us\n\n\n\nReferences\nOverall the book is full of interesting references, has some good insights on societies, but has too many wild unsupported claims from the author. I’m really glad I read it and it has made me more interested in Anthropology; understanding how other societies function. Here are some references from the book that would make for interesting further reading:\n\nAnything by Durkheim\nDescares’ Error, by the neuroscientist Antonio Damasio\nPatterns, Thinking, and Cognition by Howard Margolis\nSwitch, by Chip Heath and Dan Heath\nKass, Wisdom of Repugnance\nHierarchy in the Forest, Boehm\nDarwin’s Cathedral, Wilson\nThe Weirdest People in the World, by Joe Henrich, Steve Heine, and Ara Norenzayan\n\n\n\nIn closing\nTowards the end of the book Haidt makes an interesting claim about how to seek truth, which is relevant for someone like me who uses evidence to help decision makes get better outcomes.\n\nWe should not expect individuals to produce good, open-minded, truth-seeking reasoning, particularly when self-interest or reputational concerns are in play. But if you put individuals together in the right way, such that some individuals can use their reasoning powers to disconfirm the claims of others, and all individuals feel some common bond or shared fate that allows them to interact civilly, you can create a group that ends up producing good reasoning as an emergent property of the social system. This is why it’s so important to have intellectual and ideological diversity within any group or institution whose goal is to find truth."
  },
  {
    "objectID": "analysis-decision/index.html",
    "href": "analysis-decision/index.html",
    "title": "Analysis Needs to Change A Decision",
    "section": "",
    "text": "There’s lots of reasons people want an analysis. Sometimes it’s to confirm what they already believe (and they’ll discount anything that tells them otherwise). Sometimes it’s to prove to others something they believe; possibly to inform a decision someone else is making. But it’s most valuable when it effects a decision they can make with an outcome they care about.\nI always find it useful to clarify this and do some scenario modelling before even planning out an analysis. Some useful questions to ask are:\n\nWhat’s the problem you’re trying to solve?\nWhat does a good outcome look like?\nWhat outcome do you expect? How sure are you?\nWhat would you do if the result looked like this?\nHow big is the impact of getting it wrong?\n\nWhen you understand this you can be clear how rigorous you need to be in the analysis.\nFor example suppose that you want to know whether a more personalised subject line will improve click through rates. There’s a lot of industry knowledge suggesting that’s the case, but it’s not really clear how much it will improve things. You could run an A/B test to see whether it improves click through rates by more than 2 percentage points, say. If it makes the click through rates worse you would definitely stick to your current version. But if it doesn’t make any difference (null case) would you stick to your current version?\nIndustry best practice says that you should personalise your subject line, and you’re pretty happy with what you’ve got. You may as well stick with the new version, it’s not making things any worse.\nHow bad is it if you’re wrong? Maybe you’re going to be doing a bunch of experimentation on the subject line so it doesn’t matter too much and you set your confidence thresholds lower.\nStandard statistical analysis often focuses more on confidence that a result is different than 0, than how significant the difference is. It also doesn’t focus on how impactful the difference is on downstream decisions, but this is crucial in understanding what analysis to do."
  },
  {
    "objectID": "rule-of-five/index.html",
    "href": "rule-of-five/index.html",
    "title": "94% confidence with 5 measurements",
    "section": "",
    "text": "There are many things that are valuable to know in business but are hard to measure. For example the time from when a customer has a need to purchase, the number of related products customers use or the or the actual value your products are delivering. However you don’t need a sample size of hundreds to get an estimate; in fact you can get a statistically significant result from measuring just 5 random customers.\nFor any of these measurements there’s a 94% chance that the median value is between the biggest and smallest value of 5 random samples. The median value is a “typical” value; half of measurements will have a larger value and half will have a smaller value. The median is often more useful than the mean average because it isn’t impacted by outliers.\nIt might sound surprising that you can get this much confidence with so few measurements, but it’s true. When you measure a random sample it’s a 50/50 chance as to whether it’s above or below the median. So the chance that 5 random samples are all above or below the median is the same as the chance of flipping a coin 5 times and getting all heads or all tails. The probability of this is 1 in 16, or about 6%, and so 94% of the time there will be one sample above and one below the median. And so 94% of the time the median will be between the smallest and largest value.\nWhen things are highly uncertain but can impact decisions it’s be worth investing a little to get some measurement of it. The key assumption here is that the sample is truly random; but for impactful and uncertain measures it’s worth the investment of sampling.\nThe range between the maximum and minimum value may be huge (in statistical jargon: the test is low power). To reduce the variability you could the median as the value between the 2nd largest and 2nd smallest value of 8 items, or between the 3rd largest and 3rd smallest value of 11 items. This table shows the tradeoff between sample size and confidence."
  },
  {
    "objectID": "rule-of-five/index.html#calculating-it-yourself",
    "href": "rule-of-five/index.html#calculating-it-yourself",
    "title": "94% confidence with 5 measurements",
    "section": "Calculating it yourself",
    "text": "Calculating it yourself\nThe idea is from the wonderful book “How to Measure Anything” by Douglas W. Hubbard.\nHere’s some R code that generates the table and plots above.\n# Exact calculation\nci_quantile_extreme <- function(num_samples, nth_smallest=1, nth_largest=1, quantile=0.5) {\n    stopifnot(nth_largest + nth_smallest < num_samples)\n    stopifnot((0 < quantile) && (quantile < 1))\n    # Number of ways all but the nth largest values could be bigger than quantile\n    freq_larger <- sum(choose(num_samples, seq(0, nth_largest - 1)))\n    # Number of ways all but the nth smallest values could be bigger than quantile\n    freq_smaller <- sum(choose(num_samples, seq(0, nth_smallest - 1)))\n    1 - (freq_larger * quantile ^ num_samples + freq_smaller * (1 - quantile) ^num_samples)\n}\n\n# Test calculation with random sample\nsample_quantile_extreme <- function(sample_size, nth_smallest, nth_largest, num_sample=10000, quantile=0.5) {\n    distribution <- runif\n    distribution_median <- quantile\n\n    samples <- distribution(sample_size * num_sample)\n    dim(samples) <- c(sample_size, num_sample)\n\n    sorted_samples <- apply(samples, 2, sort)\n    mean((sorted_samples[nth_smallest,] <= distribution_median) &\n         (sorted_samples[sample_size+1-nth_largest,] >= distribution_median))\n}\n\n\n# Calculate the number of samples at nth largest to reach at least ci\nmedian_extreme_samples_at_ci <- function(nth, ci=0.9) {\n    sample_size <- 2*nth + 1\n    while(ci_quantile_extreme(sample_size, nth, nth) < ci) {\n        sample_size <- sample_size + 1\n    }\n    sample_size\n}\n\n# Generate a table of confidences\nci_median_extreme_table <- function(n_max=10, ci=0.9) {\n    nth_largest <- seq(1, n_max)\n    sample_size <- vapply(nth_largest,\n                          function(n) median_extreme_samples_at_ci(n, ci),\n                          double(1))\n    confidence_interval <- mapply(ci_quantile_extreme, sample_size, nth_largest, nth_largest)\n\n    data.frame(nth_largest, sample_size, confidence_interval, ci=ci)\n}\n\n# Power depends a lot on the distribution\n\ndf <- rbind(\nci_median_extreme_table(20, 0.75),\nci_median_extreme_table(20, 0.9),\nci_median_extreme_table(20, 0.95),\nci_median_extreme_table(20, 0.99)\n)\n\ndf$confidence_interval = paste0(as.character(df$ci * 100), '%')\n\nlibrary(ggplot2)\n\nggplot(df, mapping=aes(nth_largest, sample_size, color=confidence_interval, group=confidence_interval)) +\n    geom_line() +\n    scale_x_continuous(minor_breaks=seq(1, 20)) +\n    scale_y_continuous(minor_breaks=seq(0, 100, by=5)) +\n    labs(x=\"nth largest/smallest items\", y=\"Sample Size\", color=\"Confidence Interval\",\n         title=\"Median confidence interval between nth extreme items\")"
  },
  {
    "objectID": "prodigy-diff/index.html",
    "href": "prodigy-diff/index.html",
    "title": "Creating a Diff Recipe in Prodigy",
    "section": "",
    "text": "I’ve been extracting job titles and skills from the job ads in the Adzuna Job Salary Predictions Kaggle Competition. One thing I noticed is there are a lot of job ads that are almost exactly the se; sometimes between the train and test set which is a data leak. For information extraction it’s useful to remove these duplicates to make sure frequency counts aren’t skewed by one ad appearing dozens of times.\nI thought Prodigy could be a good tool for annotating them. Even though it doesn’t have a way of annotating duplicates it has a diff view which is almost perfect for showing the duplicates.\nFirst I created a example.jsonl file with examples like theirs (make sure the file doesn’t end in a newline or you’ll get an inscrutable error about malformed JSON):\n{ \"accept\": { \"text\": \"Cyber researchers have linked the vulnerability exploited by the latest ransomware to “WannaCry”. Both versions of malicious software rely on weaknesses discovered by the National Security Agency years ago, Kaspersky said.\" }, \"reject\": { \"text\": \"Cyber researchers linked the vulnerability exploited by the latest ransomware to 'Wanna Cry'. Both versions of malicious software rely on weaknessses, discovered by the National Security Agency years ago, Kaspersky said\" } }\n{ \"accept\": { \"text\": \"Java J****EE Developer  ****k  ****k  Music, Film & TV  London Java J****EE Developers required for software house with client sectors of music, film and TV. Salary: Maximum ****: Discretionary bonus and benefits package. Location: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java Developer. The working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forward. This is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounder. EXPERIENCE REQUIRED: The experience required for this role is as follows:  A minimum of 5 years experience in the development of web applications for the J****EE development platform.  A minimum of 5 years experience in Java  Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologies.  Good knowledge of CSS, XML and DHTML  A personality suited to clientfacing situations  good communication skills.  A good standard of written English The above experience is essential. You require all of the above experience in order for to be eligible for this role. The following experience is desirable, though not essential:  Knowledge of the WebSphere development environment and Application Server.  Knowledge of and experience with AJAX (Asynchronous JavaScript XML)  Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and support. You'll be involved in different technologies across the board from Front Office to Back Office. Please note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworks. THE OPPORTUNITY Why work here? As for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of development. Therefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technology. This is a central role that essentially can take off in any direction. Here, you will have enough autonomy to define your own role. Therefore, if you take the initiative you can shape your role for the future and drive your own progression. Overall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a say. Being music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreign. Location: Near Euston and King's Cross, London\"}, \"reject\": {\"text\": \"NEW  Java J****EE Developer – ****k  ****k  Music, Film TV  London Java J****EE Developers required for software house with client sectors of music, film and TV. Salary: Maximum ****: Discretionary bonus and benefits package. Location: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java Developer. The working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forward. This is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounder. EXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platform. A minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologies. Good knowledge of CSS, XML and DHTML A personality suited to clientfacing situations  good communication skills. A good standard of written English The above experience is essential. You require all of the above experience in order for to be eligible for this role. The following experience is desirable, though not essential: Knowledge of the Websphere development environment and application server. Knowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and support. You'll be involved in different technologies across the board from front office to back office. Please note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworks. THE OPPORTUNITY: Why work here? As for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of development. Therefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technology. This is a central role that essentially can take off in any direction. Here, you will have enough autonomy to define your own role. Therefore, if you take the initiative you can shape your role for the future and drive your own progression. Overall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a say. Being music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreign. Location: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****\"}}\nI then copied their examples to make a minimal example duplicate.py:\nimport prodigy\nfrom prodigy.components.loaders import JSONL\n\n@prodigy.recipe(\n    \"duplicate\",\n    dataset=(\"Dataset to save answers to\", \"positional\", None, str),\n    file_path=(\"Path to file\", \"positional\", None, str)\n)\ndef duplicate(dataset, file_path):\n    stream = JSONL(file_path)\n\n    return {\n        \"dataset\": dataset,\n        \"view_id\": \"diff\",\n        \"stream\": stream,\n    }\nThen I invoked it from the command line:\nprodigy duplicate -F duplicate.py test_dataset example.jsonl\nBut after opening the browser I got an informative error in the console:\n✨ ERROR: Invalid task format for view ID 'diff'\n'id' is a required property\n'mapping' is a required property\nSo I copied another example to add ids and mappings to the stream:\ndef duplicate(dataset, file_path):\n    raw_stream = JSONL(file_path)\n    stream = add_options(raw_stream)\n\n    return {\n        \"dataset\": dataset,\n        \"view_id\": \"diff\",\n        \"stream\": stream,\n    }\n\ndef add_options(stream):\n    for idx, task in enumerate(stream):\n        task[\"mapping\"] = {'A': 'accept', 'B': 'reject'}\n        task[\"id\"] = idx\n        yield task\nThen it worked very well; the colour of the text matches the action (accept/reject) which is actually meaningless in our case (we want to know if they are the same or different), but it’s good enough.\n\n\n\nProdigy diff interface\n\n\nHowever it’s messy for text that’s very different:\n\n\n\ndiff on very different text\n\n\nApparently you can configure it to do diffs at a sentence level which may be a little neater. Otherwise I may need to roll my own HTML interface.\nFinally I tested the output with prodigy db-out test_dataset which looked like:\n{\n  \"accept\": {\n    \"text\": \"Cyber researchers have linked the vulnerability exploited by the latest ransomware to “WannaCry”. Both versions of malicious software rely on weaknesses d iscovered by the National Security Agency years ago, Kaspersky said.\"\n  },\n  \"reject\": {\n    \"text\": \"Cyber researchers linked the vulnerability exploited by the latest ransomware to 'Wanna Cry'. Both versions of malicious software rely on weaknessses, discovered by the National Security Agency years ago, Kaspersky said\"\n  },\n  \"mapping\": {\n    \"A\": \"accept\",\n    \"B\": \"reject\"\n  },\n  \"id\": 1,\n  \"_input_hash\": 1111295394,\n  \"_task_hash\": -415472751,\n  \"answer\": \"accept\"\n}\nThe answer is accept or reject which doesn’t really line up with the labels, so the data is a bit messy, but usable enough. I’ll need to prepare some more sample data to see whether this is a usable way to annotate duplicates."
  },
  {
    "objectID": "athena-exhausted-resources/index.html",
    "href": "athena-exhausted-resources/index.html",
    "title": "Running out of Resources on AWS Athena",
    "section": "",
    "text": "Today I was running some queries for a regular reporting pipeline in Athena when I got failure with the error Query exhausted resources at this scale factor. The query was running out of memory, but I had no idea why. I had run this query before with no issues. I reran the pipeline and then it failed with the same error at a different step.\nThe problem is that there is no visibility on why things are failing, and no levers to get more resources. There’s just enough differences between Athena and Presto that if I spun up my own Presto cluster, which I could scale to any size, I’d have to make some small changes to my queries to have them run successfully. There was a good risk that the process was broken for a couple of days.\nI kept on retrying and eventually it reran. I talked to someone else who had similar problems, and it sounds like it may have been an issue on the AWS end. But I’ll never really know and this is the risk.\nA managed service with no levers like Athena, or Google BigQuery, is extremely convenient to run data pipelines with. But the problem is that if your data grows or the service changes your pipeline might hit the limits and you may have to interrupt your service and either rewrite your pipeline or migrate to another service. It’s worth considering this risk and it may be worth investing in a solution that allows you to scale up the infrastructure such as Spark."
  },
  {
    "objectID": "finding-hugo-blogs/index.html",
    "href": "finding-hugo-blogs/index.html",
    "title": "Finding Hugo Blogs with BigQuery",
    "section": "",
    "text": "The strategy is that most Hugo blogs will contain a /themes folder, a /content folder a /static folder and a config.toml file. They don’t have to have these, but many of them will as these are very standard structure. We can then search for them in Bigquery (this is just a 1% sample, to get them all replace sample_files with files):\nselect repo_name, markdown_files from (\nSELECT repo_name,\n      logical_or(REGEXP_CONTAINS(path, '^themes/')) as has_theme,\n      logical_or(REGEXP_CONTAINS(path, '^content/')) as has_content,\n      logical_or(REGEXP_CONTAINS(path, '^static/')) as has_static,\n      logical_or(path = 'config.toml') as has_config,\n      count(case when ends_with(path, '.md') then 1 end) as markdown_files\nFROM `bigquery-public-data.github_repos.sample_files`\ngroup by 1\n)\nwhere has_theme and has_content and has_static and has_config\nlimit 100\nThe results aren’t all blogs, some are examples like hugo-lightslider or hugo-deploy. But they are all related to Hugo in some way.\nThis could then be used for further queries, like finding the most popular themes, the distribution of number of posts, extracting tags from blog posts for a classifier and other things."
  },
  {
    "objectID": "ngram-sentence-boundaries/index.html",
    "href": "ngram-sentence-boundaries/index.html",
    "title": "Sentence Boundaries in N-gram Language Models",
    "section": "",
    "text": "I’m reading through the December 30, 2020 draft of Speech and Language Processing, by Jurafsky and Martin, in particular Chapter 3 N-gram Language Models. To create the language model you simply count how often each word follows the previous N-1 words to create a count matrix, and then normalise each row to 1 so it’s a conditional probability. Then, under the assumption that the probability of a word just depends on the previous N-1 words, you can calculate the probability of every sentence. As shown in Exercise 3.5 of the text if you don’t have an end sentence marker then the probabilities over every possible sentence don’t add up to 1.\nAs an example consider the corpus of two sentences, with the special markers to start a sentence, <s>, and end a sentence </s>:\n<s> I am Sam </s>\n<s> Sam I am </s>\nThen we can generate the counts for a bigram model on words (i.e. N=2), with the first word on the left\n\n\n\nCounts\n<s>\nI\nam\nSam\n</s>\n\n\n\n\n<s>\n0\n1\n0\n1\n0\n\n\nI\n0\n0\n2\n0\n0\n\n\nam\n0\n0\n0\n1\n1\n\n\nSam\n0\n1\n1\n0\n0\n\n\n</s>\n0\n0\n0\n0\n0\n\n\n\nNotice that structurally the first column and last row have to be zero; a start sentence marker can never occur after any token and no token can follow the end sentence marker. Now if we wanted to use add-k smoothing to allow for combinations of words not in the text, then we can’t add k to the first column or last row, but we want to add it to every other cell. I suggest instead unifying <s> and </s> as the same token, giving the count matrix:\n\n\n\nCounts\n<s>\nI\nam\nSam\n\n\n\n\n<s>\n0\n1\n0\n1\n\n\nI\n0\n0\n2\n0\n\n\nam\n1\n0\n0\n1\n\n\nSam\n0\n1\n1\n0\n\n\n\nIn this matrix any cell can be non-zero, as long as we allow empty sentences <s> <s>. In this case we can convert them to conditional probabilities by dividing each row by its sum:\n\n\n\nP(column | row)\n<s>\nI\nam\nSam\n\n\n\n\n<s>\n0\n0.5\n0\n0.5\n\n\nI\n0\n0\n1\n0\n\n\nam\n0.5\n0\n0\n0.5\n\n\nSam\n0\n0.5\n0.5\n0\n\n\n\nWe can use this to calculate the probability of any sentence under the bigram model, for example the sentence <s> Sam I am <s> has probability (using . in place of <s>)\n\\[P(\\rm{Sam\\ I\\ am}) = P(\\rm{Sam} \\vert .) P(\\rm{I} \\vert \\rm{Sam}) P(\\rm{am} \\vert \\rm{I}) P(. \\vert \\rm{am})= 0.5 \\times 0.5 \\times 1 \\times 0.5 = 0.125\\]\nNote this generalises and we can get sentences like <s> I am <s> or <s> Sam I am Sam am <s>, but we can’t possibly get <s> am I Sam <s> since \\(P(\\rm{am}| .) = 0\\).\nThe add-k method adds a constant k to every element of the matrix. As discussed we can only do this naively if we’ve got a single sentence boundary token.\n\n\n\nAdd 0.5 counts\n<s>\nI\nam\nSam\n\n\n\n\n<s>\n0.5\n1.5\n0.5\n1.5\n\n\nI\n0.5\n0.5\n2.5\n0.5\n\n\nam\n1.5\n0.5\n0.5\n1.5\n\n\nSam\n0.5\n1.5\n1.5\n0.5\n\n\n\nWe can then normalise these into probabilities:\n\n\n\n**P*(column | row)**\n<s>\nI\nam\nSam\n\n\n\n\n<s>\n0.125\n0.375\n0.125\n0.375\n\n\nI\n0.125\n0.125\n0.625\n0.125\n\n\nam\n0.375\n0.125\n0.125\n0.375\n\n\nSam\n0.125\n0.375\n0.375\n0.125\n\n\n\nNow any combination of the words is possible, even the empty sentence <s> <s> (with probability 12.5%).\nIf we move to a higher order model such as a trigram model we need to add more padding tokens at the start of the sentence, which again should all be the same. For example with our corpus the counts would look like:\n<s> <s> I am Sam <s>\n<s> <s> Sam I am <s>\n\n\n\nCounts\n<s>\nI\nam\nSam\n\n\n\n\n<s> <s>\n0\n1\n0\n1\n\n\n<s> I\n0\n0\n1\n0\n\n\n<s> Sam\n0\n1\n0\n0\n\n\nI am\n1\n0\n0\n1\n\n\nam Sam\n1\n0\n0\n0\n\n\nSam I\n0\n0\n1\n0\n\n\n\nNote that we don’t want an additional padding token at the end, because given any token followed by <s> the next token must be constrained to be <s>.\nThis is one of those little tricks in how you frame a problem that makes the calculations much easier in practice."
  },
  {
    "objectID": "nbdev/index.html",
    "href": "nbdev/index.html",
    "title": "Getting Started with nbdev",
    "section": "",
    "text": "Nbdev is a tool to make it possible to develop Python libraries in Jupyter notebooks. At first I found this idea scary, but after watching the talk I like Notebooks and seeing how it works I think it’s got the best of all worlds.\nIt lets you put code, documentation, examples and tests all together in context and provides tooling to extract the code into an installable library, run the tests and produce great hyperlinked documentation. In this sense it’s reminiscent of Knuth’s WEB System for literate programming, in which you would write the text and code together and there was a command to TANGLE compilable code or WEAVE TeX documentation from the source. However it’s much richer than WEB because Jupyter notebooks provide an interactive live coding environment, and can embed HTML, images and other rich media. It’s natural in Jupyter notebooks to see what happens when you run a piece of code, and with nbdev this then becomes an example in the documentation and optionally a test you can run.\nThere are lots of rough edges with making Jupyter notebooks reproducible and easy to diff, like the order of execution and numbers in cells. Nbdev provides tools that make all this easy, and allows you to sync code two ways between notebooks and python source files. This means you can make code changes in an IDE and then sync it back to the notebooks. However there are still some limitations here (especially being consistent with the documentations) and I suspect a large refactor would be considerably harder."
  },
  {
    "objectID": "nbdev/index.html#settings.ini",
    "href": "nbdev/index.html#settings.ini",
    "title": "Getting Started with nbdev",
    "section": "settings.ini",
    "text": "settings.ini\nThere’s a settings.ini file that contains fields used for generating the documentation, setup.py and some other things. In the template there are a bunch of fields commented out and it’s not immediately obvious which ones you need to fill in, and what you should put there.\nThe first time I ran nbdev_build_lib to tangle the code from the notebooks I got KeyError: 'copyright'. I then uncommented and updated the copyright setting to copyright = 2021 Edward Ross, only to see in the documentation:\n\nCopyright: 2021 Edward Ross 2021\n\nClearly I should have just set the field as copyright = Edward Ross to get the correct output\n\nCopyright: Edward Ross 2021\n\nThen when I tried to install the package from the generated setup.py with pip install . I got this stack trace\n    ERROR: Command errored out with exit status 1:\n    ...\n\n    AssertionError: missing expected setting: keywords\n    ----------------------------------------\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\nApparently I did need to set the keywords field (I had no idea what to put there, I thought it was just for publishing on pypi) and a description.\nWith these fields set it worked, but it would have been easier if I had more guidance over what these fields should be and which ones were required."
  },
  {
    "objectID": "shell-etl/index.html",
    "href": "shell-etl/index.html",
    "title": "Data Transformations in the Shell",
    "section": "",
    "text": "I’ll show how to get the most frequent commands from your bash history and their frequency. But the same types of techniques are useful for tallying many types of files, and once you get practiced they’re really quick to write. The final output will look like this:\n> cut -d' ' -f1 ~/.bash_history | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed -e 's/^ *//' -e 's/ /,/g' | \\\n  awk -F',' -v total=\"$total\" \\\n  'BEGIN {OFS=\",\"} \n  {cumsum+=$1; print $2, $1, $1/total * 100 \"%\", cumsum/total * 100 \"%\"}' | \\\n  head -n5 | \\\n  column -ts,\n\nls   2470  20.7703%  20.7703%\ncd   1509  12.6892%  33.4595%\ngit  794   6.67676%  40.1362%\nvim  468   3.93542%  44.0716%\naws  464   3.90178%  47.9734%\n\nBuilding the pipeline\nSuppose that you want to know the most frequent commands that you type at the shell. If you use bash your recent commands will be stored in ~/.bash_history (and you can configure it to store all your history). You can glimpse the most recent commands with tail which prints the first few rows (similar to limit in most SQL engines).\n> tail -n5 ~/.bash_history\nls\nls ~/tmp\nrm ~/tmp/*\nls ~/tmp\ndf -h\nLet’s try to extract the commands and not the arguments; we use cut a little like select in SQL, but we have to specify how the columns are separated. We set the delimiter (-d) to a ’ ’ and the fields to select is the first field -f1. Note that in bash you can specify a tab separated file with -d$'\\t'.\n> cut -d' ' -f1 ~/.bash_history | tail -n5\nls\nls\nrm\nls\ndf\nNow we want to count the commands; we can use uniq -c which counts repeated lines\n> cut -d' ' -f1 ~/.bash_history | tail -n5 | uniq -c\n      2 ls\n      1 rm\n      1 ls\n      1 df\nThis is great for counting streaks, but to count the command we need to put repeated instances of a command together with sort:\n> cut -d' ' -f1 ~/.bash_history | tail -n5 | sort | uniq -c\n      1 df\n      3 ls\n      1 rm\nFinally we want to put them in order by sorting them in reverse (-r) numerical (-n) order:\n> cut -d' ' -f1 ~/.bash_history | tail -n5 | sort | uniq -c | sort -nr\n      3 ls\n      1 df\n      1 rm\nLet’s use a bit of sed magic to make the formatting a nice CSV that we could use in another tool (assuming there are no commas in the data itself!) Also we’ll start breaking the lines to make it more readable.\n> cut -d' ' -f1 ~/.bash_history | \\\n  tail -n5 | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed -e 's/^ *//' -e 's/ /,/g'\n\n3,ls\n1,df\n1,rm\nWe can now ://superuser.com/questions/137438/how-to-unlimited-bash-shell-historyltake the tail out of the pipe and see our most frequent commands using head to only get the top ones:\n> cut -d' ' -f1 ~/.bash_history | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed -e 's/^ *//' -e 's/ /,/g' | \\\n  head -n5\n\n2470,ls\n1509,cd\n794,git\n468,vim\n464,aws\nClearly I spend a bit of time navigating directories, git repositories and using AWS. But how much of my whole shell activity does this represent? We can get the total lines counting lines with the word count utility: wc -l (the count might change under us but it doesn’t matter too much). Note that wc prints out the name of the file as well so to just get the number we’ll have to select it with cut.\n> wc -l ~/.bash_history\n11890 ~/.bash_history\n> export total=$(wc -l ~/.bash_history | cut -d' ' -f1); echo $total\n11891\nWe can now use awk to calculate the percentage of time we spend on each command, and finally use column to make the output aligned for reading.\n> cut -d' ' -f1 ~/.bash_history | \\\n  sort | \\\n  uniq -c | \\\n  sort -nr | \\\n  sed -e 's/^ *//' -e 's/ /,/g' | \\\n  awk -F',' -v total=\"$total\" \\\n  'BEGIN {OFS=\",\"}\n  {cumsum+=$1; print $2, $1, $1/total * 100 \"%\", cumsum/total * 100 \"%\"}' | \\\n  head -n5 | \\\n  column -ts,\n\nls   2470  20.7703%  20.7703%\ncd   1509  12.6892%  33.4595%\ngit  794   6.67676%  40.1362%\nvim  468   3.93542%  44.0716%\naws  464   3.90178%  47.9734%\nThe output is the command, number of times types, percentage of all commands typed and cumulative percentage of all commands typed. So I can see that these 5 are nearly half of all the commands I type!\nThis only scratches the surface of what’s possible; you can filter rows using grep (analogous to a where clause in SQL) and combine these in novel ways. For more complex (or robust!) transformations and aggregations it’s worthwhile moving to more featureful languages like Python/R/SQL, but being able to quickly generate summaries is a handy tool - especially when something breaks!"
  },
  {
    "objectID": "seq-weak/index.html",
    "href": "seq-weak/index.html",
    "title": "Sequential Weak Labelling for NER",
    "section": "",
    "text": "There is another approach in the middle between: weak labelling. This is where you take a bunch of sources of labels that are individually not great, such as heuristic rules, or maybe other models and datasets. Then you aggregate the information together in an unsupervised manner to get a new labeller. This approach lets you apply domain knowledge in a more scalable way than labelling individual examples, and lets you combine multiple sources of information. It’s also effective in practice and part of the open source Snorkel tool.\nSnorkel works for classification problems, but in sequential modelling there’s a lot of additional useful constraints. For example you can’t follow inside and org to inside a person. This is the problem solved in Named Entity Recognition without Labelled Data: A Weak Supervision Approach. They use a Hidden Markov Model with one emission per labelling function, estimating the parameters in an unsupervised manner.\nIn particular they combined 50 labelling functions including out-of-domain NER models, Gazetteers, heuristic function and document level relations. The latter includes constraints like “an entity with the same label is likely to have the same type throughout the document”, or a long form of an entity (like Kathleen McKeown) is typically followed by a shortened form in the rest of the document (like McKeown). These were very effective on the news datasets they tested on.\nThey used a Dirchlet distribution with an informative prior (which is useful on smaller datasets) for the HMM. They tried simpler distributions and priors and found these to be useful. This was then used to train a typical neural sequence tagger.\nThe result was very effective; it works better than AdaptaBERT and is much simpler and quicker to train.\nThis seems like a really effective technique; using domain knowledge to quickly bootstrap a good NER. The source code is available."
  },
  {
    "objectID": "jupyter-as-logs/index.html",
    "href": "jupyter-as-logs/index.html",
    "title": "Jupyter Notebooks as Logs for Batch Processes",
    "section": "",
    "text": "Jupyter Notebooks allow you to write your code sequentially as you usually would in a batch script; importing libraries, running functions and having assertions. But additionally the outputs can be displayed directly in the notebook, without needing to resort to separate log statements. Moreover the outputs can be more than text; they can be images (such as graphs) or rich HTML markup or even interactive elements. When you run into an issue you can just interactively step through the notebook, which gives you a much richer experience than pdb.\nYou want the notebook to be like a script and not contain the output, so before committing it to source control you want to clear all the outputs (and may want to set up a git hook for this):\njupyter nbconvert --inplace --clear-output batch_script.ipynb\nThen using nbconvert you can execute the notebook, and render it as a HTML logfile. For example this runs the notebook, removes the prompts (the [1], …) and outputs it to a logfile with the UTC run date.\njupyter nbconvert batch_script.ipynb --to html --execute \\\n  --output logs/`date --utc +%Y%m%dT%H%M%SZ`_batch_script.html \\\n  --no-prompt\nOf course sometimes you’ll want logging statements from the libraries you import, and you can display them in your Jupyter notebooks:\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nIf you want to go further and parameterise your scripts you could read in a config file in the notebook, or use something like Papermill to pass the parameters."
  },
  {
    "objectID": "git-repo-activity/index.html",
    "href": "git-repo-activity/index.html",
    "title": "Heuristics for Active Open Source Project",
    "section": "",
    "text": "For example I was recently looking at TextBlob for making plural words singular. It doesn’t work on already singular words; for example “gas”, “bus” or “analysis” have their final “s” removed, although this is not documented. Here’s the Contributors graphs from the insights pane of Github;\n\n\n\nText Blob Contributions\n\n\nAlmost all the contributions have been made by a single person, and there are not many commits in the last 12 months. It’s unlikely this will get fixed by itself. Searching the issues (both open and closed) shows this problem is an open issue with no responses. Looking at the open pull requests there are a few reasonable looking requests that have no comments. From the closed pull requests the primary author has been doing basic maintenance work but no enhancements. If I submitted a change for this complex task there’s a low chance it would be included into the library.\nNone of this is a criticism of TextBlob or its author. I’m just pragmatically assessing what my experience will be like if I adopt this library in my code base or try to contribute to it. It is a very useful library and will continue functioning, but is unlikely to change substantially and I will need to take it as it is. Maintaining an open source project is an extremely time consuming and thankless task, and the sole maintainer probably has other priorities. Rich Hickey’s Open Source is Not About You talks about what it can be like running a popular open source project.\nTextBlob’s singular is based on the inflect library’s singularize_noun function, which is documented to only work on singular nouns. There’s an open issue about it, but as in the thread it is working as documented. Unlike TextBlob which is a general purpose tool, inflect is a very focused library that just doe one thing, inflect English words. As such it wouldn’t need much maintenance to be useful once it is stable, since English doesn’t change much.\n\n\n\nInflect Contribution\n\n\nInflect only one primary contributor, but they are making regular contributions, responding to issues and pull requests and the scope of the project is rather small. People are adding new grammatical exceptions. This seems like a good library to rely on or contribute to.\nFor a different example look at SpaCy. This is part of explosion.ai which provide SpaCy as an open source package of easy to use NLP tools, and monetise on the annotation tool prodigy which has great contribution. Because of this they have more significant resources and have 3 people very actively contributing to it.\n\n\n\nSpaCy Contribution\n\n\nNLTK provides a very broad range of NLP tools. It’s supported through an academic community, but it has two very active contributors and keeps growing. I would definitely build applications on NLTK.\n\n\n\nNLTK Contribution\n\n\nThese are just heuristics, that while useful have to be taken with a grain of salt. A project that changes too quickly, especially changing the API, can be a burden to integrate and upgrade. An extremely mature and bug free project like TeX is almost completely unchanging, but it is so stable you can easily build on it, which ConTeXt and LaTeX did.\nI also wouldn’t rely too much on them because they are easily rigged. For example someone tried to take over PhantomJS by making spurious commits like adding and removing the same file, reformatting. There’s also the h-index and other author level metrics in academia that encourage people to reference their own papers, or actively build communities that cross-reference each other, just to raise their academic standing.\nHowever for quickly getting an idea for how fast a project is moving and responding to change, before doing a more thorough investigation, looking at the number of active contributors and how frequently they have contributed recently can be useful. This isn’t specific to github either, you could easily build something to do this from any version control history (but it’s very convenient having it easily accessible in the browser)."
  },
  {
    "objectID": "test-and-roll/index.html",
    "href": "test-and-roll/index.html",
    "title": "More Profitable A/B with Test and Roll",
    "section": "",
    "text": "Test & Roll: Profit-Maximizing A/B Tests by Elea McDonnell Feit and Ron Berman tackles this problem giving estimates for small samples. In the context of a one-off opportunity (like advertising, promotional offers, campaign launches, products that will only be bought once) we actually don’t care if one version is significantly better than the other; all we care about is maximising our outcome. Standard practice is to run an A/B test to determine which alternative is better, and then roll it out to 100%. There’s a tradeoff in how long we explore to find the optimal alternative, and how long we have left to exploit this opportunity.\nThey tackle this problem with Bayesian Decision Theory. They run both A and B in parallel on a limited sample, until we get enough evidence to just pick one. But here enough evidence means “good enough” to run with; if they’re very similar then it doesn’t matter too much and instead of running a high fidelity statistical test to get evidence on which is better, just pick one. In particular they look to maximise the expected (average) return, which makes sense if you’re running lots of these processes. If you have a rough idea of how the data is distributed and the size of the difference between them (something you need for a classical statistical test), you can work out the optimal sample size for your A/B test, before rolling it out to 100%.\nIn particular they assume that the returns are normally distributed. In my experience if there are opportunities to make multiple purchases the returns tend to be long tailed; there’s a few big spenders and a lot of low spenders. But it’s a fine approximation for one-off purchases and conversions (since binomials can be approximated by normal distribution), and the framework can be extended to this case. They define variables for a symmetric A/B test: \\(Y_{A} \\sim \\mathcal{N}(m_A, s^2)\\), \\(Y_{B} \\sim \\mathcal{N}(m_B, s^2)\\) where \\(m_{A}, m_{B} \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). So in the case of a conversion rate:\n\n\\(Y_A\\) and \\(Y_B\\) represent the results we get from alternative A and B respectively\n\\(m_A\\) and \\(m_B\\) are the true conversion rates of each alternative\n\\(\\mu\\) is the expected conversion rate\ns is the standard deviation of the data; it’s approximately \\(\\sqrt{\\mu(1-\\mu)}\\)\n\\(\\sigma\\) is the expected variation in the conversions between the conversion rate\n\nThe hardest of these parameters to understand is \\(\\sigma\\); one way to understand it is the expected difference of \\(\\vert m_A - m_B \\vert\\) is \\(\\frac{2}{\\sqrt{\\pi}} \\sigma\\) (note in the paper the square root in the numerator is a typo!). So if we expect the difference between A and B to be about 2 percentage points, then we should set \\(\\sigma\\) as \\(0.02 \\frac{\\sqrt{\\pi}}{2} \\approx 0.018\\). In general \\(\\sigma\\) is around 89% of the mean difference, and 95% of the median difference, so it’s pretty close to the expected effect size.\nUnder these conditions they find the optimum sample size for each of the A and B groups are:\n\\[\\sqrt{\\frac{N}{4} \\left(\\frac{s}{\\sigma}\\right)^2 + \\left(\\frac{3}{4} \\left(\\frac{s}{\\sigma}\\right)^2\\right)^2} - \\frac{3}{4} \\left(\\frac{s}{\\sigma}\\right)^2\\]\nIt’s interesting to note the key variable is the ratio \\(\\left(\\frac{s}{\\sigma}\\right)^2\\). The smaller our effect size the larger the sample we need, the smaller the variability in the data the smaller sample size we need. In our conversion example above this ratio is around 600.\nIn the limit of very large populations, large effect sizes and small variation \\(N \\gg \\left(\\frac{s}{\\sigma}\\right)^2\\), the sample size approaches \\(\\sqrt{N} \\frac{s}{2 \\sigma}\\) from below. In the limit of very small populations, small effect sizes and large variation \\(N \\ll \\left(\\frac{s}{\\sigma}\\right)^2\\) it approaches \\(N/6\\). Finally for \\(N = \\left(\\frac{s}{\\sigma}\\right)^2\\) the optimal sample size is \\(\\frac{\\sqrt{13} - 3}{4} N \\approx 0.15 N\\).\nSo for small and moderate population sizes (relative to \\(\\left(\\frac{s}{\\sigma}\\right)^2\\) ) the optimum process is to run both A and B on about 1/6 of the population each, and then run the winner on the remaining 2/3 of the population. As the population increases, effect size increases, or variation decreases the size of the A/B tests decreases, and in the limit scales as \\(\\sqrt{N}\\), and so we can run the test on a small sample.\nThe upshot of all this is on average we capture some percentage of the expected improvement. Indeed the expected profit with this optimum sample sizes in each group A and B, n* is\n\\[N \\left(\\mu + \\frac{2}{\\sqrt{\\pi}} \\sigma \\left(1 - \\frac{2n^*}{N} \\right) \\frac{1}{\\sqrt{2 + \\frac{4}{n^*} \\left( \\frac{s}{\\sigma}\\right)^2}}  \\right)\\]\nAs explained the term \\(\\frac{2}{\\sqrt{\\pi}} \\sigma\\) is the optimum expected gain. This is then attenuated by the exploration cost \\(\\left(1 - \\frac{2n^*}{N} \\right)\\), which is the percentage of cases not in the test phase. Finally it’s attenuated by the expected cost of picking the wrong alternative due to chance, \\(\\frac{1}{\\sqrt{2 + \\frac{4}{n^*} \\left( \\frac{s}{\\sigma}\\right)^2}}\\). For very low populations or small effect sizes relative to the variation in the data, the gain over random choice \\(N \\mu\\) is small. But as the effect size increases and populations increase this procedure captures more of the gains.\nThere’s a good blog post from co-author Ron Berman that explains this further, and an online sample size calculator. In the paper they compare it to a Multi-Armed Bandit with a Thompson sampler, which does better, but is much more complex to implement and control. I really like the framework of Bayesian Decision analysis; it seems flexible enough that a different set of assumptions or goals could be easily implemented.\nMy biggest concern with this procedure is choosing priors, and the effect of choosing bad ones. If your prior deviation is too small you’ll test longer and gain less; there may be worse effects if you pick the wrong distribution. Because we’re not performing a whole test we also will never really know if we’ve chosen the better alternative, and so it’s hard to incorporate this feedback into updating priors. It’s going to be really hard to know if we’re optimising correctly.\nAnother consideration is what to do with a situation where there are repeat purchases on a long-lived website. The test-and-roll framework doesn’t really apply, but there are still opportunity costs with testing and rolling out. Chris Said has a blog series which comes to similar conclusions but frames it in terms of the time opportunity cost - I look forward to reading this more.\nI’m also curious what happens if we’ve got multiple alternatives. There are a few scenarios where we could have many similar effect size alternatives, or quite risky alternatives, and I wonder what the best procedure is that maximises expected return. Also for uncommon opportunities we may be more conservative and consider the whole distribution; maybe we maximise the chance of hitting some revenue target, or maximising a percentile of returns. These should be straightforward extensions of the theory, and could be evaluated numerically through simulations."
  },
  {
    "objectID": "kaggle-project-structure/index.html",
    "href": "kaggle-project-structure/index.html",
    "title": "Structuring a Project Like a Kaggle Competition",
    "section": "",
    "text": "The modelling part of analytics projects will go smoothly only if you have clear evaluation criteria. There are methodologies like CRISP-DM or Jeremy Howard’s Data Project Checklist for running a successful project. Modelling and analytics is only one (often small) step in the process, but it’s what defines it as an analytics project. It’s easy to spend a lot of time digging into the data and trying different approaches, but if you don’t have a clear evaluation criteria it’s really hard to compare them.\nAn imperfect evaluation criteria is often better than none. Your evaluation criteria should align with the business goals, but will often have to be a proxy for the real thing. Because of this getting the absolutely best score is often not worthwhile; there will be diminishing returns as models get more complex. But it gives you a guidepost and helps identify whether something is working at all. Ideally you have a simple evaluation metric that you can communicate back to stakeholders to demonstrate you’re actually doing something. Try to come up with just one primary metric, with at most a couple of guardrail metrics (i.e. constraints), otherwise you can spend a lot of time deciding between tradeoffs.\nOnce you’ve got an evaluation criteria, and created a good training/development/test set data split, then it’s easy to compare solutions. You should start with a very simple model as a baseline and then build on that. While it can be fun trying to build bigger better models, if you’re developing a new analytics product or service try to get to “good enough”. Often picking a small subset of the possible data (say a particular region or time period) will make it much faster to prove an approach. If you can prove it’s viable in the field then you can come back and expand and improve it.\nIt’s easy to get stuck with a mediocre implementation if you don’t set a clear structure. If you start with a solution you’ll start building your evaluation around the structure of your solution. This makes it harder to test alternative solutions later on, and the evaluation may even get stuck to implementation details.\nWhen the solution is in production you can then evaluate your metrics online and reconcile them with your offline test metrics (which may be different if you inadvertently had a data leak!) Then it can be refined and optimised over time as it becomes more obvious what’s important in production. But you can keep the same project structure for future improvement cycles.\nThis technique won’t work for all problems, and it’s not always easy to come up with an offline evaluation metric and set. But when you can using this structure will make your modelling iterate faster and set up success for monitoring and improvement if the solution gets adopted."
  },
  {
    "objectID": "dont-stop-pretraining/index.html",
    "href": "dont-stop-pretraining/index.html",
    "title": "Don’t Stop Pretraining",
    "section": "",
    "text": "Take the Hyperpartisan News Detection task from SemEval 19; given a News Article text determine whether it is partisan. One approach is to train a classifier on a modern transformer such as RoBERTa. But maybe performing domain adaptation by further training the language model on a large amount of text from the news domain, such as the realnews dataset, before training the classifier will improve results. Or we could perform task adaptation by further training the language model on the Hyperpartisan News dataset before training the classifier could improve results.\nIn Don’t Stop Pretraining they pick 8 classification tasks from 4 different domains; News, Reviews, Biomedical and Computer Science. They show in each case that performing domain adaptation (DAPT) for RoBERTa gives a better classifier (and adapting to an irrelevant domain gives a worse classifier). They also show that task adaptation (TAPT) also generally results in a better classifier. And the best results consistently come from performing DAPT and then TAPT. See below for the F1 scores (with standard deviation in subscript) for each strategy.\n\n\n\nResults on different phases of adaptive pretraining\n\n\nThis is a very clean paper; they’ve carefully shown in many different cases fine tuning a language model on data closer to the target classification task improves performance. The result is not very surprising but they’ve amassed enough evidence to demonstrate this strategy works consistently and can contribute significant improvements over baselines. They’ve also documented the hyperparameters and training strategy and released the code; hopefully this means its relatively straightforward to implement. They comment that the experiments were performed on a single Google Cloud v3-8 TPU; so I would expect this kind of adapatation could be performed for several hundred dollars."
  },
  {
    "objectID": "stanza/index.html",
    "href": "stanza/index.html",
    "title": "Stanza for NLP",
    "section": "",
    "text": "It’s quite easy to use:\nimport stanza\nstanza.download('en')\nnlp = stanza.Pipeline('en')\ndoc = nlp('Edward thinks Stanza is really neat in July 2020')\nprint(doc.entities)\nWhich prints out the detected entities\n[\n{\"text\": \"Edward\", \"type\": \"PERSON\", \"start_char\": 0, \"end_char\": 6},\n{\"text\": \"Stanza\", \"type\": \"PERSON\", \"start_char\": 14, \"end_char\": 20},\n{\"text\": \"July 2020\", \"type\": \"DATE\", \"start_char\": 39, \"end_char\": 48}\n]\nWe can look at individual tokens; e.g. doc.sentences[0].tokens[1]. It gives not only the part of speech, lemma and dependency relation, but also a list of features from Universal Features. The word “likes” is singular form, 3rd person, present tense, indicative mood and a finite verb form. My linguistics isn’t good enough to understand all of this, but I don’t know another way to determine singular versus plural form so it seems useful to me.\n[\n  {\n    \"id\": \"2\",\n    \"text\": \"thinks\",\n    \"lemma\": \"think\",\n    \"upos\": \"VERB\",\n    \"xpos\": \"VBZ\",\n    \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n    \"head\": 0,\n    \"deprel\": \"root\",\n    \"misc\": \"start_char=7|end_char=13\"\n  }\n]\nStanza was introduced in an ACL 2020 demo. It’s a production version of the neural pipeline described in Universal Dependency Parsing from Scratch. I would like to dig more into this later, including Deep Biaffine Attention for Neural Dependency Parsing.\nMore traditional pipelines such as spaCy 2 use rules for tokenization and lemmatisation. By using neural networks it means to extend Stanza to another language you just need enough annotated data; which are easier to obtain than linguistic expertise to craft rules. In practice there’s now enough data and good enough techniques that neural methods are more accurate than rules. The main drawback is that the rules are slower to train and run. According to their benchmarks going from raw text to dependencies is about 10x slower than spaCy (but only 3x on a GPU).\nThe NER algorithm is based on Contextual String Embeddings for Sequence Labeling used in flair. The NER performance on benchmarks is similar to flair, but it runs much faster. I’m really interested in the idea of using it to train custom NER models.\nIt’s also straightforward to use in spaCy with spaCy-Stanza by wrapping it in a StanzaLanguage. This means it can be used with spaCy’s matcher and displacy tools for convenience.\nI still need to experiment with it more but it looks like a very promising library for NLP."
  },
  {
    "objectID": "south-sea-bubble/index.html",
    "href": "south-sea-bubble/index.html",
    "title": "South Sea Bubble",
    "section": "",
    "text": "The chapters on the South Sea Bubble and the following craze over investing in South America sound thoroughly modern; except they happened 200-300 years ago. In fact Isaac Newton lost £20,000 by investing in the bubble. Chancellor describes futures, options, and margin loans - things I had wrongly assumed were more modern inventions.\nA lot of the businesses he describes as ridiculous sound reminiscent of modern tech businesses. Companies to look for treasure on sunken ships (after a major lucky success of this kind), ways to invest in the new emerging South American market purportedly filled with unimaginable quantities of gold (on sketchy information). This makes me think of the current AI boom and preceding booms in Bitcoin, big data and the early internet where there was real progress followed by a bunch of sketchy “businesses” jumping on the bandwagon. I’m sure it’s the same in other industries too, but emerging markets and technologies are often rife for speculation.\nHe even talks about the London Umbrella Company in the early 19th century that set up “umbrella stations” from which umbrellas could be rented for a small charge. This reminds me of the modern bike and scooter rental businesses, now run via GPS and phone applications.\nOf course it’s really hard to tell in advance what is ridiculous and what is revolutionary. I wouldn’t have guessed that a better web search algorithm would be worth billions of dollars, but Google has effectively monetised the attention and information they acquired by selling targeted advertising. But for every Facebook, Google, or Amazon there are a thousand failed companies and it would have been really hard to distinguish them in advance.\nThe history of businesses and finance runs very long, and I really wonder how much we could learn today by studying the past. Certainly it’s easy to get caught up in the swells of unsound investments, like those that led to the 2007-08 Global Financial Crisis, if you follow the crowds without looking at the fundamentals. Picking “winning” stocks is ridiculously hard; managed funds can’t beat the market consistently.\nI started reading this as part of William J Bernstein’s excellent If You Can curriculum, under the hurdle those who ignore financial history are doomed to repeat it. I highly recommend this reading the booklet and the books he recommends inside; it’s a great practical financial education."
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html",
    "title": "skeptric",
    "section": "",
    "text": "Prototype methods classify objects by finding their proximity to a prototype in the feature space. These methods are flexible, and can be locally interpretable by looking at nearby examples.\nWe are going to look at examples of prototype methods on Fashion MNIST, a set of 28x28 pixel black and white images of different items of clothing. You could use any kind of features but these images are so simple that we can use the pixels themselves as features.\nThis notebook trains a classifier that gets about 85% accuracy on this dataset using K nearest neighbours, but on the way explores examining data, average prototypes, predicting accuracy with training data size, and approximate nearest neighbours methods."
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html#data-format",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html#data-format",
    "title": "skeptric",
    "section": "Data format",
    "text": "Data format\nWe can read out the label and image of any pixel\n\nidx = 0\n\nlabel = df.iloc[idx, 0]\nimage = df.iloc[idx, 1:].to_numpy().reshape(28, 28)\nprint(label)\nax = plt.imshow(image, cmap=\"Greys\")\n\n2\n\n\n\n\n\nEach image is a series of digits from 0 to 255. Here’s the top left corner.\n\ndata = image[:12, :12]\ndata\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   4,   0,   0,   0,   0,   0,  62,  61],\n       [  0,   0,   0,   0,   0,   0,   0,  88, 201, 228, 225, 255],\n       [  0,   0,   0,   0,   0,  47, 252, 234, 238, 224, 215, 215],\n       [  0,   0,   1,   0,   0, 214, 222, 210, 213, 224, 225, 217],\n       [  1,   0,   0,   0, 128, 237, 207, 224, 224, 207, 216, 214],\n       [  0,   2,   0,   0, 237, 222, 215, 207, 210, 212, 213, 206],\n       [  0,   4,   0,  85, 228, 210, 218, 200, 211, 208, 203, 215],\n       [  0,   0,   0, 217, 224, 215, 206, 205, 204, 217, 230, 222],\n       [  1,   0,  21, 225, 212, 212, 203, 211, 225, 193, 139, 136]])\n\n\nEach number maps the corresponding pixel to a color; the larger the number the more black to use.\n\ndef show_data(data, figsize=(10,10), ax=None, formatter='{:}'.format):\n    assert data.ndim == 2\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    im = ax.imshow(data, cmap=\"Greys\")\n\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            text = ax.text(j, i, formatter(data[i, j].item()), ha=\"center\", va=\"center\", color=\"magenta\")\n    return ax\n\nax = show_data(data)\n\n\n\n\nIt’s more convenient to use floating point numbers. Renormalise between 0 and 1 and convert it to a pytorch tensor.\n\nimage_tensor = image / 255.\n\ndata_tensor = image_tensor[:12, :12]\ndata_tensor\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.01568627,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.24313725, 0.23921569],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.34509804, 0.78823529, 0.89411765,\n        0.88235294, 1.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.18431373, 0.98823529, 0.91764706, 0.93333333, 0.87843137,\n        0.84313725, 0.84313725],\n       [0.        , 0.        , 0.00392157, 0.        , 0.        ,\n        0.83921569, 0.87058824, 0.82352941, 0.83529412, 0.87843137,\n        0.88235294, 0.85098039],\n       [0.00392157, 0.        , 0.        , 0.        , 0.50196078,\n        0.92941176, 0.81176471, 0.87843137, 0.87843137, 0.81176471,\n        0.84705882, 0.83921569],\n       [0.        , 0.00784314, 0.        , 0.        , 0.92941176,\n        0.87058824, 0.84313725, 0.81176471, 0.82352941, 0.83137255,\n        0.83529412, 0.80784314],\n       [0.        , 0.01568627, 0.        , 0.33333333, 0.89411765,\n        0.82352941, 0.85490196, 0.78431373, 0.82745098, 0.81568627,\n        0.79607843, 0.84313725],\n       [0.        , 0.        , 0.        , 0.85098039, 0.87843137,\n        0.84313725, 0.80784314, 0.80392157, 0.8       , 0.85098039,\n        0.90196078, 0.87058824],\n       [0.00392157, 0.        , 0.08235294, 0.88235294, 0.83137255,\n        0.83137255, 0.79607843, 0.82745098, 0.88235294, 0.75686275,\n        0.54509804, 0.53333333]])\n\n\nIt still looks the same but the numbers are scaled down\n\nax = show_data(data_tensor, formatter='{:0.3f}'.format)\n\n\n\n\nHow does imshow know how dark to make the cells?\nBy default it makex the smallest value whitest and the largest black. From the documentation this can be controlled with vmin and vmax\n\nvmin, vmax: float, optional\n\n\nWhen using scalar data and no explicit norm, vmin and vmax define the data range that the colormap covers. By default, the colormap covers the complete value range of the supplied data. It is an error to use vmin/vmax when norm is given. When using RGB(A) data, parameters vmin/vmax are ignored.\n\nSo if we double vmax the image appears fainter.\n\nplt.imshow(image_tensor, cmap='Greys', vmin=0., vmax=2.)\n\n<matplotlib.image.AxesImage at 0x7f6761405cd0>"
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html#convert-data",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html#convert-data",
    "title": "skeptric",
    "section": "Convert data",
    "text": "Convert data\nLet’s put all the data into a large normalised array\n\nimages = df.filter(regex='^pixel[0-9]+$', axis=1).to_numpy().reshape((-1, 28, 28)) / 255.\n\nimages_test = df_test.filter(regex='^pixel[0-9]+$', axis=1).to_numpy().reshape((-1, 28, 28)) / 255.\n\n\nimages.shape\n\n(60000, 28, 28)\n\n\n\n_ = plt.imshow(images[0], cmap='Greys')\n\n\n\n\nSo we understand the data let’s use human readable labels. This helps us understand how to classify the data and understand if we’ve made any mistakes. These are copied from the documentation for Fashion MNIST.\n\nlabels_txt = \"\"\"\nLabel   Description\n0   T-shirt/top\n1   Trouser\n2   Pullover\n3   Dress\n4   Coat\n5   Sandal\n6   Shirt\n7   Sneaker\n8   Bag\n9   Ankle boot\n\"\"\".strip()\n\n\nfrom io import StringIO\n\ndf_labels = pd.read_csv(StringIO(labels_txt), sep='\\t').set_index('Label')['Description']\n\nWe can then use this to convert the numeric labels to categories. So we can still access a numeric representation we use Pandas categorical dtype.\n\ncat_type = pd.CategoricalDtype(categories=df_labels)\n\nlabels = df['label'].map(df_labels).astype(cat_type)\nlabels_test = df_test['label'].map(df_labels).astype(cat_type)\n\nlabels\n\n0           Pullover\n1         Ankle boot\n2              Shirt\n3        T-shirt/top\n4              Dress\n            ...     \n59995     Ankle boot\n59996        Trouser\n59997            Bag\n59998            Bag\n59999        Sneaker\nName: label, Length: 60000, dtype: category\nCategories (10, object): ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', ..., 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\n\nLabels are same as cateogries\n\nassert (df['label'] == labels.cat.codes).all()"
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html#examining-data",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html#examining-data",
    "title": "skeptric",
    "section": "Examining data",
    "text": "Examining data\nWe have 6000 of each image in labels\n\nlabels.value_counts()\n\nT-shirt/top    6000\nTrouser        6000\nPullover       6000\nDress          6000\nCoat           6000\nSandal         6000\nShirt          6000\nSneaker        6000\nBag            6000\nAnkle boot     6000\nName: label, dtype: int64\n\n\nThe test set contains 1000 of each image\n\nlabels_test.value_counts()\n\nT-shirt/top    1000\nTrouser        1000\nPullover       1000\nDress          1000\nCoat           1000\nSandal         1000\nShirt          1000\nSneaker        1000\nBag            1000\nAnkle boot     1000\nName: label, dtype: int64\n\n\nLet’s look at some example images from the data with their labels.\n\ndef show_images(data, labels=None, nrows=5, ncols=10, figsize=(16,8), indices=None):\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n    \n    if indices is None:\n        indices=np.random.choice(len(data), size=nrows*ncols, replace=False)\n    \n    for i, ax in enumerate(axs.ravel()):\n        idx = indices[i]\n        im = ax.imshow(data[idx], cmap=\"Greys\", norm=matplotlib.colors.Normalize(0., 1.))\n        \n        if labels is not None:\n            ax.set_title(labels[idx], pad=0.0)\n        \n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    return fig, axs\n\n    \nfig, ax = show_images(images, labels)\n\n\n\n\nLooking through the categories we can see things like:\n\nImages are mostly centred, aligned, and cropped\nTrousers are distinctive from their shape\nShirts are particularly hard to distinguish from tshirt, jacket, pullover and dresshttps://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes\n\n\nfor label in labels.cat.categories:\n    mask = labels == label\n    show_images(images[mask], labels[mask].reset_index(drop=True))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s also have a look at the test data to make sure it’s from the same distribution.\n\nfor label in labels_test.cat.categories:\n    mask = labels_test == label\n    show_images(images_test[mask], labels_test[mask].reset_index(drop=True), nrows=1, ncols=10, figsize=(16,2))"
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html#duplicate-detection",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html#duplicate-detection",
    "title": "skeptric",
    "section": "Duplicate detection",
    "text": "Duplicate detection\nSurprisingly often many duplicates can end up in the training data, or worse leak between the training and test data. It’s always worth doing a check for duplicates.\nThere a small amount of duplicated in the training set. We could drop them but they’re not too concerning here.\n\nn_train_dup = df.filter(regex='pixel').duplicated().sum()\nn_train_dup\n\n43\n\n\nThere’s one duplicate in the test set\n\nn_test_dup = df_test.filter(regex='pixel').duplicated().sum()\nn_test_dup\n\n1\n\n\nWe also see there are 10 images in the test set from the training set. This amount of leakage is quite small so we won’t worry about it.\n\npd.concat([df, df_test], axis=0, ignore_index=True).filter(regex='pixel').duplicated().sum() - (n_train_dup + n_test_dup)\n\n10\n\n\nIf we were being more careful we would also look into near duplicates; there may be images that are practically identical but have slightly different pixel representations."
  },
  {
    "objectID": "notebooks/fashion-mnist-with-prototype-methods.html#making-it-faster-with-approximate-nearest-neighbours",
    "href": "notebooks/fashion-mnist-with-prototype-methods.html#making-it-faster-with-approximate-nearest-neighbours",
    "title": "skeptric",
    "section": "Making it faster with Approximate Nearest Neighbours",
    "text": "Making it faster with Approximate Nearest Neighbours\nInstead of brute force searching for the closest point we can use Approximate Nearest Neighbours to speed it up. There are many good libraries including faiss and hnswlib, but we’ll use annoy.\nIt uses Locality Sensitive Hashing; see Chapter 3 of Mining of Massive Datasets, by Leskovec, Rajaraman and Ullman for a good overview.\n\nfrom annoy import AnnoyIndex\n\nFirst we initialise an index and add all the vectors\n\n%%time\nt = AnnoyIndex(28*28, 'euclidean')\nfor i, v in enumerate(images_train):\n    t.add_item(i, v.flatten())\n\nCPU times: user 7.88 s, sys: 229 ms, total: 8.11 s\nWall time: 7.9 s\n\n\nThen we build the index, specifying the number of trees. 10 seems to work fine in this case.\n\nt.build(10)\n\nTrue\n\n\nWe then adapt our search to use the nearest neighbour from the index rather than brute forcing the search.\n\ndef ann_pred(v, k=1, search_k=-1):\n    n = t.get_nns_by_vector(v.flatten(), k, search_k=search_k, include_distances=False)\n    pred_labels = labels_train.loc[n]\n    pred_counts = Counter(pred_labels)\n    pred_label = pred_counts.most_common(n=1)[0][0]\n    \n    return pred_label\n\nWe can then get results in seconds rather than minutes.\n\n%time preds = [ann_pred(i, k=5) for i in images_valid]\n\nCPU times: user 13.7 s, sys: 7.48 ms, total: 13.7 s\nWall time: 13.7 s\n\n\nAnd we get around 85% accuracy, matching the ~15% error rate we expected from an exact solution\n\nnp.mean(preds == labels_valid)\n\n0.84875\n\n\nWe’re getting much more even predictions and more understandable confusion between:\n\ntop/pullover/coat/shirt\nankle boot/sneaker/sandal\n\nThe worst performing is shirt, which was the hardest for me to identify\n\n_ = plot_confusion_matrix(labels_valid, preds)\n\n\n\n\nLet’s grab an example where we the label was Shirt and we predicted T-shirt/top.\nIt looks like a t-shirt to me\n\nimg = images_valid[(np.array(preds) == 'T-shirt/top') & (labels_valid == 'Shirt')][0]\nplt.imshow(img, cmap='Greys')\n\n<matplotlib.image.AxesImage at 0x7f6730683a50>\n\n\n\n\n\nAnd we can see the nearby items that led to the prediction.\n\nk = 5\n\nn = t.get_nns_by_vector(img.flatten(), k)\n\n_ = show_images(images_train, labels_train, nrows=1, ncols=k, indices=n)\n\n\n\n\nAnother example is a pair of trousers the model guessed was a dress\n\nimg = images_valid[(np.array(preds) == 'Dress') & (labels_valid == 'Trouser')][0]\nplt.imshow(img, cmap='Greys')\n\n<matplotlib.image.AxesImage at 0x7f67302bb290>\n\n\n\n\n\n\nn = t.get_nns_by_vector(img.flatten(), k)\n_ = show_images(images_train, labels_train, nrows=1, ncols=k, indices=n)\n\n\n\n\nThere’s no obvious way to improve the model, so let’s do a hyperparameter search for the best k\n\n%%time\n\naccs = {}\nfor k in [1,5,10,20,100]:\n    preds = [ann_pred(i, k=k) for i in images_valid]\n    accs[k] = np.mean(preds == labels_valid)\n\nCPU times: user 1min 8s, sys: 10 ms, total: 1min 8s\nWall time: 1min 8s\n\n\nIt looks like 5 is marginally better\n\npd.Series(accs).sort_values(ascending=False).to_frame()\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      5\n      0.848750\n    \n    \n      10\n      0.841750\n    \n    \n      1\n      0.841083\n    \n    \n      20\n      0.832833\n    \n    \n      100\n      0.806250\n    \n  \n\n\n\n\nNow we have our best prototype model lets evaluate on the test set, and we get an accuracy of around 85%. Comparing with the benchmarks using sklearn this is about what they get with KNN methods (and it looks like using Manhattan Distance, aka l^p with p=1, would do slightly better).\nGiven the very best methods get around 90% (or up to 94% with convolutional neural networks) this is quite good!\n\n%%time\npreds = [ann_pred(i, k=5) for i in images_test]\nnp.mean(preds == labels_test)\n\nCPU times: user 10.5 s, sys: 502 µs, total: 10.5 s\nWall time: 10.5 s\n\n\n0.849\n\n\nOf course we could have done this all in a few lines of sklearn. But by looking through understanding how it works and looking at the images we get a much better idea of why we get these results, and maybe ideas on how to improve it. (I expect the sklearn method below does better by standardizing the features before computing distances; that would be an interesting exercise.)\n\n%%time\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier()\nknn.fit(df.filter(like='pixel', axis=1), df['label'])\n\npreds = knn.predict(df_test.filter(like='pixel', axis=1))\naccuracy_score(df_test['label'], preds)\n\nCPU times: user 1min 9s, sys: 8.63 s, total: 1min 18s\nWall time: 30.9 s\n\n\n0.8589\n\n\nPrototype methods are remarkably simple, quick to train and flexible. Because the images here are so simple we get good results on the pixels directly. For more complex datatypes we could use derived features; in particular the activations from a pretrained neural network such as Resnet, BERT, or CLIP can work well.\nFor more on prototype methods read Chapter 13 of The Elements of Statistical Learning."
  },
  {
    "objectID": "notebooks/Parsing Experience from Adzuna Job Ads.html",
    "href": "notebooks/Parsing Experience from Adzuna Job Ads.html",
    "title": "skeptric",
    "section": "",
    "text": "Next Steps\nWe could keep building out a rule based approach:\n\nDo analysis of this list to build up list of positive/negative phrases\nSearch the document for those phrases\nLook at the results and build new rules to get those phrases\n\nOr we could use this as the seed of a model based approach:\n\nBuild an NER model on these base phrases\nAnnotate the predictions and refine the model\n\nOr we could use some hybrid of the two"
  },
  {
    "objectID": "notebooks/restoring_wayback_html.html",
    "href": "notebooks/restoring_wayback_html.html",
    "title": "skeptric",
    "section": "",
    "text": "Fetch Original and Archived Content\nWe can get the version available from the Wayback Machine like this:\n\nwayback_url = f'http://web.archive.org/web/{record.timestamp}/{record.original}'\nwayback_content = requests.get(wayback_url).content\n\nThe digests don’t match because the Internet Archive changes the HTML\n\nfrom hashlib import sha1\nfrom base64 import b32encode\n\ndef sha1_digest(content: bytes) -> str:\n    return b32encode(sha1(content).digest()).decode('ascii')\n\n\nsha1_digest(wayback_content)\n\n'OXZ5C2VPDFFRV6U3CCNM6QT7VKND6SSE'\n\n\nHowever we can get the original HTML captured:\n\noriginal_url = f'http://web.archive.org/web/{record.timestamp}id_/{record.original}'\noriginal_content = requests.get(original_url).content\n\nAnd the SHA-1 matches the CDX record\n\nsha1_digest(original_content) == record['digest']\n\nTrue\n\n\nHere’s a capture of the website I made on 2021-12-01\n\ndec21_content = requests.get('https://raw.githubusercontent.com/EdwardJRoss/skeptric/98419583bc0c7b71ab9469250bbed924cdac448d/static/resources/about.html').content\n\nAnd it’s byte-for-byte the same are the snapshot\n\ndec21_content == original_content\n\nTrue\n\n\n\nsha1_digest(dec21_content)\n\n'Z5NRUTRW3XTKZDCJFDKGPJ5BWIBNQCG7'\n\n\n\n\nRemoving Headers\nThe Wayback Machine version injects a header just after the <head> tag down to <-- End Wayback Rewrite JS Include-->.\nIt looks like a bit of javascript and some CSS (likely for tracking and adding banners, search, etc)\n\nprint(wayback_content[:1500].decode('utf-8'))\n\n<!DOCTYPE html>\n<html lang=\"en-us\">\n<head><script src=\"//archive.org/includes/analytics.js?v=cf34f82\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app216.us.archive.org';v.server_ms=347;archive_analytics.send_pageview({});});</script>\n<script type=\"text/javascript\" src=\"/_static/js/bundle-playback.js?v=UfTkgsKx\" charset=\"utf-8\"></script>\n<script type=\"text/javascript\" src=\"/_static/js/wombat.js?v=UHAOicsW\" charset=\"utf-8\"></script>\n<script type=\"text/javascript\">\n  __wm.init(\"http://web.archive.org/web\");\n  __wm.wombat(\"https://skeptric.com/about/\",\"20211120235913\",\"http://web.archive.org/\",\"web\",\"/_static/\",\n          \"1637452753\");\n</script>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/_static/css/banner-styles.css?v=omkqRugM\" />\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/_static/css/iconochive.css?v=qtvMKcIJ\" />\n<!-- End Wayback Rewrite JS Include -->\n\n    <meta charset=\"utf-8\"/>\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"/>\n\n    \n    \n    <title>About Skeptric · </title>\n\n    <meta name=\"HandheldFriendly\" content=\"True\"/>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n\n    \n    <link rel=\"stylesheet\" href=\"http://web.archive.org/web/20211120235913cs_/https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css\"/>\n<script async src=\"http://web.archiv\n\n\nIt’s missing from the original\n\nprint(original_content[:500].decode('utf-8'))\n\n<!DOCTYPE html>\n<html lang=\"en-us\">\n<head>\n    <meta charset=\"utf-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\" />\n\n    \n    \n    <title>About Skeptric · </title>\n\n    <meta name=\"HandheldFriendly\" content=\"True\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n\n    \n    <link rel=\"stylesheet\" href=\"https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css\" />\n<script async src=\"https://www.googletagm\n\n\n\ndef remove_wayback_header(content):\n    _start = b'<script src=\"//archive.org/includes/analytics.js'\n    _end = b'<!-- End Wayback Rewrite JS Include -->\\n'\n    start_idx = content.find(_start)\n    end_idx = content.find(_end)\n    if start_idx < 0 or end_idx < 0:\n        raise ValueError(\"Could not find\")\n    return content[:start_idx] + content[end_idx+len(_end):]\n\nAfter removing the header the start looks the same (except for the URL rewriting at the end)\n\nprint(remove_wayback_header(wayback_content)[:500].decode('utf-8'))\n\n<!DOCTYPE html>\n<html lang=\"en-us\">\n<head>\n    <meta charset=\"utf-8\"/>\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"/>\n\n    \n    \n    <title>About Skeptric · </title>\n\n    <meta name=\"HandheldFriendly\" content=\"True\"/>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"/>\n\n    \n    <link rel=\"stylesheet\" href=\"http://web.archive.org/web/20211120235913cs_/https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css\"/>\n<\n\n\n\n\nRemoving Footers\nThe Wayback Machine adds a bunch of footers about the capture and the archival and copyright notice.\n\nprint(wayback_content[-1000:].decode('utf-8'))\n\n=\"MathJax-script\" async src=\"http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n\n<script src=\"http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose' });</script>\n\n</footer>\n\n    </div>\n\n</body>\n</html>\n<!--\n     FILE ARCHIVED ON 23:59:13 Nov 20, 2021 AND RETRIEVED FROM THE\n     INTERNET ARCHIVE ON 10:20:48 Dec 01, 2021.\n     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.\n\n     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.\n     SECTION 108(a)(3)).\n-->\n<!--\nplayback timings (ms):\n  captures_list: 204.223\n  exclusion.robots: 0.095\n  exclusion.robots.policy: 0.087\n  RedisCDXSource: 21.732\n  esindex: 0.008\n  LoadShardBlock: 161.202 (3)\n  PetaboxLoader3.datanode: 170.534 (4)\n  CDXLines.iter: 18.668 (3)\n  load_resource: 137.66\n  PetaboxLoader3.resolve: 57.198\n-->\n\n\n\nprint(original_content[-500:].decode('utf-8'))\n\nard-ross-4909ba13a/\" target=\"_blank\" rel=\"noopener\">LinkedIn</a>\n                </nav>\n            </div>\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n<script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n\n<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose' });</script>\n\n</footer>\n\n    </div>\n\n</body>\n</html>\n\n\n\nWe can roughly remove this by looking for the FILE ARCHIVED ON:\n\ndef remove_wayback_footer(content):\n    _prefix = b'</html>\\n'\n    _start = _prefix + b'<!--\\n     FILE ARCHIVED ON '\n    start_idx = content.find(_start)\n    if start_idx < 0:\n        raise ValueError(\"Could not find\")\n    return content[:start_idx + len(_prefix)]\n\n\nprint(remove_wayback_footer(wayback_content)[-500:].decode('utf-8'))\n\nive.org/web/20211120235913js_/https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n<script id=\"MathJax-script\" async src=\"http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n\n<script src=\"http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>\n<script>mermaid.initialize({ startOnLoad: true, securityLevel: 'loose' });</script>\n\n</footer>\n\n    </div>\n\n</body>\n</html>\n\n\n\n\n\nRestoring Links\nBy looking through the links we can see that there arey are prefixed with http://web.archive.org/web/<TIMESTAMP> with an extra cs_ for CSS and js_ for Javascript (and im_ for images, not shown here).\n\nimport re\nre.findall(b'(?:href|src)=\"([^\"]*)\"', wayback_content)\n\n[b'//archive.org/includes/analytics.js?v=cf34f82',\n b'/_static/js/bundle-playback.js?v=UfTkgsKx',\n b'/_static/js/wombat.js?v=UHAOicsW',\n b'/_static/css/banner-styles.css?v=omkqRugM',\n b'/_static/css/iconochive.css?v=qtvMKcIJ',\n b'http://web.archive.org/web/20211120235913cs_/https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css',\n b'http://web.archive.org/web/20211120235913js_/https://www.googletagmanager.com/gtag/js?id=UA-167481545-1',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/',\n b'/web/20211120235913/https://skeptric.com/about/',\n b'/web/20211120235913/https://skeptric.com/',\n b'http://web.archive.org/web/20211120235913/https://www.whatcar.xyz/',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/whatcar',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/job-advert-analysis',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/moving-averages-sql/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/searching-100b-pages-cdx/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/common-crawl-index-athena/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/minhash-lsh/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/calculate-centroid-on-sphere/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/constant-models/',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/all_of_statistics_exercises',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/regression_stories',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/regression_stories',\n b'http://web.archive.org/web/20211120235913/https://github.com/EdwardJRoss/mlzero',\n b'http://web.archive.org/web/20211120235913/mailto:webmaster@skeptric.com',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/reading-list/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/',\n b'http://web.archive.org/web/20211120235913/https://skeptric.com/',\n b'http://web.archive.org/web/20211120235913/https://twitter.com/EddingtonRoss',\n b'http://web.archive.org/web/20211120235913/https://github.com/edwardjross/',\n b'http://web.archive.org/web/20211120235913/https://www.linkedin.com/in/edward-ross-4909ba13a/',\n b'http://web.archive.org/web/20211120235913js_/https://polyfill.io/v3/polyfill.min.js?features=es6',\n b'http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',\n b'http://web.archive.org/web/20211120235913js_/https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js']\n\n\nWe can rewrite them like this:\n\ndef remove_wayback_links(content: bytes, timestamp: str) -> bytes:\n    # Remove web links\n    timestamp = timestamp.encode('ascii') \n    content = content.replace(b'http://web.archive.org', b'')\n    for prefix in [b'', b'im_', b'js_', b'cs_']:\n        content = content.replace(b'/web/' + timestamp + prefix + b'/', b'')\n    return content\n\n\nre.findall(b'(?:href|src)=\"([^\"]*)\"', remove_wayback_links(wayback_content, record.timestamp))\n\n[b'//archive.org/includes/analytics.js?v=cf34f82',\n b'/_static/js/bundle-playback.js?v=UfTkgsKx',\n b'/_static/js/wombat.js?v=UHAOicsW',\n b'/_static/css/banner-styles.css?v=omkqRugM',\n b'/_static/css/iconochive.css?v=qtvMKcIJ',\n b'https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css',\n b'https://www.googletagmanager.com/gtag/js?id=UA-167481545-1',\n b'https://skeptric.com/',\n b'https://skeptric.com/about/',\n b'https://skeptric.com/',\n b'https://www.whatcar.xyz/',\n b'https://github.com/EdwardJRoss/whatcar',\n b'https://github.com/EdwardJRoss/job-advert-analysis',\n b'https://skeptric.com/',\n b'https://skeptric.com/moving-averages-sql/',\n b'https://skeptric.com/searching-100b-pages-cdx/',\n b'https://skeptric.com/common-crawl-index-athena/',\n b'https://skeptric.com/minhash-lsh/',\n b'https://skeptric.com/calculate-centroid-on-sphere/',\n b'https://skeptric.com/constant-models/',\n b'https://github.com/EdwardJRoss/all_of_statistics_exercises',\n b'https://github.com/EdwardJRoss/regression_stories',\n b'https://github.com/EdwardJRoss/regression_stories',\n b'https://github.com/EdwardJRoss/mlzero',\n b'mailto:webmaster@skeptric.com',\n b'https://skeptric.com/reading-list/',\n b'https://skeptric.com/',\n b'https://skeptric.com/',\n b'https://twitter.com/EddingtonRoss',\n b'https://github.com/edwardjross/',\n b'https://www.linkedin.com/in/edward-ross-4909ba13a/',\n b'https://polyfill.io/v3/polyfill.min.js?features=es6',\n b'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',\n b'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js']\n\n\n\n\nAnd the rest\nWe can put all our changes together\n\ndef remove_wayback_changes(content, timestamp):\n    content = remove_wayback_header(content)\n    content = remove_wayback_footer(content)\n    content = remove_wayback_links(content, timestamp)\n    return content\n\n\nclean_wayback_content = remove_wayback_changes(wayback_content, record['timestamp'])\n\n\nclean_wayback_content == original_content\n\nFalse\n\n\n\nfrom difflib import SequenceMatcher\nseqmatcher = SequenceMatcher(isjunk=None,\n                             a=original_content,\n                             b=clean_wayback_content,\n                             autojunk=False)\n\ncontext_before = context_after = 20\n\nfor tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        if tag == 'equal':\n            continue\n\n        a_min = max(a0 - context_before, 0)\n        a_max = min(a1 + context_after, len(seqmatcher.a))\n        print(seqmatcher.a[a_min:a_max])\n\n        b_min = max(b0 - context_before, 0)\n        b_max = min(b1 + context_after, len(seqmatcher.b))\n        print(seqmatcher.b[b_min:b_max])\n        print()\n\nb'meta charset=\"utf-8\" />\\n    <meta http-eq'\nb'meta charset=\"utf-8\"/>\\n    <meta http-eq'\n\nb'e\" content=\"IE=edge\" />\\n\\n    \\n    \\n    <t'\nb'e\" content=\"IE=edge\"/>\\n\\n    \\n    \\n    <t'\n\nb'ndly\" content=\"True\" />\\n    <meta name=\"v'\nb'ndly\" content=\"True\"/>\\n    <meta name=\"v'\n\nb', initial-scale=1.0\" />\\n\\n    \\n    <link r'\nb', initial-scale=1.0\"/>\\n\\n    \\n    <link r'\n\nb'015bf2d95d914e5.css\" />\\n<script async src'\nb'015bf2d95d914e5.css\"/>\\n<script async src'\n\nb'\"menuitem\"><a href=\"/about/\">About</a></'\nb'\"menuitem\"><a href=\"https://skeptric.com/about/\">About</a></'\n\nb'\"menuitem\"><a href=\"/\">Home</a></li>\\n   '\nb'\"menuitem\"><a href=\"https://skeptric.com/\">Home</a></li>\\n   '\n\nb'https://skeptric.com\">skeptric.com</a>.<'\nb'https://skeptric.com/\">skeptric.com</a>.<'\n\n\n\nWe can mangle the original HTML to get the same result. I doubt this would be general enough to work on other pages, but gives a flavour of the changes.\n\nimport re\ndef wayback_normalise_content(content, base_url):\n    url = base_url.encode('ascii')\n    content = re.sub(b' */>', b'/>', content)\n    content = content.replace(b'href=\"/', b'href=\"' + url + b'/')\n    content = re.sub(b'href=\"' + url + b'\"', b'href=\"' + url + b'/\"', content)\n    return content\n\n\nassert wayback_normalise_content(original_content, 'https://skeptric.com') == clean_wayback_content"
  },
  {
    "objectID": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html",
    "href": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html",
    "title": "skeptric",
    "section": "",
    "text": "from pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#overall-minimum-salary",
    "href": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#overall-minimum-salary",
    "title": "skeptric",
    "section": "Overall Minimum Salary",
    "text": "Overall Minimum Salary\n\n%time sns.displot(df, x=\"salary_low\", log_scale=True, kde=True)\n\nCPU times: user 350 ms, sys: 103 ms, total: 452 ms\nWall time: 326 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bf1c00fa0>\n\n\n\n\n\n\n%time sns.displot(df, x=\"salary_hi\", log_scale=True, kde=True)\n\nCPU times: user 379 ms, sys: 65.8 ms, total: 445 ms\nWall time: 319 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bcbecbbb0>\n\n\n\n\n\nAbove about 500k it gets suss\n\ndf[df.salary_low > 500_000].sort_values(\"salary_low\").T\n\n\n\n\n\n  \n    \n      \n      34383\n      32963\n      36900\n      6001\n      5873\n      4212\n      4138\n      39504\n      5599\n      3990\n      13992\n      12789\n      3827\n      380\n      9323\n      32722\n      34105\n      127\n      3315\n      4491\n      9064\n      330\n      3678\n      1657\n      17909\n      ...\n      60805\n      60404\n      58749\n      66306\n      61941\n      57673\n      61590\n      61461\n      61308\n      63620\n      69190\n      30719\n      22825\n      60435\n      64851\n      67031\n      56031\n      68308\n      55116\n      55205\n      54382\n      55609\n      54317\n      60277\n      56599\n    \n  \n  \n    \n      title\n      Receptionist POC\n      Receptionist POC\n      Draftsperson\n      UX/UI Designer\n      Front End Developer\n      UX/UI Designer\n      Front End Developer\n      Marketing and Events Coordinator\n      Document Controller\n      Document Controller\n      Office Manager / Accountant\n      Office Manager / Accountant\n      Senior Android Engineer\n      Senior Android Engineer\n      Senior Android Engineer\n      Full Stack Developer\n      Full Stack Developer\n      Signallers\n      Senior Quantity Surveyor\n      Senior Quantity Surveyor\n      Lead Commissioning Engineer\n      Lead Commissioning Engineer\n      Lead Commissioning Engineer\n      Lead Commissioning Engineer\n      General Foreman\n      ...\n      Principal Structure Engineer - Up to $211K. Accelerate your Career?\n      12d Designer $108-85K. Great time to choose\n      Registered Surveyors $161-128K Multiple Choices\n      Business Services Manager $130-105K\n      Contracts Administrator - Commercial\n      Contracts Administrator - Construction\n      Estimator (QS) - Commercial Construction Projects over $40M\n      Contracts Administrator - Commercial\n      Estimator (QS) - Commercial Construction Projects over $40M\n      Outbound Sales Consultant - Join a Market Leader - Legal Tech Company\n      Debt Collection Officer\n      Corporate/Commercial Counsel 2-5 PAE- 3 month contract - Large IT Company!!\n      Inhouse Corporate Legal Counsel (4 year PAE +) - National Property Advisory Firm\n      Business Services Accountants. SYDNEY ROLES\n      CT Radiographer - Regional NSW\n      MRI Technician / MRI Radiographer\n      Rookie GP Representative - Eastern Suburbs - multiple positions\n      Employment and IR 2-5Yrs PQE\n      Temporary Conveyancing Paralegal Required\n      Estimator\n      Civil Project Manager\n      Contract Administrator\n      Project Engineer\n      Structural Drafter REVIT - Immediate Start!\n      Maintenance Planner - HME\n    \n    \n      description\n      Our client, a multi-service IT provider is currently seeking an experienced Receptionist for a permanent full time position. In this diverse role ...\n      Our client, a multi-service IT provider is currently seeking an experienced Receptionist for a permanent full time position. In this diverse role ...\n      Your new company\\nHays are currently working with a reputable company based in the Canberra region. This particular company work on large scale co...\n      My client is a large corporation with operations across the world. They have their headquarters based in Brisbane, giving you access to global pro...\n      My client is a large corporation with operations across the world. They are currently expanding their team due to increased project work. They are...\n      My client is a large corporation with operations across the world. They have their headquarters based in Brisbane, giving you access to global pro...\n      My client is a large corporation with operations across the world. They are currently expanding their team due to increased project work. They are...\n      Northern Territory PHN leads the development and coordination of an equitable, comprehensive primary health care system and an engaged health work...\n      About the Company\\nDavidson are currently working with one of the world’s largest names in wind energy. This company manufactures, installs and se...\n      About the Company\\nDavidson are currently working with one of the world’s largest names in wind energy. This company manufactures, installs and se...\n      The Company\\nProject Management Consultancy who specialise in a range of project sectors valued up to $120 Million, have an exciting and extremely...\n      The Company\\nProject Management Consultancy who specialise in a range of project sectors valued up to $120 Million, have an exciting and extremely...\n      One of Melbourne's most desirable start-ups are looking to hire a Senior Android Developer to join their team of high performing developers.\\nBein...\n      One of Melbourne's most desirable start-ups are looking to hire a Senior Android Developer to join their team of high performing developers.\\nBein...\n      One of Melbourne's most desirable start-ups are looking to hire a Senior Android Developer to join their team of high performing developers.\\nBein...\n      Great opportunity working for a unique company, who roll out solutions across Australia before scaling globally.\\n\\nJoin a rapid growing organisat...\n      Great opportunity working for a unique company, who roll out solutions across Australia before scaling globally.\\n\\nJoin a rapid growing organisat...\n      Davidson is excited to be partnering with Metro Trains Melbourne to appoint four Signallers for their Network Signalling division in Melbourne.\\nM...\n      Company:\\nOur Client is a multiple award winning, Australian owned, premier QS practice that is currently seeking the capabilities of an passionat...\n      Company:\\nOur Client is a multiple award winning, Australian owned, premier QS practice that is currently seeking the capabilities of an passionat...\n      About the Company\\nFounded in Urumqi, China, Goldwind is one of the world's leading wind power companies. The largest player in the wind industry ...\n      About the Company\\nFounded in Urumqi, China, Goldwind is one of the world's leading wind power companies. The largest player in the wind industry ...\n      About the Company\\nFounded in Urumqi, China, Goldwind is one of the world's leading wind power companies. The largest player in the wind industry ...\n      About the Company\\nFounded in Urumqi, China, Goldwind is one of the world's leading wind power companies. The largest player in the wind industry ...\n      The Company\\nOur client is known local Sydney builder who partners with prestigious developers delivering multi million-dollar projects across the...\n      ...\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      Our client is a long established successful debt recovery law firm. The firm is looking for a Debt Collection Officer to join the firm and build o...\n      The Opportunity Our client is one of Australia’s major IT service providers who partner with large global corporations such as Microsoft, Cisco, A...\n      The Opportunity  Work with one of the largest independent professional property services firms in Australia for over 40 years. This company has wi...\n      \n      \n      \n      \n      \n      18th May - 1st June Competitive hourly rate Bundall location Our client is a centrally located law firm specialising in commercial and property ma...\n      The Opportunity This contractor have been thriving in the local market for close to forty years, in thanks to their dedication to providing outsta...\n      The Opportunity  Dynamic. Fresh. Vivacious. Outside Ideas encapsulate ambition and spirit, from their early beginnings to their current impressive...\n      The Opportunity With a long history in the Australian market, our client continues to grow and evolve. A quintessential Australian builder, they b...\n      We are interested in speaking with all levels of Civil Project Engineers to work with a number of our clients on major projects here across the co...\n      \n      \n    \n    \n      uri\n      https://jobs.launchrecruitment.com.au/job/100602033258906/receptionist-poc-1/\n      https://jobs.launchrecruitment.com.au/job/100602033258906/receptionist-poc-1/\n      https://www.seek.com.au/job/41362846?type=standard\n      https://www.davidsonwp.com/job/100562133327232/ux-slash-ui-designer/\n      https://www.davidsonwp.com/job/100562133319740/front-end-developer-8/\n      https://www.davidsonwp.com/job/100562133327232/ux-slash-ui-designer/\n      https://www.davidsonwp.com/job/100562133319740/front-end-developer-8/\n      https://www.seek.com.au/job/41204953?type=standout\n      https://www.davidsonwp.com/job/100562133297159/document-controller/\n      https://www.davidsonwp.com/job/100562133297159/document-controller/\n      https://www.cgcrecruitment.com/job/office-manager-slash-accountant-2/\n      https://www.cgcrecruitment.com/job/office-manager-slash-accountant-2/\n      https://www.davidsonwp.com/job/100562133270808/senior-android-engineer/\n      https://www.davidsonwp.com/job/100562133272648/senior-android-engineer-1/\n      https://www.davidsonwp.com/job/100562133270808/senior-android-engineer/\n      https://jobs.launchrecruitment.com.au/job/100602033189052/full-stack-developer/\n      https://jobs.launchrecruitment.com.au/job/100602033189052/full-stack-developer/\n      https://www.davidsonwp.com/job/100562133248138/signallers-1/\n      https://www.davidsonwp.com/job/100562133361033/senior-quantity-surveyor-5/\n      https://www.davidsonwp.com/job/100562133361033/senior-quantity-surveyor-5/\n      https://www.davidsonwp.com/job/100562133254581/lead-commissioning-engineer/\n      https://www.davidsonwp.com/job/100562133267469/lead-commissioning-engineer-1/\n      https://www.davidsonwp.com/job/100562133254581/lead-commissioning-engineer/\n      https://www.davidsonwp.com/job/100562133254581/lead-commissioning-engineer/\n      https://www.cgcrecruitment.com/job/general-foreman-3/\n      ...\n      https://www.seek.com.au/job/36553908\n      https://www.seek.com.au/job/36554083\n      https://www.seek.com.au/job/37763876\n      https://www.seek.com.au/job/37210212\n      https://www.seek.com.au/job/36499847\n      https://www.seek.com.au/job/36672624\n      https://www.seek.com.au/job/36500052\n      https://www.seek.com.au/job/36499848\n      https://www.seek.com.au/job/36500051\n      https://www.seek.com.au/job/37437418\n      https://www.seek.com.au/job/36167378\n      https://www.seek.com.au/job/40102757\n      https://www.seek.com.au/job/40055145\n      https://www.seek.com.au/job/36554031\n      https://www.seek.com.au/job/37438072\n      https://www.seek.com.au/job/37596581\n      https://www.seek.com.au/job/37060303\n      https://www.seek.com.au/job/37596615\n      https://www.seek.com.au/job/36034602\n      https://www.seek.com.au/job/36029408\n      https://www.seek.com.au/job/36034147\n      https://www.seek.com.au/job/36029376\n      https://www.seek.com.au/job/36030406\n      https://www.seek.com.au/job/36554164\n      https://www.seek.com.au/job/36741495\n    \n    \n      view_date\n      2020-04-08 20:49:20+00:00\n      2020-07-02 01:53:50+00:00\n      2020-05-31 08:52:15+00:00\n      2020-09-18 00:45:40+00:00\n      2020-09-28 22:26:46+00:00\n      2020-04-03 12:22:01+00:00\n      2020-03-29 11:13:38+00:00\n      2020-03-28 18:53:37+00:00\n      2020-09-28 20:28:34+00:00\n      2020-03-31 20:21:49+00:00\n      2020-11-27 23:43:35+00:00\n      2020-07-13 10:52:05+00:00\n      2020-04-07 16:04:01+00:00\n      2020-06-05 08:54:21+00:00\n      2020-10-22 01:04:53+00:00\n      2020-07-03 03:29:56+00:00\n      2020-04-05 23:10:42+00:00\n      2020-05-30 21:03:15+00:00\n      2020-11-23 16:23:14+00:00\n      2020-03-28 09:43:42+00:00\n      2020-10-28 17:50:29+00:00\n      2020-05-30 20:34:13+00:00\n      2020-04-02 23:46:43+00:00\n      2020-07-14 15:46:51+00:00\n      2020-10-21 05:57:39+00:00\n      ...\n      2018-06-25 16:27:19\n      2018-06-25 16:29:02\n      2018-11-21 02:16:18\n      2018-09-13 01:28:16\n      2018-06-19 01:38:56\n      2018-07-11 04:42:55\n      2018-06-19 01:50:58\n      2018-06-19 01:52:42\n      2018-06-19 01:46:04\n      2018-10-11 01:13:02\n      2018-05-09 04:33:15\n      2019-10-07 06:55:20\n      2019-09-30 10:17:41\n      2018-06-25 16:14:43\n      2018-10-11 00:57:28\n      2018-10-31 02:56:57\n      2018-08-27 00:40:28\n      2018-10-31 02:58:30\n      2018-04-20 02:07:00\n      2018-04-20 03:39:42\n      2018-04-20 02:19:21\n      2018-04-20 03:31:50\n      2018-04-20 03:22:50\n      2018-06-25 16:14:14\n      2018-07-19 06:02:35\n    \n    \n      org\n      Launch Recruitment\n      Launch Recruitment\n      Hays Architecture\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Northern Territory PHN\n      Davidson\n      Davidson\n      CGC Recruitment\n      CGC Recruitment\n      Davidson\n      Davidson\n      Davidson\n      Launch Recruitment\n      Launch Recruitment\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      CGC Recruitment\n      ...\n      ENGINEERJOBS.com.au\n      ENGINEERJOBS.com.au\n      SurveyorJobs.com.au\n      AccountantJobs.com.au\n      Gough Property & Real Estate Recruiters\n      Gough Property & Real Estate Recruiters\n      Gough Property & Real Estate Recruiters\n      Gough Property & Real Estate Recruiters\n      Gough Property & Real Estate Recruiters\n      Gatehouse Legal Recruitment\n      Gatehouse Legal Recruitment\n      Gatehouse Legal Recruitment\n      Gatehouse Legal Recruitment\n      AccountantJobs.com.au\n      Sigma Resourcing Pty Ltd\n      Sigma Resourcing Pty Ltd\n      EvansPetersen Healthcare Pty Ltd\n      Ablethorpe Recruitment\n      legal eagles\n      Anna Roussos Recruitment + Advisory\n      Anna Roussos Recruitment + Advisory\n      Anna Roussos Recruitment + Advisory\n      Hudson\n      Constructive Recruitment Pty Ltd\n      Alchemy Recruitment Consulting Pty Ltd\n    \n    \n      salary_raw\n      $0 - $650000 per annum\n      $0 - $650000 per annum\n      $70,0000 - $100,000 + Superannuation\n      $700000 per annum\n      $700000 per annum\n      $700000 per annum\n      $700000 per annum\n      $72,6640 - $80,715 pro-rata + super\n      $750000 per annum\n      $750000 per annum\n      $85,0000 - $110,000 depending on experience.\n      $85,0000 - $110,000 depending on experience.\n      $1000000 - $1400000 per annum\n      $1000000 - $1400000 per annum\n      $1000000 - $1400000 per annum\n      $0 - $1000000 per annum\n      $0 - $1000000 per annum\n      $1100000 per annum\n      $1200000 per annum\n      $1200000 per annum\n      $1250000 per annum\n      $1250000 per annum\n      $1250000 per annum\n      $1250000 per annum\n      $130,0000 - $160,000\n      ...\n      Or call/txt Roger 0418494245 in confidence\n      Confidentially call / text Roger on 0418494245\n      Or Call/SMS Pieter on 0419 490 660 in Confidence\n      Confidentially call/txt mariam Awad 0424522052\n      Call GREG WILL 0432 498 770\n      Call GREG WILL 0432 498 770\n      Call GREG WILL 0432 498 770\n      Call GREG WILL 0432 498 770\n      Call GREG WILL 0432 498 770\n      Contact Louise Hvala on 0433 332 276\n      Contact Meg Bennett on 0436 001 675\n      Call Ravi on 0437 028 897\n      Contact Ravi on 0437 028 897\n      $$ Neg + bonuses. Dial/SMS Amy 0438 722 264\n      ilias@sigmaresourcing.com.au 0452 525 359\n      ilias@sigmaresourcing.com.au 0452 525 359\n      Call Cathy 0477029802 to discuss\n      0490 832 177\n      Contact Jo Williams on 07 3231 1209\n      Contact Anna on 08 7073 6872 for further details\n      Please call Anthony on 08 7073 6872\n      Please call 08 7073 6872 for further information\n      Call Alex Jeffries 08 8223 9908\n      Call Jana on 08 9287 5000 for more information\n      Call Nick - 1300 895 388\n    \n    \n      salary_min\n      0\n      0\n      700000\n      700000\n      700000\n      700000\n      700000\n      726640\n      750000\n      750000\n      850000\n      850000\n      1e+06\n      1e+06\n      1e+06\n      0\n      0\n      1.1e+06\n      1.2e+06\n      1.2e+06\n      1.25e+06\n      1.25e+06\n      1.25e+06\n      1.25e+06\n      1.3e+06\n      ...\n      4.18494e+08\n      4.18494e+08\n      4.19491e+08\n      4.24522e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.33332e+08\n      4.36002e+08\n      4.37029e+08\n      4.37029e+08\n      4.38722e+08\n      4.52525e+08\n      4.52525e+08\n      4.7703e+08\n      4.90832e+08\n      7.32311e+08\n      8.70737e+08\n      8.70737e+08\n      8.70737e+08\n      8.8224e+08\n      8.92875e+08\n      1.3009e+09\n    \n    \n      salary_max\n      650000\n      650000\n      100000\n      NaN\n      NaN\n      NaN\n      NaN\n      80715\n      NaN\n      NaN\n      110000\n      110000\n      1.4e+06\n      1.4e+06\n      1.4e+06\n      1e+06\n      1e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      160000\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_hours\n      2000\n      2000\n      NaN\n      2000\n      2000\n      2000\n      2000\n      NaN\n      2000\n      2000\n      NaN\n      NaN\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      location_raw\n      Sydney, AU\n      Sydney, AU\n      canberra, ACT, Australian Capital Territory, Australia\n      Brisbane, AU\n      Brisbane, AU\n      Brisbane, AU\n      Brisbane, AU\n      darwin, Darwin, Northern Territories, Australia\n      Canberra, AU\n      Canberra, AU\n      Sydney, AU\n      Sydney, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Sydney, AU\n      Sydney, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Sydney, AU\n      Melbourne, AU\n      Sydney, AU\n      Sydney, AU\n      Sydney, AU\n      ...\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Brisbane, AU\n      Brisbane, AU\n      Sunshine Coast, AU\n      Gold Coast, AU\n      Gold Coast, AU\n      Melbourne, CBD & Inner Suburbs, AU\n      Melbourne, CBD & Inner Suburbs, AU\n      Melbourne, Victoria, Australia\n      Melbourne, Victoria, Australia\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Tamworth & North West NSW, AU\n      Perth, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Melbourne, CBD & Inner Suburbs, AU\n      Gold Coast, AU\n      Adelaide, AU\n      Adelaide, AU\n      Adelaide, AU\n      Adelaide, AU\n      Adelaide, AU\n      Mackay & Coalfields, AU\n    \n    \n      loc_id\n      1.01932e+08\n      1.01932e+08\n      1.37695e+09\n      1.01934e+08\n      1.01934e+08\n      1.01934e+08\n      1.01934e+08\n      NaN\n      8.56815e+07\n      8.56815e+07\n      1.01932e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01933e+08\n      1.01932e+08\n      1.01932e+08\n      1.01932e+08\n      ...\n      8.57823e+07\n      8.57823e+07\n      8.57823e+07\n      8.57823e+07\n      1.01934e+08\n      1.01934e+08\n      1.02049e+08\n      1.02049e+08\n      1.02049e+08\n      8.57823e+07\n      8.57823e+07\n      1.01933e+08\n      1.01933e+08\n      8.57823e+07\n      1.01932e+08\n      1.01938e+08\n      8.57823e+07\n      8.57823e+07\n      1.02049e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01935e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      ...\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Sydney\n      Sydney\n      None\n      Brisbane\n      Brisbane\n      Brisbane\n      Brisbane\n      None\n      None\n      None\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Sydney\n      Sydney\n      Sydney\n      ...\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Brisbane\n      Sunshine Coast\n      Gold Coast\n      Gold Coast\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Tamworth Regional\n      Perth\n      Melbourne\n      Melbourne\n      Gold Coast\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Mackay\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      Sydney\n      Sydney\n      None\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      None\n      None\n      None\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Sydney\n      Sydney\n      Sydney\n      ...\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Brisbane City\n      None\n      None\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Tamworth\n      Perth\n      Melbourne\n      Melbourne\n      None\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Mackay\n    \n    \n      loc_locality\n      Sydney\n      Sydney\n      None\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      None\n      None\n      None\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Sydney\n      Sydney\n      Sydney\n      ...\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Brisbane City\n      None\n      None\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Tamworth\n      Perth\n      Melbourne\n      Melbourne\n      None\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Mackay\n    \n    \n      loc_macrocounty\n      Sydney\n      Sydney\n      Canberra\n      Brisbane\n      Brisbane\n      Brisbane\n      Brisbane\n      None\n      None\n      None\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Sydney\n      Sydney\n      Sydney\n      ...\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Brisbane\n      Sunshine Coast\n      Gold Coast\n      Gold Coast\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Perth\n      Melbourne\n      Melbourne\n      Gold Coast\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      None\n    \n    \n      loc_region\n      New South Wales\n      New South Wales\n      Australian Capital Territory\n      Queensland\n      Queensland\n      Queensland\n      Queensland\n      None\n      Australian Capital Territory\n      Australian Capital Territory\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Victoria\n      New South Wales\n      New South Wales\n      New South Wales\n      ...\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Queensland\n      Queensland\n      Queensland\n      Queensland\n      Queensland\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Western Australia\n      Victoria\n      Victoria\n      Queensland\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      Queensland\n    \n    \n      processor\n      launchrecruitment\n      launchrecruitment\n      seek\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      seek\n      davidsonwp\n      davidsonwp\n      cgcrecruitment\n      cgcrecruitment\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      launchrecruitment\n      launchrecruitment\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      cgcrecruitment\n      ...\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n    \n    \n      source\n      CC-MAIN-2020-16\n      CC-MAIN-2020-29\n      CC-MAIN-2020-24\n      CC-MAIN-2020-40\n      CC-MAIN-2020-40\n      CC-MAIN-2020-16\n      CC-MAIN-2020-16\n      CC-MAIN-2020-16\n      CC-MAIN-2020-40\n      CC-MAIN-2020-16\n      CC-MAIN-2020-50\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      CC-MAIN-2020-24\n      CC-MAIN-2020-45\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      CC-MAIN-2020-24\n      CC-MAIN-2020-50\n      CC-MAIN-2020-16\n      CC-MAIN-2020-45\n      CC-MAIN-2020-24\n      CC-MAIN-2020-16\n      CC-MAIN-2020-29\n      CC-MAIN-2020-45\n      ...\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau_2019q3\n      seekau_2019q3\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n    \n    \n      loc_neighbourhood\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      NaN\n      NaN\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      NaN\n      ...\n      Melbourne CBD\n      Melbourne CBD\n      Melbourne CBD\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      Melbourne CBD\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      Melbourne CBD\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n    \n    \n      annual\n      True\n      True\n      False\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      650000\n      650000\n      700000\n      700000\n      700000\n      700000\n      700000\n      726640\n      750000\n      750000\n      850000\n      850000\n      1e+06\n      1e+06\n      1e+06\n      1e+06\n      1e+06\n      1.1e+06\n      1.2e+06\n      1.2e+06\n      1.25e+06\n      1.25e+06\n      1.25e+06\n      1.25e+06\n      1.3e+06\n      ...\n      4.18494e+08\n      4.18494e+08\n      4.19491e+08\n      4.24522e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.32499e+08\n      4.33332e+08\n      4.36002e+08\n      4.37029e+08\n      4.37029e+08\n      4.38722e+08\n      4.52525e+08\n      4.52525e+08\n      4.7703e+08\n      4.90832e+08\n      7.32311e+08\n      8.70737e+08\n      8.70737e+08\n      8.70737e+08\n      8.8224e+08\n      8.92875e+08\n      1.3009e+09\n    \n    \n      salary_hi\n      NaN\n      NaN\n      100000\n      NaN\n      NaN\n      NaN\n      NaN\n      80715\n      NaN\n      NaN\n      110000\n      110000\n      1.4e+06\n      1.4e+06\n      1.4e+06\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      160000\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_valid\n      True\n      True\n      False\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      -1.5\n      NaN\n      NaN\n      NaN\n      NaN\n      -1.6001\n      NaN\n      NaN\n      -1.54167\n      -1.54167\n      0.333333\n      0.333333\n      0.333333\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -1.56164\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n30 rows × 169 columns\n\n\n\nBelow 15 it’s mostly garbage\n\ndf[df.salary_low < 15].sort_values(\"salary_low\", ascending=False).T\n\n\n\n\n\n  \n    \n      \n      22495\n      70099\n      33446\n      56590\n      4756\n      35215\n      34908\n      70436\n      34342\n      27168\n      35941\n      40049\n      33740\n      32932\n      60928\n      60179\n      18705\n      12027\n      10360\n      34056\n      35491\n      58947\n      40428\n      54330\n      36064\n      ...\n      21332\n      32918\n      34321\n      21477\n      38223\n      53332\n      20340\n      60406\n      73404\n      62523\n      55618\n      71605\n      72094\n      72335\n      20251\n      60471\n      30895\n      53743\n      57695\n      65419\n      65418\n      64792\n      59921\n      60391\n      36538\n    \n  \n  \n    \n      title\n      Fundraising and Support Officer\n      Apprentice Gardener\n      Lifecycle Marketing Manager\n      Project Engineer - Substations\n      C# .NET Software Developer\n      Direct Product Manager - Insurance\n      Lifecycle Marketing Manager\n      Junior Java Software Developer\n      Executive Manager of Product and Services\n      Senior Test Analyst - 12m Fixed Term\n      Direct Product Manager - Insurance\n      Business Operations Manager\n      Direct Product Manager - Insurance\n      Executive Manager of Product and Services\n      Diplomatic Safety Assurance and Audit Leader\n      Sponsorship & Events Producer\n      Senior Tunnel Engineer\n      Senior Tunnel Engineer\n      C# .NET Software Developer\n      Test post from idibu\n      Test post from idibu\n      Customer Support Team Leader\n      Refrigeration Apprenticeship\n      Looking for a 1st or 2nd year Landscape Apprentice\n      Fulfillment Centre Operations Manager - Night Shifts\n      ...\n      Looking for female cleaner and cook\n      SAP Customer Management\n      SAP Customer Management\n      Graduate Accountant Trainee Internship 3 months\n      Behaviour Support Practitioner\n      Senior ServiceNow Technical Consultant / Developer\n      Living Skills Support Worker\n      Senior Occupational Therapist - General Medicine and Surgery\n      Executive Director - People & Performance\n      Air Track Driller - Contract role\n      Light Vehicle Fitter - Mt Morgans\n      Chief Engineer - Department of Planning, Transport and Infrastructure\n      Chief Engineer - Department of Planning, Transport and Infrastructure\n      Chief Engineer - Department of Planning, Transport and Infrastructure\n      DFV Residential Support Worker\n      Senior Manager - Product Communications\n      Cadet Program 2020 - Information Technology Stream, Sydney\n      Construction Lawyer\n      Business Analyst\n      Project Lawyer\n      Project Lawyer\n      VR GP - Melbourne\n      ServiceNow Professional? Looking For a New Role?\n      Hospital Physiotherapist - Rural Generalist - permanent - July/August start date\n      Animal Attendant\n    \n    \n      description\n      About Canteen We help young people cope with cancer in their family. Through CanTeen, they learn to explore and deal with their feelings about can...\n      \n      Our Client\\nBased in the CBD, our client is a global eCommerce provider and a household name for consumers to sell and purchase online. They provi...\n      \n      My client is looking for experienced .NET Developers to join their agile environment working on multiple complex systems.\\nWork on high traffic, p...\n      Direct Product Manager required to work on a large Insurance integration. This role requires a Product Manager with Insurance experience to drive,...\n      Our Client\\nBased in the CBD, our client is a global eCommerce provider and a household name for consumers to sell and purchase online. They provi...\n      \n      A national Telco company has a senior position available for an Executive Manager to lead the end to end product management of their key products ...\n      H2R Technology has an amazing opportunity for a Senior Test Analyst to join a dynamic team responsible for providing testing solutions for everyth...\n      Direct Product Manager required to work on a large Insurance integration. This role requires a Product Manager with Insurance experience to drive,...\n      ​​​​​​ About Us\\nWebcentral Group is Australia’s largest full-service digital services partner for small and medium businesses. We’ve supported th...\n      Direct Product Manager required to work on a large Insurance integration. This role requires a Product Manager with Insurance experience to drive,...\n      A national Telco company has a senior position available for an Executive Manager to lead the end to end product management of their key products ...\n      \n      \n      The Company\\nWith offices across the globe, this top tier engineering design consultancy are at the forefront of design.  They have an outstanding...\n      The Company\\nWith offices across the globe, this top tier engineering design consultancy are at the forefront of design.  They have an outstanding...\n      My client is looking for experienced .NET Developers to join their agile environment working on multiple complex systems.\\nWork on high traffic, p...\n      This is a test post from idibu\\n\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test p...\n      This is a test post from idibu\\n\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test p...\n      \n      Ford and Doonan Air Conditioning Bunbury are seeking a full time apprentice refrigeration mechanic to add to our team.\\nPrinciple Duties/ Responsi...\n      -Our company specializes in all aspects of Residential landscaping projects. -You will be required to have your drivers license and vehicle. -Yo...\n      My client is seeking an experienced, Operations Manager to join our Customer Fulfillment team in Dandenong\\nKey points of interest:\\n\\nNight shift...\n      ...\n      We are busy couple. Looking for someone female from Bangladeshi India Nepal or Pakistani to clean our unit 3 days every week and helping cooking I...\n      SAP Customer Management – SAP CRM – SAP IS-U – 12-month initial contract\\nThis is a key role is a large SAP implementation and Business Transforma...\n      SAP Customer Management – SAP CRM – SAP IS-U – 12-month initial contract\\nThis is a key role is a large SAP implementation and Business Transforma...\n      We are a Bookkeeping practice assist small and medium clients around Australia who use could accounting software such as Xero, MYOB and QBO. Due t...\n      We believe : Everyone belongs\\nAbout us\\nFor almost 30 years, Interchange has supported West Australians with disability to achieve their goals th...\n      \n      About the role\\n\\nCasual positions available\\n\\nSutherland Shire Location\\n\\nSCHCADS Level 2.1 – $34.17 hourly rate inclusive of casual loading\\n\\...\n      \n      Opportunity to work with a diverse and talented team to set and deliver the people agenda\\nOngoing, full-time role\\nFlexible NSW location\\nTRP fro...\n      \n      Macmahon is a leading Australian contract mining company with projects throughout Australia and South East Asia. Delivering a comprehensive range ...\n      Deliver Expert Advice | Major Infrastructure Projects | F/T or P/T\\nOur client, the Department of Planning, Transport and Infrastructure plays a c...\n      Deliver Expert Advice | Major Infrastructure Projects | F/T or P/T\\nOur client, the Department of Planning, Transport and Infrastructure plays a c...\n      Deliver Expert Advice | Major Infrastructure Projects | F/T or P/T\\nOur client, the Department of Planning, Transport and Infrastructure plays a c...\n      About the role\\nThe Opportunity\\nWe currently have a part-time opportunity for 12 months for a DFV Residential Support Worker to provide crisis re...\n      \n      Applications are now open for our 2020 Cadet Program! Are you ready to grow your career and take on new challenges?  Join us!  Right now we’re del...\n      The Firm This is one of the largest firms Australia-wide in terms of size, yet they continue to experience unprecedented growth. You will be invol...\n      \n      \n      \n      \n      \n      \n      The Royal Society for Prevention of Cruelty to Animals (RSPCA) NSW is Australia's pre-eminent animal welfare organisation. We provide a strong voi...\n    \n    \n      uri\n      https://www.seek.com.au/job/39856340\n      https://www.seek.com.au/job/37563482\n      https://jobs.launchrecruitment.com.au/job/100602033360111/lifecycle-marketing-manager/\n      https://www.seek.com.au/job/36740858\n      https://www.davidsonwp.com/job/c-number-net-software-developer-1/\n      https://jobs.launchrecruitment.com.au/job/100602033391109/direct-product-manager-insurance/\n      https://jobs.launchrecruitment.com.au/job/100602033360111/lifecycle-marketing-manager/\n      https://www.seek.com.au/job/37563519\n      https://jobs.launchrecruitment.com.au/job/100602033250174/executive-manager-of-product-and-services/\n      https://www.seek.co.nz/job/40236720\n      https://jobs.launchrecruitment.com.au/job/100602033391109/direct-product-manager-insurance/\n      https://www.seek.com.au/job/50224723?type=standout\n      https://jobs.launchrecruitment.com.au/job/100602033391109/direct-product-manager-insurance/\n      https://jobs.launchrecruitment.com.au/job/100602033250174/executive-manager-of-product-and-services/\n      https://www.seek.com.au/job/36500995\n      https://www.seek.com.au/job/36554264\n      https://www.cgcrecruitment.com/job/senior-tunnel-engineer/\n      https://www.cgcrecruitment.com/job/senior-tunnel-engineer/\n      https://www.davidsonwp.com/job/c-number-net-software-developer-1/\n      https://jobs.launchrecruitment.com.au/job/test-post-from-idibu/\n      https://jobs.launchrecruitment.com.au/job/test-post-from-idibu/\n      https://www.seek.com.au/job/37751145\n      https://www.seek.com.au/job/50696090?type=standard\n      https://www.seek.com.au/job/36031584\n      https://jobs.launchrecruitment.com.au/job/fulfillment-centre-operations-manager-night-shifts/\n      ...\n      https://www.gumtree.com.au/s-ad/lakemba/other/looking-for-female-cleaner-and-cook/1248628669\n      https://jobs.launchrecruitment.com.au/job/100602033243699/sap-customer-management/\n      https://jobs.launchrecruitment.com.au/job/100602033243699/sap-customer-management/\n      https://www.gumtree.com.au/s-ad/hebersham/bookkeeping-small-practice-accounting/graduate-accountant-trainee-internship-3-months/1259904626\n      https://www.seek.com.au/job/50951351?type=standout\n      https://www.seek.com.au/job/37116326\n      https://probonoaustralia.com.au/jobs/2020/03/living-skills-support-worker/\n      https://www.seek.com.au/job/36552442\n      https://iworkfor.nsw.gov.au/job/executive-director-people-performance-200494\n      https://www.seek.com.au/job/36789249\n      https://www.seek.com.au/job/36029884\n      https://www.engineeringjobs.com.au/job/chief-engineer-department-of-planning-transport-and-infrastructure/\n      https://www.engineeringjobs.com.au/job/chief-engineer-department-of-planning-transport-and-infrastructure/\n      https://www.engineeringjobs.com.au/job/chief-engineer-department-of-planning-transport-and-infrastructure/\n      https://probonoaustralia.com.au/jobs/2020/02/dfv-residential-support-worker/\n      https://www.seek.com.au/job/36554586\n      https://www.seek.com.au/job/40092108\n      https://www.seek.com.au/job/36226243\n      https://www.seek.com.au/job/36673983\n      https://www.seek.com.au/job/36749366\n      https://www.seek.com.au/job/36749353\n      https://www.seek.com.au/job/37438675\n      https://www.seek.com.au/job/36551707\n      https://www.seek.com.au/job/36551261\n      https://www.seek.com.au/job/41348948?type=standout\n    \n    \n      view_date\n      2019-09-04 08:19:23\n      2018-10-26 04:32:05\n      2020-07-16 05:44:09+00:00\n      2018-07-19 06:10:01\n      2020-04-02 23:30:16+00:00\n      2020-04-05 23:32:21+00:00\n      2020-04-09 15:42:31+00:00\n      2018-10-26 04:46:59\n      2020-04-07 00:39:07+00:00\n      2019-10-26 11:33:44\n      2020-09-29 17:32:37+00:00\n      2020-08-05 22:08:45+00:00\n      2020-07-08 12:44:57+00:00\n      2020-07-05 17:07:59+00:00\n      2018-06-19 01:36:16\n      2018-06-25 16:07:24\n      2020-10-27 22:21:39+00:00\n      2020-06-05 16:21:23+00:00\n      2020-10-24 00:31:19+00:00\n      2020-07-05 17:04:34+00:00\n      2020-04-07 00:35:49+00:00\n      2018-11-21 02:14:29\n      2020-10-24 09:29:22+00:00\n      2018-04-20 02:55:48\n      2020-09-29 01:24:17+00:00\n      ...\n      2020-05-24 23:19:10+00:00\n      2020-07-05 06:05:36+00:00\n      2020-04-04 06:30:27+00:00\n      2020-12-03 08:58:08+00:00\n      2020-11-29 06:17:59+00:00\n      2018-09-02 00:33:45\n      2020-03-31 05:13:59+00:00\n      2018-06-25 17:14:28\n      2020-07-04 09:05:56+00:00\n      2018-07-24 08:35:06\n      2018-04-20 03:27:20\n      2020-07-05 20:47:04+00:00\n      2020-03-30 09:12:28+00:00\n      2020-09-26 22:02:48+00:00\n      2020-03-29 07:41:19+00:00\n      2018-06-25 16:26:41\n      2019-10-06 06:56:57\n      2018-05-16 03:39:19\n      2018-07-11 04:06:23\n      2018-07-19 02:03:59\n      2018-07-19 02:25:19\n      2018-10-11 00:57:01\n      2018-06-25 17:36:01\n      2018-06-25 17:31:21\n      2020-06-01 16:02:47+00:00\n    \n    \n      org\n      CanTeen\n      Treetops garden maintenance\n      Launch Recruitment\n      Metro Trains Melbourne\n      Davidson\n      Launch Recruitment\n      Launch Recruitment\n      The Citadel Group\n      Launch Recruitment\n      H2R Consulting\n      Launch Recruitment\n      Webcentral Group\n      Launch Recruitment\n      Launch Recruitment\n      Safety Australia Group\n      Screenwest Australia Limited\n      CGC Recruitment\n      CGC Recruitment\n      Davidson\n      Launch Recruitment\n      Launch Recruitment\n      Johns Lyng Group\n      Ford and Doonan Air Conditioning Bunbury\n      BDW Landscapes\n      Launch Recruitment\n      ...\n      None\n      Launch Recruitment\n      Launch Recruitment\n      None\n      Interchange Inc.\n      GMT People\n      St Vincent De Paul Society\n      Alfred Health\n      People Performance & Culture\n      Taurus Recruitment\n      Macmahon Contractors\n      Stillwell Management Consultants\n      Stillwell Management Consultants\n      Stillwell Management Consultants\n      Open Support\n      GM Holden\n      Transport for NSW\n      Naiman Clarke\n      Michael Page Technology\n      Naiman Clarke\n      Naiman Clarke\n      Global Medics Australia\n      Washington Frank\n      Allied Health\n      RSPCA NSW\n    \n    \n      salary_raw\n      Salary + Super | Full time contract til 13 Dec 19\n      $12.75 P/H\n      12 month contract\n      12 month, fixed-term contract\n      12 Mth Fixed Term - Salary Based\n      12 month contract\n      12 month contract\n      Excellent rates!\\n12 month contract\n      12-Month Contract\n      12 Month Fixed Term Contract\n      12 month contract\n      Competitive Package + 12-month cover\n      12 month contract\n      12-Month Contract\n      Attractive Rate - 12 month contract and may extend\n      12 month fixed term contract\n      $12-140k\n      $12-140k\n      12 Mth Fixed Term - Salary Based\n      $11 - $111 per annum\n      $11 - $111 per annum\n      11 Month Contract with Career Growth Opportunities\n      $10 - $14.99 per hour\n      $10 - $14.99 per hour\n      Hourly rate, 4 day weeks, Shifts 9.5 hours per day\n      ...\n      3 Or 4 Hours\n      3 year Transformation Project\n      3 year Transformation Project\n      After completion of 3 months\n      SCHADS award Level 3-4\n      3 months initially\n      CHCADS Level 2.1 - $34.17 hourly rate inclusive of casual loading\n      Occupational Therapist Grade 2 or 3\n      PSSE Band 2\n      Short term contract approx 2 months.\n      Mt Morgans Gold Mine Project; 2:1 Roster\n      SAES 1 Level\n      SAES 1 Level\n      SAES 1 Level\n      SCHADS Level 1\n      1 RDO a month. On-site Gym. Discount vehicles\n      Transport Grade 1 $51,666 p.a. + super (prorata)\n      Tier 1 team! Growth role!\n      1-1\n      Tier 1 training + relocation package\n      Tier 1 training + relocation package\n      Up to $1 p.a.\n      Up to $1 p.a.\n      Grade 1 or Grade 2\n      Grade 1\n    \n    \n      salary_min\n      13\n      12.75\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      11\n      11\n      11\n      10\n      10\n      9.5\n      ...\n      3\n      3\n      3\n      3\n      3\n      3\n      2.1\n      2\n      2\n      2\n      2\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      salary_max\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      140000\n      140000\n      NaN\n      111\n      111\n      NaN\n      14.99\n      14.99\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4\n      NaN\n      34.17\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_hours\n      NaN\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2000\n      2000\n      NaN\n      1\n      1\n      1\n      ...\n      NaN\n      2000\n      2000\n      NaN\n      NaN\n      NaN\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2000\n      NaN\n      NaN\n      NaN\n      NaN\n      2000\n      2000\n      NaN\n      NaN\n    \n    \n      location_raw\n      Sydney, New South Wales, Australia\n      Sydney, North West & Hills District, AU\n      Sydney, AU\n      Melbourne, Bayside & South Eastern Suburbs, AU\n      Melbourne CBD,  Victoria, AU\n      Sydney, AU\n      Sydney, AU\n      ACT, AU\n      North Sydney, AU\n      Wellington, New Zealand, New zealand\n      Sydney, AU\n      melbourne, Melbourne, Victoria, Australia\n      Sydney, AU\n      North Sydney, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Perth, CBD, Inner & Western Suburbs, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne CBD,  Victoria, AU\n      Sydney, Nova Scotia, CA\n      Sydney, Nova Scotia, CA\n      Sydney, Parramatta & Western Suburbs, AU\n      bunbury, Bunbury & South West, Western Australia, Australia\n      Melbourne, CBD & Inner Suburbs, AU\n      Melbourne, AU\n      ...\n      Lakemba, Lakemba New South Wales, Australia\n      Sydney, AU\n      Sydney, AU\n      54 Stevenage rd hebersham\n      perth, Perth, Western Australia, Australia\n      Melbourne, CBD & Inner Suburbs, AU\n      NSW Regional(Sutherland )\n      Melbourne, Eastern Suburbs, AU\n      Statewide\n      Lismore & Far North Coast, AU\n      Kalgoorlie, Goldfields & Esperance, AU\n      Adelaide, AU\n      Adelaide, AU\n      Adelaide, AU\n      Sydney(South West Sydney)\n      Melbourne, CBD & Inner Suburbs, AU\n      Sydney, New South Wales, Australia\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Melbourne, Eastern Suburbs, AU\n      Perth, CBD, Inner & Western Suburbs, AU\n      Brisbane, CBD & Inner Suburbs, AU\n      Melbourne, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Melbourne, CBD & Inner Suburbs, AU\n      yagoona, Sydney, New South Wales, Australia\n    \n    \n      loc_id\n      1.01932e+08\n      1.01932e+08\n      1.01932e+08\n      1.01933e+08\n      8.57823e+07\n      1.01932e+08\n      1.01932e+08\n      8.56815e+07\n      1.02049e+08\n      8.56872e+07\n      1.01932e+08\n      1.01933e+08\n      1.01932e+08\n      1.02049e+08\n      8.57823e+07\n      8.57823e+07\n      1.01933e+08\n      1.01933e+08\n      8.57823e+07\n      NaN\n      NaN\n      1.01932e+08\n      1.01937e+08\n      8.57823e+07\n      1.01933e+08\n      ...\n      1.01931e+08\n      1.01932e+08\n      1.01932e+08\n      1.01931e+08\n      1.01938e+08\n      8.57823e+07\n      NaN\n      1.01933e+08\n      8.56815e+07\n      1.02079e+08\n      1.02049e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01932e+08\n      8.57823e+07\n      1.01932e+08\n      8.57823e+07\n      1.01933e+08\n      8.57823e+07\n      8.57823e+07\n      1.01933e+08\n      8.57823e+07\n      8.57823e+07\n      1.01932e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      ...\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      New Zealand\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      New Zealand\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Sydney\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      None\n      North Sydney\n      None\n      Sydney\n      Melbourne\n      Sydney\n      North Sydney\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      None\n      Sydney\n      Bunbury\n      Melbourne\n      Melbourne\n      ...\n      Canterbury\n      Sydney\n      Sydney\n      Blacktown\n      Perth\n      Melbourne\n      None\n      Melbourne\n      None\n      Far North District\n      Esperance\n      Adelaide\n      Adelaide\n      Adelaide\n      Sydney\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Bankstown\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      Sydney\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      None\n      None\n      None\n      Sydney\n      Melbourne\n      Sydney\n      None\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      None\n      Sydney\n      Bunbury\n      Melbourne\n      Melbourne\n      ...\n      Lakemba\n      Sydney\n      Sydney\n      Hebersham\n      Perth\n      Melbourne\n      None\n      Melbourne\n      None\n      None\n      None\n      Adelaide\n      Adelaide\n      Adelaide\n      Sydney\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Yagoona\n    \n    \n      loc_locality\n      Sydney\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      None\n      None\n      None\n      Sydney\n      Melbourne\n      Sydney\n      None\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      None\n      Sydney\n      Bunbury\n      Melbourne\n      Melbourne\n      ...\n      Lakemba\n      Sydney\n      Sydney\n      Hebersham\n      Perth\n      Melbourne\n      None\n      Melbourne\n      None\n      None\n      None\n      Adelaide\n      Adelaide\n      Adelaide\n      Sydney\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Yagoona\n    \n    \n      loc_macrocounty\n      Sydney\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      None\n      Sydney\n      None\n      Sydney\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      None\n      Sydney\n      Bunbury\n      Melbourne\n      Melbourne\n      ...\n      Sydney\n      Sydney\n      Sydney\n      Sydney\n      Perth\n      Melbourne\n      None\n      Melbourne\n      None\n      None\n      None\n      Adelaide\n      Adelaide\n      Adelaide\n      Sydney\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Perth\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n    \n    \n      loc_region\n      New South Wales\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      New South Wales\n      New South Wales\n      Australian Capital Territory\n      New South Wales\n      Wellington Region\n      New South Wales\n      Victoria\n      New South Wales\n      New South Wales\n      Victoria\n      Western Australia\n      Victoria\n      Victoria\n      Victoria\n      None\n      None\n      New South Wales\n      Western Australia\n      Victoria\n      Victoria\n      ...\n      New South Wales\n      New South Wales\n      New South Wales\n      New South Wales\n      Western Australia\n      Victoria\n      None\n      Victoria\n      New South Wales\n      Northland Region\n      Western Australia\n      South Australia\n      South Australia\n      South Australia\n      New South Wales\n      Victoria\n      New South Wales\n      Victoria\n      Victoria\n      Western Australia\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n    \n    \n      processor\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      launchrecruitment\n      kaggle_promptcloud_listings\n      davidsonwp\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_listings\n      launchrecruitment\n      kaggle_promptcloud_latest\n      launchrecruitment\n      seek\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      cgcrecruitment\n      cgcrecruitment\n      davidsonwp\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_listings\n      seek\n      kaggle_promptcloud_listings\n      launchrecruitment\n      ...\n      gumtree\n      launchrecruitment\n      launchrecruitment\n      gumtree\n      seek\n      kaggle_promptcloud_listings\n      probono\n      kaggle_promptcloud_listings\n      iworkfornsw\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      engineeringjobs\n      engineeringjobs\n      engineeringjobs\n      probono\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      seek\n    \n    \n      source\n      seekau_2019q3\n      seekau\n      CC-MAIN-2020-29\n      seekau\n      CC-MAIN-2020-16\n      CC-MAIN-2020-16\n      CC-MAIN-2020-16\n      seekau\n      CC-MAIN-2020-16\n      seekau_2019q3\n      CC-MAIN-2020-40\n      CC-MAIN-2020-34\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      seekau\n      seekau\n      CC-MAIN-2020-45\n      CC-MAIN-2020-24\n      CC-MAIN-2020-45\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      seekau\n      CC-MAIN-2020-45\n      seekau\n      CC-MAIN-2020-40\n      ...\n      CC-MAIN-2020-24\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      CC-MAIN-2020-50\n      CC-MAIN-2020-50\n      seekau\n      CC-MAIN-2020-16\n      seekau\n      CC-MAIN-2020-29\n      seekau\n      seekau\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      CC-MAIN-2020-40\n      CC-MAIN-2020-16\n      seekau\n      seekau_2019q3\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      seekau\n      CC-MAIN-2020-24\n    \n    \n      loc_neighbourhood\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      Perth CBD\n      NaN\n      NaN\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      ...\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      NaN\n      None\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      Melbourne CBD\n      None\n      Perth CBD\n      Melbourne CBD\n      None\n      Melbourne CBD\n      Melbourne CBD\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      False\n      False\n      False\n      False\n      ...\n      False\n      True\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      True\n      False\n      False\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      True\n      True\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      13\n      12.75\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n      11\n      11\n      11\n      10\n      10\n      9.5\n      ...\n      3\n      3\n      3\n      3\n      3\n      3\n      2.1\n      2\n      2\n      2\n      2\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      salary_hi\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      140000\n      140000\n      NaN\n      111\n      111\n      NaN\n      14.99\n      14.99\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      4\n      NaN\n      34.17\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      True\n      False\n      False\n      True\n      True\n      True\n      True\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.99966\n      1.99966\n      NaN\n      1.63934\n      1.63934\n      NaN\n      0.39936\n      0.39936\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      0.285714\n      NaN\n      1.7684\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n30 rows × 127 columns\n\n\n\n\nSALARY_MIN_AUD = 15\nSALARY_MAX_AUD = 500_000\n\n\ndf.loc[df.salary_low < SALARY_MIN_AUD, \"salary_valid\"] = False\ndf.loc[df.salary_low > SALARY_MAX_AUD, \"salary_valid\"] = False\n\n\nYearly\n\n%time sns.displot(df[df.annual], x=\"salary_low\", log_scale=True, kde=True)\n\nCPU times: user 629 ms, sys: 69.1 ms, total: 698 ms\nWall time: 593 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc9f14ac0>\n\n\n\n\n\nMost of these are data mistakes\n\nSALARY_ANNUAL_MIN_AUD = 20000\n\n\ndf[(df.salary_low < SALARY_ANNUAL_MIN_AUD) & df.annual].sort_values(\"salary_low\", ascending=False).T\n\n\n\n\n\n  \n    \n      \n      10461\n      8542\n      10890\n      62274\n      3696\n      1673\n      9121\n      7153\n      6617\n      26908\n      6616\n      6561\n      4555\n      2269\n      10082\n      6553\n      979\n      61982\n      9210\n      19168\n      20004\n      4493\n      947\n      10047\n      10017\n      ...\n      70814\n      66290\n      24791\n      31959\n      69580\n      62304\n      56684\n      34056\n      35491\n      66166\n      20657\n      32237\n      20630\n      54428\n      20658\n      68051\n      68537\n      54476\n      61120\n      20837\n      32918\n      34321\n      59921\n      64792\n      30895\n    \n  \n  \n    \n      title\n      Director of Client Services /  Financial Controller\n      Safety Systems Engineer\n      Safety Systems Engineer\n      Digital Success Consultant - Education & Online Learning\n      Senior Project Manager - Developer, Construction\n      Senior Project Manager - Developer, Construction\n      Senior Project Manager - Developer, Construction\n      Installation Supervisor (5months)\n      Technical Project Manager\n      4 Assurance Mgrs- 6 Month Max Term contracts - $125k package\n      Manager, Executive Services (Chief of Staff)\n      Executive Manager\n      Manager, Executive Services (Chief of Staff)\n      Manager, Executive Services (Chief of Staff)\n      Manager, Executive Services (Chief of Staff)\n      Executive Manager\n      Manager, Executive Services (Chief of Staff)\n      Systems / DevOps Engineer - MS Azure skills required\n      PMO Analyst\n      Clinical Educator\n      Solicitor for NSW and Tasmania\n      BI Specialist\n      BI Specialist\n      BI Specialist\n      BI Specialist\n      ...\n      Customer Care Representative\n      Customer Servce Officer\n      EOI - General Laborers Required\n      Electrical Trade Assistant\n      Experienced Civil Workers\n      Machine op / Forklift driver\n      Traffic Controllers\n      Test post from idibu\n      Test post from idibu\n      Crew Leader\n      Lead Practitioner Children & Family Services\n      Registered Nurse\n      Case Manager Community Mental Health\n      Lead .Net Developer\n      Men’s Behaviour Change Program Facilitators x 2\n      Tyre Fitting Traineeship - Hawthorn\n      Tyre Fitting Traineeship - Chadstone\n      2nd or 3rd Year Carpentry Apprenticeship\n      Gas Supply Apprenticeships - Melb\n      Lawyer – Aboriginal Families Practice Team\n      SAP Customer Management\n      SAP Customer Management\n      ServiceNow Professional? Looking For a New Role?\n      VR GP - Melbourne\n      Cadet Program 2020 - Information Technology Stream, Sydney\n    \n    \n      description\n      About the Company\\nMy client is a bespoke outsourced finance function for small to medium enterprises which includes reporting, virtual CFO functi...\n      About the Company\\nDavidson Technology have partnered with a multi-national defence, security and aerospace company who are building one of the wo...\n      About the Company\\nDavidson Technology have partnered with a multi-national defence, security and aerospace company who are building one of the wo...\n      \n      About the Company\\n\\nIndustrial developer which develops and manages real estate on behalf of key clients\\nPublicly listed, highly regarded global...\n      About the Company\\n\\nIndustrial developer which develops and manages real estate on behalf of key clients\\nPublicly listed, highly regarded global...\n      About the Company\\n\\nIndustrial developer which develops and manages real estate on behalf of key clients\\nPublicly listed, highly regarded global...\n      The Company\\nDavidson is delighted to be partnering with Goldwind, one of the world's leading wind power companies in their search for an Installa...\n      About the Company\\nOur client is a multinational construction and infrastructure company they help people, clients, and communities do great thing...\n      This leading bank is seeking 4 Assurance professionals with excellent stakeholder management skills to join their Line 2 Assurance team on 6 month...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      A unique and rewarding opportunity has become available to join a growing, evolving and well-respected Not for Profit organisation. You will step ...\n      \n      About the Company\\nOne of Ausrtalia's largest professional membership organisations.\\nAbout the Role\\nThis role has two aspects to it...\\n\\nTo hel...\n      About the role\\n\\nNFP Salary Packaging options available\\nLeading mental health organisation\\nParkville location with onsite parking + close to pu...\n      About the role\\nSUMMARY\\n\\nMust have or soon will have at least 3 years post-admission experience within Australia. Experience in New South Wale a...\n      Our client have their headquarters based in Brisbane, giving you access to global professional development and career opportunities!\\nFantastic op...\n      Our client have their headquarters based in Brisbane, giving you access to global professional development and career opportunities!\\nFantastic op...\n      Our client have their headquarters based in Brisbane, giving you access to global professional development and career opportunities!\\nFantastic op...\n      Our client have their headquarters based in Brisbane, giving you access to global professional development and career opportunities!\\nFantastic op...\n      ...\n      \n      \n      Our clients are based in Brisbane Southern Suburbs /Logan Area. We are looking for labourers to work on on-call, on-going and adhoc roles. Candida...\n      Electrical Trade Assistant Amazing opportunity to work with a fast-growing New Zealand companyGain valuable skills in a company who puts an emphas...\n      Civil Construction / Drainage Workers Wanted Urgently We are looking for experienced civil construction workers of all levels to join one of New Z...\n      \n      \n      This is a test post from idibu\\n\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test p...\n      This is a test post from idibu\\n\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test post from idibu\\nThis is a test p...\n      \n      About the role\\nWe have a newly created position for a Lead Practitioner to join our Berrimah team within CatholicCare NT. This position plays a k...\n      No Night Shifts!  A full paid 6 week induction and training program Be part of our community (Free Gym onsite, monthly BBQ and more)   Are you loo...\n      About the role\\n\\nSCHADS Award Level 5 – $84,989 trp (including salary $76,673 per annum)\\nSalary Packaging Option (up to an additional $15,909.00...\n      The Job As a key member of our Trading Solutions function at Sportsbet, you will be responsible for leading a small team of high performing engine...\n      About the role\\n\\n2 x Casual positions, up to 10 hours per week\\n\\nWe have an exciting opportunity for two Men’s Behaviour Change Program Facilita...\n      \n      \n      About the Company MEGT is an organisation that uses the group training model. As your employer, we will manage your apprenticeship, provide you wi...\n      \n      About the role\\nOverview\\nTo provide high quality legal services, including information, advice, representation, casework, negotiation and dispute...\n      SAP Customer Management – SAP CRM – SAP IS-U – 12-month initial contract\\nThis is a key role is a large SAP implementation and Business Transforma...\n      SAP Customer Management – SAP CRM – SAP IS-U – 12-month initial contract\\nThis is a key role is a large SAP implementation and Business Transforma...\n      \n      \n      Applications are now open for our 2020 Cadet Program! Are you ready to grow your career and take on new challenges?  Join us!  Right now we’re del...\n    \n    \n      uri\n      https://www.davidsonwp.com/job/director-of-client-services-slash-financial-controller/\n      https://www.davidsonwp.com/job/safety-systems-engineer/\n      https://www.davidsonwp.com/job/safety-systems-engineer/\n      https://www.seek.com.au/job/36500923\n      https://www.davidsonwp.com/job/100562133257277/senior-project-manager-developer-construction/\n      https://www.davidsonwp.com/job/100562133257277/senior-project-manager-developer-construction/\n      https://www.davidsonwp.com/job/100562133257277/senior-project-manager-developer-construction/\n      https://www.davidsonwp.com/job/installation-supervisor-5months/\n      https://www.davidsonwp.com/job/100562133367068/technical-project-manager-1/\n      https://www.seek.com.au/job/39894794\n      https://www.davidsonwp.com/job/100562133367048/manager-executive-services-chief-of-staff-1/\n      https://www.davidsonwp.com/job/100562133364046/executive-manager-1/\n      https://www.davidsonwp.com/job/100562133367025/manager-executive-services-chief-of-staff/\n      https://www.davidsonwp.com/job/100562133367025/manager-executive-services-chief-of-staff/\n      https://www.davidsonwp.com/job/100562133367025/manager-executive-services-chief-of-staff/\n      https://www.davidsonwp.com/job/100562133363681/executive-manager/\n      https://www.davidsonwp.com/job/100562133367048/manager-executive-services-chief-of-staff-1/\n      https://www.seek.com.au/job/36500248\n      https://www.davidsonwp.com/job/100562133263296/pmo-analyst/\n      https://probonoaustralia.com.au/jobs/2019/09/clinical-educator/\n      https://probonoaustralia.com.au/jobs/2020/10/solicitor-for-nsw-and-tasmania/\n      https://www.davidsonwp.com/job/100562133360565/bi-specialist-2/\n      https://www.davidsonwp.com/job/100562133361996/bi-specialist-3/\n      https://www.davidsonwp.com/job/100562133361996/bi-specialist-3/\n      https://www.davidsonwp.com/job/100562133360565/bi-specialist-2/\n      ...\n      https://www.seek.com.au/job/37563347\n      https://www.seek.com.au/job/37211622\n      https://www.seek.com.au/job/40042253\n      https://www.seek.co.nz/job/40273289\n      https://www.seek.co.nz/job/35911503\n      https://www.seek.com.au/job/36499986\n      https://www.seek.co.nz/job/37343370\n      https://jobs.launchrecruitment.com.au/job/test-post-from-idibu/\n      https://jobs.launchrecruitment.com.au/job/test-post-from-idibu/\n      https://www.seek.com.au/job/37213523\n      https://probonoaustralia.com.au/jobs/2019/09/lead-practitioner-children-family-services/\n      https://www.seek.com.au/job/40146581\n      https://probonoaustralia.com.au/jobs/2019/09/case-manager-community-mental-health-2/\n      https://www.seek.com.au/job/36033974\n      https://probonoaustralia.com.au/jobs/2019/09/mens-behaviour-change-program-facilitators-x-2/\n      https://www.seek.com.au/job/37598516\n      https://www.seek.com.au/job/37598531\n      https://www.seek.com.au/job/36029790\n      https://www.seek.com.au/job/36496918\n      https://probonoaustralia.com.au/jobs/2020/01/lawyer-aboriginal-families-practice-team/\n      https://jobs.launchrecruitment.com.au/job/100602033243699/sap-customer-management/\n      https://jobs.launchrecruitment.com.au/job/100602033243699/sap-customer-management/\n      https://www.seek.com.au/job/36551707\n      https://www.seek.com.au/job/37438675\n      https://www.seek.com.au/job/40092108\n    \n    \n      view_date\n      2020-10-30 12:10:24+00:00\n      2020-08-08 07:23:54+00:00\n      2020-10-25 05:17:32+00:00\n      2018-06-19 01:13:09\n      2020-04-05 09:29:40+00:00\n      2020-07-12 08:56:32+00:00\n      2020-10-22 06:37:25+00:00\n      2020-09-27 09:23:33+00:00\n      2020-09-27 18:44:42+00:00\n      2019-09-09 09:56:47\n      2020-09-29 02:36:37+00:00\n      2020-09-23 13:26:54+00:00\n      2020-04-05 09:56:10+00:00\n      2020-07-10 18:54:50+00:00\n      2020-10-29 03:08:18+00:00\n      2020-09-27 04:32:33+00:00\n      2020-05-30 10:16:55+00:00\n      2018-06-19 01:43:39\n      2020-10-22 23:28:02+00:00\n      2020-05-29 10:07:49+00:00\n      2020-12-04 03:48:55+00:00\n      2020-03-30 03:19:37+00:00\n      2020-05-29 02:34:13+00:00\n      2020-10-25 10:40:59+00:00\n      2020-10-20 10:07:22+00:00\n      ...\n      2018-10-26 04:37:05\n      2018-09-13 01:09:35\n      2019-09-27 07:27:25\n      2019-10-30 08:58:41\n      2018-04-06 08:08:16\n      2018-06-19 02:02:22\n      2018-09-29 03:10:45\n      2020-07-05 17:04:34+00:00\n      2020-04-07 00:35:49+00:00\n      2018-09-13 00:41:52\n      2020-08-07 16:45:47+00:00\n      2019-10-13 09:52:59\n      2020-08-07 17:23:14+00:00\n      2018-04-20 02:23:17\n      2020-08-07 17:32:56+00:00\n      2018-10-31 01:51:48\n      2018-10-31 01:52:55\n      2018-04-20 03:29:48\n      2018-06-19 02:39:25\n      2020-08-09 06:45:09+00:00\n      2020-07-05 06:05:36+00:00\n      2020-04-04 06:30:27+00:00\n      2018-06-25 17:36:01\n      2018-10-11 00:57:01\n      2019-10-06 06:56:57\n    \n    \n      org\n      Davidson\n      Davidson\n      Davidson\n      BMS Sales Performance\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Bluefin Resources Pty Limited\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      Spark Recruitment\n      Davidson\n      Orygen, The National Centre of Excellence in Youth Mental Health\n      Youth Law Australia\n      Davidson\n      Davidson\n      Davidson\n      Davidson\n      ...\n      people2people\n      Page Personnel\n      Adecco\n      Tech 5 - Recruiting for the Built Environment\n      Franklin Smith Ltd\n      Frontline Human Resources\n      OneStaff Palmerston North\n      Launch Recruitment\n      Launch Recruitment\n      YMCA Victoria\n      CatholicCare NT via BigSplash\n      International SOS (Australasia) P/L\n      CatholicCare NT via BigSplash\n      sportsbet.com.au\n      CatholicCare NT via BigSplash\n      MEGT\n      MEGT\n      MEGT\n      MEGT\n      Victorian Aboriginal Legal Service\n      Launch Recruitment\n      Launch Recruitment\n      Washington Frank\n      Global Medics Australia\n      Transport for NSW\n    \n    \n      salary_raw\n      AU$18000 - AU$150000 per annum + Plus Super\n      AU$18000 - AU$200000 per annum + training,commission\n      AU$18000 - AU$200000 per annum + training,commission\n      Up to $100000.00 p.a. + Car + $15 - $20k Commissio\n      $14000 - $165000 per annum\n      $14000 - $165000 per annum\n      $14000 - $165000 per annum\n      AU$13000 - AU$140000 per annum\n      $12000 - $16000 per annum\n      $11500.00 - $140000.00 p.a. + super\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000 - $120000 per annum\n      $10000.00 - $110k p.a.\n      $10000 - $120000 per annum\n      $95,00 - $110,000 per annum (pro rata) (commensurate with skills and experience).\n      $90,00 - $105,000 p.a. (pro rata), negotiable + 9.5% Superannuation + Leave Loading + FBT Salary Packaging available.\n      $7500 - $98550 per annum\n      $7500 - $98550 per annum\n      $7500 - $98550 per annum\n      $7500 - $98550 per annum\n      ...\n      $25 - $30 per annum, Bens: $28.00 + Super\n      $25 - $26 p.a.\n      $24 - $30 p.a.\n      NZD23 - NZD26 per annum\n      $20 - $30 p.h. + 8% Annual leave paid out weekly\n      $19 - $21 p.a.\n      Up to $17 p.a.\n      $11 - $111 per annum\n      $11 - $111 per annum\n      S/S Grade 8 - $68,622 p.a. + super\n      SCHADS Award Level 7 - $99,897 trp (including salary $90,123) Salary Packaging Option (up to an additional $15,895 p/a)\n      Base salary + Super + 6 weeks annual leave\n      SCHADS Award Level 5 - $84,989 trp (including salary $76,673 per annum)\n      5 weeks annual leave + Company bonus\n      SCHADS Award Level 5 - $48.50 per hour, Salary Packaging Option (up to an additional $15,895 p/a)\n      super, 4 weeks annual leave, sick leave,\n      super, 4 weeks annual leave, sick leave,\n      super, 4 weeks annual leave, sick leave,\n      super, 4 weeks annual leave, sick leave,\n      LO Level 3 (year depending on experience) / SCHADS Level 4\n      3 year Transformation Project\n      3 year Transformation Project\n      Up to $1 p.a.\n      Up to $1 p.a.\n      Transport Grade 1 $51,666 p.a. + super (prorata)\n    \n    \n      salary_min\n      18000\n      18000\n      18000\n      15000\n      14000\n      14000\n      14000\n      13000\n      12000\n      11500\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      9500\n      9000\n      7500\n      7500\n      7500\n      7500\n      ...\n      25\n      25\n      24\n      23\n      20\n      19\n      17\n      11\n      11\n      8\n      7\n      6\n      5\n      5\n      5\n      4\n      4\n      4\n      4\n      3\n      3\n      3\n      1\n      1\n      1\n    \n    \n      salary_max\n      150000\n      200000\n      200000\n      20000\n      165000\n      165000\n      165000\n      140000\n      16000\n      140000\n      120000\n      120000\n      120000\n      120000\n      120000\n      120000\n      120000\n      110000\n      120000\n      110000\n      105000\n      98550\n      98550\n      98550\n      98550\n      ...\n      30\n      26\n      30\n      26\n      30\n      21\n      NaN\n      111\n      111\n      68622\n      99897\n      NaN\n      84989\n      NaN\n      48.5\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_hours\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      ...\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n    \n    \n      location_raw\n      Brisbane,  Queensland, AU\n      Melbourne CBD,  Victoria, AU\n      Melbourne CBD,  Victoria, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Goulburn,  New South Wales, AU\n      Melbourne, AU\n      Sydney, New South Wales, Australia\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Brisbane Adelaide Street, AU\n      Sydney, North Shore & Northern Beaches, AU\n      East Melbourne, AU\n      Melbourne(Parkville)\n      Hobart, Sydney, TAS Regional\n      Brisbane, AU\n      Brisbane, AU\n      Brisbane, AU\n      Brisbane, AU\n      ...\n      Sydney, Parramatta & Western Suburbs, AU\n      Perth, Northern Suburbs & Joondalup, AU\n      Brisbane, Queensland, Australia\n      Auckland, New Zealand, New zealand\n      Auckland, NZ\n      Melbourne, Eastern Suburbs, AU\n      Manawatu, Palmerston North, NZ\n      Sydney, Nova Scotia, CA\n      Sydney, Nova Scotia, CA\n      Melbourne, AU\n      Darwin(Berrimah )\n      Sydney, New South Wales, Australia\n      Darwin(Malak)\n      Melbourne, CBD & Inner Suburbs, AU\n      Darwin(Berrimah )\n      Melbourne, Eastern Suburbs, AU\n      Melbourne, Bayside & South Eastern Suburbs, AU\n      Melbourne, AU\n      Melbourne, CBD & Inner Suburbs, AU\n      Melbourne(Preston)\n      Sydney, AU\n      Sydney, AU\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Melbourne, AU\n      Sydney, New South Wales, Australia\n    \n    \n      loc_id\n      1.01934e+08\n      8.57823e+07\n      8.57823e+07\n      8.57823e+07\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.0193e+08\n      1.01933e+08\n      1.01932e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      1.01936e+08\n      NaN\n      1.01933e+08\n      1.01933e+08\n      NaN\n      1.01934e+08\n      1.01934e+08\n      1.01934e+08\n      1.01934e+08\n      ...\n      1.01932e+08\n      1.01938e+08\n      1.01934e+08\n      1.01914e+08\n      1.01914e+08\n      1.01933e+08\n      1.01914e+08\n      NaN\n      NaN\n      1.01933e+08\n      1.01938e+08\n      1.01932e+08\n      1.01939e+08\n      8.57823e+07\n      1.01938e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      8.57823e+07\n      1.01933e+08\n      1.01932e+08\n      1.01932e+08\n      8.57823e+07\n      1.01933e+08\n      1.01932e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      Oceania\n      Oceania\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      ...\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      New Zealand\n      New Zealand\n      Australia\n      New Zealand\n      None\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Goulburn Mulwaree\n      Melbourne\n      Sydney\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      None\n      Melbourne\n      Melbourne\n      None\n      Brisbane\n      Brisbane\n      Brisbane\n      Brisbane\n      ...\n      Sydney\n      Perth\n      Brisbane\n      Auckland\n      Auckland\n      Melbourne\n      Palmerston North City\n      None\n      None\n      Melbourne\n      None\n      Sydney\n      Darwin\n      Melbourne\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Darebin\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      ...\n      Australia\n      Australia\n      Australia\n      None\n      None\n      Australia\n      None\n      None\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Goulburn\n      Melbourne\n      Sydney\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      None\n      East Melbourne\n      Parkville\n      None\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      ...\n      Sydney\n      Perth\n      Brisbane City\n      None\n      None\n      Melbourne\n      None\n      None\n      None\n      Melbourne\n      Berrimah\n      Sydney\n      None\n      Melbourne\n      Berrimah\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Preston\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n    \n    \n      loc_locality\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Goulburn\n      Melbourne\n      Sydney\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      None\n      East Melbourne\n      Parkville\n      None\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      Brisbane City\n      ...\n      Sydney\n      Perth\n      Brisbane City\n      Auckland\n      Auckland\n      Melbourne\n      Palmerston North\n      None\n      None\n      Melbourne\n      Berrimah\n      Sydney\n      Malak\n      Melbourne\n      Berrimah\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Preston\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n    \n    \n      loc_macrocounty\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Melbourne\n      Sydney\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      Adelaide\n      None\n      Melbourne\n      Melbourne\n      None\n      Brisbane\n      Brisbane\n      Brisbane\n      Brisbane\n      ...\n      Sydney\n      Perth\n      Brisbane\n      None\n      None\n      Melbourne\n      None\n      None\n      None\n      Melbourne\n      None\n      Sydney\n      Darwin\n      Melbourne\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n    \n    \n      loc_region\n      Queensland\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Victoria\n      New South Wales\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      South Australia\n      None\n      Victoria\n      Victoria\n      None\n      Queensland\n      Queensland\n      Queensland\n      Queensland\n      ...\n      New South Wales\n      Western Australia\n      Queensland\n      Auckland Region\n      Auckland Region\n      Victoria\n      Manawatu-Wanganui Region\n      None\n      None\n      Victoria\n      Northern Territory\n      New South Wales\n      Northern Territory\n      Victoria\n      Northern Territory\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      New South Wales\n    \n    \n      processor\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      kaggle_promptcloud_listings\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      kaggle_promptcloud_latest\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      kaggle_promptcloud_listings\n      davidsonwp\n      probono\n      probono\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      ...\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_listings\n      probono\n      kaggle_promptcloud_latest\n      probono\n      kaggle_promptcloud_listings\n      probono\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      probono\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n    \n    \n      source\n      CC-MAIN-2020-45\n      CC-MAIN-2020-34\n      CC-MAIN-2020-45\n      seekau\n      CC-MAIN-2020-16\n      CC-MAIN-2020-29\n      CC-MAIN-2020-45\n      CC-MAIN-2020-40\n      CC-MAIN-2020-40\n      seekau_2019q3\n      CC-MAIN-2020-40\n      CC-MAIN-2020-40\n      CC-MAIN-2020-16\n      CC-MAIN-2020-29\n      CC-MAIN-2020-45\n      CC-MAIN-2020-40\n      CC-MAIN-2020-24\n      seekau\n      CC-MAIN-2020-45\n      CC-MAIN-2020-24\n      CC-MAIN-2020-50\n      CC-MAIN-2020-16\n      CC-MAIN-2020-24\n      CC-MAIN-2020-45\n      CC-MAIN-2020-45\n      ...\n      seekau\n      seekau\n      seekau_2019q3\n      seekau_2019q3\n      seekau\n      seekau\n      seekau\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      seekau\n      CC-MAIN-2020-34\n      seekau_2019q3\n      CC-MAIN-2020-34\n      seekau\n      CC-MAIN-2020-34\n      seekau\n      seekau\n      seekau\n      seekau\n      CC-MAIN-2020-34\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      seekau\n      seekau\n      seekau_2019q3\n    \n    \n      loc_neighbourhood\n      None\n      Melbourne CBD\n      Melbourne CBD\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      ...\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n    \n    \n      annual\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      18000\n      18000\n      18000\n      15000\n      14000\n      14000\n      14000\n      13000\n      12000\n      11500\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      10000\n      9500\n      9000\n      7500\n      7500\n      7500\n      7500\n      ...\n      25\n      25\n      24\n      23\n      20\n      19\n      17\n      11\n      11\n      8\n      7\n      6\n      5\n      5\n      5\n      4\n      4\n      4\n      4\n      3\n      3\n      3\n      1\n      1\n      1\n    \n    \n      salary_hi\n      150000\n      200000\n      200000\n      20000\n      165000\n      165000\n      165000\n      140000\n      16000\n      140000\n      120000\n      120000\n      120000\n      120000\n      120000\n      120000\n      120000\n      110000\n      120000\n      110000\n      105000\n      98550\n      98550\n      98550\n      98550\n      ...\n      30\n      26\n      30\n      26\n      30\n      21\n      NaN\n      111\n      111\n      68622\n      99897\n      NaN\n      84989\n      NaN\n      48.5\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_valid\n      False\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      pct_salary_range\n      1.57143\n      1.66972\n      1.66972\n      0.285714\n      1.68715\n      1.68715\n      1.68715\n      1.66013\n      0.285714\n      1.69637\n      1.69231\n      1.69231\n      1.69231\n      1.69231\n      1.69231\n      1.69231\n      1.69231\n      1.66667\n      1.69231\n      1.68201\n      1.68421\n      1.71711\n      1.71711\n      1.71711\n      1.71711\n      ...\n      0.181818\n      0.0392157\n      0.222222\n      0.122449\n      0.4\n      0.1\n      NaN\n      1.63934\n      1.63934\n      1.99953\n      1.99972\n      NaN\n      1.99976\n      NaN\n      1.62617\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n30 rows × 431 columns\n\n\n\n\ndf.loc[(df.salary_low < SALARY_ANNUAL_MIN_AUD) & df.annual, \"salary_valid\"] = False\n\n\n%time sns.displot(df[(df.salary_hours==2000) & (df.salary_valid)], x=\"salary_low\", log_scale=True, kde=True)\n\nCPU times: user 438 ms, sys: 61.2 ms, total: 500 ms\nWall time: 397 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc471f6a0>"
  },
  {
    "objectID": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#weekly",
    "href": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#weekly",
    "title": "skeptric",
    "section": "Weekly",
    "text": "Weekly\n\n%time sns.displot(df[df.weekly], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 523 ms, sys: 89.7 ms, total: 613 ms\nWall time: 522 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc4701d00>\n\n\n\n\n\nThese are mainly bad parsing; could tighen up parsing here\n\nSALARY_WEEKLY_MAX_AUD = 5000\n\n\ndf[df.weekly & (df.salary_low > SALARY_WEEKLY_MAX_AUD)].sort_values(\"salary_low\").T\n\n\n\n\n\n  \n    \n      \n      21565\n      40654\n      62250\n      25836\n      68789\n      19335\n      20444\n      19957\n      59419\n      59863\n      60712\n    \n  \n  \n    \n      title\n      Become a taxi driver\n      Social Media Marketing Assistant (Part Time 3 days per week)\n      Store Manager - Melbourne\n      Talent Resourcer\n      Run-Off Manager for Dairy Support\n      Office Manager – Community Colleges Australia\n      Registered Psychologist\n      HR Advisor\n      Election Managers\n      Election Managers\n      Election Manager\n    \n    \n      description\n      Need full driving victorian australian licence at least 1 year old . National police check and australian victorian driving license is must.\\nIf a...\n      About us\\nWe are a dynamic and integrated creative marketing and brand agency, offering thoughtful creative design, strategic marketing and manage...\n      \n      In this fast paced role, you'll work in a team of 3 Recruiters handling all office and call centre recruitment for our Sydney branch, along with p...\n      \n      About the role\\nCommunity Colleges Australia (CCA: www.cca.edu.au), the peak national body that represents community-owned, not-for-profit educati...\n      About the role\\n\\nAre you passionate about improving the health and wellbeing of Aboriginal people?\\nJoin an influential and supportive Not-For-Pr...\n      About the role\\nCHOICE is one of Australia’s leading independent online media organisations; one that plays a unique role in the Australian consum...\n      \n      \n      \n    \n    \n      uri\n      https://www.gumtree.com.au/s-ad/the-basin/other-jobs/become-a-taxi-driver/1258503131\n      https://www.seek.com.au/job/50730953?type=standard\n      https://www.seek.com.au/job/36497803\n      https://www.seek.com.au/job/40179061\n      https://www.seek.co.nz/job/36421441\n      https://probonoaustralia.com.au/jobs/2019/10/office-manager-community-colleges-australia/\n      https://probonoaustralia.com.au/jobs/2020/08/registered-psychologist/\n      https://probonoaustralia.com.au/jobs/2020/07/hr-advisor/\n      https://www.seek.com.au/job/36554346\n      https://www.seek.com.au/job/36554233\n      https://www.seek.com.au/job/36554178\n    \n    \n      view_date\n      2020-11-27 09:14:12+00:00\n      2020-10-28 20:38:22+00:00\n      2018-06-19 02:23:09\n      2019-10-17 09:07:41\n      2018-06-08 07:23:50\n      2020-05-29 10:49:54+00:00\n      2020-09-26 11:40:24+00:00\n      2020-11-26 22:39:23+00:00\n      2018-06-25 16:11:41\n      2018-06-25 16:29:12\n      2018-06-25 16:31:15\n    \n    \n      org\n      None\n      Lunchbox Agency\n      Hospoworld Resourcing\n      2evolve Pty Ltd\n      Rural People Ltd\n      Community Colleges Australia\n      Biripi Aboriginal Corporation Medical Centre\n      CHOICE\n      Hudson\n      Hudson\n      Hudson\n    \n    \n      salary_raw\n      $30000 plus if working 4 shifts a week\n      $45,000 base (pro rata 3 days/week $27,000)\n      $60-65k + super + bonuses + 40 hours per week\n      To $65K (37.5hr Week)\n      $70,400 including a house. Rent $200 pw\n      up to $80,000 on a full-time equivalent basis (pro rata for days/hours worked: 3.5 to 4 days/week)\n      $83,000-$100,000. Flexible PT role (24hrs/week)!\n      $100,000 FTE Package including Superannuation (12 months fixed | 3 days per week)\n      $3500 - $4000k p.w.\n      $3500 - $4000k p.w.\n      $3500 - $4000k p.w.\n    \n    \n      salary_min\n      30000\n      45000\n      60000\n      65000\n      70400\n      80000\n      83000\n      100000\n      3.5e+06\n      3.5e+06\n      3.5e+06\n    \n    \n      salary_max\n      NaN\n      NaN\n      65000\n      NaN\n      NaN\n      NaN\n      100000\n      NaN\n      4e+06\n      4e+06\n      4e+06\n    \n    \n      salary_hours\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n    \n    \n      location_raw\n      The Basin, VIC\n      melbourne, Melbourne, Victoria, Australia\n      Melbourne, CBD & Inner Suburbs, AU\n      Sydney, New South Wales, Australia\n      Canterbury, North Canterbury, NZ\n      Sydney(Sydney CBD)\n      NSW Regional(Taree)\n      Sydney(Marrickville)\n      Wollongong, Illawarra & South Coast, AU\n      Sydney, AU\n      Lismore & Far North Coast, AU\n    \n    \n      loc_id\n      1.01934e+08\n      1.01933e+08\n      8.57823e+07\n      1.01932e+08\n      8.56872e+07\n      1.01932e+08\n      NaN\n      1.02049e+08\n      1.01932e+08\n      1.01932e+08\n      1.02079e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      None\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      New Zealand\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      New Zealand\n    \n    \n      loc_county\n      Knox\n      Melbourne\n      Melbourne\n      Sydney\n      None\n      Sydney\n      None\n      Marrickville\n      Wollongong\n      Sydney\n      Far North District\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      None\n    \n    \n      loc_localadmin\n      The Basin\n      Melbourne\n      Melbourne\n      Sydney\n      None\n      Sydney\n      None\n      None\n      Wollongong\n      Sydney\n      None\n    \n    \n      loc_locality\n      The Basin\n      Melbourne\n      Melbourne\n      Sydney\n      None\n      Sydney\n      None\n      None\n      Wollongong\n      Sydney\n      None\n    \n    \n      loc_macrocounty\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      None\n      Sydney\n      None\n      Sydney\n      None\n      Sydney\n      None\n    \n    \n      loc_region\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Canterbury Region\n      New South Wales\n      None\n      New South Wales\n      New South Wales\n      New South Wales\n      Northland Region\n    \n    \n      processor\n      gumtree\n      seek\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      probono\n      probono\n      probono\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n    \n    \n      source\n      CC-MAIN-2020-50\n      CC-MAIN-2020-45\n      seekau\n      seekau_2019q3\n      seekau\n      CC-MAIN-2020-24\n      CC-MAIN-2020-40\n      CC-MAIN-2020-50\n      seekau\n      seekau\n      seekau\n    \n    \n      loc_neighbourhood\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      30000\n      45000\n      60000\n      65000\n      70400\n      80000\n      83000\n      100000\n      3.5e+06\n      3.5e+06\n      3.5e+06\n    \n    \n      salary_hi\n      NaN\n      NaN\n      65000\n      NaN\n      NaN\n      NaN\n      100000\n      NaN\n      4e+06\n      4e+06\n      4e+06\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      False\n      False\n      False\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      0.08\n      NaN\n      NaN\n      NaN\n      0.185792\n      NaN\n      0.133333\n      0.133333\n      0.133333\n    \n  \n\n\n\n\nA mix of bad parsing and bad data\n\nSALARY_WEEKLY_MIN_AUD = 300\n\n\ndf[df.weekly & (df.salary_low < SALARY_WEEKLY_MIN_AUD)].sort_values(\"salary_low\", ascending=False).T\n\n\n\n\n\n  \n    \n      \n      21392\n      21814\n      59948\n      64898\n      26218\n      41154\n      37066\n      27951\n      24477\n      31541\n    \n  \n  \n    \n      title\n      Need a car for on-demand delivery and Uber?\n      Drivers Wanted Apply Now | Weekend work available\n      Communication and Media Manager\n      Credit Application Assessor\n      Warehouse Administrative Assistant\n      Data Entry | Casual\n      In-house Customer Workshop Assistant\n      RF Scanners/Voice Pickers\n      Labourer/ Truck Driver\n      Confident Brand Ambassadors\n    \n    \n      description\n      How does Splend work?\\nWe provide eco-friendly and affordable cars so drivers can focus on earning an income.\\nOur weekly Rent plans give ultimate...\n      How does Splend work?\\nWe provide eco-friendly and affordable cars so drivers can focus on earning an income.\\nOur weekly Rent plans give ultimate...\n      \n      \n      Fantastic opportunity for an Administrative Assistant to join the Warehouse team in this newly created role! Our Operations team have recently int...\n      About the business\\nOur client has been providing manufacturing and related services to the Authentication, Payment and Secure Access sector since...\n      Our client is a National Distributor and 3PL Supply Chain provider located in Mile End (SA). They proudly boast solid partnerships with their clie...\n      Our clients are looking for experienced RF Scanners and/or Voice pickers to work in Southern and South Eastern Suburbs casually on either day or a...\n      Perth based scrap metal recycling company is seeking a casual person to assist in both yard operations and product pick-ups. To be considered for ...\n      Our client Acquire Direct is in search of 5 outgoing personalities who thrive on a fast-paced environment and are looking for a challenge. Special...\n    \n    \n      uri\n      https://www.gumtree.com.au/s-ad/north-adelaide/other/need-a-car-for-on-demand-delivery-and-uber-/1226531757\n      https://www.gumtree.com.au/s-ad/brunswick/other/drivers-wanted-apply-now-weekend-work-available/1226530424\n      https://www.seek.com.au/job/36554632\n      https://www.seek.co.nz/job/37438784\n      https://www.seek.com.au/job/39936999\n      https://www.seek.com.au/job/50759000?type=standard\n      https://www.seek.com.au/job/41366535?type=standout\n      https://www.seek.com.au/job/40002217\n      https://www.seek.com.au/job/39870184\n      https://www.seek.com.au/job/39887756\n    \n    \n      view_date\n      2020-07-04 17:30:54+00:00\n      2020-10-27 15:58:15+00:00\n      2018-06-25 16:32:50\n      2018-10-11 01:47:47\n      2019-09-13 06:49:34\n      2020-10-24 09:14:45+00:00\n      2020-05-25 11:47:54+00:00\n      2019-09-23 07:08:05\n      2019-09-05 08:42:40\n      2019-09-07 07:33:18\n    \n    \n      org\n      None\n      None\n      Corin Forest Mountain Retreat\n      Madison Recruitment Ltd\n      Boardriders\n      ProQuest Recruitment  - Dandenong\n      Jobwire\n      ProQuest Recruitment  - Dandenong\n      Wolfram Metal Recyclers\n      Wow Recruitment\n    \n    \n      salary_raw\n      Prices from $199&#47;week\n      Rent from $199&#47;week\n      $30 - $40 per hourUp to 38hrs p/week\n      Competitive salary for 30 hours/ week\n      28 hours/week across Monday to Friday\n      $27 - $28 p.w.\n      Up to $26 p.w.\n      $25 - $36 p.w.\n      $20 - $29.99 per hour30-38 hours a week\n      $20.00-$24.00/hourOTE $1000 + /week\n    \n    \n      salary_min\n      199\n      199\n      30\n      30\n      28\n      27\n      26\n      25\n      20\n      20\n    \n    \n      salary_max\n      None\n      NaN\n      40\n      NaN\n      NaN\n      28\n      NaN\n      36\n      29.99\n      24\n    \n    \n      salary_hours\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n      40\n    \n    \n      location_raw\n      North Adelaide SA\n      Brunswick VIC\n      ACT, AU\n      Auckland, Manukau & East Auckland, NZ\n      South West Coast VIC, Victoria, Australia\n      dandenongsouth, Melbourne, Victoria, Australia\n      mileend, Adelaide, South Australia, Australia\n      Melbourne, Victoria, Australia\n      Perth, Western Australia, Australia\n      Sydney, New South Wales, Australia\n    \n    \n      loc_id\n      1.01936e+08\n      1.01933e+08\n      8.56815e+07\n      1.12598e+09\n      8.56815e+07\n      1.01933e+08\n      1.01936e+08\n      1.01933e+08\n      1.01938e+08\n      1.01932e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      New Zealand\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Adelaide\n      Moreland\n      None\n      Auckland\n      None\n      Melbourne\n      Adelaide\n      Melbourne\n      Perth\n      Sydney\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      North Adelaide\n      Brunswick\n      None\n      None\n      None\n      Melbourne\n      Adelaide\n      Melbourne\n      Perth\n      Sydney\n    \n    \n      loc_locality\n      North Adelaide\n      Brunswick\n      None\n      Manukau City\n      None\n      Melbourne\n      Adelaide\n      Melbourne\n      Perth\n      Sydney\n    \n    \n      loc_macrocounty\n      Adelaide\n      Melbourne\n      None\n      None\n      None\n      Melbourne\n      Adelaide\n      Melbourne\n      Perth\n      Sydney\n    \n    \n      loc_region\n      South Australia\n      Victoria\n      Australian Capital Territory\n      Auckland Region\n      Victoria\n      Victoria\n      South Australia\n      Victoria\n      Western Australia\n      New South Wales\n    \n    \n      processor\n      gumtree\n      gumtree\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      seek\n      seek\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n    \n    \n      source\n      CC-MAIN-2020-29\n      CC-MAIN-2020-45\n      seekau\n      seekau\n      seekau_2019q3\n      CC-MAIN-2020-45\n      CC-MAIN-2020-24\n      seekau_2019q3\n      seekau_2019q3\n      seekau_2019q3\n    \n    \n      loc_neighbourhood\n      NaN\n      NaN\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      199\n      199\n      30\n      30\n      28\n      27\n      26\n      25\n      20\n      20\n    \n    \n      salary_hi\n      NaN\n      NaN\n      40\n      NaN\n      NaN\n      28\n      NaN\n      36\n      29.99\n      24\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      0.285714\n      NaN\n      NaN\n      0.0363636\n      NaN\n      0.360656\n      0.39968\n      0.181818\n    \n  \n\n\n\n\nWe could mark these as annual… slight bias to exclude them, but small in number\n\ndf.loc[df.weekly & (df.salary_low < SALARY_WEEKLY_MIN_AUD), \"salary_valid\"] = False\n\n\ndf.loc[df.weekly & (df.salary_low > SALARY_WEEKLY_MAX_AUD), \"salary_valid\"] = False\n\n\n%time sns.displot(df[df.weekly & df.salary_valid], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 323 ms, sys: 50.6 ms, total: 374 ms\nWall time: 269 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc47b0d30>"
  },
  {
    "objectID": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#daily",
    "href": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#daily",
    "title": "skeptric",
    "section": "Daily",
    "text": "Daily\n\n%time sns.displot(df[df.daily], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 593 ms, sys: 58.6 ms, total: 651 ms\nWall time: 550 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc44949d0>\n\n\n\n\n\nMix of bad data and parsing\n\ndf[df.daily & (df.salary_low > 3000)].sort_values('salary_low').T\n\n\n\n\n\n  \n    \n      \n      73423\n      29338\n      32448\n      35501\n    \n  \n  \n    \n      title\n      Aboriginal Identified - Clerk General Scale - Cadetship Program - Various Locations - Temporary\n      Management Accountant - 9 day fortnight\n      Data Dev and Gov Analyst\n      Data Dev and Gov Analyst\n    \n    \n      description\n      ABOUT LEGAL AID NSW\\nLegal Aid NSW is the largest legal aid agency in Australia, comprising of a Central Sydney office and 26 regional offices in ...\n      Our client is a high profile public-sector organisation located in the inner Bayside / Georges River Council suburban area within easy access of t...\n      Government agency looking for a to be a strong player in the implementation of a green field Big Data environment whilst cross-skilling with other...\n      Government agency looking for a to be a strong player in the implementation of a green field Big Data environment whilst cross-skilling with other...\n    \n    \n      uri\n      https://iworkfor.nsw.gov.au/job/aboriginal-identified-clerk-general-scale-cadetship-program-various-locations-temporary-212912\n      https://www.seek.com.au/job/39843065\n      https://jobs.launchrecruitment.com.au/job/100602033311507/data-dev-and-gov-analyst/\n      https://jobs.launchrecruitment.com.au/job/100602033311507/data-dev-and-gov-analyst/\n    \n    \n      view_date\n      2020-10-21 22:35:08+00:00\n      2019-09-02 10:54:58\n      2020-05-26 20:13:28+00:00\n      2020-09-29 13:50:58+00:00\n    \n    \n      org\n      Legal Aid Commission, Office of the\n      Pacific Search Partners\n      Launch Recruitment\n      Launch Recruitment\n    \n    \n      salary_raw\n      • Salary package of up to $27,700 including study and book allowances for a 60-day placement\n      Circa $100K + super for a 9 day/fortnight\n      $150000 - $180000 per day\n      $150000 - $180000 per day\n    \n    \n      salary_min\n      27700\n      100000\n      150000\n      150000\n    \n    \n      salary_max\n      NaN\n      NaN\n      180000\n      180000\n    \n    \n      salary_hours\n      8\n      8\n      8\n      8\n    \n    \n      location_raw\n      Sydney Region\n      Sydney, New South Wales, Australia\n      Parramatta, AU\n      Parramatta, AU\n    \n    \n      loc_id\n      1.01932e+08\n      1.01932e+08\n      1.0205e+08\n      1.0205e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Sydney\n      Sydney\n      Parramatta\n      Parramatta\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      Sydney\n      Sydney\n      None\n      None\n    \n    \n      loc_locality\n      Sydney\n      Sydney\n      None\n      None\n    \n    \n      loc_macrocounty\n      Sydney\n      Sydney\n      Sydney\n      Sydney\n    \n    \n      loc_region\n      New South Wales\n      New South Wales\n      New South Wales\n      New South Wales\n    \n    \n      processor\n      iworkfornsw\n      kaggle_promptcloud_latest\n      launchrecruitment\n      launchrecruitment\n    \n    \n      source\n      CC-MAIN-2020-45\n      seekau_2019q3\n      CC-MAIN-2020-24\n      CC-MAIN-2020-40\n    \n    \n      loc_neighbourhood\n      NaN\n      None\n      None\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      True\n      True\n      True\n      True\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      27700\n      100000\n      150000\n      150000\n    \n    \n      salary_hi\n      NaN\n      NaN\n      180000\n      180000\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      0.181818\n      0.181818\n    \n  \n\n\n\n\nMany of these seem suss..\n\ndf[df.daily & (df.salary_min < 100)].sort_values('salary_min', ascending=False).T\n\n\n\n\n\n  \n    \n      \n      62595\n      71103\n      29321\n      35628\n      33287\n      34739\n      26735\n      32806\n      35832\n      32513\n      38755\n      34213\n      38782\n      65932\n      9808\n      35074\n      7886\n      33609\n      35842\n      28062\n      37935\n      64256\n      31186\n      31221\n      63834\n      ...\n      33639\n      33475\n      33466\n      33462\n      33450\n      33424\n      33413\n      33502\n      33509\n      33560\n      33568\n      33573\n      33574\n      33578\n      33590\n      33591\n      33594\n      33598\n      33612\n      33615\n      33625\n      33628\n      33630\n      33635\n      34612\n    \n  \n  \n    \n      title\n      PHP Developer\n      Development Services Officer\n      Child Protection Assessment Officer\n      Training Coordinator\n      Training Coordinator\n      Training Coordinator\n      Records and Scanning Administration Officers (work with a Qld Government dept)\n      Team Leader - Speak Out Program\n      Call Centre Operator\n      Call Centre Operator\n      Production Assistant\n      Team Leader - Speak Out Program\n      Warehouse Staff Needed\n      Customer Service Representative (Inbound)\n      Contact Centre Consultant| HR Advice|Payroll|Casual Pool\n      Receptionist\n      Contact Centre Consultant| HR Advice|Payroll|Casual Pool\n      Receptionist\n      Receptionist\n      Warehouse Operators\n      Assistant Educator\n      Forklift Driver - Day Shift\n      Packing Staff - Dairy Industry - Day and Afternoon Shift\n      Container Unloaders/General Labourers\n      Class 1 Driver - $20.50 per hour\n      ...\n      Scrum Master\n      Senior Release / Deployment Manager\n      Web Automation Test Specialist\n      Team Assistant\n      Team Assistant\n      MYSQL Architect\n      Personal Assistant\n      .Net developer\n      System Access Administrator\n      Front End developer- Angular\n      IT recruiter\n      Full Stack developer\n      Solution Architect\n      Back end developer\n      Field Supervisor\n      Network Design role\n      iOS Developer\n      AWS Architect\n      Citrix SME\n      Splunk Architect\n      Scrum Master\n      Technology Analyst\n      IT support role\n      Test Analyst (Salesforce & Insurance Experience)\n      Recruitment Coodinator Lead\n    \n    \n      description\n      \n      \n      BENEFITSDarlinghurst location 8 Month Temporary AssignmentChild protection THE COMPANYpeople2people are collaborating with the NSW Government who ...\n      Fantastic opportunity to work for a leading government agency who specialise in health and safety. Our client is undergoing a large scale transfor...\n      Fantastic opportunity to work for a leading government agency who specialise in health and safety. Our client is undergoing a large scale transfor...\n      Fantastic opportunity to work for a leading government agency who specialise in health and safety. Our client is undergoing a large scale transfor...\n      We are urgently seeking Records and Scanning Administration Officers with strong attention to detail to join our temporary team working with a pro...\n      Weave is seeking an experienced Team Leader to join the team.\\nProviding leadership, support and coordination of Weave’s Speak Out Dual Diagnosis ...\n      Opportunity to work with a leading reputable government agency who are building a new online platform. This role will be shift work - 5 days per d...\n      Opportunity to work with a leading reputable government agency who are building a new online platform. This role will be shift work - 5 days per d...\n      We have a great opportunity for experienced Production Assistants to join a reputable printing organisation located in Craigieburn.\\nOur client, l...\n      Weave is seeking an experienced Team Leader to join the team.\\nProviding leadership, support and coordination of Weave’s Speak Out Dual Diagnosis ...\n      Labourforce has potions available for suitable candidates with our valued client based in Acacia Ridge due to increased work load, they are an ind...\n      \n      About the company:\\nOur client is a leading national organisation with a fantastic reputation for providing a valuable service to its customers an...\n      Join an exciting Receptionist role for one of our IT clients based in North Sydney for an initial 6 months contract with a view to extend. You wil...\n      About the company:\\nOur client is a leading national organisation with a fantastic reputation for providing a valuable service to its customers an...\n      Join an exciting Receptionist role for one of our IT clients based in North Sydney for an initial 6 months contract with a view to extend. You wil...\n      Join an exciting Receptionist role for one of our IT clients based in North Sydney for an initial 6 months contract with a view to extend. You wil...\n      Warehouse Operators needed for ALDI Prestons Do you like a physically challenging role? About the company:Aldi have been operating in Australia si...\n      About Story House Early Learning:\\nStory House Early Learning is an emerging early childhood provider with services in QLD, NSW and VIC. We strive...\n      \n      Immediate Start Day Shift 5am - 3pm Afternoon Shift 2pm - 12am Ongoing Casual Shifts Mon - Friday Production and Warehouse work experience essenti...\n      Randstad is currently recruiting for casual container unloaders/general hands. As it is approaching closer to the silly season, our clients are ge...\n      \n      ...\n      Join one of the largest IT clients based in Melbourne CBD for an initial 6 months contract for a Scrum Master role.\\nMandatory skills needed:\\n\\nH...\n      Senior Release / Deployment Manager - Perth - Day Rate = $772.72 inc. Super\\nScope:\\n\\nTracked and implemented deliverables against the scope and ...\n      Web Automation Test Specialist | 6 month contract | Circa - $750 inc. super per day\\nThe Company:\\nThis is a quintessential Australian household n...\n      A leading Telecommunications organisation is seeking an experienced Team Assistant to provide support to a number of General Managers and execs.\\n...\n      Our client is a leading telecommunications organisation who are seeking a Team Assistant who will be supporting General Managers with regards to f...\n      Join an exciting MySql Architect role for a 1 month contract based in South Melbourne. The role is initially for 4 weeks but can extend further.\\n...\n      Join an exciting PA role for one of the largest Telco companies based in Sydney CBD.You will be supporting administration and general EA duties wi...\n      Our client is a global professional services consultant who are looking for strong .Net developers for a 12 months contract.\\nThis client has a ma...\n      Join an exciting System Access Administrator role for a 6 months contract for one of the largest Telco companies based in CBD.You will be responsi...\n      Our client based in Melbourne CBD is looking for a front-end developer for a short-term contract based in Melbourne CBD.Its a contract till end of...\n      Join one of the largest IT corporates based in Canberra for IT Recruiter role for an initial 6 months contract.\\nThe skills looking for this role ...\n      Join an exciting Full Stack developer role for an initial 6 months contract based in Brisbane for one of our clients.\\nMandatory skills for the ro...\n      Join an exciting Solutions Architect role based in Melbourne for an initial 6 months contract.\\nYou will be responsible for:\\n\\nMaintaining the ST...\n      Join an exciting Back end developer(Java) with one of largest Telco clients based in Melbourne CBD for an initial 3 months contract that might ext...\n      Join one of the largest Telco companies based in North and East metro and some coutry area for a field Supervisor role for an initial 6 months con...\n      Join one of our IT clients based in Brisbane for a Site Network Study role for an initial 6 months contract.\\nYou will be performing Site Audit fo...\n      Join one of the largest IT companies based in Melbourne CBD for an iOS developer role for an initial 6 months contract.\\nYou will gave the followi...\n      Join one of the largest IT clients based in CBD for an AWS Architect role for an initial 6 months contract.\\nYou responsibilities and experience w...\n      Join an exciting Citrix SME role for one of our largest clients based in Melbourne CBD.Its an initial 6 months contract with the possibility of ex...\n      Join an exciting Splunk Developer role for a 6 months contract position based in Melbourne CBD for one of our clients.The role requires someone wh...\n      Join one of the largest IT companies as a Scrum Master for an initial 6 months contract based in CBD.\\nMandatory skills needed:\\n\\nHands-on experi...\n      Join one of the largest IT clients based in Melbourne CBD for a Technology Analyst role. The role is an initial 6 months contract role.\\nMandatory...\n      Join one of the largest IT clients for an initial 2 years contract based in Adelaide.\\nTechnical Support & Execution\\n\\nEnsure IT & OT Systems inc...\n      Test & Quality Analyst - 12 Month Day Rate Contract - $680 Inc Super a Day\\nSALESFORCE & previous INSURANCE experience required. \\nThe Company:\\nT...\n      An exciting opportunity to join a leading digital company based in Melbourne CBD for a Recruitment coodinator role.The role is a 6 months contract...\n    \n    \n      uri\n      https://www.seek.com.au/job/36789537\n      https://www.seek.com.au/job/37565087\n      https://www.seek.com.au/job/40208499\n      https://jobs.launchrecruitment.com.au/job/100602033335650/training-coordinator-2/\n      https://jobs.launchrecruitment.com.au/job/100602033335650/training-coordinator-2/\n      https://jobs.launchrecruitment.com.au/job/100602033335650/training-coordinator-2/\n      https://www.seek.com.au/job/39934310\n      https://jobs.launchrecruitment.com.au/job/100602033217942/team-leader-speak-out-program/\n      https://jobs.launchrecruitment.com.au/job/100602033372517/call-centre-operator-1/\n      https://jobs.launchrecruitment.com.au/job/100602033372517/call-centre-operator-1/\n      https://www.seek.com.au/job/50994456?type=standard\n      https://jobs.launchrecruitment.com.au/job/100602033217942/team-leader-speak-out-program/\n      https://www.seek.com.au/job/50995587?type=standard\n      https://www.seek.com.au/job/37203654\n      https://www.davidsonwp.com/job/100562133333296/contact-centre-consultant-hr-advice-payroll-casual-pool/\n      https://jobs.launchrecruitment.com.au/job/100602033373812/receptionist-9/\n      https://www.davidsonwp.com/job/100562133333296/contact-centre-consultant-hr-advice-payroll-casual-pool/\n      https://jobs.launchrecruitment.com.au/job/100602033373812/receptionist-9/\n      https://jobs.launchrecruitment.com.au/job/100602033373812/receptionist-9/\n      https://www.seek.com.au/job/40269764\n      https://www.seek.com.au/job/50783642?cid=SKL-CareerGuide\n      https://www.seek.com.au/job/37439153\n      https://www.seek.com.au/job/39950345\n      https://www.seek.com.au/job/40162654\n      https://www.seek.co.nz/job/37434117\n      ...\n      https://jobs.launchrecruitment.com.au/job/100602033376518/scrum-master-4/\n      https://jobs.launchrecruitment.com.au/job/100602033362452/senior-release-slash-deployment-manager/\n      https://jobs.launchrecruitment.com.au/job/100602033361046/web-automation-test-specialist/\n      https://jobs.launchrecruitment.com.au/job/100602033361906/team-assistant-28/\n      https://jobs.launchrecruitment.com.au/job/100602033360118/team-assistant-26/\n      https://jobs.launchrecruitment.com.au/job/100602033356253/mysql-developer/\n      https://jobs.launchrecruitment.com.au/job/100602033355763/personal-assistant-7/\n      https://jobs.launchrecruitment.com.au/job/100602033364943/dot-net-developer-11/\n      https://jobs.launchrecruitment.com.au/job/100602033365315/system-access-administrator-2/\n      https://jobs.launchrecruitment.com.au/job/100602033370133/front-end-developer-angular/\n      https://jobs.launchrecruitment.com.au/job/100602033371423/it-recruiter/\n      https://jobs.launchrecruitment.com.au/job/100602033371767/full-stack-developer-4/\n      https://jobs.launchrecruitment.com.au/job/100602033371863/solution-architect-15/\n      https://jobs.launchrecruitment.com.au/job/100602033372136/back-end-developer-1/\n      https://jobs.launchrecruitment.com.au/job/100602033372610/field-supervisor-1/\n      https://jobs.launchrecruitment.com.au/job/100602033373023/site-network-study/\n      https://jobs.launchrecruitment.com.au/job/100602033373743/ios-developer-3/\n      https://jobs.launchrecruitment.com.au/job/100602033373452/aws-architect/\n      https://jobs.launchrecruitment.com.au/job/100602033375052/citrix-sme/\n      https://jobs.launchrecruitment.com.au/job/100602033375051/splunk-architect/\n      https://jobs.launchrecruitment.com.au/job/100602033376480/scrum-master-3/\n      https://jobs.launchrecruitment.com.au/job/100602033375320/technology-analyst/\n      https://jobs.launchrecruitment.com.au/job/100602033376491/it-support-role/\n      https://jobs.launchrecruitment.com.au/job/100602033377291/test-analyst-salesforce-and-insurance-experience/\n      https://jobs.launchrecruitment.com.au/job/100602033319244/recruitment-coodinator-1/\n    \n    \n      view_date\n      2018-07-24 08:36:19\n      2018-10-26 04:21:31\n      2019-10-21 08:22:12\n      2020-09-24 03:55:28+00:00\n      2020-07-07 14:34:53+00:00\n      2020-03-29 22:32:56+00:00\n      2019-09-13 05:47:40\n      2020-07-05 07:01:30+00:00\n      2020-09-29 13:37:04+00:00\n      2020-05-29 16:02:55+00:00\n      2020-12-03 21:24:56+00:00\n      2020-04-04 07:15:18+00:00\n      2020-12-02 01:03:33+00:00\n      2018-09-13 00:55:45\n      2020-10-20 20:16:51+00:00\n      2020-03-29 23:10:58+00:00\n      2020-08-08 10:27:12+00:00\n      2020-07-14 22:43:41+00:00\n      2020-09-25 19:11:59+00:00\n      2019-10-29 05:31:44\n      2020-11-27 18:14:20+00:00\n      2018-10-11 00:40:16\n      2019-09-16 05:38:03\n      2019-10-15 07:52:12\n      2018-10-11 01:56:36\n      ...\n      2020-07-07 15:40:26+00:00\n      2020-07-03 03:06:52+00:00\n      2020-07-05 22:55:47+00:00\n      2020-07-08 22:37:45+00:00\n      2020-07-09 03:22:19+00:00\n      2020-07-09 08:11:48+00:00\n      2020-07-07 01:36:56+00:00\n      2020-07-11 08:19:54+00:00\n      2020-07-11 06:55:37+00:00\n      2020-07-02 08:24:49+00:00\n      2020-07-02 12:04:10+00:00\n      2020-07-11 08:43:27+00:00\n      2020-07-14 23:23:06+00:00\n      2020-07-05 22:00:15+00:00\n      2020-07-08 22:46:10+00:00\n      2020-07-14 22:08:25+00:00\n      2020-07-14 12:11:47+00:00\n      2020-07-08 23:44:49+00:00\n      2020-07-11 08:00:08+00:00\n      2020-07-14 12:30:51+00:00\n      2020-07-06 00:03:35+00:00\n      2020-07-14 23:53:51+00:00\n      2020-07-14 23:21:54+00:00\n      2020-07-03 04:13:09+00:00\n      2020-04-09 04:49:08+00:00\n    \n    \n      org\n      Chandler Macleod Group\n      Smalls, GWS & JHA trading as Spinifex Recruiting\n      people2people\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Recruitment@Top\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Staff Australia Recruitment Services P/L\n      Launch Recruitment\n      Labourforce\n      Kelly Services\n      Davidson\n      Launch Recruitment\n      Davidson\n      Launch Recruitment\n      Launch Recruitment\n      IPA\n      Story House Early Learning\n      HORNER Recruitment\n      Dekro Recruitment\n      Randstad - Industrial\n      Adecco Transport & Logistics\n      ...\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n      Launch Recruitment\n    \n    \n      salary_raw\n      $60 - $70 p.d. + Super\n      $50 - $60 p.d. + Super\n      $45.00 per day\n      $35 - $38 per day\n      $35 - $38 per day\n      $35 - $38 per day\n      Casual $30.19(day shift)- $33.81(afternoon shift)\n      * Part Time - 9 day fortnight (until 30 June 2019)\n      $30 - $32 per day\n      $30 - $32 per day\n      $30 - $32 p.d.\n      * Part Time - 9 day fortnight (until 30 June 2019)\n      $27.17 DAY $29.30 ARVO\n      $28.06 p.d. + Super\n      $28 per day\n      $28 - $30 per day\n      $28 per day\n      $28 - $30 per day\n      $28 - $30 per day\n      $27.33 - $28.33 p.d.\n      $27.21 - $28.03 + super + uniform + PD\n      $26 - $27 per day\n      Day Shift $25.07 Afternoon Shift $28.83+ Super\n      $25 - $30 p.d.\n      $20 - $21 p.d. + Over time\n      ...\n      $0 - $700 per day\n      $0 - $772.72 per day\n      $0 - $760 per day\n      $0 - $300 per day\n      $0 - $460 per day\n      $0 - $800 per day\n      $0 - $450 per day\n      $0 - $732 per day\n      $0 - $525 per day\n      $0 - $600 per day\n      $0 - $400 per day\n      $0 - $500 per day\n      $0 - $800 per day\n      $0 - $800 per day\n      $0 - $700 per day\n      $0 - $450 per day\n      $0 - $700 per day\n      $0 - $700 per day\n      $0 - $500 per day\n      $0 - $500 per day\n      $0 - $700 per day\n      $0 - $500 per day\n      $0 - $500 per day\n      $0 - $680 per day, Benefits: 12 Month Day Rate Contract\n      $0 - $330 per day\n    \n    \n      salary_min\n      60\n      50\n      45\n      35\n      35\n      35\n      30.19\n      30\n      30\n      30\n      30\n      30\n      29.3\n      28.06\n      28\n      28\n      28\n      28\n      28\n      27.33\n      27.21\n      26\n      25.07\n      25\n      20\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      salary_max\n      70\n      60\n      NaN\n      38\n      38\n      38\n      NaN\n      NaN\n      32\n      32\n      32\n      NaN\n      NaN\n      NaN\n      NaN\n      30\n      NaN\n      30\n      30\n      28.33\n      28.03\n      27\n      NaN\n      30\n      21\n      ...\n      700\n      772.72\n      760\n      300\n      460\n      800\n      450\n      732\n      525\n      600\n      400\n      500\n      800\n      800\n      700\n      450\n      700\n      700\n      500\n      500\n      700\n      500\n      500\n      680\n      330\n    \n    \n      salary_hours\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      ...\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n      8\n    \n    \n      location_raw\n      Sydney, CBD, Inner West & Eastern Suburbs, AU\n      Sydney, Parramatta & Western Suburbs, AU\n      Sydney, New South Wales, Australia\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Brisbane, Queensland, Australia\n      Sydney, AU\n      Geelong, AU\n      Geelong, AU\n      craigieburn, Melbourne, Victoria, Australia\n      Sydney, AU\n      acaciaridge, Brisbane, Queensland, Australia\n      Sydney, South West & M5 Corridor, AU\n      Brisbane, AU\n      Melbourne, AU\n      Brisbane, AU\n      Melbourne, AU\n      Melbourne, AU\n      Sydney, New South Wales, Australia\n      mascot, Sydney, New South Wales, Australia\n      Melbourne, Bayside & South Eastern Suburbs, AU\n      Melbourne, Victoria, Australia\n      Melbourne, Victoria, Australia\n      Auckland, Manukau & East Auckland, NZ\n      ...\n      Melbourne, AU\n      Perth, AU\n      Sydney, AU\n      Sydney, AU\n      Melbourne, AU\n      Melbourne, AU\n      Sydney, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Canberra, AU\n      Brisbane, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Brisbane, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Adelaide, AU\n      Sydney, AU\n      Melbourne, AU\n    \n    \n      loc_id\n      8.57823e+07\n      1.01932e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01934e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01934e+08\n      8.57744e+07\n      1.01934e+08\n      1.01933e+08\n      1.01934e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01931e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.12598e+09\n      ...\n      1.01933e+08\n      1.01938e+08\n      1.01932e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      8.56815e+07\n      1.01934e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01934e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01936e+08\n      1.01932e+08\n      1.01933e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      ...\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      New Zealand\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Sydney\n      Greater Geelong\n      Greater Geelong\n      Hume\n      Sydney\n      Brisbane\n      Sydney\n      Brisbane\n      Melbourne\n      Brisbane\n      Melbourne\n      Melbourne\n      Sydney\n      Botany Bay\n      Melbourne\n      Melbourne\n      Melbourne\n      Auckland\n      ...\n      Melbourne\n      Perth\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Adelaide\n      Sydney\n      Melbourne\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      ...\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Sydney\n      Geelong\n      Geelong\n      Craigieburn\n      Sydney\n      Brisbane City\n      Sydney\n      Brisbane City\n      Melbourne\n      Brisbane City\n      Melbourne\n      Melbourne\n      Sydney\n      Mascot\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      ...\n      Melbourne\n      Perth\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Adelaide\n      Sydney\n      Melbourne\n    \n    \n      loc_locality\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Sydney\n      Geelong\n      Geelong\n      Craigieburn\n      Sydney\n      Brisbane City\n      Sydney\n      Brisbane City\n      Melbourne\n      Brisbane City\n      Melbourne\n      Melbourne\n      Sydney\n      Mascot\n      Melbourne\n      Melbourne\n      Melbourne\n      Manukau City\n      ...\n      Melbourne\n      Perth\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane City\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Adelaide\n      Sydney\n      Melbourne\n    \n    \n      loc_macrocounty\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Sydney\n      Geelong\n      Geelong\n      Melbourne\n      Sydney\n      Brisbane\n      Sydney\n      Brisbane\n      Melbourne\n      Brisbane\n      Melbourne\n      Melbourne\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      ...\n      Melbourne\n      Perth\n      Sydney\n      Sydney\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Brisbane\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      Adelaide\n      Sydney\n      Melbourne\n    \n    \n      loc_region\n      Victoria\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      Queensland\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Queensland\n      New South Wales\n      Queensland\n      Victoria\n      Queensland\n      Victoria\n      Victoria\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      Auckland Region\n      ...\n      Victoria\n      Western Australia\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      Australian Capital Territory\n      Queensland\n      Victoria\n      Victoria\n      Victoria\n      Queensland\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      South Australia\n      New South Wales\n      Victoria\n    \n    \n      processor\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_latest\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      seek\n      launchrecruitment\n      seek\n      kaggle_promptcloud_listings\n      davidsonwp\n      launchrecruitment\n      davidsonwp\n      launchrecruitment\n      launchrecruitment\n      kaggle_promptcloud_latest\n      seek\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      ...\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n      launchrecruitment\n    \n    \n      source\n      seekau\n      seekau\n      seekau_2019q3\n      CC-MAIN-2020-40\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n      seekau_2019q3\n      CC-MAIN-2020-29\n      CC-MAIN-2020-40\n      CC-MAIN-2020-24\n      CC-MAIN-2020-50\n      CC-MAIN-2020-16\n      CC-MAIN-2020-50\n      seekau\n      CC-MAIN-2020-45\n      CC-MAIN-2020-16\n      CC-MAIN-2020-34\n      CC-MAIN-2020-29\n      CC-MAIN-2020-40\n      seekau_2019q3\n      CC-MAIN-2020-50\n      seekau\n      seekau_2019q3\n      seekau_2019q3\n      seekau\n      ...\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-29\n      CC-MAIN-2020-16\n    \n    \n      loc_neighbourhood\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      Sydney South\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      ...\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      hourly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      ...\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      salary_low\n      60\n      50\n      45\n      35\n      35\n      35\n      30.19\n      30\n      30\n      30\n      30\n      30\n      29.3\n      28.06\n      28\n      28\n      28\n      28\n      28\n      27.33\n      27.21\n      26\n      25.07\n      25\n      20\n      ...\n      700\n      772.72\n      760\n      300\n      460\n      800\n      450\n      732\n      525\n      600\n      400\n      500\n      800\n      800\n      700\n      450\n      700\n      700\n      500\n      500\n      700\n      500\n      500\n      680\n      330\n    \n    \n      salary_hi\n      70\n      60\n      NaN\n      38\n      38\n      38\n      NaN\n      NaN\n      32\n      32\n      32\n      NaN\n      NaN\n      NaN\n      NaN\n      30\n      NaN\n      30\n      30\n      28.33\n      28.03\n      27\n      NaN\n      30\n      21\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      ...\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      0.153846\n      0.181818\n      NaN\n      0.0821918\n      0.0821918\n      0.0821918\n      NaN\n      NaN\n      0.0645161\n      0.0645161\n      0.0645161\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0689655\n      NaN\n      0.0689655\n      0.0689655\n      0.0359324\n      0.0296886\n      0.0377358\n      NaN\n      0.181818\n      0.0487805\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n30 rows × 295 columns\n\n\n\n\nSALARY_DAILY_MAX_AUD = 3000\n\n\nSALARY_DAILY_MIN_AUD = 100\n\n\ndf.loc[df.daily & (df.salary_low < SALARY_DAILY_MIN_AUD), \"salary_valid\"] = False\n\n\ndf.loc[df.daily & (df.salary_low > SALARY_DAILY_MAX_AUD), \"salary_valid\"] = False\n\n\n%time sns.displot(df[df.salary_valid & df.daily], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 374 ms, sys: 69.6 ms, total: 444 ms\nWall time: 343 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc41a92b0>"
  },
  {
    "objectID": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#hourly",
    "href": "notebooks/Analysing Salary Extracted From CommonCrawl Job Data.html#hourly",
    "title": "skeptric",
    "section": "Hourly",
    "text": "Hourly\n\n%time sns.displot(df[df.hourly], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 954 ms, sys: 60.7 ms, total: 1.01 s\nWall time: 897 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc9ef1190>\n\n\n\n\n\nBad parsing and bad data\n\ndf[df.hourly & (df.salary_low > 250)].sort_values(\"salary_low\").T\n\n\n\n\n\n  \n    \n      \n      5238\n      8787\n      70468\n      212\n      9041\n      9107\n      51798\n      30332\n      30199\n      27985\n      24474\n      24053\n      19691\n      69134\n      26038\n      53100\n      53139\n      19659\n      65741\n      40904\n      70924\n    \n  \n  \n    \n      title\n      Strategic Innovation Manager - National Initiatives and Performance\n      Strategic Innovation Manager - National Initiatives and Performance\n      PERSONAL CARE ASSISTANT\n      Senior Business Implementation Consultant\n      Business Implementation Consultant\n      Senior Business Implementation Consultant\n      Children's Services Officer - Room Leader\n      Retail Assistants Wanted - Sales & Promotions!\n      Hospitality Workers - Promotion People Wanted!\n      Customer Service And Marketing Consultants Wanted! Immediate Starts Available!\n      Campaign Managers Wanted! New Summer Sales Campaigns About To Begin!\n      Retail Assistants Wanted - Promotion Sales!\n      Financial Counsellor (Integrated Services Project)\n      Spray Painter- Kitchens\n      Mid-Level Finished Artist\n      Radiographers (Shift) - Medical Imaging Department, Cairns\n      Radiographers (Shift) - Medical Imaging Department, Cairns\n      Community Lawyer (Integrated Services Project)\n      Leading Hand\n      F45 Head Coach / Trainer\n      Senior Contract Administrator\n    \n    \n      description\n      About the Role\\nThe main purpose of this role is to manage the implementation of the Catalyst Lab Innovation Program which aims to support the ide...\n      About the Role\\nThe main purpose of this role is to manage the implementation of the Catalyst Lab Innovation Program which aims to support the ide...\n      \n      Our client is a major insurance organisation that is well recognized in the industry. They provide innovative, competitive and flexible solutions ...\n      Our client is a major insurance organisation that is well recognized in the industry. They provide innovative, competitive and flexible solutions ...\n      Our client is a major insurance organisation that is well recognized in the industry. They provide innovative, competitive and flexible solutions ...\n      Full Time Scone The Role: Council is seeking an energetic and enthusiastic individual to undertake the above position.  The successful candidate w...\n      Our client, HQ Direct are looking for individuals who have huge personalities to employ their skills to the fast paced and highly social face to f...\n      Our client, HQ Direct are looking for individuals who have huge personalities to employ their skills to the fast paced and highly social face to f...\n      Wow Recruitment is on the hunt for social and bubbly guys and girls to join our client’s team. HQ Direct are launching some brand-new promotional ...\n      POP Advertising is looking for the next wave of campaign managers and sales agents to kickstart our new summer campaigns. We are looking for indiv...\n      Our client, HQ Direct are looking for individuals who have huge personalities to employ their skills to the fast paced and highly social face to f...\n      About the role\\n18 hours per week – 12 month fixed term, based in Fitzroy, Victoria\\nSocial Security Rights Victoria and the Financial and Consume...\n      Spray Painter Required. Our family run property services business, based on the North Side of Brisbane is currently looking for a experienced 2 Pa...\n      At the Y, we believe in the power of inspired young people. A community not-for-profit organisation with 17 million participations annually across...\n      \n      \n      About the role\\n30.4 hours per week – 12 month fixed term, based in Fitzroy, Victoria\\nSocial Security Rights Victoria and the Financial and Consu...\n      \n      JOIN SYDNEY’S PREMIER F45 STUDIO!\\nApply for this role at www.bit.ly/F45job\\nDo you get freakin' excited and pride yourself on providing fun, fres...\n      \n    \n    \n      uri\n      https://www.davidsonwp.com/job/100562133239422/strategic-innovation-manager-national-initiatives-and-performance-1/\n      https://www.davidsonwp.com/job/100562133239422/strategic-innovation-manager-national-initiatives-and-performance/\n      https://www.seek.com.au/job/37565380\n      https://www.davidsonwp.com/job/100562133256145/senior-business-implementation-consultant-1/\n      https://www.davidsonwp.com/job/100562133253698/business-implementation-consultant/\n      https://www.davidsonwp.com/job/100562133256145/senior-business-implementation-consultant-1/\n      https://www.seek.com.au/job/36025488\n      https://www.seek.com.au/job/40047761\n      https://www.seek.com.au/job/39944385\n      https://www.seek.com.au/job/39835797\n      https://www.seek.com.au/job/40101847\n      https://www.seek.com.au/job/40201201\n      https://probonoaustralia.com.au/jobs/2019/11/financial-counsellor-integrated-services-project/\n      https://www.seek.com.au/job/36166736\n      https://www.seek.com.au/job/40071743\n      https://www.seek.com.au/job/37116531\n      https://www.seek.com.au/job/37116530\n      https://probonoaustralia.com.au/jobs/2019/11/community-lawyer-integrated-services-project/\n      https://www.seek.com.au/job/37209437\n      https://www.seek.com.au/job/50749266?type=promoted\n      https://www.seek.com.au/job/37565417\n    \n    \n      view_date\n      2020-09-18 15:51:42+00:00\n      2020-10-19 05:08:00+00:00\n      2018-10-26 04:23:32\n      2020-05-27 20:05:51+00:00\n      2020-10-19 05:03:28+00:00\n      2020-10-22 04:58:01+00:00\n      2018-04-20 04:32:01\n      2019-09-28 08:41:44\n      2019-09-15 09:15:28\n      2019-09-01 07:04:43\n      2019-10-05 11:57:46\n      2019-10-19 07:56:47\n      2020-07-13 21:58:47+00:00\n      2018-05-09 04:42:46\n      2019-10-02 09:35:09\n      2018-09-02 00:36:35\n      2018-09-02 00:38:14\n      2020-07-13 21:35:19+00:00\n      2018-09-13 01:53:15\n      2020-10-30 20:01:37+00:00\n      2018-10-26 04:06:21\n    \n    \n      org\n      Davidson\n      Davidson\n      Upper Hunter Shire Council\n      Davidson\n      Davidson\n      Davidson\n      Upper Hunter Shire Council\n      Wow Recruitment\n      Wow Recruitment\n      Wow Recruitment\n      Wow Recruitment\n      Wow Recruitment\n      Social Security Rights Victoria\n      Regen Resurfacing PTY LTD\n      YMCA Victoria\n      Cairns & Hinterland Hospital and Health Service\n      Cairns & Hinterland Hospital and Health Service\n      Social Security Rights Victoria\n      Design & Build\n      F45 Training Artarmon\n      Hays Construction\n    \n    \n      salary_raw\n      $500 per day, Benefits: 7 hour day, 35 hour week with State Government.\n      $500 per day, Benefits: 7 hour day, 35 hour week with State Government.\n      $781.40 - $830.50 per 38 hour week\n      $800 - $1000 per hour\n      $800 - $1000 per hour\n      $800 - $1000 per hour\n      $972.10 - $1088.80 per 38 hour week\n      Hourly rate + Super + Bonuses OTE $1000 - $1800\n      Hourly rate + Super + Bonuses OTE $1000 - $1800\n      Hourly rate + Super + Bonuses OTE $1000 - $1800\n      $24.37/hour + Super + Bonuses OTE $1000 - $1800 PW\n      Hourly rate + Super + Bonuses OTE $1000 - $1800\n      $36,000-$38,500 (for 18 hour week)\n      $50,000 - $54,999 plus super + Vehicle & Mobile Ph\n      $62,936 - $70,682 per hour + super\n      $64287 - $97438 p.h.\n      $64287 - $97438 p.h.\n      $67,000-$70,000 for 30.4 hour week\n      $80 - $100k or hourly rate\n      Competitive hourly rate plus commission (OTE 90k)\n      $130000.00 - $150000.00 p.h. + +Super\n    \n    \n      salary_min\n      500\n      500\n      781.4\n      800\n      800\n      800\n      972.1\n      1000\n      1000\n      1000\n      1000\n      1000\n      36000\n      50000\n      62936\n      64287\n      64287\n      67000\n      80000\n      90000\n      130000\n    \n    \n      salary_max\n      NaN\n      NaN\n      830.5\n      1000\n      1000\n      1000\n      1088.8\n      1800\n      1800\n      1800\n      1800\n      1800\n      38500\n      54999\n      70682\n      97438\n      97438\n      70000\n      100000\n      NaN\n      150000\n    \n    \n      salary_hours\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n    \n      location_raw\n      Parramatta, AU\n      Parramatta, AU\n      Newcastle, Maitland & Hunter, AU\n      Melbourne, AU\n      Melbourne, AU\n      Melbourne, AU\n      Newcastle, Maitland & Hunter, AU\n      Melbourne, Victoria, Australia\n      Melbourne, Victoria, Australia\n      Melbourne, Victoria, Australia\n      Sydney, New South Wales, Australia\n      Melbourne, Victoria, Australia\n      Melbourne(Fitzroy)\n      Brisbane, CBD & Inner Suburbs, AU\n      Melbourne, Victoria, Australia\n      Cairns & Far North, AU\n      Cairns & Far North, AU\n      Melbourne(Fitzroy)\n      Melbourne, CBD & Inner Suburbs, AU\n      lanecovenorth, Sydney, New South Wales, Australia\n      Newcastle, Maitland & Hunter, AU\n    \n    \n      loc_id\n      1.0205e+08\n      1.0205e+08\n      4.04539e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      4.04539e+08\n      1.01933e+08\n      1.01933e+08\n      1.01933e+08\n      1.01932e+08\n      1.01933e+08\n      1.01933e+08\n      8.57823e+07\n      1.01933e+08\n      1.02079e+08\n      1.02079e+08\n      1.01933e+08\n      8.57823e+07\n      1.01932e+08\n      4.04539e+08\n    \n    \n      loc_continent\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n      Oceania\n    \n    \n      loc_country\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      New Zealand\n      New Zealand\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_county\n      Parramatta\n      Parramatta\n      Greater Bendigo\n      Melbourne\n      Melbourne\n      Melbourne\n      Greater Bendigo\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Yarra\n      Melbourne\n      Melbourne\n      Far North District\n      Far North District\n      Yarra\n      Melbourne\n      Sydney\n      Greater Bendigo\n    \n    \n      loc_empire\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      Australia\n      None\n      None\n      Australia\n      Australia\n      Australia\n      Australia\n    \n    \n      loc_localadmin\n      None\n      None\n      Hunter\n      Melbourne\n      Melbourne\n      Melbourne\n      Hunter\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Fitzroy\n      Melbourne\n      Melbourne\n      None\n      None\n      Fitzroy\n      Melbourne\n      Sydney\n      Hunter\n    \n    \n      loc_locality\n      None\n      None\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Fitzroy\n      Melbourne\n      Melbourne\n      None\n      None\n      Fitzroy\n      Melbourne\n      Sydney\n      None\n    \n    \n      loc_macrocounty\n      Sydney\n      Sydney\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      Melbourne\n      Melbourne\n      Melbourne\n      Sydney\n      Melbourne\n      Melbourne\n      Melbourne\n      Melbourne\n      None\n      None\n      Melbourne\n      Melbourne\n      Sydney\n      None\n    \n    \n      loc_region\n      New South Wales\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      New South Wales\n      Victoria\n      Victoria\n      Victoria\n      Victoria\n      Northland Region\n      Northland Region\n      Victoria\n      Victoria\n      New South Wales\n      Victoria\n    \n    \n      processor\n      davidsonwp\n      davidsonwp\n      kaggle_promptcloud_listings\n      davidsonwp\n      davidsonwp\n      davidsonwp\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_latest\n      probono\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_latest\n      kaggle_promptcloud_listings\n      kaggle_promptcloud_listings\n      probono\n      kaggle_promptcloud_listings\n      seek\n      kaggle_promptcloud_listings\n    \n    \n      source\n      CC-MAIN-2020-40\n      CC-MAIN-2020-45\n      seekau\n      CC-MAIN-2020-24\n      CC-MAIN-2020-45\n      CC-MAIN-2020-45\n      seekau\n      seekau_2019q3\n      seekau_2019q3\n      seekau_2019q3\n      seekau_2019q3\n      seekau_2019q3\n      CC-MAIN-2020-29\n      seekau\n      seekau_2019q3\n      seekau\n      seekau\n      CC-MAIN-2020-29\n      seekau\n      CC-MAIN-2020-45\n      seekau\n    \n    \n      loc_neighbourhood\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n      None\n      None\n      Melbourne CBD\n      None\n      None\n    \n    \n      annual\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      weekly\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      daily\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      hourly\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      salary_low\n      500\n      500\n      781.4\n      800\n      800\n      800\n      972.1\n      1000\n      1000\n      1000\n      1000\n      1000\n      36000\n      50000\n      62936\n      64287\n      64287\n      67000\n      80000\n      90000\n      130000\n    \n    \n      salary_hi\n      NaN\n      NaN\n      830.5\n      1000\n      1000\n      1000\n      1088.8\n      1800\n      1800\n      1800\n      1800\n      1800\n      38500\n      54999\n      70682\n      97438\n      97438\n      70000\n      100000\n      NaN\n      150000\n    \n    \n      salary_valid\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n      True\n    \n    \n      pct_salary_range\n      NaN\n      NaN\n      0.0609219\n      0.222222\n      0.222222\n      0.222222\n      0.113251\n      0.571429\n      0.571429\n      0.571429\n      0.571429\n      0.571429\n      0.0671141\n      0.09522\n      0.115942\n      0.409968\n      0.409968\n      0.0437956\n      0.222222\n      NaN\n      0.142857\n    \n  \n\n\n\n\n\nSALARY_HOURLY_MAX_AUD = 300\n\n\ndf.loc[df.hourly & (df.salary_low > SALARY_HOURLY_MAX_AUD), \"salary_valid\"] = False\n\n\n%time sns.displot(df[df.hourly & df.salary_valid], x=\"salary_low\", kde=True, log_scale=True)\n\nCPU times: user 347 ms, sys: 49.8 ms, total: 397 ms\nWall time: 295 ms\n\n\n<seaborn.axisgrid.FacetGrid at 0x7f2bc42190a0>"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#importing-libraries",
    "href": "notebooks/Detecting duplicate job ads.html#importing-libraries",
    "title": "skeptric",
    "section": "Importing Libraries",
    "text": "Importing Libraries\n\nimport pandas as pd\nimport re\n\n\nfrom IPython.display import HTML, display\n\nWe’ll define a type TokenList for convenience\n\nfrom typing import List, Any, Callable, Tuple, Union\n\nToken = str\nTokenList = List[Token]\n\n\npd.options.display.max_colwidth=100"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#highlighting-the-differences",
    "href": "notebooks/Detecting duplicate job ads.html#highlighting-the-differences",
    "title": "skeptric",
    "section": "Highlighting the Differences",
    "text": "Highlighting the Differences\nWe can use Python’s difflib to find and highlight differences between the two texts.\n\nimport difflib\nimport html\n\nWe can markup the differences using HTML; we’ll colour and bold each token.\nWe could mark each token:\n\ndef mark_text(text:str) -> str:\n    return f'<span style=\"color: red;\">{text}</span>'\n\ndef mark_span(text:TokenList) -> TokenList:\n    return [mark_text(token) for token in text]\n\nor the whole span:\n\ndef mark_span(text:TokenList) -> TokenList:\n    if len(text) > 0:\n        text[0] = '<span style=\"background: #69E2FB;\">' + text[0]\n        text[-1] += '</span>'\n    return text\n\nIdentity function for convenience\n\ndef identity(x):\n    return x\n\nmarkup_diff processes the tokenlists, marking any tokens that are different.\nTo understand autojunk=False see Dear DiffLib; it’s turning off some clever magic I don’t want to use.\nNote: The output should actually be something like a MakedUpTokenList, but I’m not really sure about the abstraction I would want to use.\n\ndef markup_diff(a:TokenList, b:TokenList,\n                mark:Callable[[TokenList], TokenList]=mark_span,\n                mark_equal: Callable[[TokenList], TokenList]=identity,\n                isjunk:Union[None, Callable[[Token], bool]]=None) -> Tuple[TokenList, TokenList]:\n    \"\"\"Returns a and b with any differences processed by mark\n    \n    Junk is ignored by the differ\n    \"\"\"\n    seqmatcher = difflib.SequenceMatcher(isjunk=isjunk, a=a, b=b, autojunk=False)\n    out_a, out_b = [], []\n    for tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        markup = mark_equal if tag == 'equal' else mark\n        out_a += markup(a[a0:a1])\n        out_b += markup(b[b0:b1])\n    assert len(out_a) == len(a)\n    assert len(out_b) == len(b)\n    return out_a, out_b\n\nMake sure to escape HTML special characters because our markup is in HTML\n\ntok_a, tok_b = tokenize(html.escape(ad_a)), tokenize(html.escape(ad_b))\n\n\nout_a, out_b = markup_diff(tok_a, tok_b)\n\nIt’s now clear that there are some small differences in punctuation and whitespace between the two ads, but are otherwise the same (except for the last line).\n\nHTML(untokenize(out_a))\n\nJava J****EE Developer ****k ****k Music, Film & TV London Java J****EE Developers required for software house with client sectors of music, film and TV. Salary: Maximum ****: Discretionary bonus and benefits package. Location: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java Developer. The working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forward. This is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounder. EXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platform. A minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologies. Good knowledge of CSS, XML and DHTML A personality suited to clientfacing situations good communication skills. A good standard of written English The above experience is essential. You require all of the above experience in order for to be eligible for this role. The following experience is desirable, though not essential: Knowledge of the WebSphere development environment and Application Server. Knowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and support. You'll be involved in different technologies across the board from Front Office to Back Office. Please note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworks. THE OPPORTUNITY Why work here? As for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of development. Therefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technology. This is a central role that essentially can take off in any direction. Here, you will have enough autonomy to define your own role. Therefore, if you take the initiative you can shape your role for the future and drive your own progression. Overall, this is a lovely place to work it's a privatelyowned company and feels more like a family company, not at all institutionalised everyone has a stake, everyone has a say. Being music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreign. Location: Near Euston and King's Cross, London\n\n\n\nHTML(untokenize(out_b))\n\nNEW Java J****EE Developer – ****k ****k Music, Film TV London Java J****EE Developers required for software house with client sectors of music, film and TV. Salary: Maximum ****: Discretionary bonus and benefits package. Location: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java Developer. The working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forward. This is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounder. EXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platform. A minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologies. Good knowledge of CSS, XML and DHTML A personality suited to clientfacing situations good communication skills. A good standard of written English The above experience is essential. You require all of the above experience in order for to be eligible for this role. The following experience is desirable, though not essential: Knowledge of the Websphere development environment and application server. Knowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and support. You'll be involved in different technologies across the board from front office to back office. Please note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworks. THE OPPORTUNITY: Why work here? As for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of development. Therefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technology. This is a central role that essentially can take off in any direction. Here, you will have enough autonomy to define your own role. Therefore, if you take the initiative you can shape your role for the future and drive your own progression. Overall, this is a lovely place to work it's a privatelyowned company and feels more like a family company, not at all institutionalised everyone has a stake, everyone has a say. Being music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreign. Location: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#putting-ads-side-by-side",
    "href": "notebooks/Detecting duplicate job ads.html#putting-ads-side-by-side",
    "title": "skeptric",
    "section": "Putting Ads Side by Side",
    "text": "Putting Ads Side by Side\nIt’s still hard to compare two ads below each other, so let’s try to put them side by side.\nWe could in fact show differences the ads at a sentence level instead of a word level\n\nsent_a, sent_b = sentencize(html.escape(ad_a)), sentencize(html.escape(ad_b))\nout_a, out_b = markup_diff(sent_a, sent_b)\n\n\nHTML(html_unsentencise(out_a))\n\nJava J****EE Developer  ****k  ****k  Music, Film & TV  London Java J****EE Developers required for software house with client sectors of music, film and TVSalary: Maximum ****: Discretionary bonus and benefits packageLocation: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThis is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounderEXPERIENCE REQUIRED: The experience required for this role is as follows:  A minimum of 5 years experience in the development of web applications for the J****EE development platformA minimum of 5 years experience in Java  Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesGood knowledge of CSS, XML and DHTML  A personality suited to clientfacing situations  good communication skillsA good standard of written English The above experience is essentialYou require all of the above experience in order for to be eligible for this roleThe following experience is desirable, though not essential:  Knowledge of the WebSphere development environment and Application ServerKnowledge of and experience with AJAX (Asynchronous JavaScript XML)  Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportYou'll be involved in different technologies across the board from Front Office to Back OfficePlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksTHE OPPORTUNITY Why work hereAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyThis is a central role that essentially can take off in any directionHere, you will have enough autonomy to define your own roleTherefore, if you take the initiative you can shape your role for the future and drive your own progressionOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignLocation: Near Euston and King's Cross, London\n\n\n\nHTML(html_unsentencise(out_b))\n\nNEW  Java J****EE Developer – ****k  ****k  Music, Film TV  London Java J****EE Developers required for software house with client sectors of music, film and TVSalary: Maximum ****: Discretionary bonus and benefits packageLocation: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThis is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounderEXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platformA minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesGood knowledge of CSS, XML and DHTML A personality suited to clientfacing situations  good communication skillsA good standard of written English The above experience is essentialYou require all of the above experience in order for to be eligible for this roleThe following experience is desirable, though not essential: Knowledge of the Websphere development environment and application serverKnowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportYou'll be involved in different technologies across the board from front office to back officePlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksTHE OPPORTUNITY: Why work hereAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyThis is a central role that essentially can take off in any directionHere, you will have enough autonomy to define your own roleTherefore, if you take the initiative you can shape your role for the future and drive your own progressionOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignLocation: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****\n\n\nWe could use some CSS to display them side by side\n\nfrom itertools import zip_longest\n\n\ndef html_sidebyside(a, b):\n    # Set the panel display\n    out = '<div style=\"display: grid;grid-template-columns: 1fr 1fr;grid-gap: 20px;\">'\n    # There's some CSS in Jupyter notebooks that makes the first pair unalign. This is a workaround\n    out += '<p></p><p></p>'\n    for left, right in zip_longest(a, b, fillvalue=''):\n        out += f'<p>{left}</p>'\n        out += f'<p>{right}</p>'\n    out += '</div>'\n    return out\n\nHTML(html_sidebyside(out_a, out_b))\n\nJava J****EE Developer  ****k  ****k  Music, Film & TV  London Java J****EE Developers required for software house with client sectors of music, film and TVNEW  Java J****EE Developer – ****k  ****k  Music, Film TV  London Java J****EE Developers required for software house with client sectors of music, film and TVSalary: Maximum ****: Discretionary bonus and benefits packageSalary: Maximum ****: Discretionary bonus and benefits packageLocation: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperLocation: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThis is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounderThis is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounderEXPERIENCE REQUIRED: The experience required for this role is as follows:  A minimum of 5 years experience in the development of web applications for the J****EE development platformEXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platformA minimum of 5 years experience in Java  Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesA minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesGood knowledge of CSS, XML and DHTML  A personality suited to clientfacing situations  good communication skillsGood knowledge of CSS, XML and DHTML A personality suited to clientfacing situations  good communication skillsA good standard of written English The above experience is essentialA good standard of written English The above experience is essentialYou require all of the above experience in order for to be eligible for this roleYou require all of the above experience in order for to be eligible for this roleThe following experience is desirable, though not essential:  Knowledge of the WebSphere development environment and Application ServerThe following experience is desirable, though not essential: Knowledge of the Websphere development environment and application serverKnowledge of and experience with AJAX (Asynchronous JavaScript XML)  Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportKnowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportYou'll be involved in different technologies across the board from Front Office to Back OfficeYou'll be involved in different technologies across the board from front office to back officePlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksPlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksTHE OPPORTUNITY Why work hereTHE OPPORTUNITY: Why work hereAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyThis is a central role that essentially can take off in any directionThis is a central role that essentially can take off in any directionHere, you will have enough autonomy to define your own roleHere, you will have enough autonomy to define your own roleTherefore, if you take the initiative you can shape your role for the future and drive your own progressionTherefore, if you take the initiative you can shape your role for the future and drive your own progressionOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignLocation: Near Euston and King's Cross, LondonLocation: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****\n\n\nBecause many sentences are the same we could use difflib to align them at a sentence level; adding in padding when sentences are added or deleted.\nThis way we’ll get paired sentences between the texts (this may not work well if every line is a little different, or if the punctuation results in the ads having different sentence structure).\n\ndef align_seqs(a: TokenList, b: TokenList, fill:Token='') -> Tuple[TokenList, TokenList]:\n    out_a, out_b = [], []\n    seqmatcher = difflib.SequenceMatcher(a=a, b=b, autojunk=False)\n    for tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        delta = (a1 - a0) - (b1 - b0)\n        out_a += a[a0:a1] + [fill] * max(-delta, 0)\n        out_b += b[b0:b1] + [fill] * max(delta, 0)\n    assert len(out_a) == len(out_b)\n    return out_a, out_b\n\n\nout_a, out_b = align_seqs(sent_a, sent_b)\n\nHTML(html_sidebyside(out_a, out_b))\n\nJava J****EE Developer  ****k  ****k  Music, Film & TV  London Java J****EE Developers required for software house with client sectors of music, film and TVNEW  Java J****EE Developer – ****k  ****k  Music, Film TV  London Java J****EE Developers required for software house with client sectors of music, film and TVSalary: Maximum ****: Discretionary bonus and benefits packageSalary: Maximum ****: Discretionary bonus and benefits packageLocation: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperLocation: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThis is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounderThis is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounderEXPERIENCE REQUIRED: The experience required for this role is as follows:  A minimum of 5 years experience in the development of web applications for the J****EE development platformEXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platformA minimum of 5 years experience in Java  Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesA minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesGood knowledge of CSS, XML and DHTML  A personality suited to clientfacing situations  good communication skillsGood knowledge of CSS, XML and DHTML A personality suited to clientfacing situations  good communication skillsA good standard of written English The above experience is essentialA good standard of written English The above experience is essentialYou require all of the above experience in order for to be eligible for this roleYou require all of the above experience in order for to be eligible for this roleThe following experience is desirable, though not essential:  Knowledge of the WebSphere development environment and Application ServerThe following experience is desirable, though not essential: Knowledge of the Websphere development environment and application serverKnowledge of and experience with AJAX (Asynchronous JavaScript XML)  Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportKnowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportYou'll be involved in different technologies across the board from Front Office to Back OfficeYou'll be involved in different technologies across the board from front office to back officePlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksPlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksTHE OPPORTUNITY Why work hereTHE OPPORTUNITY: Why work hereAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyThis is a central role that essentially can take off in any directionThis is a central role that essentially can take off in any directionHere, you will have enough autonomy to define your own roleHere, you will have enough autonomy to define your own roleTherefore, if you take the initiative you can shape your role for the future and drive your own progressionTherefore, if you take the initiative you can shape your role for the future and drive your own progressionOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayOverall, this is a lovely place to work  it's a privatelyowned company and feels more like a family company, not at all institutionalised  everyone has a stake, everyone has a sayBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignLocation: Near Euston and King's Cross, LondonLocation: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****\n\n\nWe can combine all of these to show the token differences between aligned sentences\n\ndef html_diffs(a, b):\n    a = html.escape(a)\n    b = html.escape(b)\n    \n    out_a, out_b = [], []\n    for sent_a, sent_b in zip(*align_seqs(sentencize(a), sentencize(b))):\n        mark_a, mark_b = markup_diff(tokenize(sent_a), tokenize(sent_b))\n        out_a.append(untokenize(mark_a))\n        out_b.append(untokenize(mark_b))\n    \n    return html_sidebyside(out_a, out_b)\n\nNow it’s quite easy to see the differences\n\nHTML(html_diffs(ad_a, ad_b))\n\nJava J****EE Developer ****k ****k Music, Film & TV London Java J****EE Developers required for software house with client sectors of music, film and TVNEW Java J****EE Developer – ****k ****k Music, Film TV London Java J****EE Developers required for software house with client sectors of music, film and TVSalary: Maximum ****: Discretionary bonus and benefits packageSalary: Maximum ****: Discretionary bonus and benefits packageLocation: Near Euston and King's Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperLocation: Near Euston and King’s Cross, London THE COMPANY: Consistent new business wins for the world leader in the provision of software solutions to the Music and Entertainment industry has given rise to the need for an experienced Java DeveloperThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThe working environment here is very pleasant with a casual dress code, laid back and friendly atmosphere, but also hardworking and dynamic with the autonomy to drive your job role forwardThis is predominantly a development role, but you will be involved in the full product life cycle including design and clientfacing duties, so they need a good allrounderThis is predominantly a development role, but you will be involved in the full product lifecycle including design and clientfacing duties, so they need a good allrounderEXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platformEXPERIENCE REQUIRED: The experience required for this role is as follows: A minimum of 5 years experience in the development of web applications for the J****EE development platformA minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesA minimum of 5 years experience in Java Strong knowledge in all of JSP, Servlet, JDBC, JavaScript, SQL and HTML technologiesGood knowledge of CSS, XML and DHTML A personality suited to clientfacing situations good communication skillsGood knowledge of CSS, XML and DHTML A personality suited to clientfacing situations good communication skillsA good standard of written English The above experience is essentialA good standard of written English The above experience is essentialYou require all of the above experience in order for to be eligible for this roleYou require all of the above experience in order for to be eligible for this roleThe following experience is desirable, though not essential: Knowledge of the WebSphere development environment and Application ServerThe following experience is desirable, though not essential: Knowledge of the Websphere development environment and application serverKnowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportKnowledge of and experience with AJAX (Asynchronous JavaScript XML) Experience with IBM DB**** THE ROLE: This is a full SDLC role: You will be involved in all stages of the software development cycle from requirements gathering and specification through development, implementation, QA and supportYou'll be involved in different technologies across the board from Front Office to Back OfficeYou'll be involved in different technologies across the board from front office to back officePlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksPlease note there is no Spring or Hibernate: The company have instead developed their own inhouse frameworksTHE OPPORTUNITY Why work hereTHE OPPORTUNITY: Why work hereAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentAs for prospects, where you can take this role is flexible, as the role entails a wide remit across most aspects of developmentTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyTherefore, if you wish, you could become more clientfacing and progress to what is essentially a business analyst role, or you may wish to specialise more on the technical side of things and push the boundaries of the technologyThis is a central role that essentially can take off in any directionThis is a central role that essentially can take off in any directionHere, you will have enough autonomy to define your own roleHere, you will have enough autonomy to define your own roleTherefore, if you take the initiative you can shape your role for the future and drive your own progressionTherefore, if you take the initiative you can shape your role for the future and drive your own progressionOverall, this is a lovely place to work it's a privatelyowned company and feels more like a family company, not at all institutionalised everyone has a stake, everyone has a sayOverall, this is a lovely place to work it's a privatelyowned company and feels more like a family company, not at all institutionalised everyone has a stake, everyone has a sayBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignBeing music, entertainment and film it's an interesting industry to work in too, with a wide range of clients both local and foreignLocation: Near Euston and King's Cross, LondonLocation: Near Euston and King’s Cross, London This job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaJ****EEDeveloper****k****kMusicFilmTVLondon_job****\n\n\nLet’s wrap it in a function to make it easy to show in Jupyter notebook\n\ndef show_diffs(a, b):\n    display(HTML(html_diffs(a,b)))"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#edit-distance",
    "href": "notebooks/Detecting duplicate job ads.html#edit-distance",
    "title": "skeptric",
    "section": "Edit Distance",
    "text": "Edit Distance\nDuplicate ads are likely to only have some small changes, so one approach would be to calculate the Levenshtein Distance; if this is small relative to the length of the ads they are likely to be duplicates.\nSee the related article\n\nimport editdistance\n\ndef relative_editdistance(a, b):\n    return editdistance.eval(a, b) / max(len(a), len(b))\n\n\nrelative_editdistance('aaaa', 'aaaa')\n\n0.0\n\n\n\nrelative_editdistance('aaaa', 'aaba')\n\n0.25\n\n\n\nrelative_editdistance('bbbb', 'aaaa')\n\n1.0\n\n\n\nrelative_editdistance('b', 'aaaa')\n\n1.0\n\n\n\nrelative_editdistance('abab', 'baba')\n\n0.5\n\n\n\nrelative_editdistance('abc', 'xyzw')\n\n1.0\n\n\nWe can then evaluate this at a character level\n\nrelative_editdistance(ad_a, ad_b)\n\n0.04612954186413902\n\n\nOr at a token level:\n\nrelative_editdistance(tokenize(ad_a), tokenize(ad_b))\n\n0.044444444444444446\n\n\nLet’s run it on the firt 100 pairs of ads\n\n%%time\ndistance = {}\nads_sample = ads[:100]\nfor i, ad1 in enumerate(ads_sample):\n    for j, ad2 in enumerate(ads_sample):\n        if i < j:\n            distance[(i, j)] = relative_editdistance(ad1, ad2)\n\nCPU times: user 1min 54s, sys: 359 ms, total: 1min 55s\nWall time: 1min 57s\n\n\nNote this takes close to a minute with 100 ads, and scales quadratically so for the whole 400k ads would take over to 26 years\n\nfrom datetime import timedelta\n(timedelta(seconds=50) * (len(ads) / 100) ** 2) / timedelta(days=365)\n\n26.378981994545917\n\n\nLet’s look at candidate pairs with less than 10% edit distance\n\nfor k, v in distance.items():\n    if v < 0.1:\n        print(k, v)\n\n(11, 79) 0.05895691609977324\n(57, 58) 0.003784295175023652\n(74, 76) 0.04600484261501211\n(87, 96) 0.04146249528835281\n\n\nThis looks like a duplicate; the second version just has PROCESS and PHARMACEUTICAL, where the first has PROJECT.\nNote that here a missing fullstop has broken the sentence alignment.\n\nshow_diffs(ads[11], ads[79])\n\nPROJECT ENGINEER PHAMACEUTICAL, c**** Excellent Benefits, Oxfordshire ****/****/mg PROJECT ENGINEER : JOB ROLE: Responsible for execution of Continuous improvement and compliance projectsPROCESS PROJECT ENGINEER PHAMACEUTICAL, c**** **** Excellent Benefits, Oxfordshire ****/****/dh PROCESS ENGINEER PHARMACEUTICAL: JOB ROLE: Responsible for execution of Continuous improvement and compliance projectsResponsible too for subprojects related to introduction of new products and processes in order to meet strategic business plans.The position is jointly responsible fo reporting of designated capital and expenses (actual VsResponsible too for subprojects related To introduction of new products and processes in order to meet strategic business plansThe position is jointly responsible fo reporting of designated capital and expensive (actual VsPlan) associated with implementation of new process equipment or modifications to Existing process equipmentPlan) associated with implementation of new process equipment or modifications to Existing process equipmentPROJECT ENGINEER: KEY RESPONSIBILITIES: Manage project tasks & deliverables within agreed time, cost, quality Responsbile for process equipment validation, material validation, remediation activity Maintain effectiveness of Quality System in accordance with corporate requirements Effective liaison with Engineering functions, external suppliers and regulatory bodies PROJECT ENGINEER: SKILLS, EXPERIENCE, QUALIFICATION: Proven problem solving skills to quickly implement process improvements Understanding of technical drawings, dealing with suppliers and subcontractors Experience of Project Engineering/Project Management within regulated manufacturing Working knowledge of quality/regulatory systems BSc/HNC with related experiencePROCESS ENGINEER: PHARMACEUTICAL: KEY RESPONSIBILITIES: Manage project tasks & deliverables within agreed time, cost, quality Responsbile for process equipment validation, material validation, remediation activity Maintain effectiveness of Quality System in accordance with corporate requirements Effective liaison with Engineering functions, external suppliers and regulatory bodies PROCESS ENGINEER: PHARMACEUTICAL: SKILLS, EXPERIENCE, QUALIFICATION: Proven problem solving skills to quickly implement process improvements Understanding of technical drawings, dealing with suppliers and subcontractors Experience of Project Engineering/Project Management within regulated manufacturing Working knowledge of quality/regulatory systems BSc/HNC with related experience\n\n\nThey were both posted by the same recruiter, but to different categories (Healthcare & Nursing seems like a mistake) with slightly different salaries\n\ndf.iloc[[11, 79]]\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      11\n      20199757\n      PROJECT ENGINEER, PHARMACEUTICAL\n      PROJECT ENGINEER PHAMACEUTICAL, c****  Excellent Benefits, Oxfordshire ****/****/mg PROJECT ENGI...\n      Witney, Oxfordshire\n      Witney\n      NaN\n      permanent\n      MatchBox Recruiting Ltd\n      Healthcare & Nursing Jobs\n      35000 - 40000/annum c40000 + Excellent Benefits\n      37500.0\n      cv-library.co.uk\n      Train\n    \n    \n      79\n      32992148\n      Process Project Engineer\n      PROCESS PROJECT ENGINEER PHAMACEUTICAL, c****  ****  Excellent Benefits, Oxfordshire ****/****/d...\n      Witney, Oxfordshire\n      Witney\n      NaN\n      permanent\n      MatchBox Recruiting Ltd\n      Manufacturing Jobs\n      30000 - 40000/annum c3500 + excellent benefits\n      35000.0\n      cv-library.co.uk\n      Train\n    \n  \n\n\n\n\nThe location is changed (in a minor way), but probably still the same ad\n\nshow_diffs(ads[57], ads[58])\n\n\nSenior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas sectors Our client requires a Senior Mechanical Engineer with design and substantiation experience to join their expanding teamSenior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas sectors Our client requires a Senior Mechanical Engineer with design and substantiation experience to join their expanding teamThey have challenging projects requiring the design and substantiation of complex components and structures in high integrity environmentsThey have challenging projects requiring the design and substantiation of complex components and structures in high integrity environmentsThe Mechanical Design and Analysis team is currently expanding to meet strong demand from its main clientsThe Mechanical Design and Analysis team is currently expanding to meet strong demand from its main clientsThe team s work supports a number of key Defence programmes including the UK Submarine programme (both inservice and next generation vessels, as well as dockyard equipment and facilities) and high integrity structures for new Defence establishmentsThe team s work supports a number of key Defence programmes including the UK Submarine programme (both inservice and next generation vessels, as well as dockyard equipment and facilities) and high integrity structures for new Defence establishmentsOur client also delivers a significant amount of work outside the Defence industry, making use of their strong design and analytical skills and plant knowledge to deliver projects for other internal or external clientsOur client also delivers a significant amount of work outside the Defence industry, making use of their strong design and analytical skills and plant knowledge to deliver projects for other internal or external clientsEngineers working within the team therefore have the opportunity to challenge themselves through delivering interesting and varied projects for a range of different clients and industriesEngineers working within the team therefore have the opportunity to challenge themselves through delivering interesting and varied projects for a range of different clients and industriesOur client is the largest consultant engineer to the UK Defence marketOur client is the largest consultant engineer to the UK Defence marketTheir mission is to ensure success for their clients, whatever the challengeTheir mission is to ensure success for their clients, whatever the challengeTheir multiskilled experts cover land, sea and air, as well as information and communications they work across the lifecycle of a platform, system or facilityTheir multiskilled experts cover land, sea and air, as well as information and communications they work across the lifecycle of a platform, system or facilityThey achieve success by working in close partnership with their clients, by matching their skills to their needs, being flexible, and by using their resources efficientlyThey achieve success by working in close partnership with their clients, by matching their skills to their needs, being flexible, and by using their resources efficientlyWhether they are delivering high quality engineering and technical services or programme management, they always deliver the solution that s right for the clientWhether they are delivering high quality engineering and technical services or programme management, they always deliver the solution that s right for the clientRequirements Senior Mechanical Engineer Design and Substantiation The successful candidate will have a track record of supervising delivery of technical solutions in challenging timescalesRequirements Senior Mechanical Engineer Design and Substantiation The successful candidate will have a track record of supervising delivery of technical solutions in challenging timescalesQualifications: Degreequalified or equivalent in Mechanical Engineering or a related discipline, Chartered Engineer Qualifications: Degreequalified or equivalent in Mechanical Engineering or a related discipline, Chartered Engineer Essential Criteria Significant demonstrable experience in two or more of the following technical disciplines is requiredConcept design and developmentEssential Criteria Significant demonstrable experience in two or more of the following technical disciplines is requiredConcept design and developmentDesign for manufactureDesign for manufactureCAD (preferably Unigraphics or Pro/ENGINEER)CAD (preferably Unigraphics or Pro/ENGINEER)Design substantiation by hand calculation or Finite Element analysis (preferably Abaqus or Ansys)Design substantiation by hand calculation or Finite Element analysis (preferably Abaqus or Ansys)Furthermore, the successful candidate will demonstrate: A sound understanding of mechanical engineering principles and their application to varied projects and domains A desire to be challenged and to contribute to the success of the business Desirable Criteria Experience in one or more of the following industry domains is also desirable: Aero Engines or Aero StructuresFurthermore, the successful candidate will demonstrate: A sound understanding of mechanical engineering principles and their application to varied projects and domains A desire to be challenged and to contribute to the success of the business Desirable Criteria Experience in one or more of the following industry domains is also desirable: Aero Engines or Aero StructuresSteamraising power plant and components (pressure vessels, heat exchangers, pumps, valves, pipework etc)Steamraising power plant and components (pressure vessels, heat exchangers, pumps, valves, pipework etc)Nuclear industry knowledge and safety case appreciationNuclear industry knowledge and safety case appreciationAlthough the team is based in Silchester, opportunities also exist at various client sites around the countryAlthough the team is based in Silchester, opportunities also exist at various client sites around the countryFlexibility on work location and a willingness to undertake secondments to other sites is beneficialFlexibility on work location and a willingness to undertake secondments to other sites is beneficialResponsibilities Senior Mechanical Engineer Design and Substantiation As a successful applicant, you would be expected to: Identify, supervise and produce designs supported by calculations to demonstrate acceptable performance or identify design issues Author and update major technical documents to present technical findings (e.gResponsibilities Senior Mechanical Engineer Design and Substantiation As a successful applicant, you would be expected to: Identify, supervise and produce designs supported by calculations to demonstrate acceptable performance or identify design issues Author and update major technical documents to present technical findings (e.gdesign substantiation reports, safety justifications)design substantiation reports, safety justifications)Lead requirements capture activities to identify the scope of work requiredLead requirements capture activities to identify the scope of work requiredSupport bidding activities and produce proposal documentsSupport bidding activities and produce proposal documentsManage task delivery against Time, Cost and Quality requirementsManage task delivery against Time, Cost and Quality requirementsSupport our QA process through review or checking colleagues workSupport our QA process through review or checking colleagues workLead presentations to clientsLead presentations to clientsLead or work within project teams (though some independent work, including placements at clients sites, may be required)Lead or work within project teams (though some independent work, including placements at clients sites, may be required)Take responsibility for the quality and delivery of your own work and maintain our clients reputationTake responsibility for the quality and delivery of your own work and maintain our clients reputationSenior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas sectors Salary: **** to **** Location: Reading, BerkshireSenior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas sectors Salary: **** to **** Location: Reading/Basingstoke border\n\n\nMy guess is the ad was updated/reposted with a more specific location\n\ndf.iloc[[57, 58]]\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      57\n      31301427\n      Senior Mechanical Engineer Design and Substantiation\n      Senior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas se...\n      Berkshire, South East\n      Berkshire\n      NaN\n      permanent\n      Gregory Martin International\n      Engineering Jobs\n      40000 - 60000/annum 40K-60K\n      50000.0\n      cv-library.co.uk\n      Train\n    \n    \n      58\n      31301430\n      Senior Mechanical Engineer Design and Substantiation\n      Senior Mechanical Engineer Design and Substantiation Defence, Aerospace, Nuclear, Oil and Gas se...\n      Hampshire, South East\n      Hampshire\n      NaN\n      permanent\n      Gregory Martin International\n      Engineering Jobs\n      40000 - 60000/annum 40K-60K\n      50000.0\n      cv-library.co.uk\n      Train\n    \n  \n\n\n\n\nThis pair have some punctuation changes\n\nshow_diffs(ads[74], ads[76])\n\nChef De Partie up to **** Tips Ipswich Outskirts Clear Selection are pleased to working with this new fresh food client based in on the outskirts of the town of IpswichChef De Partie up to **** Tips Ipswich Outskirts Clear Selection are pleased to working with this new fresh food client based in on the outskirts of the town of IpswichOur client runs an extremely busy modern public house where the food is seasonal and freshOur client runs an extremely busy modern public house where the food is seasonal and freshWe are seeking a proactive, Chef De Partie who wishes to work within a professional environmentWe are seeking a proactive, Chef De Partie who wishes to work within a professional environmentThis is a chance to work within a very talented team, where a chef can continue to learn new techniques and recipes, as Chef de partie you will have good solid experience of working with fresh food, you will need to have a British culinary knowledge with great knife skillsThis is a chance to work within a very talented team, where a chef can continue to learn new techniques and recipes, as Chef de partie you will have good solid experience of working with fresh food, you will need to have a British culinary knowledge with great knife skillsI cannot emphasise enough this job is great, it's a bustling, busy organised environment, with great cohesion between front and back of house and a great team to become a permanent long term fixture inI cannot emphasise enough this job is great, it s a bustling, busy organised environment, with great cohesion between front and back of house and a great team to become a permanent long term fixture inAs Chef de Partie you will want to achieve great things, you will need to have a real passion for good food, you will be able to add ideas to the team and effectively run different sectionsAs Chef de Partie you will want to achieve great things, you will need to have a real passion for good food, you will be able to add ideas to the team and effectively run different sectionsInterested with a great solid background with fresh food, must have transport due to location of the property Please email CV Salary up to **** Ref ****JC Jemma Collins Recruitment Manager Leads the Recruitment Team for Clear Selection ensuring we are one of the finest in the business / **** **** Want a quick job search without the hassle of registering then paste www.clearselection.co.uk/search.php to your web browser and enjoy the experienceInterested with a great solid background with fresh food, must have transport due to location of the property Please email CV Salary up to **** Ref ****JC Jemma Collins Recruitment Manager Leads the Recruitment Team for Clear Selection ensuring we are one of the finest in the business / **** **** Want a quick job search without the hassle of registering then paste www.clearselection.co.uk/search.php to your web browser and enjoy the experienceWe are always busy and are constantly receiving new instructions from a number of different client's nationwide, which makes it good for candidates and usWe are always busy and are constantly receiving new instructions from a number of different client s nationwide, which makes it good for candidates and usIn the majority of cases we have visited the client's establishment and even spent work days with them and have a very enviable reputation for finding the best for the bestIn the majority of cases we have visited the client s establishment and even spent work days with them and have a very enviable reputation for finding the best for the bestIf you desire a professional, honest and calm approach with the reassurance that your application will be treated confidentially if requested, we look forward to working on your behalfIf you desire a professional, honest and calm approach with the reassurance that your application will be treated confidentially if requested, we look forward to working on your behalfOur Consultants love their jobs and its always worth trying to contact them on their mobiles in the evenings or at weekends, if you are to busy during the dayOur Consultants love their jobs and its always worth trying to contact them on their mobiles in the evenings or at weekends, if you are to busy during the dayRemember if they do not contact you within **** hours, unfortunately you have not been successful in your application for this positionRemember if they do not contact you within **** hours, unfortunately you have not been successful in your application for this positionClear Selection Recruitment is part of the Clear Selection Group EstClear Selection Recruitment is part of the Clear Selection Group Est**** This job was originally posted as www.caterer.com/JobSeeking/ChefDePartieupto****TipsIpswichOutskirts_job********\n\n\nIt looks like the same job ad was posted to 2 different sources, and the second one mangled the punctuation\n\ndf.iloc[[74, 76]]\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      74\n      32621550\n      Chef De Partie up to ****  Tips Ipswich Outskirts\n      Chef De Partie up to ****  Tips Ipswich Outskirts Clear Selection are pleased to working with th...\n      Ipswich Suffolk East Anglia\n      UK\n      NaN\n      NaN\n      Clear Selection\n      Hospitality & Catering Jobs\n      17000 per annum\n      17000.0\n      caterer.com\n      Train\n    \n    \n      76\n      32695796\n      Chef De Partie up to ****  Tips Ipswich Outskirts\n      Chef De Partie up to **** Tips Ipswich Outskirts Clear Selection are pleased to working with thi...\n      Ipswich, Suffolk, UK, Suffolk\n      Ipswich\n      NaN\n      NaN\n      Clear Selection\n      Hospitality & Catering Jobs\n      17000 per annum\n      17000.0\n      jobs.catererandhotelkeeper.com\n      Train\n    \n  \n\n\n\n\nThis is the same kind of duplication as the previous\n\nshow_diffs(ads[87], ads[96])\n\n\nChef De Partie Norfolk Live In up to **** Tips Are you seeking a new role as a chef de partie in a fantastic fresh food rosette gastro pubChef De Partie Norfolk Live In up to **** Tips Are you seeking a new role as a chef de partie in a fantastic fresh food rosette gastro pubOur retained client runs a very successful pub and this small group are continually looking for properties to expand, giving all staff the chance to stay within the company and move up the career ladderOur retained client runs a very successful pub and this small group are continually looking for properties to expand, giving all staff the chance to stay within the company and move up the career ladderThe right chef de partie will have experience in working with fresh food, the menu consists of exciting, seasonal dishes including lots of fresh fish and creative dessertsThe right chef de partie will have experience in working with fresh food, the menu consists of exciting, seasonal dishes including lots of fresh fish and creative dessertsAs with all coastal properties in the height of the season you could be doing between **** **** covers a service dropping **** in the winter months, tips could equate between **** **** per month which is a great boost to an already generous salaryAs with all coastal properties in the height of the season you could be doing between **** **** covers a service dropping **** in the winter months, tips could equate between **** **** per month which is a great boost to an already generous salaryIf you are looking to work to a two roseete standard, be open minded to learn new skills and techniques and bring ideas to the table then this could be the position for youIf you are looking to work to a two roseete standard, be open minded to learn new skills and techniques and bring ideas to the table then this could be the position for youSituated over looking the stunning Norfolk coastline the venue is a fantastic place to workSituated over looking the stunning Norfolk coastline the venue is a fantastic place to workShifts will be splits and accommodation is available for the right candidate, the team are creative, friendly and have a work hard play hard mentality, for any up and coming chef looking for a solid job with great career prospects then please send in a CV illustrating all relevant experience to: Salary Up to **** Ref: ****JC Jemma Collins Recruitment Manager Leads the Recruitment Team for Clear Selection ensuring we are one of the finest in the business / **** **** Want a quick job search without the hassle of registering then paste www.clearselection.co.uk/search.php to your web browser and enjoy the experienceShifts will be splits and accommodation is available for the right candidate, the team are creative, friendly and have a work hard play hard mentality, for any up and coming chef looking for a solid job with great career prospects then please send in a CV illustrating all relevant experience to: Salary Up to **** Ref: ****JC Jemma Collins Recruitment Manager Leads the Recruitment Team for Clear Selection ensuring we are one of the finest in the business / **** **** Want a quick job search without the hassle of registering then paste www.clearselection.co.uk/search.php to your web browser and enjoy the experienceWe are always busy and are constantly receiving new instructions from a number of different client's nationwide, which makes it good for candidates and usWe are always busy and are constantly receiving new instructions from a number of different client s nationwide, which makes it good for candidates and usIn the majority of cases we have visited the client's establishment and even spent work days with them and have a very enviable reputation for finding the best for the bestIn the majority of cases we have visited the client s establishment and even spent work days with them and have a very enviable reputation for finding the best for the bestIf you desire a professional, honest and calm approach with the reassurance that your application will be treated confidentially if requested, we look forward to working on your behalfIf you desire a professional, honest and calm approach with the reassurance that your application will be treated confidentially if requested, we look forward to working on your behalfOur Consultants love their jobs and its always worth trying to contact them on their mobiles in the evenings or at weekends, if you are to busy during the dayOur Consultants love their jobs and its always worth trying to contact them on their mobiles in the evenings or at weekends, if you are to busy during the dayRemember if they do not contact you within **** hours, unfortunately you have not been successful in your application for this positionRemember if they do not contact you within **** hours, unfortunately you have not been successful in your application for this positionClear Selection Recruitment is part of the Clear Selection Group EstClear Selection Recruitment is part of the Clear Selection Group Est**** This job was originally posted as www.caterer.com/JobSeeking/ChefDePartieNorfolkLiveInupto****Tips_job********"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#detecting-duplicates-at-scale",
    "href": "notebooks/Detecting duplicate job ads.html#detecting-duplicates-at-scale",
    "title": "skeptric",
    "section": "Detecting Duplicates at Scale",
    "text": "Detecting Duplicates at Scale\nThe problem with the above approach that finding duplicates it’s like trying to find two needles in a haystack of ads\nIn particular it’s quadratic in the number of ads because we need to compare every pair. If 100 ads takes a minute, then 400k ads will take 30 years.\nInstead we use MinHash. Essentially we treat each text as a set of tokens and try to calculate the Jaccard Similarity between the two texts. Doing this directly would be roughly as slow as edit distance, and require storing all the tokens in memory, so we use a probabalistic approach.\nIf we pick a random ordering of the elements and just store the smallest one for each document, the probability it is the same for two different documents is (# of tokens in both texts) / (# of tokens in either text), which is exactly the Jaccard Similarity. So if we do this with lots of different random orderings the average number of smallest elements that are the same is approximately the Jaccard Similarity.\nWe can approximate a random ordering by the output of a family of hash functions generated by Universal Hashing (assuming collisions are relatively low).\nSo using MinHash reduces the problem of finding sets with high Jaccard Similarity to the problem of finding fixed length sequences with a large number of equal elements. We do this by splitting the hashes into groups called bands; storing the bands of each document in a hashtable and searching for similar documents by doing a lookup on each band in the hashtable. There are some heuristics for finding the best number of bands for a particular Jaccard cutoff.\nThinking about this: another way you could do this would be to keep a sorted collection of each band; this would make it likely that neighbouring elements are close in Jaccard. Is this how LSH Forest works?\nAnyway the details can be found in Chapter 3 of Mining Massive Datasets by Ullman et al. or chapter 6 of Gakhov’s Probabilistic Data Structures and Algorithms for Big Data Applications, and are implemented in the nice datasketch module."
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#jaccard-distance",
    "href": "notebooks/Detecting duplicate job ads.html#jaccard-distance",
    "title": "skeptric",
    "section": "Jaccard Distance",
    "text": "Jaccard Distance\nSee the related article\n\ndef jaccard(a, b):\n    a = set(a)\n    return len(a.intersection(b)) / len(a.union(b))\n\nWe can calculate the overlap at a character level (which is a terrible idea because most job ads will contain the English alphabet!)\n\njaccard(ad_a, ad_b)\n\n0.9230769230769231\n\n\nOr at a token level\n\njaccard(tokenize(ad_a), tokenize(ad_b))\n\n0.9081272084805654\n\n\nBut often it’s the way words are arranged that defines a document; so we can represent a document as a set of shingles or n-grams, that is subsequences of a given length.\n\ndef subseq(seq:List[Any], n:int=1) -> List[Tuple[Any]]:\n    \"\"\"Returns all contiguous subsequences of seq of length n\n    \n    Example: subseq([1,2,3,4], n=2) == [(1,2), (2,3), (3,4)]\n    \"\"\"\n    return [tuple(seq[i:i+n]) for i in range(0, len(seq)+1-n)]\n\n\nsubseq([1,2,3,4], n=1)\n\n[(1,), (2,), (3,), (4,)]\n\n\n\nsubseq([1,2,3,4], n=2)\n\n[(1, 2), (2, 3), (3, 4)]\n\n\n\nsubseq([1,2,3,4], n=3)\n\n[(1, 2, 3), (2, 3, 4)]\n\n\n\nsubseq([1,2,3,4], n=4)\n\n[(1, 2, 3, 4)]\n\n\n\nsubseq([1,2,3,4], n=5)\n\n[]\n\n\n\ndef shingle(seq:List[str], n:int=1) -> List[str]:\n    return [untokenize(s) for s in subseq(seq, n)]\n\n\nshingle(['a', 'b', 'c', 'd'], 2)\n\n['a b', 'b c', 'c d']\n\n\nThen we could e.g. represent a document by all sequences of characters of length 15\n\njaccard(shingle(ad_a, 15), shingle(ad_b, 15))\n\n0.8047604700210906\n\n\nor all sequences of words of length 4\n\njaccard(shingle(tokenize(ad_a), 4), shingle(tokenize(ad_b), 4))\n\n0.8442307692307692"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#distribution-of-values",
    "href": "notebooks/Detecting duplicate job ads.html#distribution-of-values",
    "title": "skeptric",
    "section": "Distribution of Values",
    "text": "Distribution of Values\nNote the lobe to the left will get thicker much more quickly than the spikes to the right because it will grow quadratically in the side of the items\n\np = shingle_df['jaccard_1'].plot.hist(bins=50, log=True)\np.set_xlabel('1-Jaccard Distance')\n\nText(0.5, 0, '1-Jaccard Distance')\n\n\n\n\n\n\np = shingle_df['jaccard_2'].plot.hist(bins=50, log=True)\np.set_xlabel('2-Jaccard Distance')\n\nText(0.5, 0, '2-Jaccard Distance')\n\n\n\n\n\n\np = shingle_df['jaccard_3'].plot.hist(bins=50, log=True)\np.set_xlabel('3-Jaccard Distance')\n\nText(0.5, 0, '3-Jaccard Distance')\n\n\n\n\n\nBy 4 and 5 we have a pretty clear separaratino around 0.4\n\np = shingle_df['jaccard_4'].plot.hist(bins=50, log=True)\np.set_xlabel('4-Jaccard Distance')\n\nText(0.5, 0, '4-Jaccard Distance')\n\n\n\n\n\nCan we separate this a bit more?\n\nshingle_df['jaccard_5'].plot.hist(bins=50, log=True)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8b4327ae10>\n\n\n\n\n\n\nshingle_df['jaccard_6'].plot.hist(bins=50, log=True)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8b430896a0>\n\n\n\n\n\n\nshingle_df['jaccard_7'].plot.hist(bins=50, log=True)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8b42e5e6d8>"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#randomly-sampling-pairs-instead-of-ads",
    "href": "notebooks/Detecting duplicate job ads.html#randomly-sampling-pairs-instead-of-ads",
    "title": "skeptric",
    "section": "Randomly sampling pairs instead of ads",
    "text": "Randomly sampling pairs instead of ads\n\nnp.random.seed(737)\n\nsample_indices = np.random.choice(len(ads), size=2000, replace=False)\n\n\nall_pairs = np.random.randint(0, len(ads), size=(100, 2))\n\npairs = frozenset(map(tuple, all_pairs[all_pairs[:,0] < all_pairs[:,1]]))\nlen(pairs)\n\n48\n\n\n\nall_pairs[all_pairs[:,0] < all_pairs[:,1]]\n\narray([[208634, 273069],\n       [ 16934, 120237],\n       [102010, 323173],\n       [313412, 332382],\n       [195177, 248533],\n       [145468, 218645],\n       [256321, 378370],\n       [  9091, 167775],\n       [315874, 318059],\n       [ 74204, 271667],\n       [123657, 276394],\n       [ 66053, 272459],\n       [161843, 390347],\n       [282031, 371204],\n       [ 12046, 289250],\n       [ 67703, 302343],\n       [ 24604, 196422],\n       [312456, 368154],\n       [136707, 348576],\n       [129108, 355085],\n       [ 82818, 299170],\n       [150315, 236328],\n       [ 86498, 382748],\n       [225974, 261476],\n       [104509, 299685],\n       [256889, 367033],\n       [176619, 206752],\n       [ 26982, 286734],\n       [312488, 320187],\n       [231973, 330229],\n       [ 80250, 342288],\n       [ 66770, 277829],\n       [ 53405, 229829],\n       [329647, 407037],\n       [155261, 364779],\n       [ 86418, 269651],\n       [ 45429, 327386],\n       [107896, 271578],\n       [ 98860, 267928],\n       [122075, 283689],\n       [ 86552, 136495],\n       [214751, 261046],\n       [ 58443, 311120],\n       [120637, 282351],\n       [308928, 403657],\n       [230742, 282225],\n       [ 93236, 244141],\n       [221542, 264707]])"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#comparing-examples-at-different-cutoffs",
    "href": "notebooks/Detecting duplicate job ads.html#comparing-examples-at-different-cutoffs",
    "title": "skeptric",
    "section": "Comparing examples at different cutoffs",
    "text": "Comparing examples at different cutoffs\nArbitrarily let’s look at 4-grams\n\nAt ~0.7 (3 per million ad pairs): Basically the same ad\nAt ~0.5 (6 per million ad pairs): Slight edits (which may include the role title!)\nAt ~0.2 (1 per 10k ad pairs): From the same hirer\nAt ~0.1 (6 per 10k ad pairs): From the same recruiter\nAt ~0.05 (1 per 100 ad pairs): Basically unrelated\n\nBecause advertisers often have common boilerplate this is actually an effective way of finding the same hirer.\n\nk = 4\n\n\nExactly the same; just some punctuation munging\nCopied ad (maybe duplicate): Assistant Manager vs General Manager\nThis is a copied ad; Store Manager vs Deputy Manager at different locations\nLook identical (?) Financial Media vs International\nBasically the same role in two different locations\n\n\ncutoff = 0.7\nfor i, (_idx, row) in enumerate(shingle_df[shingle_df[f'jaccard_{k}'] >= cutoff].sort_values(f'jaccard_{k}').head().iterrows()):\n    similarity = row[f'jaccard_{k}']\n    display(HTML(f'<h2> {i+1}. {k}-Similarity between {int(row.a)} and {int(row.b)}: {similarity}</h2>'))\n    show_diffs(ads[int(row.a)], ads[int(row.b)])\n\n 1. 4-Similarity between 50211 and 195018: 0.7563291139240507\n\n\nGraduate Sales Consultant/ Graduate Account ManagerSector: Express Parcel Delivery Location: CoventrySalary: Up to **** Basic Plus **** Bonus The Company: Global brand leader in the parcel delivery and logistics sector seeks a graduate with a commercial edge to join their thriving sales teamGRADUATE SALES CONSULTANT/ GRADUATE ACCOUNT MANAGERSector: Express Parcel Delivery Location: CoventrySalary: **** Basic salary plus **** BonusGlobally recongised parcel delivery companyMulti billion pound turnoverFantastic progression opportunitiesFull sales training and inductionRole: After your training and induction programme you will work alongside an experienced Regional Sales Manager to help you in your developmentRole: After your training and induction programme you will work alongside an experienced Regional Sales Manager to help you in your developmentTargeting medium sized businesses you will be selling fixed term contracts for all their express delivery needsTargeting medium sized businesses you will be selling fixed term contracts for all their express delivery needsWith a business related degree you will need to be well presented and have the ability to develop new and existing businessWith a business related degree you will need to be well presented and have the ability to develop new and existing businessResponsibilities as a Sales Executive will include:Contacting clients on a daily basisNew business development Developing accounts at a variety of levels Cold callingAccount managementFor this Graduate Trainee Sales position, we are looking for individuals who meet the following criteria:Educated to degree level or equivalent in a business related subjectTenacityAbility to think on your feet Target focussedWell presentedConsultative natureThe Package for this Graduate Sales role:Up to **** Basic Plus **** Bonus Plus additional benefitsAbout BMS Graduate Recruitment LLP: We focus on helping future sales professionals to find graduate jobs and specifically graduates sales jobsThe Company: Global brand leader in the parcel delivery and logistics sector seeks a graduate with a commercial edge to join their thriving sales teamAs a Graduate Sales Executive your responsibilities will include:Contacting clients on a daily basisNew business development Developing accounts at a variety of levels Cold callingAccount managementFor this Graduate Trainee Sales position, we are looking for individuals who meet the following criteria:Educated to degree level or equivalent in a business related subjectTenacityAbility to think on your feet Target focussedWell presentedConsultative natureThe package for this Graduate Sales role:Up to **** Basic Plus **** Bonus Plus additional benefitsAbout BMS Graduate Recruitment LLP: We focus on helping future sales professionals to find graduate jobs and specifically graduates sales jobsWe specialise in graduate assessment, placement and sales trainingWe specialise in graduate assessment, placement and sales trainingEstablished in 1990, we've developed a strong brand, synonymous with graduate sales recruitment and sales trainingEstablished in 1990, we've developed a strong brand, synonymous with graduate sales recruitment and sales trainingWe work with a prestigious client base of global companies and provide them with a range of services to ensure they attract, retain and develop the best graduate talent in the UKWe work with a prestigious client base of global companies and provide them with a range of services to ensure they attract, retain and develop the best graduate talent in the UKPlease visit our website at www.bmsgraduates.com or call Sharon Wright (Graduate Manager) on **** **** to discuss your job search.Please visit our website at www.bmsgraduates.com or call Sharon Wright (Graduate Manager) on **** **** to discuss your job search.\n\n\n 2. 4-Similarity between 164508 and 391470: 0.7983539094650206\n\n\nASSISTANT MANAGER We are currently recruiting for Assistant Manager positions for a number of our key clients across the UK, many of whom have registered their vacancies with us and us alone, choosing Cherryred as the consultancy of choiceGENERAL MANAGER We are currently recruiting for General Manager positions for a number of our key clients across the UK, many of whom have registered their vacancies with us and us alone, choosing Cherryred as the consultancy of choiceAre you a Assistant Manager who wants to develop your career with a well known and expanding groupAre you a General Manager who wants to develop your career with a well known and expanding groupAre you a Assistant Manager who would you prefer to work for an independent business with an excellent local reputation, but wants to raise their profileAre you a General Manager who would you prefer to work for an independent business with an excellent local reputation, but wants to raise their profileAre you a Assistant Manager who just wants to work for a company who will dedicate their time to training and developing your careerAre you a General Manager who just wants to work for a company who will dedicate their time to training and developing your careerAre you an experienced Assistant Manager who is just ready for a changeAre you an experienced General Manager who is just ready for a changeIf you answered yes to any of these questions, we would love to hear from youIf you answered yes to any of these questions, we would love to hear from youPlease apply now, and one of our consultants will call you shortly to confidentially discuss your career goalsPlease apply now, and one of our consultants will call you shortly to confidentially discuss your career goalsCherryred is a true specialist consultancy; a business that specialises in Hospitality and Catering Managerial roles both front and back of houseCherryred is a true specialist consultancy; a business that specialises in Hospitality and Catering Managerial roles both front and back of houseWe believe that advice should be given with a sense of prideWe believe that advice should be given with a sense of prideAdvice that is specific to individuals, not the majorityAdvice that is specific to individuals, not the majorityTo describe Cherryred in one sentence “We are focused, professional, driven, exciting, funky and cool, and passionate about what we do.” This job was originally posted as www.caterer.com/JobSeeking/AssistantManagerBournemouthFantasticOpportunity_job****To describe Cherryred in one sentence “We are focused, professional, driven, exciting, funky and cool, and passionate about what we do.”\n\n\n 3. 4-Similarity between 80127 and 80142: 0.8022922636103151\n\n\nStore Manager We re making historyDeputy Manager We re making historyWhat part will you playWhat part will you playYou may not be aware that GAME has recently been bought and a very exciting future lies aheadYou may not be aware that GAME has recently been bought and a very exciting future lies aheadWe are investing in our key stores and bringing the best talent into the organisation to build the Number One specialist video games retailer in the UKWe are investing in our key stores and bringing the best talent into the organisation to build the Number One specialist video games retailer in the UKWe are currently recruiting for a Store Manager for a fantastic GAME store in Great Yarmouth, NorfolkWe are currently recruiting for a Deputy Manager for our fantastic GAME store at The Trafford Centre, ManchesterIt s an exciting place to work with a great location, with the potential to deliver fantastic results and to progress your career furtherIt s an exciting place to work with a great location, with the potential to deliver fantastic results and to progress your career furtherWe are looking for a dynamic, inspirational Store Manager to play a key role in delivering even better results whilst maintaining a welcoming, efficient and safe environment for our customers and staff alikeWe are looking for a dynamic, inspirational Deputy Manager to play a key role in delivering even better results whilst maintaining a welcoming, efficient and safe environment for our customers and staff alikeAbout You We are looking for Store Managers, ideally with a passion for gaming, who are able to demonstrate the following: Proven track record of success in a retail management position within a 4m turnover store Experience in managing P s experience a truly exceptional one so they return time and time again Ability to work on own initiative and able to cope with changing priorities Self motivation and the ability to inspire change within your team & the wider business Confidence and professionalism Team Player attitiude Excellent time management skills and able to meet deadlines Calm and resilient work ethic with the ability to work well under pressure In return we will pay you a great salary, offer fantastic opportunities for development and promotion and an exciting place to workAbout You We are looking for Deputy Managers, ideally with a passion for gaming, who are able to demonstrate the following: Proven track record of success in a management position within retail or a similar environment Experience in managing P s experience a truly exceptional one so they return time and time again Ability to work on own initiative and able to cope with changing priorities Self motivation and the ability to inspire change within your team & the wider business Confidence and professionalism Team Player attitiude Excellent time management skills and able to meet deadlines Calm and resilient work ethic with the ability to work well under pressure In return we will pay you a great salary, offer fantastic opportunities for development and promotion and an exciting place to workPlease note: Due to the number of applications we receive, we are not able to respond to each application individuallyPlease note: Due to the number of applications we receive, we are not able to respond to each application individuallyIf you have not heard from us with 2 weeks, then please assume that your application has been unsuccessful at this time, but do not be deterred from showing your interest in future vacancies that ariseIf you have not heard from us with 2 weeks, then please assume that your application has been unsuccessful at this time, but do not be deterred from showing your interest in future vacancies that arise\n\n\n 4. 4-Similarity between 53231 and 126751: 0.8495370370370371\n\n\n\nBusiness Development Executive Financial Media Central London ****k (**** ****k OTE) Are you looking for a graduate job within one of the UK's leading financial business information and publishing housesInternational Business Development Executive Central London ****k (**** ****k OTE) Are you looking for a graduate job within one of the UK's leading financial business information and publishing housesThey have global reach, a seventy per cent staff retention rate and a fantastic commission structure that rewards effort and successThey have global reach, a seventy per cent staff retention rate and a fantastic commission structure that rewards effort and successThey offer limitless career opportunities for those who are determined enough to succeedThey offer limitless career opportunities for those who are determined enough to succeedWith a thriving atmosphere and an empowered environment, this highlevel sales opportunity is second to noneWith a thriving atmosphere and an empowered environment, this highlevel sales opportunity is second to noneThis is a FTSE 25**** company with head offices in London, Hong Kong and New YorkThis is a FTSE 25**** company with head offices in London, Hong Kong and New YorkBusiness Development Executive Financial Media A Business Development Executive will be involved in selling to key accounts in the financial world, including investment banks, highnet worth individuals, hedgefunds, and corporate law firms from day one, with client facing responsibilities right awayInternational Business Development Executive A Business Development Executive will be involved in selling to key accounts in the financial world, including investment banks, highnet worth individuals, hedgefunds, and corporate law firms from day one, with client facing responsibilities right awayYou will need to be able to talk with authority and gravitas about highly complex financial products and also then have the drive and determination to close the dealYou will need to be able to talk with authority and gravitas about highly complex financial products and also then have the drive and determination to close the dealSelling across Display, Online and Sponsorship sales, the ability to cross sell is a mustSelling across Display, Online and Sponsorship sales, the ability to cross sell is a mustTHIS GRADUATE JOB INVOLVES INTERNATIONAL TRAVELTHIS GRADUATE JOB INVOLVES INTERNATIONAL TRAVELBusiness Development Executive Financial Media As a Business Development Executive, you will be a tenacious and driven candidate keen to progress in a fantastic financial media sales opportunity in financial publishing, conference sponsorship and online salesInternational Business Development Executive As a Business Development Executive, you will be a tenacious and driven candidate keen to progress in a fantastic financial media sales opportunity in financial publishing, conference sponsorship and online salesIf the financial, futures, options and derivatives markets are of interest and a fastpaced sales role with outstanding opportunities for remuneration and international travel sounds like your ideal next move then this could be for youIf the financial markets are of interest and a fastpaced sales role with outstanding opportunities for remuneration and international travel sounds like your ideal next move then this could be for youYou must be a proactive, self driven candidate with a hunger to succeed in a graduate sales jobYou must be a proactive, self driven candidate with a hunger to succeed in a graduate sales jobApply for this role today and become part of one of the world's leading financial information providers by sending me your CV via this website Please note all applications will be made in confidenceApply for this role today and become part of one of the world's leading financial information providers by sending me your CV via this website Please note all applications will be made in confidenceFeel free to call me directly on: Olivia Paviour Sector Head The Graduate Recruitment Company **** **** **** We have a variety of graduate roles on at present (****k25k), for candidates with 0 2 years experience, so if this is one is not suitable; please call me to discuss other exciting opportunities which you can apply forFeel free to call me directly on: Olivia Paviour Sector Head The Graduate Recruitment Company **** **** **** We have a variety of graduate roles on at present (****k25k), for candidates with 0 2 years experience, so if this is one is not suitable; please call me to discuss other exciting opportunities which you can apply forPlease check out my profile on LinkedIn: Search for Olivia Paviour And follow us on Twitter: GradRecCo Job Sectors: Media Sales Jobs | Conference Exhibitions Sales | Digital Sales Jobs | Corporate Sales Jobs This job was originally posted as www.totaljobs.com/JobSeeking/BusinessDevelopmentExecutiveFinancialMedia_job****Please check out my profile on LinkedIn: Search for Olivia Paviour And follow us on Twitter: GradRecCo Job Sectors: Media Sales Jobs | Conference & Exhibitions Sales | Digital Sales Jobs | Corporate Sales Jobs\n\n\n 5. 4-Similarity between 122405 and 403679: 0.8675324675324675\n\n\n\nAbout the role We protect everything from bats to buildings to beaches – and we want to share them with everyone tooAbout the role We protect everything from bats to buildings to beaches – and we want to share them with everyone tooBut how do we care for special places like Mottisfont Abbey, while helping everyone to enjoy them and their fascinating storiesBut how do we care for special places like Charlecote Park, while helping everyone to enjoy them and their fascinating storiesIt’s no small featIt’s no small featIn fact, it’s a big team effort and it’s why our Conservation Assistants are so importantIn fact, it’s a big team effort and it’s why our Conservation Assistants are so importantYou’ll be working with a great housekeeping team in an incredible place, helping with the daytoday cleaning and care of the collectionsYou’ll be working with a great housekeeping team in an incredible place, helping with the daytoday cleaning and care of the collectionsNo detail will go unnoticed, and you’ll keep a keen eye on everything from the environmental conditions in a room, to signs of wear and tearNo detail will go unnoticed, and you’ll keep a keen eye on everything from the environmental conditions in a room, to signs of wear and tearYou’ll keep accurate records tooYou’ll keep accurate records tooWe’d also love you to talk to visitors about the work you’re doingWe’d also love you to talk to visitors about the work you’re doingAbout you You’ll share our love for magical places like Mottisfont Abbey, and you’ll love the opportunity to help us protect them for future generations to enjoyAbout you You’ll share our love for magical places like Charlecote Park, and you’ll love the opportunity to help us protect them for future generations to enjoyYou’ll already have an interest in historic houses and collections, plus an understanding of preventative conservation cleaning techniquesYou’ll already have an interest in historic houses and collections, plus an understanding of preventative conservation cleaning techniquesSome practical, handson conservation cleaning experience would be great, but we’ll give you plenty of training, guidance and supportSome practical, handson conservation cleaning experience would be great, but we’ll give you plenty of training, guidance and supportYou’ll also need to be comfortable working at heights from step ladders and scaffoldIncredibly rewarding We’ll give you all the training and support you needIncredibly rewarding We’ll give you all the training and support you needYou can also look forward to 25 days’ holiday, (pro rata for parttime), as well as free admission to all our incredible places, and a helpful 20% off at our shops, cafes and restaurantsYou can also look forward to 25 days’ holiday, (pro rata for parttime), pension scheme and flexible working as well as free admission to all our incredible places, and a helpful 20% off at our shops, cafes and restaurantsBut they’re just the benefits we can tell you about hereBut they’re just the benefits we can tell you about hereAbout us Here at the National Trust, we want even more people to enjoy our extraordinary placesAbout us Here at the National Trust, we want even more people to enjoy our extraordinary placesWe want people of all ages and backgrounds to get involved with them, be inspired by them, and love them as much as we doWe want people of all ages and backgrounds to get involved with them, be inspired by them, and love them as much as we doThat’s why we’ve put some bold ambitions in placeThat’s why we’ve put some bold ambitions in placeWe want everyone in England,Wales and Northern Ireland to feel like a member of the National Trust and, by **** we want to have grown our membership to more than five millionWe want everyone in England,Wales and Northern Ireland to feel like a member of the National Trust and, by **** we want to have grown our membership to more than five millionCommitted to equal opportunitiesCommitted to equal opportunitiesRegistered charity number **** To apply: Please visit our website nationaltrustjobs.org.uk and search for ‘Seasonal Vacancies’Registered charity number **** To apply: Please visit our website nationaltrustjobs.org.uk and search for ‘Seasonal Vacancies’\n\n\n\n(shingle_df[f'jaccard_{k}'] >= cutoff).mean()\n\n3.0015007503751877e-06\n\n\n\nLooks like a rewrite of the same ad\nSame except for punctuation munging\nVery similar? Technician vs Bodyshop MET Technician\nLikely copied ad; same role in Edinburgh and Surry resp.\nCarer vs Live in carer\n\n\ncutoff = 0.5\nfor i, (_idx, row) in enumerate(shingle_df[shingle_df[f'jaccard_{k}'] >= cutoff].sort_values(f'jaccard_{k}').head().iterrows()):\n    similarity = row[f'jaccard_{k}']\n    display(HTML(f'<h2> {i+1}. {k}-Similarity between {int(row.a)} and {int(row.b)}: {similarity}</h2>'))\n    show_diffs(ads[int(row.a)], ads[int(row.b)])\n\n 1. 4-Similarity between 181955 and 325340: 0.5117845117845118\n\n\nAssociate Quantity Surveyor Residential/MixedUse **** **** Pension Private health London REF: **** Proud of their independent status our client, a leading firm of international construction and property consultants requires an experienced Associate Quantity Surveyor to join their LondonAssociate Quantity Surveyor Residential/MixedUse **** **** plus car allowance London REF: Our client, a leading firm of international construction and property consultants requires an experienced Associate Quantity Surveyor to join their London team of **** The successful candidate will be responsible for supporting a team of six quantity surveyors, dealing with performance reviews, mentoring to RICS chartership as well as being involved in business developmentThe international company now have a team of 50 from their head office in the cityThe client is looking to grow the company by 20% over the coming year due to their healthy order book and ambitious plansThe successful candidate will be responsible for supporting a team of six quantity surveyors, dealing with performance reviews, mentoring to RICS chartership as well as being involved in business developmentThey are looking for a client facing Associate Quantity Surveyor to work on high end and large scale residential Degree in Quantity Surveying MRICS/FRICS Ability to work well with others Good communication skills both verbal and written Excellent Client facing skills (role requires to oversee a number of clients) Residential/Mixed use experience is a must Must be ambitious and keen to progress with the company If you are interested in this exciting opportunity to join a role that offers great career progression opportunities and exposure to a variety of prestigious residential projects please send through your CV or call George Reeves ((url removed)) on (Apply online only)With plans to grow the company by 20% over the coming year due to their healthy order book now is an exciting time to joinThey are looking for a client facing Associate Quantity Surveyor to work on high end and large scale residential mixed development projects in London, Brussels and ParisTheir portfolio of projects consists of high value penthouses and private homes with recent projects including The Kingdom Tower in Jeddah and a high spec build on Bishops Avenue LondonRequirements: Degree in Quantity Surveying MRICS/FRICS Ability to work well with others Good communication skills both verbal and written Excellent Client facing skills (role requires to oversee a number of clients) Residential/Mixed use experience is a must Must be ambitious and keen to progress with the company If you are interested in this exciting opportunity to join a role that offers great career progression opportunities and exposure to a variety of prestigious residential projects please send through your CV or call Ridda (g.reevesprojectresource.co.uk) on **** **** **** This job was originally posted as www.CareerStructure.com/JobSeeking/AssociateQuantitySurveyorResidential_job****\n\n\n 2. 4-Similarity between 234437 and 391470: 0.5958188153310104\n\n\nSalary:&nbsp&pound**** &nbsp&pound**** / Year Location: Sheffield Company: Cherry Red Recruitment Job type: Permanent &nbsp Job Description: We are currently recruiting for Assistant Manager positions for a number of our key clients across the UK, many of whom have registered their vacancies with us and us alone, choosing Cherryred as the consultancy of choiceGENERAL MANAGER We are currently recruiting for General Manager positions for a number of our key clients across the UK, many of whom have registered their vacancies with us and us alone, choosing Cherryred as the consultancy of choice&bullAre you a Assistant Manager who wants to develop your career with a well known and expanding groupAre you a General Manager who wants to develop your career with a well known and expanding group&bullAre you a Assistant Manager who would you prefer to work for an independent business with an excellent local reputation, but wants to raise their profileAre you a General Manager who would you prefer to work for an independent business with an excellent local reputation, but wants to raise their profile&bullAre you a Assistant Manager who just wants to work for a company who will dedicate their time to training and developing your careerAre you a General Manager who just wants to work for a company who will dedicate their time to training and developing your career&bullAre you an experienced Assistant Manager who is just ready for a changeAre you an experienced General Manager who is just ready for a changeIf you answered yes to any of these questions, we would love to hear from youIf you answered yes to any of these questions, we would love to hear from youPlease apply now, and one of our consultants will call you shortly to confidentially discuss your career goalsPlease apply now, and one of our consultants will call you shortly to confidentially discuss your career goalsCherryred is a true specialist consultancy a business that specialises in Hospitality and Catering Managerial roles both front and back of houseCherryred is a true specialist consultancy; a business that specialises in Hospitality and Catering Managerial roles both front and back of houseWe believe that advice should be given with a sense of prideWe believe that advice should be given with a sense of prideAdvice that is specific to individuals, not the majorityAdvice that is specific to individuals, not the majorityTo describe Cherryred in one sentence &ldquoWe are focused, professional, driven, exciting, funky and cool, and passionate about what we do.&rdquo PLEASE APPLY NOW&nbsp(PLEASE ONLY APPLY IF YOU LIVE WITHIN EASY COMMUTE OF THIS JOB)To describe Cherryred in one sentence “We are focused, professional, driven, exciting, funky and cool, and passionate about what we do.”\n\n\n 3. 4-Similarity between 114783 and 125520: 0.6026587887740029\n\n\nYou will see from our website and our recent financial results that we are a rapidly becoming one of the Country's leading automotive retailersBody Shop MET Technician Ford Stoke **** basic salary with OTE and company benefits Overview You will see from our website and our recent financial results that we are a rapidly becoming one of the Country's leading automotive retailersWe currently operate over **** franchised outlets throughout England and Scotland employing over **** people and have very strong relationships with the manufacturers we representWe currently operate over **** franchised outlets throughout England and Scotland employing over **** people and have very strong relationships with the manufacturers we representWe are the 7th largest retail motor group in the UK and will be expanding as we drive our business forward into 201****We are the 7th largest retail motor group in the UK and will be expanding as we drive our business forward into 2013We are very well placed in to achieve our growth and performance objectives and are constantly building our capacity to improve the business furtherWe are very well placed in to achieve our growth and performance objectives and are constantly building our capacity to improve the business furtherWe have a clear vision to become recognised as the most progressive motor retailer and our Mission Statement is 'To deliver an outstanding customer motoring experience through honesty and trust.' We are committed to supporting our colleagues through the provision of continuous training, coaching and developmentWe have a clear vision to become recognised as the most progressive motor retailer and our Mission Statement is 'To deliver an outstanding customer motoring experience through honesty and trust.' We are committed to supporting our colleagues through the provision of continuous training, coaching and developmentOur competitive advantage lies in the quality of people we employ and we are intent on building our future on the drive, energy and talent of our colleaguesOur competitive advantage lies in the quality of people we employ and we are intent on building our future on the drive, energy and talent of our colleaguesWith that in mind we are selective about who we employ and expect that they drive the business forward passionately and fully in line with our valuesWith that in mind we are selective about who we employ and expect that they drive the business forward passionately and fully in line with our valuesRole Responsibilities The key responsibilities of this role are as follows: Customer Satisfaction (CSI): To take responsibility to achieve the highest possible customer satisfaction levels by providing a quality of service that means Customers would not consider using our competitorsRole Responsibilities The key responsibilities of this role are as follows: Customer Satisfaction (CSI): To take responsibility to achieve the highest possible customer satisfaction levels by providing a quality of service that means Customers would not consider using our competitorsTechnical Knowledge: To maintain high level of technical knowledge and ensure you are compliant with manufacturer training requirementsTechnical Knowledge: To maintain high level of technical knowledge and ensure you are compliant with manufacturer training requirementsEfficiency: To consistently maintain high levels of overall efficiency (hours sold vsEfficiency: To consistently maintain high levels of overall efficiency (hours sold vshours attended)hours attended)Quality: To uphold Company and manufacturer quality standards and ensure all work undertaken meets customer expectations in relation to repairs carried out and to ensure repeat repairs are minimisedQuality: To uphold Company and manufacturer quality standards and ensure all work undertaken meets customer expectations in relation to repairs carried out and to ensure repeat repairs are minimisedDelivering our Duty of Care: To ensure 100% compliance with the Vehicle Health Check process and that repair order write ups are clear and concise specifying concern, cause and cure (the three C`s)Delivering our Duty of Care: To ensure 100% compliance with the Vehicle Health Check process and that repair order write ups are clear and concise specifying concern, cause and cure (the three C`s)Standards: To maintain your work area, tools and equipment to a high standard and ensure you understand and comply with all health and safety regulations and that customer vehicles are appropriately protected while in our careStandards: To maintain your work area, tools and equipment to a high standard and ensure you understand and comply with all health and safety regulations and that customer vehicles are appropriately protected while in our careWhat we're looking for: We are looking for talented technicians ideally with Franchise experience; however, manufacturer training will be ongoing for the successful candidate.You will possess a relevant formal qualification (City Guilds / NVQ Level 2 to 3) as a minimum requirement for this roleWhat we`re looking for Due to our expanding business we are looking for a Bodyshop MET TechnicianThis opportunity is ideal for an individual who is looking to progress their career within a franchised environment and has the desire and ability to work to and achieve targets and objectivesYou will be a fully trained Body Shop MET Technician and you MUST hold a current NVQ Level 3 or Senior ATA Qualification, evidence of which must be produced at Interview stageWe are looking for individuals who possess the following attributes: Target driven Technical Knowledge Team Working Problem Solving Time Management Attention to Detail Literacy Communication SkillsIdeally you will be experienced in the repair of all makes of vehicles, be proficient and accurate with the ability to work within time constraints and deadlinesYou will be able to interpret technical instructions and repair estimates, work within guidelines ensuring ?right first time repairsThis role is 45 hours per week with a **** minute unpaid lunch breakThis opportunity is ideal for an individual who is looking to progress their career within a franchised motor industry environment and has the ability to work to and achieve targets and objectivesWe are looking for individuals who possess the following attributes: Target driven Technical Knowledge Team Working Problem Solving Time Management Attention to Detail Literacy Communication Skills What You Can Expect If you are successful you can look forward to ongoing training opportunities, career progression and a range of benefits you would expect from an employer of choice, including a competitive salary and individual performancerelated bonus, childcare voucher scheme, share incentive plan, vertu rewards and pension schemeIf you are interested in joining the most progressive team in the industry please applyThis job was originally posted as www.totaljobs.com/JobSeeking/BodyShopMETTechnicianFordStoke_job****\n\n\n 4. 4-Similarity between 10633 and 253135: 0.6335403726708074\n\n\nEndoscopy Nurse Edinburgh **** Full Time This is an exciting new opportunity for an Endoscopy Nurse to work for a private hospital in EdinburghEndoscopy Nurse Surrey **** **** I am currently looking for an Endoscopy Nurse to work for a private hospital in the Surrey area The successful candidate will be working in the hospitals Endoscopy unit which is currently being upgraded which will provide excellent career progression and opportunitiesThe successful candidate will be working in the hospitals Endoscopy unit with excellent career progression and opportunitiesYou will form part of an already established small friendly team, and will be expected to provide and maintain a high quality serviceYou will form part of an already established small friendly team, and will be expected to provide and maintain a high quality serviceIt is essential that you are an experienced, qualified Registered Nurse with recent endoscopy experienceIt is essential that you are an experienced, qualified Registered Nurse with recent endoscopy experienceThe hospital can offer a strong management team, the opportunity to work along side expert clinicians, access to professional and personal development and experience within a dynamic working environmentThe hospital can offer a strong management team, the opportunity to work along side expert clinicians, access to professional and personal development and experience within a dynamic working environmentFor more information or to apply for the position then please contact Lydia Robinson on **** **** and email your CV to lydia.robinsonappointgroup.co.ukFor more information or to apply for the position then please contact Lydia Robinson on N/A and email your CV to lydia.robinson N/A .uk\n\n\n 5. 4-Similarity between 117585 and 372390: 0.6369047619047619\n\n\nCarer ‘I love being able to help others, no matter how little’ Are you the kind of person who thrives on making others feel valuedLive in Carer Hertfordshire ‘ I love being able to help others, no matter how little’ Are you the kind of person who thrives on making others feel valuedAre you a positive and honest person who is good at tuning into other peopleAre you a positive and honest person who is good at tuning into other peopleDo you care about doing things rightDo you care about doing things rightAnd are you a reliable, hard working personAnd are you a reliable, hard working personIf this sounds like you and you would like the chance to make a difference to other people’s lives, this could be the job for youIf this sounds like you and you would like the chance to make a difference to other people’s lives, this could be the job for youOur customers range from elderly people who need help with personal care through to severely disabled people with very complex care needsThis role will involve living in at our customer’s home for a two to three week periodWhat is common, is how much they come to trust and rely on our great teamThey range from elderly people who need help with personal care through to severely disabled people with very complex care needsWhat is common is how much they come to trust and rely on our great teamTo be in this field of work you need to REALLY careTo be in this field of work you need to REALLY careThe job is very rewarding as well as demanding at timesThe job is very rewarding as well as demanding at timesIn return for your hard work and commitment we offer competitive rates of pay and benefitsIn return for your hard work and commitment we offer competitive rates of pay and benefitsWe also pride ourselves on being supportive and as caring to our staff as we want them to be to our customersWe also pride ourselves on being supportive and as caring to our staff as we want them to be to our customers‘They have such a good heart’ Car drivers required‘They have such a good heart’ This is based in Hertfordshire and you will have a local branch for training and support but our livein work can cover the whole of the UKFull Time/Part Time Work available now in Maidstone,Tovil, Shepway, Parkwood, Bearsted, Harrietsham, Lenham and surrounding areas If this appeals to you we would love to hear from youVarious shifts available If this appeals to you and you have your own transport we would love to hear from youAll applicants will be required to complete a Criminal Records Bureau DisclosureAll applicants will be required to complete a Criminal Records Bureau DisclosureSuccessful applicants will be required to pay **** for their CRB checkSuccessful applicants will be required to pay **** for their CRB checkAllied Healthcare is an equal opportunities employer and regulated by CQCSaga Homecare is an equal opportunities employer and regulated by CQCThis job was originally posted as www.totaljobs.com/JobSeeking/CareWorkerMaidstoneandsurroundingareas_job****This job was originally posted as www.totaljobs.com/JobSeeking/LiveInCarerHertfordshireandsurroundingareas_job****\n\n\n\n(shingle_df[f'jaccard_{k}'] >= cutoff).mean()\n\n6.503251625812906e-06\n\n\n\nDifferent roles from Michael Page\nDifference roles from Michael Page\nDifferent roles from Allegis Group\nDifferent roles from ACS Recruitment Consultants\nVery similar roles in two different locations; same recruiter\n\n\ncutoff = 0.2\nfor i, (_idx, row) in enumerate(shingle_df[shingle_df[f'jaccard_{k}'] >= cutoff].sort_values(f'jaccard_{k}').head().iterrows()):\n    similarity = row[f'jaccard_{k}']\n    display(HTML(f'<h2> {i+1}. {k}-Similarity between {int(row.a)} and {int(row.b)}: {similarity}</h2>'))\n    show_diffs(ads[int(row.a)], ads[int(row.b)])\n\n 1. 4-Similarity between 372114 and 385653: 0.2\n\n\nAn equal split between developing new business and account managing with individual practitioners and regional chains Must have experience in healthcare Experience of negotiating with independent practitioners orretail clients strongly preferred Swiss based worldwide medical instruments business organisation in growth and with a strong profit line is looking for Regional Sales Manager for one of its brands to cover the South East of the UK **** **** Good Car Strong Quarterly Bonus Benefits Your application will be reviewed by Michael PageAn opportunity has become available for a Senior Associate within our London office to join the fast growing Forensic and Litigation Support Services (FLSS) teamThe individual will support the head of the team in leading, developing and maintaining the FLSS service lineThe successful candidate will be responsible for the daytoday management of client assignments, development of strategies with clients for Partner agreement, and delivery of projects within the agreed strategyThis opportunity will include aspects of practice development as the business seeks to continue to grow this service line and also business development, as the candidate will work directly with the Head of FLSS in pitching for work and actively completing assignmentsThe successful candidate will also have oppo Key Responsibilities Accountabilities: Responsible for the daytoday management of client assignments, reporting to Partners and Directors Work on a variety of client assignments financial investigations, prelitigation analysis and expert report work Draft terms of engagement on assignments Manage and plan work programmes Manage and control billings within objectives and provide budgets Develop internal processes Draft Word and PowerPoint reports and assume responsibility for the factual makeup of reports Establish and maintain client relationships Assist the Head of FLSS in developing market contacts to generate new work opportunities Top professional services firm Competitive Your application will be reviewed by Michael PagePlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsPlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsWe will only contact you within the next 14 days if you are selected for interviewWe will only contact you within the next 14 days if you are selected for interviewWhere specific UK qualifications are required we will take into account overseas equivalentsWhere specific UK qualifications are required we will take into account overseas equivalentsMichael Page is a world leading recruitment consultancyMichael Page is a world leading recruitment consultancyThis job was originally posted as www.SalesTarget.co.uk/JobSeeking/RegionalSalesManagerMedicalDevices_job****This job was originally posted as www.totaljobs.com/JobSeeking/ForensicsSeniorAssociate_job****\n\n\n 2. 4-Similarity between 104680 and 161465: 0.2\n\n\nAn exciting position within the luxury automotive segmentAs Channel Marketing Manager your role will be to plan, execute and track channel marketing campaigns to increase lead generationDevelopment of hard interior trim componentsIN addition you will also continue to develop on and offline tools, materials and collateral to improve sales enablementYou would be responsible for part of the manufacturing/production process or for the development of products across a vehicle rangeOur client is looking for a proven Channel Marketing Manager with experience in a similar roleA great opportunity for a manufacturing/production engineer with some project management understandingYou will have worked in a B2B Technology business with demonstrable experience of improving lead generationPart of a team, integrating with other departments within the OEM vehicle assembly plant, striving for continuous improvement within the product and manufacturing processOur client is a rapidly growing Technology business based in London Competitive salary packageThe ideal candidate should have knowledge of first tier Automotive product development and industrialisationYour application will be reviewed by Michael PageProduct feasibility and evaluation as well as process planning form a large portion of this role and knowledge in these will be required.Any candidate should also ideally have the following skills: Manufacturing engineering skills Project management Catia V**** interrogation Minitab Six sigma ****D / problem solving tools SPC ISO TS**** Basic SAP My client is a luxury automotive OEMc**** Excellent Benefits Your application will be reviewed by Michael PagePlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsPlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsWe will only contact you within the next 14 days if you are selected for interviewWe will only contact you within the next 14 days if you are selected for interviewApply to Tom Leaman quoting Job Ref: MPJT**** Where specific UK qualifications are required we will take into account overseas equivalentsWhere specific UK qualifications are required we will take into account overseas equivalentsMichael Page is a world leading recruitment consultancy.Michael Page is a world leading recruitment consultancyThis job was originally posted as www.totaljobs.com/JobSeeking/ChannelMarketingManager_job****\n\n\n 3. 4-Similarity between 70947 and 362358: 0.2\n\n\nC.NET **** WCF, ASP.NET, Covariance, Dynamic Language Run time, Garbage Collection/ Banking no experience required, training given This is an opportunity for an individual with expert knowledge of C .NET **** to join a team in a leading investment bank, located in the cityJava, Hedge Fund, Greenfield, Spring, Hibernate, Caching, Coherence, DataSynapse, Grids, High Frequency, Low Latency, Scalable Systems, Equities, Derivatives, Fixed Income, London, City, Scala, HaskellCandidates will require excellent core C .NET **** development/programmer skills while also being analytical with strong problem solving skillsGreenfield system at Hedge Fund requires passionate, technology driven Junior, Mid Level and Senior Java DevelopersTechnically, candidates will be expert C .NET **** developers with an in depth knowledge of asp.NET, WCF, dynamic language runtime, garbage collections and covarianceMy client ranks in the top 5 most successful hedge funds globally and is the most prestigious Hedge Fund in London They are looking for strong Java Developers to join their leading team and be passionate about contributing to what's aiming to be the fastest trading platform within the HFT spaceRelevant candidates will also have an excellent academic background in which they achieved a good degree from a top universityThe successful candidate will work on a highly complex, sophisticated and completely Greenfield Algo Trading SystemThis is an exciting opportunity for an experienced C .NET **** developer/ analyst to enter into the investment banking sectorAs this company is technology driven, they are using functional/scripting languages alongside the Core Java stack so you will get exposure to SCALA amongst many other technologies that you and the team see fit to useABSOLUTELY ESSENTIAL: 2:**** or higher from a top 20 University in Computer Science or similar (check The Guardians Uni ranking) and STRONG A / O Levels (no equivalents accepted) You'll also be facing off to traders and quants to fully understand user requirements, allowing you to gain massive exposure to the financial business whilst enhancing your already excellent Java skillsFor this reason, your communication skills must be on point tooMid and Senior Level roles require some level of previous experience within the financial industry but no previous experience is required for the Junior roles just strong development skillsCompensation ranges from: Junior ****k to ****k Mid ****k to ****k Senior ****k to ****k No matter what level you are at, this will also be topped up biannually by an infamous Bonus scheme (the last successful candidate who joined 7 months ago received their first bonus of 110% of their base salary)If you are highly educated and can prove that you are also a Java Expert then apply withinFor more information surrounding this and other similar roles please call Mindy on **** **** **** or email mjhakra [at] astoncarter.co.ukAllegis Group Limited and Aston Carter Limited operate Employment Businesses and Agencies and are companies within the Allegis Group IncAllegis Group Limited and Aston Carter Limited operate Employment Businesses and Agencies and are companies within the Allegis Group Incgroup of companies, the fourth largest staffing company in the world, (collectively referred to as the Allegis Group )group of companies, the fourth largest staffing company in the world, (collectively referred to as the 'Allegis Group')TEKsystems and Aerotek are divisions of Allegis Group LimitedTEKsystems and Aerotek are divisions of Allegis Group LimitedApplicant data will be treated in accordance with the Allegis Group s Privacy Notice http://www.allegisgroup.com/Privacy/Default.aspx)Applicant data will be treated in accordance with the Allegis Group's Privacy Notice http://www.allegisgroup.com/Privacy/Default.aspx)By submitting personal data to any company or division within the Allegis Group, the applicant is providing explicit consent to the use of such data by the Allegis Group and to the transfer of such data to and from the Allegis Group companies within the UK, Europe and outside the European Economic Area in connection with the fulfilment of the applicant s voluntary requests, and the fulfilment of other job opportunities that match the applicant s profile, and confirms that they may be contacted about such job opportunitiesBy submitting personal data to any company or division within the Allegis Group, the applicant is providing explicit consent to the use of such data by the Allegis Group and to the transfer of such data to and from the Allegis Group companies within the UK, Europe and outside the European Economic Area in connection with the fulfilment of the applicant's voluntary requests, and the fulfilment of other job opportunities that match the applicant's profile, and confirms that they may be contacted about such job opportunitiesThis job was originally posted as www.cwjobs.co.uk/JobSeeking/JavaDeveloperHedgeFundGreenfieldScalaCity_job****\n\n\n 4. 4-Similarity between 7285 and 288349: 0.2\n\n\nExperienced Costs Draftsman **** 25,000 Minimum of 2 years` experience drafting bills of Costs Negotiate schedules Litigation background with extensive experience Multitrack experience Bills of Costs Part **** Claims Good understanding of CPR rules relating to Costs Detailed assessments with advocacy skills Please note that Legal experience is essential for this roleIndustrial Disease Fee Earner Bolton Area Salary **** **** As one of the largest personal injury specialists in Bolton and due to expansion, we are now looking for a fee earner to join our ambitious companyAll Applicants will be reviewed within the next working day and successful applicants will be acknowledged within 2 working daysWith this continued growth in mind we offer a competitive salary along with exciting opportunitiesExpect a supportive team in a dynamic and thriving environmentWhat We Are Looking For: ProactiProactivelyin your own case load from inception through to conclusion Managing your case load to make sure your clients are up to date and informed Being able to work with a mixed case load from VWF, fatal disease and serious illnesses Good communication skills both spoken and written Good 'people skills' for building relationships with colleagues at all levels Having the ability to plan and prioritise your case load effectively Working from your own Action List within our Case Management System Accuracy with record keeping Experience preferred but not necessary To ensure compliance with the Solicitors Code of Conduct **** (as amended) Recognised as an award winning firm you can expect a supportive and dynamic atmosphere Please note that Legal experience is essential for this roleAll Applicants will be reviewed within the next working day and successful applicants will be acknowledged within **** working daysACS Recruitment Consultants Ltd was established in 1983ACS Recruitment Consultants Ltd was established in 1983We are the market leader within Legal Recruitment and we specialise in the placement of Qualified Solicitors, Legal Executives, Paralegals, Legal Secretaries and Support Staff at all levels.We are the market leader within Legal Recruitment and we specialise in the placement of Qualified Solicitors, Legal Executives, Paralegals, Legal Secretaries and Support Staff at all levelsPlease apply to eileenacsrecruitment.co.uk\n\n\n 5. 4-Similarity between 269144 and 299075: 0.2\n\n\nAutomotive, Motor Trade Job: Service Advisor in Watford, HertfordshireAutomotive, Motor Trade Job: Service Advisor required in Birmingham, West Midlands Salary: **** Basic, OTE **** Per Annum Term: Full Time, Permanent Motor Trade Jobs / Automotive Vacancies: Automotive Service Advisor required in BirminghamSalary: **** **** Basic, OTE **** Term: Full Time, Permanent Hours: Monday Friday 8am 6pm, and 1 in **** Saturdays 8am 1pm Motor Trade Jobs / Automotive Vacancies: Automotive Service Advisor required in WatfordOur client, an Automotive Main Car Dealership in the Birmingham area is currently looking to recruit a Service Advisor to work in their busy Aftersales departmentOur client, an Automotive Main Dealer in the Watford area, is currently looking to hire an experienced and professional Service Advisor for their busy branchOur client, a franchised main dealer in the Birmingham area deal with both retail and light commercial vehicles and are looking for a good service advisor to be able to provide a great service to all customersYou will need to be a confident relationship builder, able to influence and advise customers on minor technical and motor related issues whilst delivering a first class serviceThe successful candidate will be able to demonstrate: A proven record in delivering great customer serviceYou will also need to achieve the company objectives and targets as set out by this Motor Trade Main DealershipMain Dealer Experience Kerridge experience As an experienced Service Advisor you will have experience in completing job cards, upselling of products and treating customers as required by the manufacturers set standardsYou will be working alongside a team of Service Advisors and be responsible for providing a proactive service to customers requesting to schedule their vehicles in for a service, maintenance or repair work and arranging where necessary a courtesy vehicleMy client is looking for an experienced Service Advisor within the Automotive industry and as such you must have Main Dealer experienceYou will be greeting customers in to the service department, extensive liaison with the technicians in the workshop to chase work in progress, and keeping customers informed, and gaining authorisation for any additional work to be carried outYou will ideally live within the West Midlands area or be able to travel to the Birmingham area and not only be an experienced Service Advisor, but also be used to and experienced in working in a main dealerYou will take ownership of customer queries and resolve all issues through to your customer's satisfaction, following the protocols set out but this Main DealerOur client offers a fantastic salary package and a very good bonus schemeYou must also possess a full UK driving license as part of the role as a Service Advisor may require you to move motor vehiclesThis vacancy is based in Birmingham and our client is looking only for Main Dealer Service Advisor experienced applicantsYou will ideally live within the Watford area or be able to travel to the Watford area and have a working experience in a Main Dealer Service DepartmentTo apply please send your CV to Nick Paul, quoting Job Ref: J**** Service Advisor, BirminghamOur Client offers a fantastic salary package and a very good bonus schemeThis vacancy is based in Watford and our client is looking only for Service Advisor applicantsTo apply please send your CV to Sharron Spall quoting J**** Service Advisor, WatfordPlease remember to add your home phone number, mobile number, email address and full address to your CVPlease remember to add your home phone number, mobile number, email address and full address to your CVIf this information is missing it slows down our process and may result in us not being able to contact youIf this information is missing it slows down our process and may result in us not being able to contact youFollow us on Twitter http://twitter.com/MotorTradeJobs Perfect Placement UK Ltd is an employment agency acting as such under the Employment Act **** and Employment Agencies Act **** (and amendments)Follow us on Twitter (url removed) Perfect Placement UK Ltd is an employment agency acting as such under the Employment Act **** and Employment Agencies Act **** (and amendments)Copyright Perfect Placement UK Limited 2013We have many different Motor Trade Jobs available from Service Manager, Service Team Leader, Aftersales Manager, Sales Executive, General Sales Manager, Sales Manager, Business Manager, Sales Admin, Body Shop Manager, Paint Sprayer, Panel Beater, Bodyshop Estimator, Dealer Principal, Motor Mechanic, Service Advisor, Motor Cycle Technicians Perfect Placement UK Limited 2012The reproduction, transmission or other use of all or any part of this advert to or in any media, without Perfect Placement UK Limited's prior written permission is prohibited and may result in criminal or civil actionsThe reproduction, transmission or other use of all or any part of this advert to or in any media, without Perfect Placement UK Limited s prior written permission is prohibited and may result in criminal or civil actionsPlease contact our office on **** **** **** if you wish to discuss this copyright.Please contact our offices on (Apply online only) if you wish to discuss this copyright\n\n\n\n(shingle_df[f'jaccard_{k}'] >= cutoff).mean()\n\n0.00013406703351675838\n\n\n\nDifferent roles placed by same person\nDifferent roles places by Hays\nDifferent roles from BMS (posted by different people)\nDifferent roles from different divisions of Hays\nSame employer: To apply for this position, candidates must be eligible to live and work in the UK Matchtech is acting as an Employment Business in relation to this vacancy\n\n\ncutoff = 0.1\nfor i, (_idx, row) in enumerate(shingle_df[shingle_df[f'jaccard_{k}'] >= cutoff].sort_values(f'jaccard_{k}').head().iterrows()):\n    similarity = row[f'jaccard_{k}']\n    display(HTML(f'<h2> {i+1}. {k}-Similarity between {int(row.a)} and {int(row.b)}: {similarity}</h2>'))\n    show_diffs(ads[int(row.a)], ads[int(row.b)])\n\n 1. 4-Similarity between 225625 and 353060: 0.1\n\n\nThis company are a market leader within the soft furnishings market and due to expansion require a Regional Account Manager to cover from Liverpool to Bristol ideally living in the West Midlands areaThis company are a well known IT supplier based in Tamworth and due to expansion require **** x Telemarketers to be based out of their officesYou role will be to manage around ****ey accounts across the area, including large retail chains through to independent clients, and therefore as this is purely account management you need to have excellent communication skillsThe role will involve making appointments for the field sales teams, and therefore the successful candidates needs to have a strong background in telesales/telemarketing and be looking to work for a market leader with excellent opportunities for progressionIdeally you will have experience of working within the FMCG sector, however if you have strong sales skills and are used to developing and managing accounts then you would be consideredThey are looking to offer a basic salary of around **** plus an excellent benefits scheme and if you are interested in this role please call Richard on **** **** or email your CV to richardcharlespeters.co.uk This job was originally posted as www.totaljobs.com/JobSeeking/TelemarketingExecutive_job****They are looking to offer a basic salary of between **** **** depending upon experience, with an additional bonus of **** You will also receive a company car, ipad, phone and full office set up, plus usual benefitsIf you arte interested in this role please call Richard on **** **** or email your CV to Richardcharlespeters.co.uk This job was originally posted as www.totaljobs.com/JobSeeking/RegionalAccountManager_job****\n\n\n 2. 4-Similarity between 184369 and 299291: 0.1\n\n\nJob: Bookkeeper Finance Officer Salary: **** p/h 18 hours per week Location: Bradford A rapidly expanding company is looking find an experienced Bookkeeper to join their busy Finance team in the Bradford areaExcellent new job opportunity for a newly/recently qualified ACA, ACCA, CIMA Accountant to join this International hitech business based in Camberley areaThis is a permanent position reporting directly to the Finance Manager and will involve an experienced Accounts Assistant with a strong knowledge of financial systems, and advanced excel skillsMain responsibilities: Liaison with Sector Finance teams to ensure that accounting and reporting is in compliance with company guidelines and that the financial statements are prepared accuratelyAreas of expertise required: Payroll Purchase Ledger Sales Ledger Bank reconciliations Financial report preparation Office Administration The successful candidate will be able to work well within a busy environmentMonitor G/L account reconciliationsThis role requires someone with significant Bookkeeping experience, ideally from a charity backgroundPrepare quarterly analytical reviewsThe ideal applicant will have worked in a similar role recently.Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersEnsure that changes to IFRS and company accounting guidelines are implemented by the businesses/legal entities in a timely mannerResponsible for the formal approval and sign off of the monthly financial statements including P review the impairment tests of tangible and intangible assets Check that customer credit ratings that are being set are reasonable and comply with credit guidelines and resolve matters escalated on the AR query sheet.Assisting with the preparation of UK legal entities statutory accounts.Supervision of Part Qualified Accountants including performance reviews, staff development and coachingQualifications/Experience You should ideally be newly/recently qualified ACA/ACCA/CIMA or be studying towards the final stages You should demonstrate strong knowledge of financial accounting and control processes in a commercial environmentIt would be beneficial to have worked in a financial audit function to show experience in this area and have exposure to SOX and IFRSExperience gained in an international blue chip business would also be beneficial.Good English written and spoken with fluency is preferredIT literate, capable in Excel , Word, Outlook, SAP and ESPRITHays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersBy applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.ukBy applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.uk\n\n\n 3. 4-Similarity between 275128 and 398143: 0.1\n\n\n\nSales Executive Industrial Coatings ****k OTE ****k Car West East Midlands THE COMPANY: Our client is the largest British manufacturer in their marketplace, with a workforce of over **** talented professionals and a century of innovation behind themBrand Account Manager Branded Eyewear ****k ****k Car West East Midlands South West THE COMPANY : This well known business has a portfolio of brands that is the envy of many other fashion housesThey manufacture industrial coatings such as primers and finishers, and currently seek a new sales executive to cover the North of EnglandThey have been established since the ****'s, have an international turnover in excess of **** billion and include many iconic brands amongst their rangeYou will be selling into Tier **** component companies, large industrial contractors and local authorities, dealing at both technical and commercial levelsIn addition they invest heavily in training and developing their sales staff and offer excellent opportunities to progress through the businessThis role is a good mix of account management and new business development and you will receive full trainingYou will be responsible for selling a designer range of branded eyewear into independent opticians, boutiques and clothing storesTHE PERSON: You will have the following skills; Field sales track record You will ideally have sold industrial paints or coatings Sales of technical products into industry will also be considered You will have excellent account opening and management skills along with a hardworking ethic THE PACKAGE: **** Basic OTE **** Fully Expensed Company Car Mobile, Pension, LaptopBMS is a leading consultancy specialising in sales recruitmentThe primary focus of this role is on new business development, selling to owners, managers, buyers and opticiansEstablished in 1990, BMS has achieved a truly nationwide presence through a number of regional centresTHE PERSON : You will have the following skills; Field sales experience with a structured approach You will have an FMCG, retail and B2B sales track record The proven ability to sell a premium or luxury brand is essential Articulate and moneymotivated sales people will do well here THE PACKAGE : **** Basic **** Uncapped Fully Expensed Company Car Mobile, Pension, Laptop, Healthcare, 25 days holidays in **** BMS has achieved a truly nationwide presence through a number of regional centresThe Midlands office in Coventry formally opened in **** and was introduced to service the needs of candidates and clients alike throughout the MidlandsBMS offers sales jobs for Trainees, Sales Representatives, Sales Executives, Area Sales Managers and Account ManagersBMS offers sales jobs for Trainees, Sales Representatives, Sales Executives, Sales Engineers, Area Sales Managers and Account ManagersConveniently located just off junction 2 of the M6, we are committed to meeting all potentially suitable candidates face to faceConveniently located just off junction **** of the M6, we are committed to meeting all potentially suitable candidates face to facePlease visit our website at www.bmsuk.com or contact Katie Gell or Tony Nagra on **** **** **** This job was originally posted as www.totaljobs.com/JobSeeking/BrandAccountManagerSalesExecutive_job****Furthermore, our organisation consists of several highly focused teams, aimed at specific market sectors, enabling us to deliver a service directly tailored to your needsPlease visit our website at (url removed) or contact Andrew Bagchi or Ryan Chadwick on N/A\n\n\n 4. 4-Similarity between 253957 and 381799: 0.1\n\n\n\nA successful national retail company based in Hertfordshire are looking for a bright and eager Assistant Management Accountant to join the business on a permanent basisOracle Database Administrator Oracle ****g, WebLogic, Banking, Finance City of London **** **** Bonus Benefits Oracle ****g, WebLogic, RMAN, GRID, Banking My client, a leading prolific Banking institution, requires an expert Oracle DBA with experience using WebLogic and SQL server to join their everexpanding IT Operations supportThis role is full time and paying **** **** and the option of study support as wellThe successful Oracle DBA will not only benefit from gaining skills from this leading global financial institution but also use some of the latest and most exciting technologies available on the market today such as Oracle ****g and WebLogicDaily duties will include: Purchase and Sales Ledger reconciliations Bank reconciliations Month and Year End reconciliations Accruals and Prepayments Monitoring and reconciling intercompany accounts Assisting with the preparation of Management Accounts Month end reporting and analysis This role would suit an AAT qualified, ACCA or CIMA studier that has a good understanding of accounts and proceduresThe Successful Oracle DBA must have: Excellent Oracle ****g experience within a banking environment Excellent Oracle WebLogic experience to an administration level Advanced SQL server experience Experience of Shell scripting Strong commercial experience of RMAN Excellent experience of Oracle GRID Control Ability to lead infrastructural database solution design and implementation Experience of Infrastructure project management A financial background Essential Qualifications Certified Oracle database administrator/architect ITIL certified Relevant training in key DBMS Software My client is not only offering a superb working environment, cutting edge technologies and a range of challenging projects they are also offering the following package: A salary of up to **** Bonus Pension contribution Life insurance Health insurance Flexi hours Gym membership Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersThis role will be very hands on and require high volume reconciliations so accuracy and speed is requiredBy applying for this job you accept the T C's, Privacy Policy and Disclaimers which can be found at hays.co.uk This job was originally posted as www.cwjobs.co.uk/JobSeeking/SeniorOracleDBA_job****Experience of using MS Excel and Vlook ups and pivot tables would also be a distinct advantageThe client is looking for someone to interview ASAP with the view to start in the coming weeksHays EA is a trading division of Hays Specialist Recruitment Limited and acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersBy applying for this job you accept the T C's, Privacy Policy and Disclaimers which can be found at hays.co.uk This job was originally posted as www.totaljobs.com/JobSeeking/AssistantManagementAccountant_job****\n\n\n 5. 4-Similarity between 203870 and 329697: 0.1\n\n\nC++ Software Engineer Marine Systems A great opportunity has arisen for a C++ Software Engineer to work for a leading marine systems company in the Fareham areaYou will take a lead role in designing Lighting and control circuits, LV/HV power supply and distribution, Mechanical piping systems and HVAC systemsThis C++ Software Engineer will be working to both develop new software programmes and add enhancements to existing productsThis includes producing and checking documents/drawings, plans, sections elevations, schematic details, calculations, design risk assessments, specifications other technical documents, cost plans etcJOB DESCRIPTION Embedded C++ programming Object Oriented Design Multithreaded programming/RTOS Software development processes and tools Good communication skills Embedded systems Additional relevant skills: Device driver development Linux User Interface development Network Protocols Very occasional travel maybe required to customer sites To apply for this position, candidates must be eligible to live and work in the UK Matchtech is acting as an Employment Business in relation to this vacancyInput will need to be provided for tender documents and estimates and will need to organise and attend occasional site visits as appropriateYou will also take a leading role within the allocated project team providing mentoring to less experienced staff as requiredYou will ideally be a Chartered Engineer and will need to have extensive experience in the design of railway mechanical electrical systems ideally in both subsurface above ground stationsYou will also have familiarity with LUL/NWR/RG Standards and British/European StandardsTo apply for this position, candidates must be eligible to live and work in the UK Matchtech is acting as an Employment Business in relation to this vacancy.\n\n\n\n(shingle_df[f'jaccard_{k}'] >= cutoff).mean()\n\n0.0006073036518259129\n\n\n\nLikely unrelated\nLikely unrelated\nLikely unrelated\nLikely unrelated\nBoth from Hays\n\n\ncutoff = 0.05\nfor i, (_idx, row) in enumerate(shingle_df[shingle_df[f'jaccard_{k}'] >= cutoff].sort_values(f'jaccard_{k}').head().iterrows()):\n    similarity = row[f'jaccard_{k}']\n    display(HTML(f'<h2> {i+1}. {k}-Similarity between {int(row.a)} and {int(row.b)}: {similarity}</h2>'))\n    show_diffs(ads[int(row.a)], ads[int(row.b)])\n\n 1. 4-Similarity between 64743 and 399946: 0.05\n\n\n\nYou will be responsible for working alongside leading advisory M A Senior Mangers and Directors assisting in the deal process to include modelling and responsibilities across the following areas: Building of integrated financial forecasting modelsMechanical Engineers ****p/h Harwell A leading nuclear engineering consultancy is looking to recruit a number of mechanical engineers with current SC clearance to become part of a growing specialist teamPreparation and analysis of supporting financial informationThe ideal candidate will have experience with CAD in the nuclear sector and will have appropriate qualificationsDrafting of Information Memoranda, Teasers and Presentations Managing of information flows during the process Preparation of valuation reports Analysis of transactional data Preparation of pitch documentation Research support looking for investors, buyers and potential targets Process managementIn return a negotiable daily rate is on offer, as well as the chance to work on exciting projects with a market leaderSuitable candidates will have either existing experience in the field, within mid tier or large firm environment, or a background to date in audit, tax or recovery, looking to diversify into the corporate finance filedResponsibilities The Mechanical Designer is fundamentally a handson CAD draughting based roleYou will be based in the firms Bristol operations in a role offering a wide exposure to all areas of corporate finance, in a client facing role providing an excellent opportunity for the right accounting professional, looking for a challenging and exciting career moveMechanical Designers are responsible for maintaining and developing the engineers' designs as working drawings and ensuring design standards and codes are adhered to throughout the design processYou will be able to demonstrate a desire to develop a long term career in corporate finance, and will see yourself as an essential part of the firm, willing to assist in business development, marketing and promotional activities articulate, inquisitive, A highly successful, fast growing and expanding Bristol basedcorporate finance boutique firm **** **** Your application will be reviewed by Michael PageDesign draughting to meet technical specifications, drawing standards and codes primarily using AutoCAD` and/or Inventor Development of designs to fit within space envelope and design interfaces Preparation of manufacturing and installation drawings including detailing, dimensioning and tolerances Recording design changes following design change control procedures Liaison with Document Controllers and Design Engineers Experience required Demonstrable competence gained in a design delivery position Good understanding of generic mechanical principles is preferred e.gPlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsForces, stresses, materials selection and design principles Experience in nuclear is ideal but not always a prerequisite Technical experience could be gained from but is not limited to precision engineering, conveyer systems, remote handling systems / equipment, heavy engineering, cranes, pipes and pumps Preferably HNC qualified in a technical/engineering discipline though not essentialMinimum ONC or equivalent qualification gained in a relevant technical, engineering or design disciplineA full UK drivers licence is essential for this position as travelling between offices may be requiredTo hear more about this opportunity or be considered for similar roles in the nuclear industry, please apply todayThis advert was posted by Gold Group one of the UK's leading niche recruitment consultanciesWe span a variety of specialist industries and are the recruitment company to help you find your next career opportunityWe pride ourselves on our commitment to candidates and stick to our ethos of finding the right role for the right personVisit our website or get in touch today to discuss this role, find out what else we've got or just for a chat about the state of your industryServices advertised by Gold Group are those of an Agency and/or an Employment BusinessPlease be aware that we receive a high volume of applications for every role advertised and regularly receive applications from candidates who exceed the job credentialsWe will only contact you within the next 14 days if you are selected for interviewWe will only contact you within the next 14 days if you are selected for interviewWhere specific UK qualifications are required we will take into account overseas equivalentsThis job was originally posted as www.totaljobs.com/JobSeeking/MechanicalEngineers****phHarwell_job****Michael Page is a world leading recruitment consultancyThis job was originally posted as www.totaljobs.com/JobSeeking/CorporateFinanceExecutive_job****\n\n\n 2. 4-Similarity between 316896 and 346732: 0.05\n\n\nUNDERWRITING ASSISTANT **** Our clients, have an interesting and progressive vacancy for an underwriting assistant which will be based in their offices in BromleyPlatform Engineering Manager, Permanent, Rosyth, **** **** Job Description STR is representing a large client to oversee, manage and develop the Platform Engineering Group (Naval Architecture s to Simon Hoyle at or call Simon on (Apply online only) STR Limited is acting as an Employment Agency in relation to this vacancyIt is essential that you have attained a Law degree in order to be successful and that you have some administrative work experienceYou will be fully trained in this role and progression is availableIt is essential that you have good communication skills Only apply for this role if you have the requisite degree quoting Ref 130146 MW Appointments is acting as an Employment Agency in relation to this vacancyThis job was originally posted as www.totaljobs.com/JobSeeking/UnderwritingAssistantBromley_job****\n\n\n 3. 4-Similarity between 36401 and 399946: 0.050073637702503684\n\n\n\nThe role of Business Development Manager is a critical position within the organisationMechanical Engineers ****p/h Harwell A leading nuclear engineering consultancy is looking to recruit a number of mechanical engineers with current SC clearance to become part of a growing specialist teamYou will pick up the daytoday management of a portfolio of key clients, across a number of verticals including Financial Services, Legal and Professional ServicesThe ideal candidate will have experience with CAD in the nuclear sector and will have appropriate qualificationsThe BDM will be targeted on retention growth maintaining the existing relationship, whilst upselling and crossselling from the wider suite of products/services availableIn return a negotiable daily rate is on offer, as well as the chance to work on exciting projects with a market leaderThe successful candidate will have a background in B2B sales, ideally within the Technology industryResponsibilities The Mechanical Designer is fundamentally a handson CAD draughting based roleEssentially, you will be comfortable engaging with a wide variety of clients, from startups to SME`s and global, corporate organisationsMechanical Designers are responsible for maintaining and developing the engineers' designs as working drawings and ensuring design standards and codes are adhered to throughout the design processYou will have a trackrecord of success in Account Management, with the ability to nurture client relationships and maximise revenue opportunitiesDesign draughting to meet technical specifications, drawing standards and codes primarily using AutoCAD` and/or Inventor Development of designs to fit within space envelope and design interfaces Preparation of manufacturing and installation drawings including detailing, dimensioning and tolerances Recording design changes following design change control procedures Liaison with Document Controllers and Design Engineers Experience required Demonstrable competence gained in a design delivery position Good understanding of generic mechanical principles is preferred e.gCustomer satisfaction, retention growth are pivotal to the success of the businessForces, stresses, materials selection and design principles Experience in nuclear is ideal but not always a prerequisite Technical experience could be gained from but is not limited to precision engineering, conveyer systems, remote handling systems / equipment, heavy engineering, cranes, pipes and pumps Preferably HNC qualified in a technical/engineering discipline though not essentialAlthough a fieldbased role, you can expect to spend 23 days per week in the UK HQ, near Slough in BerkshireMinimum ONC or equivalent qualification gained in a relevant technical, engineering or design disciplineA fastgrowing, privately owned Technology businessA full UK drivers licence is essential for this position as travelling between offices may be requiredWith a suite of products that provide business support to both SME corporate level organisations, the company are quickly becoming a goto name in this spaceTo hear more about this opportunity or be considered for similar roles in the nuclear industry, please apply todayThis is a fantastic opportunity to join a business which is on the upward curve and this will reward both financially and professionallyThis advert was posted by Gold Group one of the UK's leading niche recruitment consultanciesBasic salary, commission and benefits packageWe span a variety of specialist industries and are the recruitment company to help you find your next career opportunityYour application will be reviewed by Michael PageWe pride ourselves on our commitment to candidates and stick to our ethos of finding the right role for the right personPlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsVisit our website or get in touch today to discuss this role, find out what else we've got or just for a chat about the state of your industryServices advertised by Gold Group are those of an Agency and/or an Employment BusinessPlease be aware that we receive a high volume of applications for every role advertised and regularly receive applications from candidates who exceed the job credentialsWe will only contact you within the next 14 days if you are selected for interviewWe will only contact you within the next 14 days if you are selected for interviewWhere specific UK qualifications are required we will take into account overseas equivalentsThis job was originally posted as www.totaljobs.com/JobSeeking/MechanicalEngineers****phHarwell_job****Michael Page is a world leading recruitment consultancyThis job was originally posted as www.SalesTarget.co.uk/JobSeeking/BusinessDevelopmentManager_job****\n\n\n 4. 4-Similarity between 172788 and 399946: 0.050073637702503684\n\n\nThe key responsibilities in the role are: Liaising with HR Department and Finance Manager to produce monthly payroll for a variety of employees Updating starter and leaver information for payroll Updating freelancer information and ensuring correct documentation is provided Manual calculations for SSP, SMP, holiday and tax calculations Ensuring accurate information is passed to the pay bureau in a timely manner Liaising with payroll bureau who run the payroll, ensuring all queries are successfully resolved Assisting employees with any payroll queries and resolving any issues Preparing payroll analysis for Finance Manager on a monthly basis Timely and accurate completion of P****D, and revie The perfect candidate will: Previous working knowledge of managing a payroll either via a pay bureau or in house payroll Demonstrable evidence of uptodate knowledge of current legislation and best practice in Payroll matters Solid understanding of PAYE for payroll and P****D knowledgeMechanical Engineers ****p/h Harwell A leading nuclear engineering consultancy is looking to recruit a number of mechanical engineers with current SC clearance to become part of a growing specialist teamAbility to initiative research for project work Excellent numeracy skills Intermediate level Excel skills Confidential approach essential Meticulous attention to detail Good organisational skills with the ability to manage and prioritise work load effectively Effective and confident communicator who is able to communicate with all people of all levels Page Personnel Finance are always looking for Payroll Clerks of all levels to join a variety of retail/media companies for exciting opportunities in South LondonThe ideal candidate will have experience with CAD in the nuclear sector and will have appropriate qualificationsCompetitive Salary and BenefitsIn return a negotiable daily rate is on offer, as well as the chance to work on exciting projects with a market leaderYour application will be reviewed by Page PersonnelResponsibilities The Mechanical Designer is fundamentally a handson CAD draughting based rolePlease be aware we receive a high volume of applications for every role advertised regularly receive applications from candidates who exceed the job credentialsMechanical Designers are responsible for maintaining and developing the engineers' designs as working drawings and ensuring design standards and codes are adhered to throughout the design processDesign draughting to meet technical specifications, drawing standards and codes primarily using AutoCAD` and/or Inventor Development of designs to fit within space envelope and design interfaces Preparation of manufacturing and installation drawings including detailing, dimensioning and tolerances Recording design changes following design change control procedures Liaison with Document Controllers and Design Engineers Experience required Demonstrable competence gained in a design delivery position Good understanding of generic mechanical principles is preferred e.gForces, stresses, materials selection and design principles Experience in nuclear is ideal but not always a prerequisite Technical experience could be gained from but is not limited to precision engineering, conveyer systems, remote handling systems / equipment, heavy engineering, cranes, pipes and pumps Preferably HNC qualified in a technical/engineering discipline though not essentialMinimum ONC or equivalent qualification gained in a relevant technical, engineering or design disciplineA full UK drivers licence is essential for this position as travelling between offices may be requiredTo hear more about this opportunity or be considered for similar roles in the nuclear industry, please apply todayThis advert was posted by Gold Group one of the UK's leading niche recruitment consultanciesWe span a variety of specialist industries and are the recruitment company to help you find your next career opportunityWe pride ourselves on our commitment to candidates and stick to our ethos of finding the right role for the right personVisit our website or get in touch today to discuss this role, find out what else we've got or just for a chat about the state of your industryServices advertised by Gold Group are those of an Agency and/or an Employment BusinessPlease be aware that we receive a high volume of applications for every role advertised and regularly receive applications from candidates who exceed the job credentialsWe will only contact you within the next 14 days if you are selected for interviewWe will only contact you within the next 14 days if you are selected for interviewPage Personnel is a leading UK recruitment consultancy This job was originally posted as www.totaljobs.com/JobSeeking/Payroll_job****This job was originally posted as www.totaljobs.com/JobSeeking/MechanicalEngineers****phHarwell_job****\n\n\n 5. 4-Similarity between 204450 and 407408: 0.05007587253414264\n\n\nDue to continuous growth, a global leading Manufacturing organisation based in Bristol are currently looking to recruit and General Ledger Assistant to join their busy vibrant teamHealth and Social Care Assessor Health and Social Care Assessor(**** year old) Permanent Position for the right person Locations; Leicester **** Probation period 6 months with a review at **** and 5 months Benefits Pension Scheme Life Assurance Employee Assistance Programme Mileage ****p per mile Lap Top Mobile Phone Dongle Purpose of the Health and Social Care Assessor You will be joining a leading UK Training Provider, that reaches **** learners a yearAs the General Ledger Assistant, you will be working closely with the Finance Manager with a focus on the General LedgerThey run a number of highly regarded Health and Social Care programmes, with a focus on developing a skilled workforceYou will prepare and process journal entries on general ledger, prepare account reconciliations and intercompany balances, along with being responsible for the daily cash book and assisting the Finance Manager with KPI's and the month end reporting packIts about helping individuals now, while contributing to a much bigger vision for the futureThis position would suit an ambitious part qualified ACCA or CIMA accountantYour role will be to assist learners in gaining Health and Social Care qualifications training, reviewing and assessing learners on programmes while using your enthusiasm to engage and inspireThe successful candidate with have experience of both accounts payable and accounts receivable, good reconciliations experience and excellent IT skillsHealth and Social Care Assessor job potentially working from Leicester; Are you a qualified Health and Social Care Assessor looking for a job and able to travel to the above areaIt is desirable to have experience of an ERP system and come from a high volume/ large company backgroundDo you have experience of working on apprenticeship programmesFor more information or to apply, please contact Bridie Horridge.Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersThis rewarding Health and Social Care Assessor role is an enhanced teaching role, which will enable you to utilise your teaching and external relations skillsBy applying for this job you accept the T&C's, Privacy Policy and Disclaimers which can be found at hays.co.ukYou will be working with learners who are specialising in Health and Social Care areas together with delivery of Maths, English and ITResponsibilities for this Health and Social Care Assessor job: As a core member of the team your responsibilities for this role will be to recruit and assess **** learners in their workplaceYou will be expected to ensure their technical and practical knowledge meets the apprenticeship standards, and that they reach the appropriate level to pass their NVQ/QCF qualification, and to develop training so that learners reach LSC or other professional bodies standardsOther aspects of your role will include conducting regular **** reviews, undertaking Health and Safety assessments of the learners workplace, observing learners against NVQ/Framework/Technical Certs/Key Skills/Functional Skills, setting action plans for learners, helping learners build a portfolio, document assessment decisions, submitting portfolios to the IV and providing extra help to the learnersThis role will suit an excellent communicator who has experience of assessing and training and is looking to work with and support learnersIt is essential that you are able to work independently, managing your own timeFor this Health and Social Care Assessor job you will have: You will have an Assessing Qualification Strong experience of working with apprenticeships Experience of supporting learners with personal and social needs Experience of supporting learners with Literacy, Numeracy and ICT needs Experience of assessing Health and Social Care **** s in workbased environments If you are interested in this position and would like to apply for this position, please send your CV Candidates must be eligible to live and work in the UK.Hays Specialist Recruitment Limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workersBy applying for this job you accept the T C s, Privacy Policy and Disclaimers which can be found at hays.co.uk\n\n\n\n(shingle_df[f'jaccard_{k}'] >= cutoff).mean()\n\n0.0012086043021510755\n\n\nIn a case of unrelated jobs most of the similar shingles are very common phrases\n\n(multiset(shingle(tokenize(ads[172788]), 4))).intersection((multiset(shingle(tokenize(ads[399946]), 4))))\n\n{('14 days if you', 0),\n ('This job was originally', 0),\n ('We will only contact', 0),\n ('a high volume of', 0),\n ('applications for every role', 0),\n ('applications from candidates who', 0),\n ('are selected for interview.', 0),\n ('candidates who exceed the', 0),\n ('contact you within the', 0),\n ('credentials. We will only', 0),\n ('days if you are', 0),\n ('exceed the job credentials.', 0),\n ('for every role advertised', 0),\n ('from candidates who exceed', 0),\n ('high volume of applications', 0),\n ('if you are selected', 0),\n ('job credentials. We will', 0),\n ('job was originally posted', 0),\n ('next 14 days if', 0),\n ('of applications for every', 0),\n ('only contact you within', 0),\n ('receive a high volume', 0),\n ('receive applications from candidates', 0),\n ('regularly receive applications from', 0),\n ('the job credentials. We', 0),\n ('the next 14 days', 0),\n ('volume of applications for', 0),\n ('was originally posted as', 0),\n ('we receive a high', 0),\n ('who exceed the job', 0),\n ('will only contact you', 0),\n ('within the next 14', 0),\n ('you are selected for', 0),\n ('you within the next', 0)}"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#minhash",
    "href": "notebooks/Detecting duplicate job ads.html#minhash",
    "title": "skeptric",
    "section": "MinHash",
    "text": "MinHash\nSee the related article\nWe’ll use xxhash because it’s faster than the builtin hash\n\nfrom datasketch import MinHash, MinHashLSH, LeanMinHash\nimport xxhash\n\n\nnum_perm = 128 \n\nWe need to do a little code gymnastics to return bytes\n\ndef minhash(seq:List[str], num_perm=num_perm):\n    m = MinHash(num_perm=num_perm, hashfunc=xxhash.xxh64_intdigest)\n    for s in seq:\n        m.update(s.encode('utf8'))\n    return LeanMinHash(m)\n\n\njaccard(ad_a, ad_b)\n\n0.9230769230769231\n\n\n\nminhash(ad_a).jaccard(minhash(ad_b))\n\n0.9453125\n\n\n\njaccard(tokenize(ad_a), tokenize(ad_b))\n\n0.9081272084805654\n\n\n\nminhash(tokenize(ad_a)).jaccard(minhash(tokenize(ad_b)))\n\n0.9609375\n\n\n\n%%time\njaccard_distances = {}\nfor i in range(100):\n    for j in range(100):\n        if i < j:\n            jaccard_distances[(i,j)] = jaccard(tokenize(ads[i]), tokenize(ads[j]))\n\nCPU times: user 1.22 s, sys: 46.9 ms, total: 1.27 s\nWall time: 1.25 s\n\n\n\n%%time\nminhashes = []\nfor i in range(100):\n    minhashes.append(minhash(tokenize(ads[i])))\n    \nmh_distances = {}\nfor i in range(100):\n    for j in range(100):\n        if i < j:\n            mh_distances[(i, j)] = minhashes[i].jaccard(minhashes[j])\n\nCPU times: user 453 ms, sys: 31.2 ms, total: 484 ms\nWall time: 473 ms\n\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.hist(list(jaccard_distances.values()), bins=30, log=True)\n\n(array([5.450e+02, 6.720e+02, 1.929e+03, 1.253e+03, 2.460e+02, 3.700e+01,\n        1.600e+01, 5.000e+00, 2.000e+00, 7.000e+00, 1.000e+01, 4.300e+01,\n        7.000e+01, 5.400e+01, 2.000e+01, 1.000e+01, 1.200e+01, 3.000e+00,\n        3.000e+00, 3.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n        2.000e+00, 0.000e+00, 0.000e+00, 3.000e+00, 0.000e+00, 3.000e+00]),\n array([0.        , 0.03291925, 0.06583851, 0.09875776, 0.13167702,\n        0.16459627, 0.19751553, 0.23043478, 0.26335404, 0.29627329,\n        0.32919255, 0.3621118 , 0.39503106, 0.42795031, 0.46086957,\n        0.49378882, 0.52670807, 0.55962733, 0.59254658, 0.62546584,\n        0.65838509, 0.69130435, 0.7242236 , 0.75714286, 0.79006211,\n        0.82298137, 0.85590062, 0.88881988, 0.92173913, 0.95465839,\n        0.98757764]),\n <a list of 30 Patch objects>)\n\n\n\n\n\nMinhash is a reasonable approximation of the jaccard distance\n\ndiffs = [jaccard_distances[k] - v for k,v in list(mh_distances.items())]\np = plt.hist(diffs, bins=30)\nplt.xlabel('Error in 1-Jaccard Distance')\nplt.ylabel('Frequency')\nplt.title('MinHash Error on 100 samples (128 permutations)')\n\nText(0.5, 1.0, 'MinHash Error on 100 samples (128 permutations)')\n\n\n\n\n\n\nnp.std(diffs)\n\n0.024130764261291047\n\n\n\nnp.mean(diffs)\n\n-0.01756808817777059\n\n\n\nplt.hexbin([jaccard_distances[k] for k in mh_distances], list(mh_distances.values()), bins='log')\nplt.xlabel('Jaccard Distance')\nplt.ylabel('MinHash approximation')\nplt.title('MinHash correlates very well with Jaccard')\n\nText(0.5, 1.0, 'MinHash correlates very well with Jaccard')\n\n\n\n\n\n\nplt.hexbin([jaccard_distances[k] for k in mh_distances], diffs, bins='log')\n\n<matplotlib.collections.PolyCollection at 0x7f8abac30f98>\n\n\n\n\n\n\nlen(ads)\n\n407894\n\n\nHow long will it take to calculate the 400k minhashes?\nAbout half an hour on my laptop\n\nimport pickle\nfrom tqdm.notebook import tqdm\n\n\ndef get_minhashes(corpus, n_shingle=1, preprocess=tokenize, disable_progress=False):\n    return [minhash(shingle(preprocess(text), n_shingle)) for text in tqdm(corpus, disable=disable_progress)]\n\n\nfor i in [1, 3, 5, 7]:\n    print(f'Shingle {i}')\n    minhashes = get_minhashes(ads, n_shingle=i)\n    with open(f'minhash_{i}.pkl', 'wb') as f:\n        pickle.dump(minhashes, f)\n\nShingle 1\n\n\n\n\n\n\nShingle 3\n\n\n\n\n\n\nShingle 5\n\n\n\n\n\n\nShingle 7\n\n\n\n\n\n\n\n\n\n!ls -alh *.pkl\n\n-rw-r----- 1 eross eross 218M May 22 16:38 minhash_1.pkl\n-rw-r----- 1 eross eross 218M May 22 17:08 minhash_3.pkl\n-rw-r----- 1 eross eross 218M May 22 17:38 minhash_5.pkl\n-rw-r----- 1 eross eross 218M May 22 18:07 minhash_7.pkl\n-rw-r----- 1 eross eross 178M May 11 12:40 similar_relevance_shingle3_42band_3row.pkl\n\n\n\n218*(1024)**2 / 400000\n\n571.47392"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#filtering-bad-pairs",
    "href": "notebooks/Detecting duplicate job ads.html#filtering-bad-pairs",
    "title": "skeptric",
    "section": "Filtering bad pairs",
    "text": "Filtering bad pairs\n\nlen(set(item for pair in similar_all for item in pair))\n\n331307\n\n\n\nlen(similar_all)\n\n7623164\n\n\n\n7623164 / 331307\n\n23.009365935522037\n\n\n\nrels = [relevance(ads[pair[0]], ads[pair[1]], 3) for pair in tqdm(similar_all)]\n\n\n\n\n\n\n\n\nsimilar_rel = {pair: rel for pair, rel in zip(similar_all, rels)}\n\n\nimport pickle\n\n\nwith open('similar_relevance_shingle3_42band_3row.pkl', 'wb') as f:\n    pickle.dump(similar_rel, f)\n\n\nmin(similar_rel.values())\n\n0.0015313935681470138"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#directly",
    "href": "notebooks/Detecting duplicate job ads.html#directly",
    "title": "skeptric",
    "section": "Directly",
    "text": "Directly\n\ndef find(x, parents):\n    while parents.get(x, x) != x:\n        x = parents[x]\n    return x\n\n\ndef union(a, b, parents):\n    root_a = find(a, parents)\n    root_b = find(b, parents)\n    if root_b != root_a:\n        parents[root_a] = root_b\n        if root_b not in parents:\n            parents[root_b] = root_b\n\n\nfrom collections import defaultdict\ndef find_sets(parents):\n    sets = defaultdict(list)\n    for child in parents:\n        root = find(child, parents)\n        sets[root].append(child)\n    return list(sets.values())\n\n\nparents = {}\n\n\nunion(2, 1, parents)\n\n\nunion(5, 3, parents)\n\n\nunion(3, 1, parents)\n\n\nunion(7, 9, parents)\n\n\nfind_sets(parents)\n\n[[2, 1, 5, 3], [7, 9]]\n\n\n\ndef lsh_similar_sets(minhashes, bands, rows):\n    lsh = MinHashLSH(num_perm=num_perm, params=(bands, rows))\n    for i, mh in enumerate(tqdm(minhashes)):\n        lsh.insert(i, mh)\n    \n    parents = {}\n    for hashtable in lsh.hashtables:\n        for items in hashtable._dict.values():\n            items = list(items)\n            for i in range(len(items)):\n                for j in range(len(items)):\n                    if i > j:\n                        union(items[i], items[j], parents)\n    return find_sets(parents)\n\n\nparents = lsh_similar_sets(minhashes[:1000], 8, 16)\n\n\n\n\n\n\n\n\nparents[:10]\n\n[[111, 36],\n [58, 57],\n [76, 74],\n [107, 81],\n [87, 96],\n [135, 134],\n [182, 666, 667, 616, 668, 615, 614, 665, 183],\n [265, 264, 266, 263],\n [277, 276, 450, 278],\n [301, 300, 299, 302]]\n\n\n\nlen(parents)\n\n105\n\n\n\nfrom collections import Counter\nCounter(map(len, parents))\n\nCounter({2: 71, 9: 2, 4: 12, 3: 14, 7: 2, 5: 2, 6: 1, 19: 1})"
  },
  {
    "objectID": "notebooks/Detecting duplicate job ads.html#combinatorial-explosion",
    "href": "notebooks/Detecting duplicate job ads.html#combinatorial-explosion",
    "title": "skeptric",
    "section": "Combinatorial Explosion",
    "text": "Combinatorial Explosion\nFinding all cliques is O(exp(n)), and this blows up in practice near 400 nodes.\n\nn = 65200\nlen(similar_connected[n])\n\n369\n\n\n\ndef count(x):\n    for idx, _ in enumerate(x):\n        pass\n    return idx + 1\n\n3s for 369\n\n%%time\nSG=G.subgraph(similar_connected[n])\nprint(count(nx.find_cliques(SG)))\n\n7\nCPU times: user 2.69 s, sys: 46.9 ms, total: 2.73 s\nWall time: 2.84 s\n\n\n\nn = 65201\nlen(similar_connected[n])\n\n392\n\n\n12s for 392\n\n%%time\nSG=G.subgraph(similar_connected[n])\nprint(count(nx.find_cliques(SG)))\n\n511\nCPU times: user 11.5 s, sys: 156 ms, total: 11.6 s\nWall time: 12 s\n\n\n\nn = 65202\nlen(similar_connected[n])\n\n427\n\n\n\n%%time\nSG=G.subgraph(similar_connected[n])\nprint(count(nx.find_cliques(SG)))\n\n119998\nCPU times: user 19.6 s, sys: 203 ms, total: 19.8 s\nWall time: 21.1 s\n\n\n\nn = 65204\nlen(similar_connected[n])\n\n429\n\n\nI gave it 5 mintues, no ides how long this would take.\n\n%%time\nSG=G.subgraph(similar_connected[n])\nprint(count(nx.find_cliques(SG)))\n\nKeyboardInterrupt:"
  },
  {
    "objectID": "notebooks/WAT WET WARC - Common Crawl Archives.html",
    "href": "notebooks/WAT WET WARC - Common Crawl Archives.html",
    "title": "skeptric",
    "section": "",
    "text": "Reading WARC\n\nr = requests.get(warc_url, stream=True)\nrecords = ArchiveIterator(r.raw)\n\nFirst record is warcinfo about the crawl\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'warcinfo'\n\n\n\na = record.content_stream().read()\n\n\nprint(a.decode('utf-8'))\n\nisPartOf: CC-MAIN-2020-24\npublisher: Common Crawl\ndescription: Wide crawl of the web for May/June 2020\noperator: Common Crawl Admin (info@commoncrawl.org)\nhostname: ip-10-67-67-182.ec2.internal\nsoftware: Apache Nutch 1.16 (modified, https://github.com/commoncrawl/nutch/)\nrobots: checked via crawler-commons 1.1-SNAPSHOT (https://github.com/crawler-commons/crawler-commons)\nformat: WARC File Format 1.1\nconformsTo: http://iipc.github.io/warc-specifications/specifications/warc-format/warc-1.1/\n\n\n\nThe next is details about the request to the server\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'request'\n\n\n\nrecord.rec_headers\n\nStatusAndHeaders(protocol = 'WARC/1.0', statusline = '', headers = [('WARC-Type', 'request'), ('WARC-Date', '2020-05-25T05:11:44Z'), ('WARC-Record-ID', '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>'), ('Content-Length', '330'), ('Content-Type', 'application/http; msgtype=request'), ('WARC-Warcinfo-ID', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'), ('WARC-IP-Address', '124.156.125.238'), ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619')])\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'request'),\n ('WARC-Date', '2020-05-25T05:11:44Z'),\n ('WARC-Record-ID', '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>'),\n ('Content-Length', '330'),\n ('Content-Type', 'application/http; msgtype=request'),\n ('WARC-Warcinfo-ID', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'),\n ('WARC-IP-Address', '124.156.125.238'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619')]\n\n\nShows HTTP headers in the get request\n\nrecord.http_headers\n\nStatusAndHeaders(protocol = 'GET', statusline = '/related_report/detail.php?id=866619 HTTP/1.1', headers = [('User-Agent', 'CCBot/2.0 (https://commoncrawl.org/faq/)'), ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'), ('Accept-Language', 'en-US,en;q=0.5'), ('If-Modified-Since', 'Fri, 28 Feb 2020 12:03:01 UTC'), ('Accept-Encoding', 'br,gzip'), ('Host', '002397.cn'), ('Connection', 'Keep-Alive')])\n\n\n\nrecord.http_headers.headers\n\n[('User-Agent', 'CCBot/2.0 (https://commoncrawl.org/faq/)'),\n ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'),\n ('Accept-Language', 'en-US,en;q=0.5'),\n ('If-Modified-Since', 'Fri, 28 Feb 2020 12:03:01 UTC'),\n ('Accept-Encoding', 'br,gzip'),\n ('Host', '002397.cn'),\n ('Connection', 'Keep-Alive')]\n\n\nThere’s no data in the request\n\na = record.content_stream().read()\n\n\na\n\nb''\n\n\nThe next item is the response of the previous request\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'response'\n\n\n\nrecord.rec_headers\n\nStatusAndHeaders(protocol = 'WARC/1.0', statusline = '', headers = [('WARC-Type', 'response'), ('WARC-Date', '2020-05-25T05:11:44Z'), ('WARC-Record-ID', '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>'), ('Content-Length', '14321'), ('Content-Type', 'application/http; msgtype=response'), ('WARC-Warcinfo-ID', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'), ('WARC-Concurrent-To', '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>'), ('WARC-IP-Address', '124.156.125.238'), ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'), ('WARC-Payload-Digest', 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH'), ('WARC-Block-Digest', 'sha1:CNOLET4OGLWYCKDJUDAAVYF5YS3MCW4S'), ('WARC-Identified-Payload-Type', 'text/html')])\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'response'),\n ('WARC-Date', '2020-05-25T05:11:44Z'),\n ('WARC-Record-ID', '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>'),\n ('Content-Length', '14321'),\n ('Content-Type', 'application/http; msgtype=response'),\n ('WARC-Warcinfo-ID', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'),\n ('WARC-Concurrent-To', '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>'),\n ('WARC-IP-Address', '124.156.125.238'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'),\n ('WARC-Payload-Digest', 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH'),\n ('WARC-Block-Digest', 'sha1:CNOLET4OGLWYCKDJUDAAVYF5YS3MCW4S'),\n ('WARC-Identified-Payload-Type', 'text/html')]\n\n\n\nrecord.http_headers\n\nStatusAndHeaders(protocol = 'HTTP/1.1', statusline = '200 OK', headers = [('Date', 'Mon, 25 May 2020 05:11:44 GMT'), ('Content-Type', 'text/html'), ('X-Crawler-Content-Length', '6641'), ('Content-Length', '13911'), ('Connection', 'keep-alive'), ('Set-Cookie', 'tgw_l7_route=f60eebbcd438146c92bb28cfca9251e6; Expires=Mon, 25-May-2020 06:11:44 GMT; Path=/'), ('Server', 'Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips PHP/5.4.16'), ('X-Powered-By', 'PHP/5.4.16'), ('Vary', 'Accept-Encoding'), ('X-Crawler-Content-Encoding', 'gzip')])\n\n\n\nrecord.http_headers.statusline\n\n'200 OK'\n\n\n\nrecord.http_headers.headers\n\n[('Date', 'Mon, 25 May 2020 05:11:44 GMT'),\n ('Content-Type', 'text/html'),\n ('X-Crawler-Content-Length', '6641'),\n ('Content-Length', '13911'),\n ('Connection', 'keep-alive'),\n ('Set-Cookie',\n  'tgw_l7_route=f60eebbcd438146c92bb28cfca9251e6; Expires=Mon, 25-May-2020 06:11:44 GMT; Path=/'),\n ('Server', 'Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips PHP/5.4.16'),\n ('X-Powered-By', 'PHP/5.4.16'),\n ('Vary', 'Accept-Encoding'),\n ('X-Crawler-Content-Encoding', 'gzip')]\n\n\n\na = record.content_stream().read()\n\nThis contains the full HTML\n\nprint(a.decode('utf-8')[:1000])\n\n<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"utf-8\">\n    <title>纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)</title>\n    <meta name=\"mobile-agent\" content=\"format=html5; url=detail_m.php?id=866619\" />\n    <meta name=\"mobile-agent\" content=\"format=xhtml; url=detail_m.php?id=866619\" />\n    <meta name=\"keywords\" content=\"纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397\"/>\n    <meta name=\"description\" content=\"梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证\"/>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"http://txt.inv.org.cn/ir/site/pc/css.css\"/>\n</head>\n<body>\n\n<div class=\"header clearfix\">\n    <div class=\"logo\">\n                <a href=\"/\" target=\"_blank\"><span>梦洁股份(002397)</span></a>\n    </div>\n    <div class=\"header_meun\">\n        <a href=\"/index_m.php\" target=\"_blank\" style=\"border:none;\">移动版</a>\n            </div>\n\n<div cl\n\n\nThe next record is metadata about the fetch:\n\nHow long it took to fetch the size\nDetected characterset\nLanguages detected\n\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'metadata'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'metadata'),\n ('WARC-Date', '2020-05-25T05:11:44Z'),\n ('WARC-Record-ID', '<urn:uuid:ce3946a5-f44b-417c-ab8c-3d32e7db40f7>'),\n ('Content-Length', '201'),\n ('Content-Type', 'application/warc-fields'),\n ('WARC-Warcinfo-ID', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'),\n ('WARC-Concurrent-To', '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619')]\n\n\n\na = record.content_stream().read()\n\n\nprint(a.decode('utf-8'))\n\nfetchTimeMs: 731\ncharset-detected: UTF-8\nlanguages-cld2: {\"reliable\":true,\"text-bytes\":8659,\"languages\":[{\"code\":\"zh\",\"code-iso-639-3\":\"zho\",\"text-covered\":0.98,\"score\":2026.0,\"name\":\"Chinese\"}]}\n\n\n\n\nNow we move onto the next request\n\nrecord = next(records)\n\n\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('request', 'http://003364.cn/j78/453618.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('response', 'http://003364.cn/j78/453618.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://003364.cn/j78/453618.html')\n\n\nAnd the next record\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('request', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('response', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\nAnd the next\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('request', 'http://023yc.com/az/118080.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('response', 'http://023yc.com/az/118080.html')\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://023yc.com/az/118080.html')\n\n\nAnd so on\n\nr.close()\n\n\n\nReading WET\n\nr = requests.get(wet_url, stream=True)\nrecords = ArchiveIterator(r.raw)\n\nFirst record is information about the crawl\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'warcinfo'\n\n\n\na = record.content_stream().read()\n\n\nprint(a.decode('utf-8'))\n\nSoftware-Info: ia-web-commons.1.1.10-SNAPSHOT-20200605094634\nExtracted-Date: Sun, 07 Jun 2020 16:56:24 GMT\nrobots: checked via crawler-commons 1.1-SNAPSHOT (https://github.com/crawler-commons/crawler-commons)\nisPartOf: CC-MAIN-2020-24\noperator: Common Crawl Admin (info@commoncrawl.org)\ndescription: Wide crawl of the web for May/June 2020\npublisher: Common Crawl\n\n\n\n\nThe WET file doesn’t contain the headers just the title and text.\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'conversion'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'conversion'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'),\n ('WARC-Date', '2020-05-25T05:11:44Z'),\n ('WARC-Record-ID', '<urn:uuid:3020cc7c-fd30-4f3d-bbd7-8513f33cd83a>'),\n ('WARC-Refers-To', '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>'),\n ('WARC-Block-Digest', 'sha1:QBZTGL7G53UVVTAZ5VOKXL3C7LRZ2FUR'),\n ('WARC-Identified-Content-Language', 'zho'),\n ('Content-Type', 'text/plain'),\n ('Content-Length', '9300')]\n\n\n\nrecord.http_headers\n\n\na = record.content_stream().read()\n\nThe first line is the title of the page, everything else is the text.\n\nprint(a.decode('utf-8')[:1000])\n\n纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)\n梦洁股份(002397)\n移动版\n首页\n股票行情\n媒体报道\n相关新闻\n公司公告\n研究报告\n相关研报\n纺织服装行业周报:终端零售回暖,板块业绩等待验证\n发布时间：2017-02-12 研究机构：海通证券\n投资要点:\n市场回顾:本周(20170206-20170212)纺织服装板块上涨2.21%,跑赢上证综指0.41个百分点,在申万一级行业中列第十一。其中,纺织制造板块上涨2.58%,服装家纺板块上涨1.97%。个股方面,万里马、梦洁股份(002397)、摩登大道、美欣达、金发拉比等个股涨幅居前;探路者、星期六、希努尔、比音勒芬、山东如意跌幅靠前。从PE估值水平来看,纺织服装板块目前估值32.9倍(TTM,剔除负值),其中纺织制造板块32.0倍,服装家纺板块35.4倍。\n行业数据:零售方面,春节黄金周零售大幅回升,全国百家重点大型零售企业零售额同比增长2.8%,增速相比上年回升了9.4个百分点。其中服装类商品零售额同比增长4.1%,高于上年春节10.1个百分点。2017年1月份,全国50家重点大型零售企业零售额同比增长17.8%,这一增速与同样包含了春节假期的2014年1月份增速基本持平,高于2012年同期增速4.3个百分点,消费市场显示出较强的活力。出口方面,1月份出口现开门红。2017年1月,我国纺织品出口95.84亿美元,同比增长3.50%,服装及其附件出口143.20亿美元,同比增长1.85%,纺织品服装合计出口239.04亿美元,同比增长2.5%。 周组合跑赢行业指数:跨境通(+7.34%),歌力思(+1.13%),美盛文化(+0.18%),乔治白(+2.23%),按照各1/4的权重,组合收益+2.72%。\n周观点: 本周,我们对纺织服装板块2016年业绩前瞻进行了整理,共计57家公司先后发布了2016E业绩预告,其中,33家预告业绩增长,5家预告业绩持平/下滑(+5%~-10%),19家预告业绩下滑。 纺织行业率先回暖。我们认为纺织制造子版块业绩有所改善的主要原因有:1)一方面制造业出口比例较高,受益于人民币贬值带来的出口形势的改善,以及部分汇兑损益对报表带来的正面影响;2)制造业下游客户多为优质品牌商,其中海外龙头品牌由于其全球销售的性质,更广泛享受消费复苏的影响\n\n\n\nrecord = next(records)\nrecord.rec_type\n\n'conversion'\n\n\n\nrecord.rec_headers.get_header('WARC-Target-URI')\n\n'http://003364.cn/j78/453618.html'\n\n\n\na = record.content_stream().read()\n\nThis page seems to be broken PHP?\n\nprint(a.decode('utf-8')[:1000])\n\nCan not fopen please check the file or PHP.INI\n\n\nAnd the next page\n\nrecord = next(records)\nrecord.rec_type\n\n'conversion'\n\n\n\nrecord.rec_headers.get_header('WARC-Target-URI')\n\n'http://010yingkelawyer.com/case/2018-09-25/408.html'\n\n\n\na = record.content_stream().read()\n\nMore text\n\nprint(a.decode('utf-8')[:1000])\n\n北京刑事律师 彭坤律师辩护北某某非法吸收公众存款／集资诈骗案，成功案例\n北京市盈科律师事务所\n北京著名刑事辩护律师\n13911269079\n首页\n律师简介\n律师文集\n业务领域\n贪污贿赂\n职务犯罪\n经济犯罪\n涉黑犯罪\n海关走私\n死刑复核\n刑事再审\n经典案例\n团队风采\n荣誉展示\n在线留言\n联系我们\n您现在的位置是：首页 > 经典案例\n北京刑事律师 彭坤律师辩护北某某非法吸收公众存款／集资诈骗案，成功案例\n发布时间：2018-09-25 15:10:16 浏览次数：\n案情简介：\n北某某与庞某是夫妻关系，2010年加盟青岛某某投资管理有限公司后于2010年8月31日注册成立某某县银基信息咨询有限公司，公司，非法向社会不特定人员吸收存款344443000元，为维护平台正常运营，包装假标、过期的标，一标多融、加大自融等方式继续吸收资金，最终因客观原因，平台爆雷，截止案发未偿还贷款126060800元，公安机关以非法吸收公众存款罪、集资诈骗罪立案，案件到检后，本人多次与承办检察官沟通，据理力争，成功说服检察官仅以非法吸收公众存款罪追究我的当事人刑事责任。\n案件结果：\n第一被告犯非法吸收公众存款罪、集资诈骗罪，判处无期徒刑；第二被告犯非法吸收公众存款罪判处5年有期徒刑。\n本案的意义：\n非法集资案件，对平台负责人来讲，如果平台爆雷，大多多数案件都是以非法吸收公众存款罪、集资诈骗罪追究责任，可想而知，这类案件数额都特别巨大，一旦认定为集资诈骗，就是无期徒刑，大多数平台都存在以后面吸收的资金偿还前面本息的情况，俗称有“拆东墙补西墙”情况，关键是怎么区分非吸还是集资诈骗， “拆东墙补西墙”行为、资金的去向、标的的真假是判断的主要因素，《非法集资解释理解与适用》认为，“拆东墙补西墙”不能单独评价行为是否具有“非法占有为目的”，还应当结合其他情节综合判断，支付本息是非法集资的一个基本特征，在一定意义上，按期支付本金和高额回报反而有可能说明行为人主观上没有非法占有目的，本案，本人成功说服检察官，仅以非法吸收公众存款罪追究北某某的刑事责任。\n上一篇: 北京刑事律师 彭坤办理非法利用信息网络案 成功取保候审    \n下一篇: 北京刑事律师 彭坤主办青岛赵某、孙某某诈骗案 二审撤销原判发回重审  \n首页 | 律师简介 | 法律资讯 | 业务领域 | 经典案例 | 团队风采 | 在线留言 | 联系我们\nCopy\n\n\n\nr.close()\n\n\n\nReading WAT\n\nr = requests.get(wat_url, stream=True)\nrecords = ArchiveIterator(r.raw)\n\nAgain the first record is a header\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'warcinfo'\n\n\n\na = record.content_stream().read()\nprint(a.decode('utf-8'))\n\nSoftware-Info: ia-web-commons.1.1.10-SNAPSHOT-20200605094634\nExtracted-Date: Sun, 07 Jun 2020 16:56:24 GMT\nip: 10.67.67.60\nhostname: ip-10-67-67-60.ec2.internal\nformat: WARC File Format 1.0\nconformsTo: http://bibnum.bnf.fr/WARC/WARC_ISO_28500_version1_latestdraft.pdf\n\n\n\n\nThe next one is metadata about the WARC records themselves\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'metadata'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'metadata'),\n ('WARC-Target-URI', 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz'),\n ('WARC-Date', '2020-06-07T16:56:24Z'),\n ('WARC-Record-ID', '<urn:uuid:06070eb0-5afe-4a0c-9c6c-f0d4188414ec>'),\n ('WARC-Refers-To', '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>'),\n ('Content-Type', 'application/json'),\n ('Content-Length', '1239')]\n\n\n\nrecord.http_headers\n\n\na = record.content_stream().read()\n\n\ndata = json.loads(a.decode('utf-8'))\ndata\n\n{'Container': {'Filename': 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz',\n  'Compressed': True,\n  'Offset': '0',\n  'Gzip-Metadata': {'Deflate-Length': '481',\n   'Header-Length': '10',\n   'Footer-Length': '8',\n   'Inflated-CRC': '1190498035',\n   'Inflated-Length': '766'}},\n 'Envelope': {'Payload-Metadata': {'Actual-Content-Length': '503',\n   'Block-Digest': 'sha1:XUOM4YJTGT5VXOY2XJ5KNXDHKNMYUPQA',\n   'Trailing-Slop-Length': '0',\n   'Headers-Corrupt': True,\n   'Actual-Content-Type': 'application/warc-fields',\n   'WARC-Info-Metadata': {'isPartOf': 'CC-MAIN-2020-24',\n    'publisher': 'Common Crawl',\n    'description': 'Wide crawl of the web for May/June 2020',\n    'operator': 'Common Crawl Admin (info@commoncrawl.org)',\n    'hostname': 'ip-10-67-67-182.ec2.internal',\n    'software': 'Apache Nutch 1.16 (modified, https://github.com/commoncrawl/nutch/)',\n    'robots': 'checked via crawler-commons 1.1-SNAPSHOT (https://github.com/crawler-commons/crawler-commons)',\n    'format': 'WARC File Format 1.1'}},\n  'Format': 'WARC',\n  'WARC-Header-Length': '259',\n  'WARC-Header-Metadata': {'WARC-Type': 'warcinfo',\n   'WARC-Date': '2020-05-25T03:26:36Z',\n   'WARC-Record-ID': '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>',\n   'Content-Length': '503',\n   'Content-Type': 'application/warc-fields',\n   'WARC-Filename': 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz'}}}\n\n\nThe next request contains all the metadata of the first request\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'metadata'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'metadata'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'),\n ('WARC-Date', '2020-06-07T16:56:24Z'),\n ('WARC-Record-ID', '<urn:uuid:56945f62-e374-4572-9e2c-f5954ed588ed>'),\n ('WARC-Refers-To', '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>'),\n ('Content-Type', 'application/json'),\n ('Content-Length', '1458')]\n\n\n\nrecord.http_headers\n\n\na = record.content_stream().read()\n\nContainer shows where the WARC data is, this is about the request\n\ndata = json.loads(a.decode('utf-8'))\ndata\n\n{'Container': {'Filename': 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz',\n  'Compressed': True,\n  'Offset': '481',\n  'Gzip-Metadata': {'Deflate-Length': '479',\n   'Header-Length': '10',\n   'Footer-Length': '8',\n   'Inflated-CRC': '400586359',\n   'Inflated-Length': '706'}},\n 'Envelope': {'Payload-Metadata': {'Actual-Content-Type': 'application/http; msgtype=request',\n   'HTTP-Request-Metadata': {'Request-Message': {'Method': 'GET',\n     'Path': '/related_report/detail.php?id=866619',\n     'Version': 'HTTP/1.1'},\n    'Headers-Length': '328',\n    'Headers': {'User-Agent': 'CCBot/2.0 (https://commoncrawl.org/faq/)',\n     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n     'Accept-Language': 'en-US,en;q=0.5',\n     'If-Modified-Since': 'Fri, 28 Feb 2020 12:03:01 UTC',\n     'Accept-Encoding': 'br,gzip',\n     'Host': '002397.cn',\n     'Connection': 'Keep-Alive'},\n    'Entity-Length': '0',\n    'Entity-Digest': 'sha1:3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ',\n    'Entity-Trailing-Slop-Length': '0'},\n   'Actual-Content-Length': '330',\n   'Block-Digest': 'sha1:PVCMTRJPX7C5ZEWYAALER3IPUBR5A7S7',\n   'Trailing-Slop-Length': '4'},\n  'Format': 'WARC',\n  'WARC-Header-Length': '372',\n  'WARC-Header-Metadata': {'WARC-Type': 'request',\n   'WARC-Date': '2020-05-25T05:11:44Z',\n   'WARC-Record-ID': '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>',\n   'Content-Length': '330',\n   'Content-Type': 'application/http; msgtype=request',\n   'WARC-Warcinfo-ID': '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>',\n   'WARC-IP-Address': '124.156.125.238',\n   'WARC-Target-URI': 'http://002397.cn/related_report/detail.php?id=866619'}}}\n\n\nNotice it’s HTTP-Request-Metadata\n\ndata['Envelope']\n\n{'Payload-Metadata': {'Actual-Content-Type': 'application/http; msgtype=request',\n  'HTTP-Request-Metadata': {'Request-Message': {'Method': 'GET',\n    'Path': '/related_report/detail.php?id=866619',\n    'Version': 'HTTP/1.1'},\n   'Headers-Length': '328',\n   'Headers': {'User-Agent': 'CCBot/2.0 (https://commoncrawl.org/faq/)',\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en-US,en;q=0.5',\n    'If-Modified-Since': 'Fri, 28 Feb 2020 12:03:01 UTC',\n    'Accept-Encoding': 'br,gzip',\n    'Host': '002397.cn',\n    'Connection': 'Keep-Alive'},\n   'Entity-Length': '0',\n   'Entity-Digest': 'sha1:3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ',\n   'Entity-Trailing-Slop-Length': '0'},\n  'Actual-Content-Length': '330',\n  'Block-Digest': 'sha1:PVCMTRJPX7C5ZEWYAALER3IPUBR5A7S7',\n  'Trailing-Slop-Length': '4'},\n 'Format': 'WARC',\n 'WARC-Header-Length': '372',\n 'WARC-Header-Metadata': {'WARC-Type': 'request',\n  'WARC-Date': '2020-05-25T05:11:44Z',\n  'WARC-Record-ID': '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>',\n  'Content-Length': '330',\n  'Content-Type': 'application/http; msgtype=request',\n  'WARC-Warcinfo-ID': '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>',\n  'WARC-IP-Address': '124.156.125.238',\n  'WARC-Target-URI': 'http://002397.cn/related_report/detail.php?id=866619'}}\n\n\nAnd the next one is about the response\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'metadata'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'metadata'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'),\n ('WARC-Date', '2020-06-07T16:56:25Z'),\n ('WARC-Record-ID', '<urn:uuid:c72d34ed-8279-4653-8bd3-a53b0b072dc8>'),\n ('WARC-Refers-To', '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>'),\n ('Content-Type', 'application/json'),\n ('Content-Length', '4048')]\n\n\n\nrecord.http_headers\n\n\na = record.content_stream().read()\n\nEnvelope contains the details\n\ndata = json.loads(a.decode('utf-8'))\ndata\n\n{'Container': {'Filename': 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz',\n  'Compressed': True,\n  'Offset': '960',\n  'Gzip-Metadata': {'Deflate-Length': '7317',\n   'Header-Length': '10',\n   'Footer-Length': '8',\n   'Inflated-CRC': '-219204245',\n   'Inflated-Length': '14929'}},\n 'Envelope': {'Payload-Metadata': {'Actual-Content-Type': 'application/http; msgtype=response',\n   'HTTP-Response-Metadata': {'Response-Message': {'Status': '200',\n     'Version': 'HTTP/1.1',\n     'Reason': 'OK'},\n    'Headers-Length': '410',\n    'Headers': {'Date': 'Mon, 25 May 2020 05:11:44 GMT',\n     'Content-Type': 'text/html',\n     'X-Crawler-Content-Length': '6641',\n     'Content-Length': '13911',\n     'Connection': 'keep-alive',\n     'Set-Cookie': 'tgw_l7_route=f60eebbcd438146c92bb28cfca9251e6; Expires=Mon, 25-May-2020 06:11:44 GMT; Path=/',\n     'Server': 'Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips PHP/5.4.16',\n     'X-Powered-By': 'PHP/5.4.16',\n     'Vary': 'Accept-Encoding',\n     'X-Crawler-Content-Encoding': 'gzip'},\n    'HTML-Metadata': {'Head': {'Title': '纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)',\n      'Metas': [{'name': 'mobile-agent',\n        'content': 'format=html5; url=detail_m.php?id=866619'},\n       {'name': 'mobile-agent',\n        'content': 'format=xhtml; url=detail_m.php?id=866619'},\n       {'name': 'keywords',\n        'content': '纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397'},\n       {'name': 'description',\n        'content': '梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证'}],\n      'Link': [{'path': 'LINK@/href',\n        'url': 'http://txt.inv.org.cn/ir/site/pc/css.css',\n        'rel': 'stylesheet',\n        'type': 'text/css'}],\n      'Scripts': [{'path': 'SCRIPT@/src',\n        'url': 'http://static.bshare.cn/b/buttonLite.js#style=-1&uuid=&pophcol=2&lang=zh',\n        'type': 'text/javascript'},\n       {'path': 'SCRIPT@/src',\n        'url': 'http://static.bshare.cn/b/bshareC0.js',\n        'type': 'text/javascript'},\n       {'path': 'SCRIPT@/src',\n        'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'},\n       {'path': 'SCRIPT@/src',\n        'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'}]},\n     'Links': [{'path': 'A@/href',\n       'url': '/',\n       'target': '_blank',\n       'text': '梦洁股份(002397)'},\n      {'path': 'A@/href',\n       'url': '/index_m.php',\n       'target': '_blank',\n       'text': '移动版'},\n      {'path': 'IMG@/src',\n       'url': 'http://img.inv.org.cn/broker/huasheng_pc.jpg'},\n      {'path': 'A@/href',\n       'url': 'https://hd.hstong.com/marketing/2019/0228?_scnl=OTg0NWJibzY0MTI3',\n       'target': '_blank'},\n      {'path': 'A@/href', 'url': '/', 'text': '首页'},\n      {'path': 'A@/href', 'url': '/quote/', 'text': '股票行情'},\n      {'path': 'A@/href', 'url': '/media_news/', 'text': '媒体报道'},\n      {'path': 'A@/href', 'url': '/related_news/', 'text': '相关新闻'},\n      {'path': 'A@/href', 'url': '/notice/', 'text': '公司公告'},\n      {'path': 'A@/href', 'url': '/report/', 'text': '研究报告'},\n      {'path': 'A@/href', 'url': '/related_report/', 'text': '相关研报'},\n      {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '梦洁股份'},\n      {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '002397'},\n      {'path': 'A@/href',\n       'url': 'http://www.bShare.cn/',\n       'title': '分享到',\n       'text': '分享到'},\n      {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/ad/zixun_pc.jpg'},\n      {'path': 'A@/href',\n       'url': 'http://stock.inv.org.cn',\n       'target': '_blank',\n       'text': '股票投资之家'}]},\n    'Entity-Length': '13911',\n    'Entity-Digest': 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH',\n    'Entity-Trailing-Slop-Length': '0'},\n   'Actual-Content-Length': '14321',\n   'Block-Digest': 'sha1:CNOLET4OGLWYCKDJUDAAVYF5YS3MCW4S',\n   'Trailing-Slop-Length': '4'},\n  'Format': 'WARC',\n  'WARC-Header-Length': '604',\n  'WARC-Header-Metadata': {'WARC-Type': 'response',\n   'WARC-Date': '2020-05-25T05:11:44Z',\n   'WARC-Record-ID': '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>',\n   'Content-Length': '14321',\n   'Content-Type': 'application/http; msgtype=response',\n   'WARC-Warcinfo-ID': '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>',\n   'WARC-Concurrent-To': '<urn:uuid:b14093da-51b7-4f61-8fa5-4630084209d9>',\n   'WARC-IP-Address': '124.156.125.238',\n   'WARC-Target-URI': 'http://002397.cn/related_report/detail.php?id=866619',\n   'WARC-Payload-Digest': 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH',\n   'WARC-Block-Digest': 'sha1:CNOLET4OGLWYCKDJUDAAVYF5YS3MCW4S',\n   'WARC-Identified-Payload-Type': 'text/html'}}}\n\n\nHere we’ve got the HTTP headers and response metadata\n\ndata['Envelope']['Payload-Metadata']\n\n{'Actual-Content-Type': 'application/http; msgtype=response',\n 'HTTP-Response-Metadata': {'Response-Message': {'Status': '200',\n   'Version': 'HTTP/1.1',\n   'Reason': 'OK'},\n  'Headers-Length': '410',\n  'Headers': {'Date': 'Mon, 25 May 2020 05:11:44 GMT',\n   'Content-Type': 'text/html',\n   'X-Crawler-Content-Length': '6641',\n   'Content-Length': '13911',\n   'Connection': 'keep-alive',\n   'Set-Cookie': 'tgw_l7_route=f60eebbcd438146c92bb28cfca9251e6; Expires=Mon, 25-May-2020 06:11:44 GMT; Path=/',\n   'Server': 'Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips PHP/5.4.16',\n   'X-Powered-By': 'PHP/5.4.16',\n   'Vary': 'Accept-Encoding',\n   'X-Crawler-Content-Encoding': 'gzip'},\n  'HTML-Metadata': {'Head': {'Title': '纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)',\n    'Metas': [{'name': 'mobile-agent',\n      'content': 'format=html5; url=detail_m.php?id=866619'},\n     {'name': 'mobile-agent',\n      'content': 'format=xhtml; url=detail_m.php?id=866619'},\n     {'name': 'keywords',\n      'content': '纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397'},\n     {'name': 'description',\n      'content': '梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证'}],\n    'Link': [{'path': 'LINK@/href',\n      'url': 'http://txt.inv.org.cn/ir/site/pc/css.css',\n      'rel': 'stylesheet',\n      'type': 'text/css'}],\n    'Scripts': [{'path': 'SCRIPT@/src',\n      'url': 'http://static.bshare.cn/b/buttonLite.js#style=-1&uuid=&pophcol=2&lang=zh',\n      'type': 'text/javascript'},\n     {'path': 'SCRIPT@/src',\n      'url': 'http://static.bshare.cn/b/bshareC0.js',\n      'type': 'text/javascript'},\n     {'path': 'SCRIPT@/src',\n      'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'},\n     {'path': 'SCRIPT@/src',\n      'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'}]},\n   'Links': [{'path': 'A@/href',\n     'url': '/',\n     'target': '_blank',\n     'text': '梦洁股份(002397)'},\n    {'path': 'A@/href',\n     'url': '/index_m.php',\n     'target': '_blank',\n     'text': '移动版'},\n    {'path': 'IMG@/src',\n     'url': 'http://img.inv.org.cn/broker/huasheng_pc.jpg'},\n    {'path': 'A@/href',\n     'url': 'https://hd.hstong.com/marketing/2019/0228?_scnl=OTg0NWJibzY0MTI3',\n     'target': '_blank'},\n    {'path': 'A@/href', 'url': '/', 'text': '首页'},\n    {'path': 'A@/href', 'url': '/quote/', 'text': '股票行情'},\n    {'path': 'A@/href', 'url': '/media_news/', 'text': '媒体报道'},\n    {'path': 'A@/href', 'url': '/related_news/', 'text': '相关新闻'},\n    {'path': 'A@/href', 'url': '/notice/', 'text': '公司公告'},\n    {'path': 'A@/href', 'url': '/report/', 'text': '研究报告'},\n    {'path': 'A@/href', 'url': '/related_report/', 'text': '相关研报'},\n    {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '梦洁股份'},\n    {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '002397'},\n    {'path': 'A@/href',\n     'url': 'http://www.bShare.cn/',\n     'title': '分享到',\n     'text': '分享到'},\n    {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/ad/zixun_pc.jpg'},\n    {'path': 'A@/href',\n     'url': 'http://stock.inv.org.cn',\n     'target': '_blank',\n     'text': '股票投资之家'}]},\n  'Entity-Length': '13911',\n  'Entity-Digest': 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH',\n  'Entity-Trailing-Slop-Length': '0'},\n 'Actual-Content-Length': '14321',\n 'Block-Digest': 'sha1:CNOLET4OGLWYCKDJUDAAVYF5YS3MCW4S',\n 'Trailing-Slop-Length': '4'}\n\n\n\ndata['Envelope']['Payload-Metadata']['HTTP-Response-Metadata']\n\n{'Response-Message': {'Status': '200', 'Version': 'HTTP/1.1', 'Reason': 'OK'},\n 'Headers-Length': '410',\n 'Headers': {'Date': 'Mon, 25 May 2020 05:11:44 GMT',\n  'Content-Type': 'text/html',\n  'X-Crawler-Content-Length': '6641',\n  'Content-Length': '13911',\n  'Connection': 'keep-alive',\n  'Set-Cookie': 'tgw_l7_route=f60eebbcd438146c92bb28cfca9251e6; Expires=Mon, 25-May-2020 06:11:44 GMT; Path=/',\n  'Server': 'Apache/2.4.23 (Unix) OpenSSL/1.0.1e-fips PHP/5.4.16',\n  'X-Powered-By': 'PHP/5.4.16',\n  'Vary': 'Accept-Encoding',\n  'X-Crawler-Content-Encoding': 'gzip'},\n 'HTML-Metadata': {'Head': {'Title': '纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)',\n   'Metas': [{'name': 'mobile-agent',\n     'content': 'format=html5; url=detail_m.php?id=866619'},\n    {'name': 'mobile-agent',\n     'content': 'format=xhtml; url=detail_m.php?id=866619'},\n    {'name': 'keywords',\n     'content': '纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397'},\n    {'name': 'description',\n     'content': '梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证'}],\n   'Link': [{'path': 'LINK@/href',\n     'url': 'http://txt.inv.org.cn/ir/site/pc/css.css',\n     'rel': 'stylesheet',\n     'type': 'text/css'}],\n   'Scripts': [{'path': 'SCRIPT@/src',\n     'url': 'http://static.bshare.cn/b/buttonLite.js#style=-1&uuid=&pophcol=2&lang=zh',\n     'type': 'text/javascript'},\n    {'path': 'SCRIPT@/src',\n     'url': 'http://static.bshare.cn/b/bshareC0.js',\n     'type': 'text/javascript'},\n    {'path': 'SCRIPT@/src',\n     'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'},\n    {'path': 'SCRIPT@/src',\n     'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'}]},\n  'Links': [{'path': 'A@/href',\n    'url': '/',\n    'target': '_blank',\n    'text': '梦洁股份(002397)'},\n   {'path': 'A@/href',\n    'url': '/index_m.php',\n    'target': '_blank',\n    'text': '移动版'},\n   {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/broker/huasheng_pc.jpg'},\n   {'path': 'A@/href',\n    'url': 'https://hd.hstong.com/marketing/2019/0228?_scnl=OTg0NWJibzY0MTI3',\n    'target': '_blank'},\n   {'path': 'A@/href', 'url': '/', 'text': '首页'},\n   {'path': 'A@/href', 'url': '/quote/', 'text': '股票行情'},\n   {'path': 'A@/href', 'url': '/media_news/', 'text': '媒体报道'},\n   {'path': 'A@/href', 'url': '/related_news/', 'text': '相关新闻'},\n   {'path': 'A@/href', 'url': '/notice/', 'text': '公司公告'},\n   {'path': 'A@/href', 'url': '/report/', 'text': '研究报告'},\n   {'path': 'A@/href', 'url': '/related_report/', 'text': '相关研报'},\n   {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '梦洁股份'},\n   {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '002397'},\n   {'path': 'A@/href',\n    'url': 'http://www.bShare.cn/',\n    'title': '分享到',\n    'text': '分享到'},\n   {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/ad/zixun_pc.jpg'},\n   {'path': 'A@/href',\n    'url': 'http://stock.inv.org.cn',\n    'target': '_blank',\n    'text': '股票投资之家'}]},\n 'Entity-Length': '13911',\n 'Entity-Digest': 'sha1:RWL3CQY47VCKFOXJVZXBQP64U7RCFODH',\n 'Entity-Trailing-Slop-Length': '0'}\n\n\nContains from the head the title, metas and scripts, as well as links from the text itself.\n\ndata['Envelope']['Payload-Metadata']['HTTP-Response-Metadata']['HTML-Metadata']\n\n{'Head': {'Title': '纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)',\n  'Metas': [{'name': 'mobile-agent',\n    'content': 'format=html5; url=detail_m.php?id=866619'},\n   {'name': 'mobile-agent',\n    'content': 'format=xhtml; url=detail_m.php?id=866619'},\n   {'name': 'keywords',\n    'content': '纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397'},\n   {'name': 'description',\n    'content': '梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证'}],\n  'Link': [{'path': 'LINK@/href',\n    'url': 'http://txt.inv.org.cn/ir/site/pc/css.css',\n    'rel': 'stylesheet',\n    'type': 'text/css'}],\n  'Scripts': [{'path': 'SCRIPT@/src',\n    'url': 'http://static.bshare.cn/b/buttonLite.js#style=-1&uuid=&pophcol=2&lang=zh',\n    'type': 'text/javascript'},\n   {'path': 'SCRIPT@/src',\n    'url': 'http://static.bshare.cn/b/bshareC0.js',\n    'type': 'text/javascript'},\n   {'path': 'SCRIPT@/src',\n    'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'},\n   {'path': 'SCRIPT@/src',\n    'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'}]},\n 'Links': [{'path': 'A@/href',\n   'url': '/',\n   'target': '_blank',\n   'text': '梦洁股份(002397)'},\n  {'path': 'A@/href',\n   'url': '/index_m.php',\n   'target': '_blank',\n   'text': '移动版'},\n  {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/broker/huasheng_pc.jpg'},\n  {'path': 'A@/href',\n   'url': 'https://hd.hstong.com/marketing/2019/0228?_scnl=OTg0NWJibzY0MTI3',\n   'target': '_blank'},\n  {'path': 'A@/href', 'url': '/', 'text': '首页'},\n  {'path': 'A@/href', 'url': '/quote/', 'text': '股票行情'},\n  {'path': 'A@/href', 'url': '/media_news/', 'text': '媒体报道'},\n  {'path': 'A@/href', 'url': '/related_news/', 'text': '相关新闻'},\n  {'path': 'A@/href', 'url': '/notice/', 'text': '公司公告'},\n  {'path': 'A@/href', 'url': '/report/', 'text': '研究报告'},\n  {'path': 'A@/href', 'url': '/related_report/', 'text': '相关研报'},\n  {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '梦洁股份'},\n  {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '002397'},\n  {'path': 'A@/href',\n   'url': 'http://www.bShare.cn/',\n   'title': '分享到',\n   'text': '分享到'},\n  {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/ad/zixun_pc.jpg'},\n  {'path': 'A@/href',\n   'url': 'http://stock.inv.org.cn',\n   'target': '_blank',\n   'text': '股票投资之家'}]}\n\n\nThe next record corresponds to the metadata of the request\n\nrecord = next(records)\n\n\nrecord.rec_type\n\n'metadata'\n\n\n\nrecord.rec_headers.headers\n\n[('WARC-Type', 'metadata'),\n ('WARC-Target-URI', 'http://002397.cn/related_report/detail.php?id=866619'),\n ('WARC-Date', '2020-06-07T16:56:25Z'),\n ('WARC-Record-ID', '<urn:uuid:0802c5dc-77da-405c-851a-63b642fa2cef>'),\n ('WARC-Refers-To', '<urn:uuid:ce3946a5-f44b-417c-ab8c-3d32e7db40f7>'),\n ('Content-Type', 'application/json'),\n ('Content-Length', '1243')]\n\n\n\na = record.content_stream().read()\n\nThis envelope contains WARC-Metadata-Metadata, this covers all the actual metadata in the metadata record.\n\ndata = json.loads(a)\ndata\n\n{'Container': {'Filename': 'CC-MAIN-20200525032636-20200525062636-00381.warc.gz',\n  'Compressed': True,\n  'Offset': '8277',\n  'Gzip-Metadata': {'Deflate-Length': '434',\n   'Header-Length': '10',\n   'Footer-Length': '8',\n   'Inflated-CRC': '-1977350947',\n   'Inflated-Length': '603'}},\n 'Envelope': {'Payload-Metadata': {'Actual-Content-Type': 'application/metadata-fields',\n   'WARC-Metadata-Metadata': {'Metadata-Records': [{'Name': 'fetchTimeMs',\n      'Value': '731'},\n     {'Name': 'charset-detected', 'Value': 'UTF-8'},\n     {'Name': 'languages-cld2',\n      'Value': '{\"reliable\":true,\"text-bytes\":8659,\"languages\":[{\"code\":\"zh\",\"code-iso-639-3\":\"zho\",\"text-covered\":0.98,\"score\":2026.0,\"name\":\"Chinese\"}]}'}]},\n   'Actual-Content-Length': '201',\n   'Block-Digest': 'sha1:ZFJEHS5NUU3WCOEYR63VLFIQIYHFSN7I',\n   'Trailing-Slop-Length': '0'},\n  'Format': 'WARC',\n  'WARC-Header-Length': '398',\n  'WARC-Header-Metadata': {'WARC-Type': 'metadata',\n   'WARC-Date': '2020-05-25T05:11:44Z',\n   'WARC-Record-ID': '<urn:uuid:ce3946a5-f44b-417c-ab8c-3d32e7db40f7>',\n   'Content-Length': '201',\n   'Content-Type': 'application/warc-fields',\n   'WARC-Warcinfo-ID': '<urn:uuid:40b0c676-a143-44c9-bde5-ad0e9999cb04>',\n   'WARC-Concurrent-To': '<urn:uuid:10bc1a42-8c88-4369-a04e-7b77ca106e79>',\n   'WARC-Target-URI': 'http://002397.cn/related_report/detail.php?id=866619'}}}\n\n\nAnd so on for the next few requests\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://003364.cn/j78/453618.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'HTTP-Request-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://003364.cn/j78/453618.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'HTTP-Response-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://003364.cn/j78/453618.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'WARC-Metadata-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\nAnd so on\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'HTTP-Request-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'HTTP-Response-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\n\nrecord = next(records)\nrecord.rec_type, record.rec_headers.get_header('WARC-Target-URI')\n\n('metadata', 'http://010yingkelawyer.com/case/2018-09-25/408.html')\n\n\n\ndata = json.loads(record.content_stream().read())\ndata['Envelope']['Payload-Metadata'].keys()\n\ndict_keys(['Actual-Content-Type', 'WARC-Metadata-Metadata', 'Actual-Content-Length', 'Block-Digest', 'Trailing-Slop-Length'])\n\n\n\nr.close()"
  },
  {
    "objectID": "notebooks/Extracting Jobs with Common Crawl.html",
    "href": "notebooks/Extracting Jobs with Common Crawl.html",
    "title": "skeptric",
    "section": "",
    "text": "import re\n\nimport cdx_toolkit\nfrom bs4 import BeautifulSoup\n\nimport json\nimport demjson\n\nfrom IPython.display import HTML\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/Extracting Jobs with Common Crawl.html#indeed",
    "href": "notebooks/Extracting Jobs with Common Crawl.html#indeed",
    "title": "skeptric",
    "section": "Indeed",
    "text": "Indeed\n\nobjs = list(cdx.iter('au.indeed.com/*',\n                     from_ts='202004', to='202005',\n                     limit=50, \n                     filter=['status:200']))\npd.DataFrame(objs)\n\n\n\n\n\n  \n    \n      \n      charset\n      digest\n      filename\n      languages\n      length\n      mime\n      mime-detected\n      offset\n      status\n      timestamp\n      url\n      urlkey\n    \n  \n  \n    \n      0\n      UTF-8\n      SUS5VGDGUMKJAZ7GN6T4LYMTNA2NWCSU\n      crawl-data/CC-MAIN-2020-16/segments/1585371896913.98/warc/CC-MAIN-20200410110538-20200410141038-00441.warc.gz\n      eng\n      87919\n      text/html\n      text/html\n      335167477\n      200\n      20200410140752\n      https://au.indeed.com/$110,000-jobs-in-Dinmore-QLD\n      com,indeed,au)/$110,000-jobs-in-dinmore-qld\n    \n    \n      1\n      UTF-8\n      FXO2T22MGLBZVE4S6DIOANRL5T7NUMPJ\n      crawl-data/CC-MAIN-2020-16/segments/1585371624083.66/warc/CC-MAIN-20200406102322-20200406132822-00295.warc.gz\n      eng\n      80016\n      text/html\n      text/html\n      350051882\n      200\n      20200406122949\n      https://au.indeed.com/$110,000-jobs-in-Quinns-Rocks-WA\n      com,indeed,au)/$110,000-jobs-in-quinns-rocks-wa\n    \n    \n      2\n      UTF-8\n      CYXMCOYDZZ2VI2FBC3QKT4EQB53POQTA\n      crawl-data/CC-MAIN-2020-16/segments/1585371618784.58/warc/CC-MAIN-20200406035448-20200406065948-00352.warc.gz\n      eng\n      87620\n      text/html\n      text/html\n      348171424\n      200\n      20200406060907\n      https://au.indeed.com/$110,600-jobs-in-Rodd-Point-NSW\n      com,indeed,au)/$110,600-jobs-in-rodd-point-nsw\n    \n    \n      3\n      UTF-8\n      D5QLEI7LBPZYG5IGSM7GJ3MEB7RWPA43\n      crawl-data/CC-MAIN-2020-16/segments/1585371807538.83/warc/CC-MAIN-20200408010207-20200408040707-00279.warc.gz\n      eng\n      84586\n      text/html\n      text/html\n      371777629\n      200\n      20200408011304\n      https://au.indeed.com/$110,700-jobs-in-Woolner-NT\n      com,indeed,au)/$110,700-jobs-in-woolner-nt\n    \n    \n      4\n      UTF-8\n      PS46BDU6XKSHV4HO5KOBKUUDIYXHGOYQ\n      crawl-data/CC-MAIN-2020-16/segments/1585371896913.98/warc/CC-MAIN-20200410110538-20200410141038-00012.warc.gz\n      eng\n      86821\n      text/html\n      text/html\n      351049090\n      200\n      20200410135327\n      https://au.indeed.com/$130,000-jobs-in-Flinders-Lane-VIC\n      com,indeed,au)/$130,000-jobs-in-flinders-lane-vic\n    \n    \n      5\n      UTF-8\n      ZH53DGWTQZGXTDV2RDLP6PQACUZLEPIJ\n      crawl-data/CC-MAIN-2020-16/segments/1585370506870.41/warc/CC-MAIN-20200402080824-20200402110824-00495.warc.gz\n      eng\n      84190\n      text/html\n      text/html\n      337139560\n      200\n      20200402095317\n      https://au.indeed.com/$132,700-jobs-in-Warwick-WA\n      com,indeed,au)/$132,700-jobs-in-warwick-wa\n    \n    \n      6\n      UTF-8\n      BTKJNOLRSE2CREODQPZK4JLD6UBLDPZQ\n      crawl-data/CC-MAIN-2020-16/segments/1585371806302.78/warc/CC-MAIN-20200407214925-20200408005425-00356.warc.gz\n      eng\n      79981\n      text/html\n      text/html\n      355360319\n      200\n      20200408002615\n      https://au.indeed.com/$140,000-jobs-in-Quinns-Rocks-WA\n      com,indeed,au)/$140,000-jobs-in-quinns-rocks-wa\n    \n    \n      7\n      UTF-8\n      EGN7N53REECODNQBS6QZG26AYQNVKB7U\n      crawl-data/CC-MAIN-2020-16/segments/1585370520039.50/warc/CC-MAIN-20200404042338-20200404072338-00073.warc.gz\n      eng\n      86710\n      text/html\n      text/html\n      364638210\n      200\n      20200404055103\n      https://au.indeed.com/$140,000-jobs-in-Rodd-Point-NSW\n      com,indeed,au)/$140,000-jobs-in-rodd-point-nsw\n    \n    \n      8\n      UTF-8\n      YQB5EKFD72C2K47S5UB7ANL7ATCUTHN3\n      crawl-data/CC-MAIN-2020-16/segments/1585371805747.72/warc/CC-MAIN-20200407183818-20200407214318-00495.warc.gz\n      eng\n      83900\n      text/html\n      text/html\n      347012528\n      200\n      20200407211904\n      https://au.indeed.com/$301,000-jobs-in-Peak-Crossing-QLD\n      com,indeed,au)/$301,000-jobs-in-peak-crossing-qld\n    \n    \n      9\n      UTF-8\n      3MSK4SGITTW75C5QANUX2LGS4ZCKBL23\n      crawl-data/CC-MAIN-2020-16/segments/1585371824409.86/warc/CC-MAIN-20200408202012-20200408232512-00433.warc.gz\n      eng\n      85660\n      text/html\n      text/html\n      337399156\n      200\n      20200408221826\n      https://au.indeed.com/$50,000-jobs-in-Bribie-Island-QLD\n      com,indeed,au)/$50,000-jobs-in-bribie-island-qld\n    \n    \n      10\n      UTF-8\n      3B6ZUUSXQKQHAKQU2VG3EIBIBW54WKQM\n      crawl-data/CC-MAIN-2020-16/segments/1585371821680.80/warc/CC-MAIN-20200408170717-20200408201217-00379.warc.gz\n      eng\n      80779\n      text/html\n      text/html\n      336543593\n      200\n      20200408190736\n      https://au.indeed.com/$60,700-jobs-in-Maitland-WA\n      com,indeed,au)/$60,700-jobs-in-maitland-wa\n    \n    \n      11\n      UTF-8\n      EEEC27FKAMLNKWMQD4SQ4IC2FGV4FDI6\n      crawl-data/CC-MAIN-2020-16/segments/1585370524604.46/warc/CC-MAIN-20200404165658-20200404195658-00109.warc.gz\n      eng\n      86323\n      text/html\n      text/html\n      353871155\n      200\n      20200404194532\n      https://au.indeed.com/$61,200-jobs-in-St-Georges-SA\n      com,indeed,au)/$61,200-jobs-in-st-georges-sa\n    \n    \n      12\n      UTF-8\n      XGX2X6JZ6HUINJI7UTE45ZE662OEG6TK\n      crawl-data/CC-MAIN-2020-16/segments/1585371807538.83/warc/CC-MAIN-20200408010207-20200408040707-00399.warc.gz\n      eng\n      81487\n      text/html\n      text/html\n      344326173\n      200\n      20200408024726\n      https://au.indeed.com/$70,000-jobs-in-Bribie-Island-QLD\n      com,indeed,au)/$70,000-jobs-in-bribie-island-qld\n    \n    \n      13\n      UTF-8\n      IXNGTNRWIOWBHXQQJJNFSUDX6QWFTWAV\n      crawl-data/CC-MAIN-2020-16/segments/1585371861991.79/warc/CC-MAIN-20200409154025-20200409184525-00052.warc.gz\n      eng\n      87495\n      text/html\n      text/html\n      345689513\n      200\n      20200409162539\n      https://au.indeed.com/$70,000-jobs-in-Churchill-QLD\n      com,indeed,au)/$70,000-jobs-in-churchill-qld\n    \n    \n      14\n      UTF-8\n      53WHVMCMR6K2L3TUXJHDHDUU6OIVNOQZ\n      crawl-data/CC-MAIN-2020-16/segments/1585371858664.82/warc/CC-MAIN-20200409122719-20200409153219-00123.warc.gz\n      eng\n      85231\n      text/html\n      text/html\n      351197696\n      200\n      20200409152603\n      https://au.indeed.com/$70,000-jobs-in-Clarence-Gardens-SA\n      com,indeed,au)/$70,000-jobs-in-clarence-gardens-sa\n    \n    \n      15\n      UTF-8\n      YNFHUQZCCFDQH4OV2ETYNFGIZK433XQZ\n      crawl-data/CC-MAIN-2020-16/segments/1585371618784.58/warc/CC-MAIN-20200406035448-20200406065948-00446.warc.gz\n      eng\n      84560\n      text/html\n      text/html\n      350869513\n      200\n      20200406061558\n      https://au.indeed.com/$70,000-jobs-in-Quinns-Rocks-WA\n      com,indeed,au)/$70,000-jobs-in-quinns-rocks-wa\n    \n    \n      16\n      UTF-8\n      PVHCQSZKD5MGBLYVIW3QO2M4VM3YWN4F\n      crawl-data/CC-MAIN-2020-16/segments/1585370521574.59/warc/CC-MAIN-20200404073139-20200404103139-00098.warc.gz\n      eng\n      87537\n      text/html\n      text/html\n      357067905\n      200\n      20200404092121\n      https://au.indeed.com/$70,000-jobs-in-Robertson-QLD\n      com,indeed,au)/$70,000-jobs-in-robertson-qld\n    \n    \n      17\n      UTF-8\n      EKGPB2VWFWPECMFAXYZLFWOGBEXK3N3P\n      crawl-data/CC-MAIN-2020-16/segments/1585371606067.71/warc/CC-MAIN-20200405150416-20200405180916-00460.warc.gz\n      eng\n      86928\n      text/html\n      text/html\n      368029548\n      200\n      20200405173423\n      https://au.indeed.com/$83,000-jobs-in-Canley-Heights-NSW\n      com,indeed,au)/$83,000-jobs-in-canley-heights-nsw\n    \n    \n      18\n      UTF-8\n      HFSWDG7TAM73XTUVCLDZAITGAM7ZR2HU\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00493.warc.gz\n      eng\n      79014\n      text/html\n      text/html\n      360212774\n      200\n      20200401115138\n      https://au.indeed.com/$90,000-jobs-in-Bribie-Island-QLD\n      com,indeed,au)/$90,000-jobs-in-bribie-island-qld\n    \n    \n      19\n      UTF-8\n      JISF54RGKFN2I7AXVVLUEGV2XUHSR52L\n      crawl-data/CC-MAIN-2020-16/segments/1585370506870.41/warc/CC-MAIN-20200402080824-20200402110824-00530.warc.gz\n      eng\n      86201\n      text/html\n      text/html\n      348714754\n      200\n      20200402082540\n      https://au.indeed.com/$90,000-jobs-in-Churchill-QLD\n      com,indeed,au)/$90,000-jobs-in-churchill-qld\n    \n    \n      20\n      UTF-8\n      K6QTF7L6B26U4YXU6CWIJOH5NXJU7PSD\n      crawl-data/CC-MAIN-2020-16/segments/1585370520039.50/warc/CC-MAIN-20200404042338-20200404072338-00233.warc.gz\n      eng\n      85533\n      text/html\n      text/html\n      347240989\n      200\n      20200404061242\n      https://au.indeed.com/$90,000-jobs-in-Clarence-Gardens-SA\n      com,indeed,au)/$90,000-jobs-in-clarence-gardens-sa\n    \n    \n      21\n      UTF-8\n      XUVPFFMQZ3B4P2CQOU7HXMNDZW6AHXAW\n      crawl-data/CC-MAIN-2020-16/segments/1585370518767.60/warc/CC-MAIN-20200403220847-20200404010847-00526.warc.gz\n      eng\n      85956\n      text/html\n      text/html\n      358137219\n      200\n      20200404001354\n      https://au.indeed.com/$90,000-jobs-in-Dinmore-QLD\n      com,indeed,au)/$90,000-jobs-in-dinmore-qld\n    \n    \n      22\n      UTF-8\n      6BI666K7YZPUFSCVQH5C3FCZA2WGRPHH\n      crawl-data/CC-MAIN-2020-16/segments/1585371858664.82/warc/CC-MAIN-20200409122719-20200409153219-00186.warc.gz\n      eng\n      87744\n      text/html\n      text/html\n      330499220\n      200\n      20200409150221\n      https://au.indeed.com/$90,000-jobs-in-Kareela-NSW\n      com,indeed,au)/$90,000-jobs-in-kareela-nsw\n    \n    \n      23\n      UTF-8\n      FL36UXRASQTNGLWM7543JQ7L5OIKANBW\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00200.warc.gz\n      eng\n      82935\n      text/html\n      text/html\n      350703294\n      200\n      20200406021312\n      https://au.indeed.com/$92,600-jobs-in-Peak-Crossing-QLD\n      com,indeed,au)/$92,600-jobs-in-peak-crossing-qld\n    \n    \n      24\n      UTF-8\n      7RGKKP7UBQZDMJP2LXEUF5JS35PJ6MWM\n      crawl-data/CC-MAIN-2020-16/segments/1585371805747.72/warc/CC-MAIN-20200407183818-20200407214318-00482.warc.gz\n      eng\n      88979\n      text/html\n      text/html\n      352936509\n      200\n      20200407210734\n      https://au.indeed.com/15-Year-Old,-Part-Time,-Cash-Register,-Retail-jobs-in-New-South-Wales\n      com,indeed,au)/15-year-old,-part-time,-cash-register,-retail-jobs-in-new-south-wales\n    \n    \n      25\n      UTF-8\n      IB6JAXPVX5DHJFQE6STL3RYZBNI5Y767\n      crawl-data/CC-MAIN-2020-16/segments/1585371618784.58/warc/CC-MAIN-20200406035448-20200406065948-00540.warc.gz\n      eng\n      75858\n      text/html\n      text/html\n      360597776\n      200\n      20200406060848\n      https://au.indeed.com/1800-My-Catering-jobs\n      com,indeed,au)/1800-my-catering-jobs\n    \n    \n      26\n      UTF-8\n      5DK7DP6NGRIEYN7UZENDDRDNHGM4IFWR\n      crawl-data/CC-MAIN-2020-16/segments/1585371805747.72/warc/CC-MAIN-20200407183818-20200407214318-00019.warc.gz\n      eng\n      79734\n      text/html\n      text/html\n      341547567\n      200\n      20200407194506\n      https://au.indeed.com/2-Fat-Indians-jobs\n      com,indeed,au)/2-fat-indians-jobs\n    \n    \n      27\n      UTF-8\n      OU27TVQSMLD2EDZTII5VNU2EP2P3L4GG\n      crawl-data/CC-MAIN-2020-16/segments/1585370521876.48/warc/CC-MAIN-20200404103932-20200404133932-00293.warc.gz\n      eng\n      76995\n      text/html\n      text/html\n      357508875\n      200\n      20200404125602\n      https://au.indeed.com/2discover-jobs\n      com,indeed,au)/2discover-jobs\n    \n    \n      28\n      UTF-8\n      2KEYL5RMSTK77JERPCEVGZ3CDOIEA4GT\n      crawl-data/CC-MAIN-2020-16/segments/1585370510846.12/warc/CC-MAIN-20200403092656-20200403122656-00378.warc.gz\n      eng\n      80834\n      text/html\n      text/html\n      335385226\n      200\n      20200403110521\n      https://au.indeed.com/3d-Animation-$100,000-jobs\n      com,indeed,au)/3d-animation-$100,000-jobs\n    \n    \n      29\n      UTF-8\n      IIYNVW276H3NHNCQ3IINVJQWAP4WVFDJ\n      crawl-data/CC-MAIN-2020-16/segments/1585371876625.96/warc/CC-MAIN-20200409185507-20200409220007-00498.warc.gz\n      eng\n      82519\n      text/html\n      text/html\n      373457422\n      200\n      20200409214934\n      https://au.indeed.com/3d-Animation-jobs-in-Sydney-NSW\n      com,indeed,au)/3d-animation-jobs-in-sydney-nsw\n    \n    \n      30\n      UTF-8\n      CURUYVE2IEK4KYHIIMZNE6QWP57W7KCY\n      crawl-data/CC-MAIN-2020-16/segments/1585371805747.72/warc/CC-MAIN-20200407183818-20200407214318-00253.warc.gz\n      eng\n      85117\n      text/html\n      text/html\n      361947519\n      200\n      20200407213126\n      https://au.indeed.com/3d-Artist-jobs\n      com,indeed,au)/3d-artist-jobs\n    \n    \n      31\n      UTF-8\n      DTCEUA2KUQLYMZPHXJVMFMAPB2WMOCV5\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00236.warc.gz\n      eng\n      84586\n      text/html\n      text/html\n      370123020\n      200\n      20200406031453\n      https://au.indeed.com/3d-jobs-in-Brisbane-QLD\n      com,indeed,au)/3d-jobs-in-brisbane-qld\n    \n    \n      32\n      UTF-8\n      A6KLS2SSVYXIOS75TXUU7KFCUVLQTZ26\n      crawl-data/CC-MAIN-2020-16/segments/1585370519111.47/warc/CC-MAIN-20200404011558-20200404041558-00134.warc.gz\n      eng\n      46690\n      text/html\n      text/html\n      346002220\n      200\n      20200404022316\n      https://au.indeed.com/?s_rid=theage%3Alhsnav%3Ajobs\n      com,indeed,au)/?s_rid=theage:lhsnav:jobs\n    \n    \n      33\n      UTF-8\n      3DCW3YUA2PNRPHLXFAWN5QWHQLFONWXX\n      crawl-data/CC-MAIN-2020-16/segments/1585370524604.46/warc/CC-MAIN-20200404165658-20200404195658-00155.warc.gz\n      eng\n      85623\n      text/html\n      text/html\n      355535353\n      200\n      20200404195008\n      https://au.indeed.com/A-Commercial-$60,000-jobs-in-Gold-Coast-QLD\n      com,indeed,au)/a-commercial-$60,000-jobs-in-gold-coast-qld\n    \n    \n      34\n      UTF-8\n      Y7X24ZG46NP2F6N5EGDH7EBAHKQKNN6K\n      crawl-data/CC-MAIN-2020-16/segments/1585370505359.23/warc/CC-MAIN-20200401003422-20200401033422-00213.warc.gz\n      eng\n      59755\n      text/html\n      text/html\n      348260244\n      200\n      20200401015649\n      https://au.indeed.com/A-Cut-Above-Family-Butcher-jobs\n      com,indeed,au)/a-cut-above-family-butcher-jobs\n    \n    \n      35\n      UTF-8\n      LYJUAQUGZZZ7O443OG4PJMH4VNYNO6HI\n      crawl-data/CC-MAIN-2020-16/segments/1585371893683.94/warc/CC-MAIN-20200410075105-20200410105605-00507.warc.gz\n      eng\n      60094\n      text/html\n      text/html\n      344555492\n      200\n      20200410103305\n      https://au.indeed.com/A-Mop-Above-the-Rest-jobs\n      com,indeed,au)/a-mop-above-the-rest-jobs\n    \n    \n      36\n      UTF-8\n      TTSHM5OX56URJ322RZG5RHTNDUQUKQQE\n      crawl-data/CC-MAIN-2020-16/segments/1585370506580.20/warc/CC-MAIN-20200402014600-20200402044600-00240.warc.gz\n      eng\n      76911\n      text/html\n      text/html\n      282423245\n      200\n      20200402035454\n      https://au.indeed.com/Abbott-Point-Coal-Terminal-jobs\n      com,indeed,au)/abbott-point-coal-terminal-jobs\n    \n    \n      37\n      UTF-8\n      L6QJRDSKKHRTMOMCDAEHMIGN3FVWZNAU\n      crawl-data/CC-MAIN-2020-16/segments/1585370505359.23/warc/CC-MAIN-20200401003422-20200401033422-00559.warc.gz\n      eng\n      78926\n      text/html\n      text/html\n      350455254\n      200\n      20200401025215\n      https://au.indeed.com/Aberglasslyn-Medical-Centre-jobs\n      com,indeed,au)/aberglasslyn-medical-centre-jobs\n    \n    \n      38\n      UTF-8\n      JP27BHGJENGK7JM3MNHXATFKANVROS2M\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00111.warc.gz\n      eng\n      82902\n      text/html\n      text/html\n      338150223\n      200\n      20200406024217\n      https://au.indeed.com/Aboriginal-Identified-$82,500-jobs-in-Queensland\n      com,indeed,au)/aboriginal-identified-$82,500-jobs-in-queensland\n    \n    \n      39\n      UTF-8\n      YMEETCQQ4XB77ECKGHAX3JH32OIKDMJM\n      crawl-data/CC-MAIN-2020-16/segments/1585371893683.94/warc/CC-MAIN-20200410075105-20200410105605-00129.warc.gz\n      eng\n      77929\n      text/html\n      text/html\n      351313651\n      200\n      20200410104615\n      https://au.indeed.com/Aboriginal-Liaison-Officer-jobs-in-Eagleby-QLD\n      com,indeed,au)/aboriginal-liaison-officer-jobs-in-eagleby-qld\n    \n    \n      40\n      UTF-8\n      6JD3HSDEVBO6SAAMPQ5CPVUFYJ54HRFL\n      crawl-data/CC-MAIN-2020-16/segments/1585370505359.23/warc/CC-MAIN-20200401003422-20200401033422-00024.warc.gz\n      eng\n      86013\n      text/html\n      text/html\n      367050552\n      200\n      20200401025847\n      https://au.indeed.com/Access-Corporate-Group-jobs\n      com,indeed,au)/access-corporate-group-jobs\n    \n    \n      41\n      UTF-8\n      A7EQF3RELM2JOFI2MTR7HA5GPUXWSFYM\n      crawl-data/CC-MAIN-2020-16/segments/1585371896913.98/warc/CC-MAIN-20200410110538-20200410141038-00195.warc.gz\n      eng\n      61341\n      text/html\n      text/html\n      337157476\n      200\n      20200410140805\n      https://au.indeed.com/Accessory-Jewellery-$117,500-jobs-in-New-South-Wales\n      com,indeed,au)/accessory-jewellery-$117,500-jobs-in-new-south-wales\n    \n    \n      42\n      UTF-8\n      KCG6Y2RRSKBL222OALB66F27GPIZL7OC\n      crawl-data/CC-MAIN-2020-16/segments/1585370521574.59/warc/CC-MAIN-20200404073139-20200404103139-00453.warc.gz\n      eng\n      77285\n      text/html\n      text/html\n      362548342\n      200\n      20200404100456\n      https://au.indeed.com/Accessory-Jewellery-$60,000-jobs-in-New-South-Wales\n      com,indeed,au)/accessory-jewellery-$60,000-jobs-in-new-south-wales\n    \n    \n      43\n      UTF-8\n      F2LTENG2BWNYOAPNGAT4MTMLUCS4KNCQ\n      crawl-data/CC-MAIN-2020-16/segments/1585370507738.45/warc/CC-MAIN-20200402173940-20200402203940-00479.warc.gz\n      eng\n      82949\n      text/html\n      text/html\n      361006163\n      200\n      20200402191948\n      https://au.indeed.com/Accommodation-jobs-in-Pilbara-WA\n      com,indeed,au)/accommodation-jobs-in-pilbara-wa\n    \n    \n      44\n      UTF-8\n      LMSS3CAO3GWO7DGUYQLJVP6MET7B4FCM\n      crawl-data/CC-MAIN-2020-16/segments/1585371606067.71/warc/CC-MAIN-20200405150416-20200405180916-00284.warc.gz\n      eng\n      79581\n      text/html\n      text/html\n      353990995\n      200\n      20200405180144\n      https://au.indeed.com/Accommodation-Support-Worker-$147,900-jobs-in-Queensland\n      com,indeed,au)/accommodation-support-worker-$147,900-jobs-in-queensland\n    \n    \n      45\n      UTF-8\n      4Q2LWFAID3INQM4GE3MVZYKL7NHETXIP\n      crawl-data/CC-MAIN-2020-16/segments/1585370519111.47/warc/CC-MAIN-20200404011558-20200404041558-00147.warc.gz\n      eng\n      85577\n      text/html\n      text/html\n      365859791\n      200\n      20200404033728\n      https://au.indeed.com/Accommodation-Support-Worker-$72,500-jobs-in-Queensland\n      com,indeed,au)/accommodation-support-worker-$72,500-jobs-in-queensland\n    \n    \n      46\n      UTF-8\n      FGAWLKGI3HB2JTNYPHQY7QOPD6OEQUPU\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00446.warc.gz\n      eng\n      60068\n      text/html\n      text/html\n      347527770\n      200\n      20200408100520\n      https://au.indeed.com/Accor-Hotels-jobs\n      com,indeed,au)/accor-hotels-jobs\n    \n    \n      47\n      UTF-8\n      L7FHP4OFDVWMAVI4QBWWPVECWGHYKV2D\n      crawl-data/CC-MAIN-2020-16/segments/1585371876625.96/warc/CC-MAIN-20200409185507-20200409220007-00150.warc.gz\n      eng\n      86467\n      text/html\n      text/html\n      355292446\n      200\n      20200409214336\n      https://au.indeed.com/Account-Manager-Advertising-jobs-in-Victoria\n      com,indeed,au)/account-manager-advertising-jobs-in-victoria\n    \n    \n      48\n      UTF-8\n      L6SA645B7EEPWBVLN72DOT5DN7JCMPFL\n      crawl-data/CC-MAIN-2020-16/segments/1585370504930.16/warc/CC-MAIN-20200331212647-20200401002647-00231.warc.gz\n      eng\n      84326\n      text/html\n      text/html\n      372719076\n      200\n      20200401000139\n      https://au.indeed.com/Account-Myob-jobs\n      com,indeed,au)/account-myob-jobs\n    \n    \n      49\n      UTF-8\n      LCRBCC6VI7JRL5VDT4UVV2EHO3YUYBCB\n      crawl-data/CC-MAIN-2020-16/segments/1585370510846.12/warc/CC-MAIN-20200403092656-20200403122656-00015.warc.gz\n      eng\n      86002\n      text/html\n      text/html\n      349017854\n      200\n      20200403111452\n      https://au.indeed.com/Account-Payable-Bank-jobs\n      com,indeed,au)/account-payable-bank-jobs\n    \n  \n\n\n\n\nIndeed Contains:\n\nTitle\nCompany\nLocation\nFirst 25 words of ad text\nSometimes salary\n\n\nwith open('test.html', 'w') as f:\n    f.write(objs[0].content.decode('utf-8'))\n\nNone\n\n\n\nsoup = BeautifulSoup(objs[0].content)\n\n\nurls = [a['href'] for a in soup.select('a.jobtitle')]\nurls\n\n['/rc/clk?jk=028bf2018beebedb&fccid=59b04a51f1164f7e&vjs=3',\n '/rc/clk?jk=d9ea2b72aae1bd1f&fccid=92dfe858c4b585f9&vjs=3',\n '/rc/clk?jk=34add443e5138142&fccid=a4a93a5cf946e3ad&vjs=3',\n '/rc/clk?jk=2d59eb05949f2081&fccid=edae4285faf6c2f0&vjs=3',\n '/rc/clk?jk=be8468dce830f059&fccid=ea26a03c73e2d4e9&vjs=3',\n '/rc/clk?jk=bf0dced454efd688&fccid=a7eb6e72c143133c&vjs=3',\n '/rc/clk?jk=e26927c40a677590&fccid=b155cdcdabd4ee03&vjs=3',\n '/rc/clk?jk=d5703b74c268f0b6&fccid=9eb3b6eca8bf5aac&vjs=3',\n '/rc/clk?jk=573786223f902b3b&fccid=ea26a03c73e2d4e9&vjs=3',\n '/rc/clk?jk=060a5ceb47fda90c&fccid=6e557affe98df478&vjs=3']\n\n\n\nbase_url = objs[0].data['url']\nbase_url = base_url[:base_url.find('/', 8)]\n\n\nparam = re.match('/rc/clk\\?(jk=[^&]+)', urls[0]).group(1)\nparam\n\n'jk=028bf2018beebedb'\n\n\n\nimport requests\n\nRetrieve the job by some url manipulation\n\nurl = f'{base_url}/viewjob?{param}'\n\n\nr = requests.get(url)\n\n\nr.status_code\n\n200\n\n\n\nwith open('test.html', 'wb') as f:\n    f.write(r.content)\n\nMetadata is a bit tricky to get\n\nstart = 'window._initialData='\n\n\nstart_idx = r.text.find(start) + len(start)\n\n\ndef get_object(text):\n    depth = 0\n    inquote = False\n    escape = False\n    for idx, char in enumerate(text):\n        if escape:\n            escape = False\n            continue\n        if char == '\"':\n            inquote = not inquote\n        if  char == '\\\\':\n            escape = True\n        if (not inquote) and char == '{':\n            depth += 1\n        if (not inquote) and char == '}':\n            depth -= 1\n            if depth <= 0:\n                break\n    return text[:idx+1]\n\n\nobj_text = get_object(r.text[start_idx:])\n\n\nobj_text\n\n'{\"base64EncodedJson\":\"eyJhIjp0cnVlLCJjIjp0cnVlLCJkIjpmYWxzZSwiZSI6dHJ1ZSwiZyI6Imh0dHA6Ly9hdS5pbmRlZWQuY29tL20vYmFzZWNhbXAvdmlld2pvYj9qaz0wMjhiZjIwMThiZWViZWRiIiwiaCI6IlZhcmlvdXMgUHJvamVjdCBDb250cm9scyAmIFBsYW5uaW5nIFBvc2l0aW9ucyIsImkiOiJCcmlzYmFuZSBRTEQiLCJqIjoib3JnYW5pYyIsImwiOiIifQ\",\"baseInboxUrl\":\"https:\\\\u002F\\\\u002Finbox.indeed.com\",\"baseUrl\":\"https:\\\\u002F\\\\u002Fau.indeed.com\",\"clientsideProctorGroups\":{\"mobcompanylinktst\":true,\"mobvj_hideapplyemail_tst\":false,\"mobvjpsfeedbacktst\":false,\"jasx_track_multisession_noapplies\":false,\"sal_insights_tab_redesign_tst\":false,\"jasx_hidephonenumber_tst\":false},\"companyFollowForm\":{\"addAlertUrl\":\"\\\\u002Falert?a=add&alert_params=followCompany%3Dadfbfa8ae907519e&q=company%3A%27SNC-Lavalin%27&alert_period=weekly&output=json&verified=0&tk=1eaook86k36bm000&hct=4c021f433b8dd134463a497cf3645afa\",\"cancelText\":\"By creating a company alert you agree to our <a href=\\\\\"\\\\u002Flegal\\\\\" target=\\\\\"_blank\\\\\">Terms<\\\\u002Fa>. You can change your consent settings at any time by unsubscribing, or as detailed in our terms.\",\"checkAlertUrl\":\"\\\\u002Frpc\\\\u002Fjobalert?a=check&app=acme&q=company%3A%27SNC-Lavalin%27&followCompany=adfbfa8ae907519e\",\"confirmationHeader\":\"Please check your email\",\"confirmationSubHeader\":\"we have sent a confirmation message\",\"confirmationText\":\"Click on the link in this email to start receiving your Company Alert.\",\"createAlertUrl\":\"\\\\u002Fmy\\\\u002Falerts?a=create&alert_params=followCompany%3Dadfbfa8ae907519e&alert_keywords=company%3A%27SNC-Lavalin%27&alert_period=weekly&output=json&followCompany=adfbfa8ae907519e&hct=4c021f433b8dd134463a497cf3645afa\",\"cta\":\"Get job updates from SNC-Lavalin\",\"duplicateEmailMessage\":\"You are already following this company.\",\"followButton\":{\"buttonSize\":\"sm\",\"buttonType\":\"tertiary\",\"children\":\"Follow\",\"disabled\":false,\"isBlock\":false,\"isResponsive\":false,\"size\":\"sm\"},\"followingText\":\"Following\",\"input\":{\"disabled\":false,\"errorText\":\"This field is required\",\"helpText\":null,\"id\":null,\"isSmall\":false,\"label\":\"My Email:\",\"name\":\"email\",\"type\":\"text\",\"value\":null},\"invalidEmailMessage\":\"Please provide a valid email address.\",\"saveButton\":{\"buttonSize\":null,\"buttonType\":\"secondary\",\"children\":\"Save\",\"disabled\":false,\"isBlock\":true,\"isResponsive\":false,\"size\":\"sm\"}},\"country\":\"AU\",\"ctk\":\"1eaook85m10a3000\",\"dcmModel\":{\"category\":\"jobse0\",\"source\":\"8232301\",\"type\":\"organic\"},\"desktop\":true,\"desktopSponsoredJobSeenData\":\"tk=1eaook86k36bm000\",\"dgToken\":\"B1C91F9AB5B14CDA827FB6F92A2587D5\",\"googleOneTapModel\":{\"baseSecureUrl\":\"https:\\\\u002F\\\\u002Fsecure.indeed.com\",\"googleClientID\":\"1047839414793-v442kdo3pt0vb43l8nu2c5sh9lf4bsnj.apps.googleusercontent.com\",\"redirectUrl\":null},\"indeedChatEmployerModel\":{\"chatEnabled\":false},\"jobKey\":\"028bf2018beebedb\",\"jobLocation\":\"Brisbane QLD\",\"jobSeenData\":\"tk=1eaook86k36bm000&context=viewjobrecs\",\"jobTitle\":\"Various Project Controls & Planning Positions\",\"language\":\"en\",\"locale\":\"en_AU\",\"localeData\":{\"\":[null,\"Project-Id-Version: \\\\nReport-Msgid-Bugs-To: \\\\nPOT-Creation-Date: 2020-06-11 04:00-0500\\\\nPO-Revision-Date: 2020-04-01 21:41+0000\\\\nLast-Translator: Auto Generated <noreply@indeed.com>\\\\nLanguage-Team: English (Australia) <https:\\\\u002F\\\\u002Fweblate.corp.indeed.com\\\\u002Fprojects\\\\u002Findeed\\\\u002Findeedmobile-i18n-content\\\\u002Fen_AU\\\\u002F>\\\\nLanguage: en_AU\\\\nMIME-Version: 1.0\\\\nContent-Type: text\\\\u002Fplain; charset=UTF-8\\\\nContent-Transfer-Encoding: 8bit\\\\nPlural-Forms: nplurals=2; plural=n != 1;\\\\nX-Generator: Weblate 3.9.1\\\\n\"]},\"mobtk\":\"1eaook86k36bm000\",\"notifications\":{\"inboxLinkEnabled\":false,\"messagesLabel\":\"Messages\",\"newMessagesCountPlurals\":[\"{0} new\",\"{0} new\"],\"notificationCenterEnabled\":false,\"updatingText\":\"checking...\"},\"originalJobLinkModel\":{\"cookieName\":\"RCLK\",\"cookiePath\":\"\\\\u002F\",\"cookieValue\":\"jk=028bf2018beebedb&vjtk=1eaook86k36bm000&ts=1592116519124&rd=&qd=\"},\"pageId\":\"viewjob\",\"relatedLinks\":[{\"href\":\"\\\\u002Fjobs?q=Project+Planner&l=Brisbane+QLD\",\"linkText\":\"Project Planner jobs in Brisbane QLD\"},{\"href\":\"\\\\u002Fjobs?q=SNC-Lavalin&l=Brisbane+QLD\",\"linkText\":\"Jobs at SNC-Lavalin in Brisbane QLD\"},{\"href\":\"\\\\u002Fsalary?q1=Project+Planner&l1=Brisbane+QLD\",\"linkText\":\"Project Planner salaries in Brisbane QLD\"}],\"reportJobForm\":{\"additionalInformationPlaceholder\":\"Additional information\",\"closeIconLabel\":\"Close\",\"disclaimer\":\"All Job Ads are subject to Indeed\\'s <a target=\\\\\"_blank\\\\\" href=\\\\\"\\\\u002Flegal\\\\\">Terms of Service<\\\\u002Fa>. We allow users to flag postings that may be in violation of those terms. Job Ads may also be flagged by Indeed. However, no moderation system is perfect, and flagging a posting does not ensure that it will be removed.\",\"postHref\":\"\\\\u002Fm\\\\u002Frpc\\\\u002Flog\\\\u002Freport\\\\u002Fjob?jobKey=028bf2018beebedb&mobvjtk=1eaook86k36bm000&isMobile=false&indeedcsrftoken=7UWdjDPjNZTLybz4lALPJo4q6PhYIYzh\",\"radioButtonGroup\":{\"errorText\":null,\"helpText\":null,\"isDisabled\":false,\"label\":\"Report this job\",\"name\":null,\"radioButtons\":[{\"id\":null,\"isDisabled\":false,\"label\":\"It is offensive, discriminatory\",\"name\":\"offensive\",\"value\":\"offensive\"},{\"id\":null,\"isDisabled\":false,\"label\":\"It seems like a fake job\",\"name\":\"fake\",\"value\":\"fake\"},{\"id\":null,\"isDisabled\":false,\"label\":\"It is inaccurate\",\"name\":\"inaccurate\",\"value\":\"inaccurate\"},{\"id\":null,\"isDisabled\":false,\"label\":\"It is an advertisement\",\"name\":\"advertisement\",\"value\":\"advertisement\"},{\"id\":null,\"isDisabled\":false,\"label\":\"Other\",\"name\":\"other\",\"value\":\"other\"}],\"value\":null},\"submitButtonText\":\"Submit\",\"successHeadline\":\"Job successfully reported\",\"successText\":\"Thank you for helping us identify suspicious behavior on Indeed\"},\"saveJobButtonContainerModel\":{\"alreadySavedButtonModel\":{\"actions\":[\"Saved\",\"Applied\",\"Interviewing\",\"Offered\",\"Hired\"],\"buttonSize\":\"block\",\"buttonType\":\"secondary\",\"contentHtml\":\"Saved\",\"href\":\"\\\\u002F\",\"iconSize\":null},\"applyFromComputerButtonModel\":null,\"applyFromComputerLogUrl\":\"\\\\u002Fm\\\\u002Frpc\\\\u002Flog\\\\u002Femailmyself?jk=028bf2018beebedb&mobvjtk=1eaook86k36bm000&sbt=4c021f433b8dd134463a497cf3645afa&ctk=1eaook85m10a3000&acctKey=\",\"currentJobState\":\"VISITED\",\"didYouApplyPromptModel\":{\"calloutModel\":{\"actionsList\":null,\"actionsMap\":{\"NO\":{\"children\":\"Not interested\",\"className\":null,\"href\":null,\"target\":null},\"LATER\":{\"children\":\"Maybe later\",\"className\":null,\"href\":null,\"target\":null},\"YES\":{\"children\":\"Yes\",\"className\":null,\"href\":null,\"target\":null}},\"caretPosition\":null,\"children\":null,\"dismissAriaLabel\":\"Close\",\"dismissAttributes\":null,\"dismissHref\":null,\"heading\":\"Did you apply?\"},\"jobKey\":\"028bf2018beebedb\",\"possibleResponses\":{\"NO\":\"NO\",\"LATER\":\"LATER\",\"YES\":\"YES\"},\"userCanView\":false},\"didYouApplyResponseUrl\":\"\\\\u002Fm\\\\u002Frpc\\\\u002Fdidyouapply?tk=1eaook86k36bm000&jobKey=028bf2018beebedb&originPage=viewjob&from=viewjob\",\"hashedCSRFToken\":\"4c021f433b8dd134463a497cf3645afa\",\"isAlreadySavedButtonVisible\":false,\"isDisableJobStatusChange\":false,\"isLoggedIn\":false,\"isSaveWithoutLoginEnabled\":false,\"isSticky\":false,\"isSyncJobs\":false,\"mobtk\":\"1eaook86k36bm000\",\"myIndeedLoginLink\":\"https:\\\\u002F\\\\u002Fau.indeed.com\\\\u002Faccount\\\\u002Flogin?dest=%2Fviewjob%3Fjk%3D028bf2018beebedb\",\"myJobsAPIHref\":\"\\\\u002Frpc\\\\u002Flog\\\\u002Fmyjobs\\\\u002Ftransition_job_state?client=mobile&cause=statepicker&preserveTimestamp=false&tk=1eaook86k36bm000&jobKey=028bf2018beebedb&originPage=viewjob\",\"myJobsURL\":\"\\\\u002Fmyjobs\\\\u002F?from=mobvj#\",\"pageId\":\"viewjob\",\"possibleJobActions\":{\"SAVED\":\"save\",\"APPLIED\":\"apply\",\"INTERVIEWING\":\"interview\",\"OFFERED\":\"offer\",\"HIRED\":\"hire\",\"VISITED\":\"visit\",\"ARCHIVED\":\"archive\"},\"possibleJobStates\":{\"SAVED\":\"Saved\",\"APPLIED\":\"Applied\",\"INTERVIEWING\":\"Interviewing\",\"OFFERED\":\"Offered\",\"HIRED\":\"Hired\",\"VISITED\":\"Visited\",\"ARCHIVED\":\"Archived\"},\"saveButtonModel\":{\"buttonSize\":\"block\",\"buttonType\":\"secondary\",\"contentHtml\":\"Save this job\",\"dataHref\":null,\"href\":\"\\\\u002F\",\"icon\":{\"iconTitle\":\"save-icon\",\"iconType\":\"favorite-border\"},\"isBlock\":false,\"largeScreenSizeText\":null,\"openInNewTab\":false,\"referrerpolicy\":null,\"rel\":null,\"sanitizedHref\":null,\"sanitizedHtml\":null,\"sticky\":false,\"target\":null,\"title\":null,\"viewJobDisplay\":\"DESKTOP_STANDALONE\"},\"showSaveJobInlineCallout\":true,\"smallButtonModel\":null,\"uistates\":{\"INTERVIEWING\":\"INTERVIEWING\",\"OFFERED\":\"OFFERED\",\"SAVED\":\"SAVED\",\"VISITED\":\"VISITED\",\"HIRED\":\"HIRED\",\"ARCHIVED\":\"ARCHIVED\",\"APPLIED\":\"APPLIED\"},\"viewJobDisplay\":\"DESKTOP_STANDALONE\"},\"saveJobCalloutModel\":{\"actionsList\":null,\"actionsMap\":{\"createaccount\":{\"children\":\"Create account (it\\'s free)\",\"className\":null,\"href\":\"https:\\\\u002F\\\\u002Fau.indeed.com\\\\u002Faccount\\\\u002Fregister?dest=%2Fviewjob%3Fjk%3D028bf2018beebedb\",\"target\":\"self\"},\"signin\":{\"children\":\"Sign in\",\"className\":null,\"href\":\"https:\\\\u002F\\\\u002Fau.indeed.com\\\\u002Faccount\\\\u002Flogin?dest=%2Fviewjob%3Fjk%3D028bf2018beebedb\",\"target\":\"self\"}},\"caretPosition\":null,\"children\":\"You must sign in to save jobs:\",\"dismissAriaLabel\":\"Close\",\"dismissAttributes\":null,\"dismissHref\":null,\"heading\":\"Save jobs and view them from any computer.\"},\"saveJobFailureModalModel\":{\"closeAriaLabel\":\"Close\",\"closeButtonText\":\"Close\",\"message\":\"Please retry\",\"signInButtonText\":null,\"signInHref\":null,\"title\":\"Failed to Save Job\"},\"saveJobLimitExceededModalModel\":{\"closeAriaLabel\":\"Close\",\"closeButtonText\":null,\"message\":\"You reached the limit. Please log in to save additional jobs.\",\"signInButtonText\":\"Sign in\",\"signInHref\":\"https:\\\\u002F\\\\u002Fau.indeed.com\\\\u002Faccount\\\\u002Flogin?dest=%2Fviewjob%3Fjk%3D028bf2018beebedb&from=viewjob_savejoblimitmodal\",\"title\":\"You\\'ve already saved 20 jobs\"},\"stickyType\":\"ALWAYS\",\"validationToken\":\"ZlZg1VqaEDWp2g+kdCQ9qUTkAV7sslXJUFUKPJAPMpE=\",\"viewJobButtonLinkContainerModel\":{\"clickCookieName\":\"RCLK\",\"clickCookieValue\":\"jk=028bf2018beebedb&vjtk=1eaook86k36bm000&ts=1592116519124&rd=&qd=\",\"desktopScreenerQuestionsModel\":null,\"jobKey\":\"028bf2018beebedb\",\"shouldSetClickTrackingCookie\":true,\"thirdPartyApplyCreateAccountModel\":null,\"viewJobButtonLinkModel\":{\"buttonSize\":\"block\",\"buttonType\":\"primary\",\"contentHtml\":\"Apply Now\",\"dataHref\":null,\"href\":\"https:\\\\u002F\\\\u002Fau.indeed.com\\\\u002Frc\\\\u002Fclk?jk=028bf2018beebedb&from=vj&pos=bottom&sjdu=76Cn2YAIPzFIwtaQqpG01IDDplm6SwWjHcxyoDIphKnEOEJUiVSIY7daUBaXb4E_kN0wkll9wDHc3mnStM4Hmg\",\"icon\":null,\"isBlock\":true,\"largeScreenSizeText\":\"Apply On Company Site\",\"openInNewTab\":true,\"referrerpolicy\":\"origin\",\"rel\":\"noopener\",\"sanitizedHref\":null,\"sanitizedHtml\":null,\"sticky\":false,\"target\":\"_blank\",\"title\":null,\"viewJobDisplay\":null}},\"viewJobDisplay\":\"DESKTOP_STANDALONE\"}'\n\n\n\ndata = json.loads(obj_text)\n\n\ndata.keys()\n\ndict_keys(['base64EncodedJson', 'baseInboxUrl', 'baseUrl', 'clientsideProctorGroups', 'companyFollowForm', 'country', 'ctk', 'dcmModel', 'desktop', 'desktopSponsoredJobSeenData', 'dgToken', 'googleOneTapModel', 'indeedChatEmployerModel', 'jobKey', 'jobLocation', 'jobSeenData', 'jobTitle', 'language', 'locale', 'localeData', 'mobtk', 'notifications', 'originalJobLinkModel', 'pageId', 'relatedLinks', 'reportJobForm', 'saveJobButtonContainerModel', 'saveJobCalloutModel', 'saveJobFailureModalModel', 'saveJobLimitExceededModalModel', 'stickyType', 'validationToken', 'viewJobButtonLinkContainerModel', 'viewJobDisplay'])\n\n\n\ndata['jobLocation']\n\n'Brisbane QLD'\n\n\n\ndata['jobTitle']\n\n'Various Project Controls & Planning Positions'\n\n\n\ndata['jobKey']\n\n'028bf2018beebedb'\n\n\n\nsoup = BeautifulSoup(r.content)\n\nJob Text\n\nsoup.select('#jobDescriptionText')[0]\n\n<div class=\"jobsearch-jobDescriptionText\" id=\"jobDescriptionText\"><div><p>Founded in 1911, SNC-Lavalin Atkins is a global fully integrated professional services and project management company and a major player in the ownership of infrastructure. From offices around the world, SNC-Lavalin Atkins’ employees think beyond engineering. Our teams provide comprehensive end-to-end project solutions – including capital investment, consulting, design, engineering, construction management, sustaining capital and operations and maintenance – to clients across the EDPM (Engineering, Design and Project Management), Infrastructure and Resources businesses. http://www.snclavalin.com</p><p></p><p><b>\nJoin our SNC-Lavalin Atkins team, and you’ll be a part of a diverse, ambitious business with a strong team spirit.\n</b></p><p></p><p>For more than 40 years in Australia, our people have been carving out rewarding careers on award winning projects. We think beyond engineering and push the boundaries of innovation for our clients across all major markets.</p><p>\nSNC-Lavalin Atkins is built on our core values of Safety, Integrity, Collaboration and Innovation. Our people drive results and are helping our clients transform their projects from vision into reality. Working with diverse and multi-disciplinary teams we provide consultancy, design, engineering through to self-perform construction, completions &amp; commissioning and operations &amp; maintenance, all underpinned by our digital know-how.</p><p></p><p><b><i>\nAbout the Opportunity\n</i></b></p><p>Our Programme Management Office (PMO) consultancy team currently has specialists working on complex infrastructure, aviation and transport projects in Brisbane, Sydney and Melbourne.</p><p>\nWe are seeking a variety of project, programme and portfolio (P3) PMO roles for each of our Brisbane, Sydney and Melbourne State offices within our EDPM business. Please include in your cover letter, which state office you would like to be considered for.\n</p><p></p><p>The PMO roles we are seeking include:</p><ul><li>\nProject Controls Managers</li><li>\nForensic Planners</li><li>\nP6 Planning Managers</li><li>\nP6 Planners</li><li>\nRisk Managers</li><li>\nCost Controllers</li><li>\nCost Managers</li><li>\nReporting Managers</li><li>\nEstimating Leads</li><li>\nEstimators</li><li>\nDocument Controllers</li></ul><p></p><p><b>\nEducation and Skills</b></p><ul><li>\nMinimum 5 years’ experience in chosen field</li><li>\nA Bachelor degree or higher, and/or equivalent in training and experience regarded</li><li>\nMust have experience in one of the following sectors; infrastructure, aviation, transportation or mining</li><li>\nProject, programme and portfolio (P3) knowledge and experience advantageous</li><li>\nEffective problem solving and time management abilities</li><li>\nExcellent communication and organisational skills</li><li>\nExperience with large-scale projects\n</li><li>Ability to work independently with minimal supervision or in an integrated team environment</li><li>\nProcess driven and attention to detail are necessary</li><li>\nDrive to continuously seek innovation and improvement (self, project, strategic)</li></ul><p></p><p><b><i>\nAbout the Benefits\n</i></b></p><p>We offer rewarding careers to people who want to be part of our great stories and remarkable achievements. With the opportunity to work on diverse projects of varying sizes.</p><p></p><p>\nSNC-Lavalin Atkins’ business offer a competitive compensation and benefits package with a great team environment. We have in place strong learning and development programs, training and career opportunities to keep you developing.</p><p>\nWe are looking for innovative, forward-thinking people who enjoy challenge and actively seek to develop and improve work processes and want to be part of a safe and healthy work environment.</p><p></p><p><b>\nWhy join our team?</b></p><p>\nLocated in 13 countries across the Asia Pacific region, SNC-Lavalin Atkins operates through its brands, SNC-Lavalin, Kentz and Atkins and our people have worked, and continue to work, on some of the region’s most iconic projects. So join today as a career with us opens up a world of possibilities - being part of a global organisation of over 50,000 employees opportunities await you to collaborate with colleagues on international projects or use your skills and knowledge to create a winning combination for our clients across our other markets.</p><p>\nWe’re focused on creating an inclusive, supportive workplace that will enable you to develop and thrive. You’ll work alongside some of the leaders in your field, with opportunities to reach your potential through training and professional development.</p><p></p><p>\nOnly current ‘Right to work in Australia’ applications will be considered.</p></div><p></p></div>"
  },
  {
    "objectID": "notebooks/Extracting Jobs with Common Crawl.html#seek",
    "href": "notebooks/Extracting Jobs with Common Crawl.html#seek",
    "title": "skeptric",
    "section": "Seek",
    "text": "Seek\n\nobjs = list(cdx.iter('seek.com.au/job/*',\n                     from_ts='202004', to='202005',\n                     limit=50, \n                     filter=['status:200']))\npd.DataFrame(objs)\n\n\n\n\n\n  \n    \n      \n      charset\n      digest\n      filename\n      languages\n      length\n      mime\n      mime-detected\n      offset\n      status\n      timestamp\n      url\n      urlkey\n    \n  \n  \n    \n      0\n      UTF-8\n      QZF3KBX2P77DGDACHOO6VQO6TEMTWXVZ\n      crawl-data/CC-MAIN-2020-16/segments/1585370506673.7/warc/CC-MAIN-20200402045741-20200402075741-00395.warc.gz\n      eng\n      24473\n      text/html\n      text/html\n      1137303128\n      200\n      20200402065023\n      https://www.seek.com.au/job/40480218?type=standard\n      au,com,seek)/job/40480218?type=standard\n    \n    \n      1\n      UTF-8\n      GFQWJL3GA4HB4GZBANA5EJU7MODNVVKX\n      crawl-data/CC-MAIN-2020-16/segments/1585370506959.34/warc/CC-MAIN-20200402111815-20200402141815-00366.warc.gz\n      eng\n      24220\n      text/html\n      text/html\n      1137099417\n      200\n      20200402134520\n      https://www.seek.com.au/job/40673486?type=standard\n      au,com,seek)/job/40673486?type=standard\n    \n    \n      2\n      UTF-8\n      3NM664ONG6P5DJXZSZQMTGKQPC642RQF\n      crawl-data/CC-MAIN-2020-16/segments/1585371880945.85/warc/CC-MAIN-20200409220932-20200410011432-00128.warc.gz\n      eng\n      29828\n      text/html\n      text/html\n      1127547178\n      200\n      20200410002047\n      https://www.seek.com.au/job/40778851?_ga=2.217432918.1128169088.1579487610-1596147771.1579487610\n      au,com,seek)/job/40778851?_ga=2.217432918.1128169088.1579487610-1596147771.1579487610\n    \n    \n      3\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00294.warc.gz\n      NaN\n      1715\n      text/html\n      text/html\n      1133538036\n      200\n      20200402151926\n      https://www.seek.com.au/job/40790432/apply/linkout\n      au,com,seek)/job/40790432/apply/linkout\n    \n    \n      4\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00141.warc.gz\n      NaN\n      1714\n      text/html\n      text/html\n      1134800452\n      200\n      20200406011717\n      https://www.seek.com.au/job/40800664/apply/linkout\n      au,com,seek)/job/40800664/apply/linkout\n    \n    \n      5\n      UTF-8\n      IX5KPE6S4O4HIONEMZOGEBBDH7HJDG2J\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00470.warc.gz\n      eng\n      24749\n      text/html\n      text/html\n      1112222656\n      200\n      20200410032900\n      https://www.seek.com.au/job/40832664?type=standout\n      au,com,seek)/job/40832664?type=standout\n    \n    \n      6\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00202.warc.gz\n      NaN\n      1715\n      text/html\n      text/html\n      1144074898\n      200\n      20200402162406\n      https://www.seek.com.au/job/40842263/apply/linkout\n      au,com,seek)/job/40842263/apply/linkout\n    \n    \n      7\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00083.warc.gz\n      NaN\n      1706\n      text/html\n      text/html\n      1107682991\n      200\n      20200406025458\n      https://www.seek.com.au/job/40846293/apply/linkout\n      au,com,seek)/job/40846293/apply/linkout\n    \n    \n      8\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00025.warc.gz\n      NaN\n      1708\n      text/html\n      text/html\n      1116159944\n      200\n      20200408084847\n      https://www.seek.com.au/job/40862183/apply/linkout\n      au,com,seek)/job/40862183/apply/linkout\n    \n    \n      9\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00511.warc.gz\n      NaN\n      1711\n      text/html\n      text/html\n      1109546595\n      200\n      20200402161049\n      https://www.seek.com.au/job/40862233/apply/linkout\n      au,com,seek)/job/40862233/apply/linkout\n    \n    \n      10\n      UTF-8\n      SU5PQRKEA2AVPRYDNFR4FXAB2PIBG3UC\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00410.warc.gz\n      eng\n      23324\n      text/html\n      text/html\n      1143762094\n      200\n      20200401103811\n      https://www.seek.com.au/job/40878691?type=promoted\n      au,com,seek)/job/40878691?type=promoted\n    \n    \n      11\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370526982.53/warc/CC-MAIN-20200404231315-20200405021315-00523.warc.gz\n      NaN\n      1707\n      text/html\n      text/html\n      1147942773\n      200\n      20200405004211\n      https://www.seek.com.au/job/40899398/apply/linkout\n      au,com,seek)/job/40899398/apply/linkout\n    \n    \n      12\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370507738.45/warc/CC-MAIN-20200402173940-20200402203940-00223.warc.gz\n      NaN\n      1711\n      text/html\n      text/html\n      1156221286\n      200\n      20200402184859\n      https://www.seek.com.au/job/40922447/apply/linkout\n      au,com,seek)/job/40922447/apply/linkout\n    \n    \n      13\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370526982.53/warc/CC-MAIN-20200404231315-20200405021315-00195.warc.gz\n      NaN\n      1709\n      text/html\n      text/html\n      1119039973\n      200\n      20200405003128\n      https://www.seek.com.au/job/40937285/apply/linkout\n      au,com,seek)/job/40937285/apply/linkout\n    \n    \n      14\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00300.warc.gz\n      NaN\n      1707\n      text/html\n      text/html\n      1136149459\n      200\n      20200402144654\n      https://www.seek.com.au/job/40938911/apply/linkout\n      au,com,seek)/job/40938911/apply/linkout\n    \n    \n      15\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00202.warc.gz\n      NaN\n      1706\n      text/html\n      text/html\n      1114897640\n      200\n      20200408092032\n      https://www.seek.com.au/job/40939118/apply/linkout\n      au,com,seek)/job/40939118/apply/linkout\n    \n    \n      16\n      UTF-8\n      QOJNF6KZWQD7V5RGTUHN7A3MZWVAB2IH\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00164.warc.gz\n      eng\n      24431\n      text/html\n      text/html\n      1130847247\n      200\n      20200408080650\n      https://www.seek.com.au/job/40939952\n      au,com,seek)/job/40939952\n    \n    \n      17\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00085.warc.gz\n      NaN\n      1713\n      text/html\n      text/html\n      1134732273\n      200\n      20200402162209\n      https://www.seek.com.au/job/40942814/apply/linkout\n      au,com,seek)/job/40942814/apply/linkout\n    \n    \n      18\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371660550.75/warc/CC-MAIN-20200406200320-20200406230820-00200.warc.gz\n      NaN\n      1702\n      text/html\n      text/html\n      1153471119\n      200\n      20200406201945\n      https://www.seek.com.au/job/40942905/apply/linkout\n      au,com,seek)/job/40942905/apply/linkout\n    \n    \n      19\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370515113.54/warc/CC-MAIN-20200403154746-20200403184746-00325.warc.gz\n      NaN\n      1710\n      text/html\n      text/html\n      1124048328\n      200\n      20200403171955\n      https://www.seek.com.au/job/40949035/apply/linkout\n      au,com,seek)/job/40949035/apply/linkout\n    \n    \n      20\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00525.warc.gz\n      NaN\n      1710\n      text/html\n      text/html\n      1146260722\n      200\n      20200406021412\n      https://www.seek.com.au/job/40950055/apply/linkout\n      au,com,seek)/job/40950055/apply/linkout\n    \n    \n      21\n      UTF-8\n      SBIPR2XKNNXOU6BDLCJDXYVKCW5ELTBZ\n      crawl-data/CC-MAIN-2020-16/segments/1585371880945.85/warc/CC-MAIN-20200409220932-20200410011432-00131.warc.gz\n      NaN\n      1713\n      text/html\n      text/html\n      1118334178\n      200\n      20200410003652\n      https://www.seek.com.au/job/40951337/apply/linkout\n      au,com,seek)/job/40951337/apply/linkout\n    \n    \n      22\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00127.warc.gz\n      NaN\n      1713\n      text/html\n      text/html\n      1117459171\n      200\n      20200408083658\n      https://www.seek.com.au/job/40952378/apply/linkout\n      au,com,seek)/job/40952378/apply/linkout\n    \n    \n      23\n      UTF-8\n      UPI34GKA55O3CP3DC7WVTQ6ERDEPFQBD\n      crawl-data/CC-MAIN-2020-16/segments/1585370505366.8/warc/CC-MAIN-20200401034127-20200401064127-00073.warc.gz\n      eng\n      25661\n      text/html\n      text/html\n      1146497518\n      200\n      20200401041840\n      https://www.seek.com.au/job/40953788?type=standout\n      au,com,seek)/job/40953788?type=standout\n    \n    \n      24\n      UTF-8\n      JGI3B5JH6FTUAP34DNOCVV3FPVWJPAZK\n      crawl-data/CC-MAIN-2020-16/segments/1585370515113.54/warc/CC-MAIN-20200403154746-20200403184746-00145.warc.gz\n      eng\n      28241\n      text/html\n      text/html\n      1115810277\n      200\n      20200403174236\n      https://www.seek.com.au/job/40954073\n      au,com,seek)/job/40954073\n    \n    \n      25\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371660550.75/warc/CC-MAIN-20200406200320-20200406230820-00003.warc.gz\n      NaN\n      1706\n      text/html\n      text/html\n      1131971218\n      200\n      20200406214733\n      https://www.seek.com.au/job/40956282/apply/linkout\n      au,com,seek)/job/40956282/apply/linkout\n    \n    \n      26\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370515113.54/warc/CC-MAIN-20200403154746-20200403184746-00237.warc.gz\n      NaN\n      1713\n      text/html\n      text/html\n      1137446887\n      200\n      20200403171321\n      https://www.seek.com.au/job/40956358/apply/linkout\n      au,com,seek)/job/40956358/apply/linkout\n    \n    \n      27\n      UTF-8\n      5A6GAGBZKB6QEBXXPAPUQUH3R2FEAIZI\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00552.warc.gz\n      eng\n      25662\n      text/html\n      text/html\n      1120301081\n      200\n      20200408074459\n      https://www.seek.com.au/job/40957644\n      au,com,seek)/job/40957644\n    \n    \n      28\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370505366.8/warc/CC-MAIN-20200401034127-20200401064127-00194.warc.gz\n      NaN\n      1750\n      text/html\n      text/html\n      1175885889\n      200\n      20200401052903\n      https://www.seek.com.au/job/40957854/apply/linkout?searchrequesttoken=196fd578-5779-41fa-b014-3d21cb5ca0f6\n      au,com,seek)/job/40957854/apply/linkout?searchrequesttoken=196fd578-5779-41fa-b014-3d21cb5ca0f6\n    \n    \n      29\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370507738.45/warc/CC-MAIN-20200402173940-20200402203940-00559.warc.gz\n      NaN\n      1716\n      text/html\n      text/html\n      1125843721\n      200\n      20200402185759\n      https://www.seek.com.au/job/40958604/apply/linkout\n      au,com,seek)/job/40958604/apply/linkout\n    \n    \n      30\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00094.warc.gz\n      NaN\n      1707\n      text/html\n      text/html\n      1140931785\n      200\n      20200406005647\n      https://www.seek.com.au/job/40958834/apply/linkout\n      au,com,seek)/job/40958834/apply/linkout\n    \n    \n      31\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370507738.45/warc/CC-MAIN-20200402173940-20200402203940-00178.warc.gz\n      NaN\n      1714\n      text/html\n      text/html\n      1152691856\n      200\n      20200402193733\n      https://www.seek.com.au/job/40961544/apply/linkout\n      au,com,seek)/job/40961544/apply/linkout\n    \n    \n      32\n      UTF-8\n      F2TIBEVBIBVAGTXVMKUAOJYA7MZSXZ3H\n      crawl-data/CC-MAIN-2020-16/segments/1585371612531.68/warc/CC-MAIN-20200406004220-20200406034720-00271.warc.gz\n      eng\n      25755\n      text/html\n      text/html\n      1168960284\n      200\n      20200406004849\n      https://www.seek.com.au/job/40961710\n      au,com,seek)/job/40961710\n    \n    \n      33\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00132.warc.gz\n      NaN\n      1706\n      text/html\n      text/html\n      1141736079\n      200\n      20200408082132\n      https://www.seek.com.au/job/40964291/apply/linkout\n      au,com,seek)/job/40964291/apply/linkout\n    \n    \n      34\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506988.10/warc/CC-MAIN-20200402143006-20200402173006-00338.warc.gz\n      NaN\n      1714\n      text/html\n      text/html\n      1157306432\n      200\n      20200402160448\n      https://www.seek.com.au/job/40964800/apply/linkout\n      au,com,seek)/job/40964800/apply/linkout\n    \n    \n      35\n      UTF-8\n      67FOF5FK3OGDZVD25SC5JCDTO6WMBVNK\n      crawl-data/CC-MAIN-2020-16/segments/1585371810807.81/warc/CC-MAIN-20200408072713-20200408103213-00150.warc.gz\n      NaN\n      1705\n      text/html\n      text/html\n      1104968024\n      200\n      20200408084458\n      https://www.seek.com.au/job/40966592/apply/linkout\n      au,com,seek)/job/40966592/apply/linkout\n    \n    \n      36\n      UTF-8\n      6TXVXNZCFZPVBLE3YPROENOHGQYHGBN7\n      crawl-data/CC-MAIN-2020-16/segments/1585370520039.50/warc/CC-MAIN-20200404042338-20200404072338-00502.warc.gz\n      eng\n      31254\n      text/html\n      text/html\n      1111481280\n      200\n      20200404061056\n      https://www.seek.com.au/job/40987005?type=standout\n      au,com,seek)/job/40987005?type=standout\n    \n    \n      37\n      UTF-8\n      YEH5B7Y53462ESRPORVU7WJQAENEBSSM\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00112.warc.gz\n      eng\n      26881\n      text/html\n      text/html\n      1114285420\n      200\n      20200410034736\n      https://www.seek.com.au/job/40990128?type=standout\n      au,com,seek)/job/40990128?type=standout\n    \n    \n      38\n      UTF-8\n      EMYKQVSS4YL5CDMOGCF3ADB3VEVUBETZ\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00078.warc.gz\n      eng\n      26852\n      text/html\n      text/html\n      1113488574\n      200\n      20200410033112\n      https://www.seek.com.au/job/40990643?type=standout\n      au,com,seek)/job/40990643?type=standout\n    \n    \n      39\n      UTF-8\n      6XTYNWDPTXQUJJ357R2SRKLFRQ2WMN5J\n      crawl-data/CC-MAIN-2020-16/segments/1585370506959.34/warc/CC-MAIN-20200402111815-20200402141815-00342.warc.gz\n      eng\n      23720\n      text/html\n      text/html\n      1136226194\n      200\n      20200402121900\n      https://www.seek.com.au/job/41021776?type=standard\n      au,com,seek)/job/41021776?type=standard\n    \n    \n      40\n      UTF-8\n      A4WSOKJZQEUOTDFYBFCGGQSMEWVMPSXT\n      crawl-data/CC-MAIN-2020-16/segments/1585371861991.79/warc/CC-MAIN-20200409154025-20200409184525-00489.warc.gz\n      eng\n      29094\n      text/html\n      text/html\n      1115383341\n      200\n      20200409165536\n      https://www.seek.com.au/job/41023396?type=standout\n      au,com,seek)/job/41023396?type=standout\n    \n    \n      41\n      UTF-8\n      5CCOPJ2FALWPHJMCYOVGTPLZPWD7A2JW\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00236.warc.gz\n      eng\n      31694\n      text/html\n      text/html\n      1100082493\n      200\n      20200410022530\n      https://www.seek.com.au/job/41026600?type=standout\n      au,com,seek)/job/41026600?type=standout\n    \n    \n      42\n      UTF-8\n      3JRC4NOIBQMHS3KH3BJ2TT3HP6ZBKFVC\n      crawl-data/CC-MAIN-2020-16/segments/1585370519111.47/warc/CC-MAIN-20200404011558-20200404041558-00352.warc.gz\n      eng\n      23326\n      text/html\n      text/html\n      1137698543\n      200\n      20200404021509\n      https://www.seek.com.au/job/41041152?type=standard\n      au,com,seek)/job/41041152?type=standard\n    \n    \n      43\n      UTF-8\n      UNITV4SZELSTZ2SUY5CTNFFNHQXJW5SE\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00258.warc.gz\n      eng\n      24930\n      text/html\n      text/html\n      1110674535\n      200\n      20200410034700\n      https://www.seek.com.au/job/41043865?type=standout\n      au,com,seek)/job/41043865?type=standout\n    \n    \n      44\n      UTF-8\n      NNVFM3GLRVU3WCKOAWLVYH5ER6LY53QE\n      crawl-data/CC-MAIN-2020-16/segments/1585370506959.34/warc/CC-MAIN-20200402111815-20200402141815-00213.warc.gz\n      eng\n      24224\n      text/html\n      text/html\n      1136423327\n      200\n      20200402130715\n      https://www.seek.com.au/job/41046217?type=standard\n      au,com,seek)/job/41046217?type=standard\n    \n    \n      45\n      UTF-8\n      FNB3SNDV4PC4H7N5HYE3A5Y4HLMQOXO5\n      crawl-data/CC-MAIN-2020-16/segments/1585370521574.59/warc/CC-MAIN-20200404073139-20200404103139-00344.warc.gz\n      eng\n      25365\n      text/html\n      text/html\n      1125398898\n      200\n      20200404082309\n      https://www.seek.com.au/job/41047214?_ga=2.203597463.624167984.1582928577-333199963.1581114360\n      au,com,seek)/job/41047214?_ga=2.203597463.624167984.1582928577-333199963.1581114360\n    \n    \n      46\n      UTF-8\n      N4XD7VHWB7TCCCP4IUCYQGWLM4ZQUNXB\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00326.warc.gz\n      eng\n      27174\n      text/html\n      text/html\n      1155685826\n      200\n      20200401110801\n      https://www.seek.com.au/job/41051514?type=standout\n      au,com,seek)/job/41051514?type=standout\n    \n    \n      47\n      UTF-8\n      ISMTOOTNSH4XUAEOVCKJARS2RCWDROWS\n      crawl-data/CC-MAIN-2020-16/segments/1585370524604.46/warc/CC-MAIN-20200404165658-20200404195658-00285.warc.gz\n      eng\n      23851\n      text/html\n      text/html\n      1137407096\n      200\n      20200404174519\n      https://www.seek.com.au/job/41082355?_ga=2.144734647.241037096.1583184597-1542963780.1583184597&_gac=1.207905446.1583184597.EAIaIQobChMI-MXYmd785w...\n      au,com,seek)/job/41082355?_ga=2.144734647.241037096.1583184597-1542963780.1583184597&_gac=1.207905446.1583184597.eaiaiqobchmi-mxymd785wivrkwwch3zi...\n    \n    \n      48\n      UTF-8\n      C23EWFC5G4SZO5HTN324MB57CQZJJHFK\n      crawl-data/CC-MAIN-2020-16/segments/1585370506673.7/warc/CC-MAIN-20200402045741-20200402075741-00413.warc.gz\n      eng\n      27069\n      text/html\n      text/html\n      1133160885\n      200\n      20200402071245\n      https://www.seek.com.au/job/41090313?type=standard\n      au,com,seek)/job/41090313?type=standard\n    \n    \n      49\n      UTF-8\n      6VVJ4EBK4MHG5ORVCVMB5AWGN2J55IIS\n      crawl-data/CC-MAIN-2020-16/segments/1585370506959.34/warc/CC-MAIN-20200402111815-20200402141815-00214.warc.gz\n      eng\n      27803\n      text/html\n      text/html\n      1142507080\n      200\n      20200402120415\n      https://www.seek.com.au/job/41096558?_ga=2.123495755.1320293108.1583192246-1372768251.1563851836\n      au,com,seek)/job/41096558?_ga=2.123495755.1320293108.1583192246-1372768251.1563851836\n    \n  \n\n\n\n\nFull add (unless has /apply…)\n\nwith open('test.html', 'wb') as f:\n    f.write(objs[0].content)\n\nNone\n\n\nLooking at the source it looks like all the relevant data is in a javascript object on a single line\n\ndata_re = re.compile('_REDUX_DATA = ([^\\n]+);')\ndef sk_extract(text):\n    return demjson.decode(data_re.search(text).group(1))\n\n\nobj = sk_extract(objs[0].content.decode('utf-8'))\n\n\nobj.keys()\n\ndict_keys(['dashboard', 'experiments', 'featureFlags', 'jobdetails', 'joblistitem', 'lastSearch', 'lmis', 'location', 'nudges', 'results', 'savedJobs', 'saveSearch', 'search', 'seo', 'user', 'fitme', '@@redux-hotjar-state'])\n\n\nAll the data is in here\n\nobj['jobdetails']['result']\n\n{'id': 40480218,\n 'listingDate': '2020-03-17T23:41:15.000Z',\n 'expiryDate': '2020-04-25T13:00:00.000Z',\n 'title': 'Transmission Coordinator - Broadcast TV Playout - Sydney',\n 'teaser': \"Immediate, full-time TV media operations job in Sydney's North. Coordinate live multi-channel TV content to air in a digital TV playout facility.\",\n 'advertiser': {'id': 30979201,\n  'description': 'Lang Deacon',\n  'searchParams': {'keywords': 'Lang Deacon'}},\n 'locationHierarchy': {'nation': 'Australia',\n  'state': 'New South Wales',\n  'city': 'Sydney',\n  'area': 'North Shore & Northern Beaches',\n  'suburb': 'northsydney'},\n 'locationId': 1000,\n 'stateId': 3101,\n 'workType': 'Full Time',\n 'classification': {'id': 6304, 'description': 'Advertising, Arts & Media'},\n 'subClassification': {'id': 6314, 'description': 'Programming & Production'},\n 'salary': None,\n 'salaryType': 'AnnualPackage',\n 'automaticInclusion': False,\n 'isLinkOut': False,\n 'isScreenAssigned': False,\n 'isSelectionCriteriaEnabled': False,\n 'status': 'Active',\n 'isRightToWorkRequired': False,\n 'hasRoleRequirements': True,\n 'roleRequirements': ['Which of the following statements best describes your right to work in Australia?',\n  \"What's your expected annual base salary?\",\n  'How much notice are you required to give your current employer?'],\n 'mobileAdTemplate': '<ul> <li><strong>Live media TV operations; </strong></li> <li><strong>Based Sydney North; </strong></li> <li><strong>Career development &amp; team support. </strong></li></ul> <p>\\xa0</p>  <p>Immediate opportunity based in Sydney’s North to join one of Australia’s leading multi-channel broadcast playout organisations and develop your media operations career to the next level.</p>  <p>\\xa0</p>  <p><strong>The Job:</strong></p>  <p>Coordinate multiple live and scheduled channels to air in a state-of-the-art digital TV playout centre.\\xa0 The role of Transmission Coordinator, sometimes referred to as Presentation Coordinator or Presentation Director, challenges your ability to coordinate multiple tasks, your attention to detail, and your calm nature under pressure.</p>  <p>\\xa0</p>  <p>Working with a supportive team, the Transmission Coordinator will liaise with broadcast clients to ensure schedules and playlists are accurate, content is appropriate and transmission of multi-channel content to air runs smoothly.</p>  <p>\\xa0</p>  <p><strong>What we need:</strong></p>  <p>We’re looking for an understanding of automated playout workflows in a contemporary TV or video environment combined with a genuine passion for working in the media industry.\\xa0</p>  <p>Broadcasting is a 24 hour business and as such the Transmission Coordinator must be comfortable working varied hours across a 24/7 shift roster.</p>  <p><br /><strong>We need:</strong></p> <ul> <li>Career experience in media TV operations;</li> <li>Exposure to digital TV environment / workflows;</li> <li>Attention to detail, methodical and responsive;</li> <li>Experience managing multiple tasks and priorities;</li> <li>Ability to work to a 24/7 roster.</li> <li>Previous TX Coord experience highly regarded.</li></ul> <p>\\xa0</p>  <p>In return the Transmission Coordinator will enjoy a genuinely supportive and enjoyable team culture, where work-life balance and simple team work is valued.\\xa0 The Transmission Coordinator will earn a competitive salary and benefit from defined career development opportunities.</p>  <p>\\xa0</p>  <p>At this stage only applicants with the right to live and work in Australia can be considered for this position.</p>  <p>\\xa0</p>  <p>If you fulfil the above criteria and would be interested in a new full-time challenge, then apply online including a Word version of your CV immediately.</p>',\n 'companyReview': None,\n 'contactMatches': [],\n 'hasCustomTemplate': False,\n 'roleTitles': 'coordinator',\n 'isPrivateAdvertiser': False}\n\n\nHow many objects?\n\nobjs = list(cdx.iter('seek.com.au/job/*',\n                     from_ts='202004', to='202005',\n                     filter=['status:200', '!~url:.*/apply/']))\npd.DataFrame(objs)\n\n\n\n\n\n  \n    \n      \n      charset\n      digest\n      filename\n      languages\n      length\n      mime\n      mime-detected\n      offset\n      status\n      timestamp\n      url\n      urlkey\n    \n  \n  \n    \n      0\n      UTF-8\n      QZF3KBX2P77DGDACHOO6VQO6TEMTWXVZ\n      crawl-data/CC-MAIN-2020-16/segments/1585370506673.7/warc/CC-MAIN-20200402045741-20200402075741-00395.warc.gz\n      eng\n      24473\n      text/html\n      text/html\n      1137303128\n      200\n      20200402065023\n      https://www.seek.com.au/job/40480218?type=standard\n      au,com,seek)/job/40480218?type=standard\n    \n    \n      1\n      UTF-8\n      GFQWJL3GA4HB4GZBANA5EJU7MODNVVKX\n      crawl-data/CC-MAIN-2020-16/segments/1585370506959.34/warc/CC-MAIN-20200402111815-20200402141815-00366.warc.gz\n      eng\n      24220\n      text/html\n      text/html\n      1137099417\n      200\n      20200402134520\n      https://www.seek.com.au/job/40673486?type=standard\n      au,com,seek)/job/40673486?type=standard\n    \n    \n      2\n      UTF-8\n      3NM664ONG6P5DJXZSZQMTGKQPC642RQF\n      crawl-data/CC-MAIN-2020-16/segments/1585371880945.85/warc/CC-MAIN-20200409220932-20200410011432-00128.warc.gz\n      eng\n      29828\n      text/html\n      text/html\n      1127547178\n      200\n      20200410002047\n      https://www.seek.com.au/job/40778851?_ga=2.217432918.1128169088.1579487610-1596147771.1579487610\n      au,com,seek)/job/40778851?_ga=2.217432918.1128169088.1579487610-1596147771.1579487610\n    \n    \n      3\n      UTF-8\n      IX5KPE6S4O4HIONEMZOGEBBDH7HJDG2J\n      crawl-data/CC-MAIN-2020-16/segments/1585371883359.91/warc/CC-MAIN-20200410012405-20200410042905-00470.warc.gz\n      eng\n      24749\n      text/html\n      text/html\n      1112222656\n      200\n      20200410032900\n      https://www.seek.com.au/job/40832664?type=standout\n      au,com,seek)/job/40832664?type=standout\n    \n    \n      4\n      UTF-8\n      SU5PQRKEA2AVPRYDNFR4FXAB2PIBG3UC\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00410.warc.gz\n      eng\n      23324\n      text/html\n      text/html\n      1143762094\n      200\n      20200401103811\n      https://www.seek.com.au/job/40878691?type=promoted\n      au,com,seek)/job/40878691?type=promoted\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      713\n      UTF-8\n      X565OQM2DRRQZ6FQC7S2NAMRXCAWI2LT\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00503.warc.gz\n      eng\n      27607\n      text/html\n      text/html\n      1151045210\n      200\n      20200401121846\n      https://www.seek.com.au/job/41214294?type=standard\n      au,com,seek)/job/41214294?type=standard\n    \n    \n      714\n      UTF-8\n      ZRO6RIHF3VULV5JBPUOGJLSUTOI7UNX6\n      crawl-data/CC-MAIN-2020-16/segments/1585370519111.47/warc/CC-MAIN-20200404011558-20200404041558-00100.warc.gz\n      eng\n      28557\n      text/html\n      text/html\n      1140480506\n      200\n      20200404033221\n      https://www.seek.com.au/job/41214307?type=standard\n      au,com,seek)/job/41214307?type=standard\n    \n    \n      715\n      UTF-8\n      4H72NRAA3UFVRCGPSKNJHVKAX2553JKI\n      crawl-data/CC-MAIN-2020-16/segments/1585370505730.14/warc/CC-MAIN-20200401100029-20200401130029-00421.warc.gz\n      eng\n      28671\n      text/html\n      text/html\n      1174895213\n      200\n      20200401113202\n      https://www.seek.com.au/job/41214308?type=standard\n      au,com,seek)/job/41214308?type=standard\n    \n    \n      716\n      UTF-8\n      BBQKJO27RIE6J26SUXV4ZC3X3AXMQBEF\n      crawl-data/CC-MAIN-2020-16/segments/1585370519111.47/warc/CC-MAIN-20200404011558-20200404041558-00292.warc.gz\n      eng\n      27902\n      text/html\n      text/html\n      1137053959\n      200\n      20200404023324\n      https://www.seek.com.au/job/41214450?type=standout\n      au,com,seek)/job/41214450?type=standout\n    \n    \n      717\n      UTF-8\n      6XEMII3GSEUSYBIREZT7L5LZLWOAYXBZ\n      crawl-data/CC-MAIN-2020-16/segments/1585370506121.24/warc/CC-MAIN-20200401192839-20200401222839-00381.warc.gz\n      eng\n      26222\n      text/html\n      text/html\n      1161048394\n      200\n      20200401212405\n      https://www.seek.com.au/job/41214657?type=standout\n      au,com,seek)/job/41214657?type=standout\n    \n  \n\n718 rows × 12 columns\n\n\n\n\ndata = [sk_extract(obj.content.decode('utf-8'))['jobdetails']['result'] for obj in objs[:5]]\n\nNone\nNone\nNone\nNone\nNone\n\n\n\npd.DataFrame(data)\n\n\n\n\n\n  \n    \n      \n      id\n      listingDate\n      expiryDate\n      title\n      teaser\n      advertiser\n      locationHierarchy\n      locationId\n      stateId\n      workType\n      ...\n      roleRequirements\n      mobileAdTemplate\n      companyReview\n      contactMatches\n      hasCustomTemplate\n      roleTitles\n      isPrivateAdvertiser\n      desktopAdTemplate\n      video\n      branding\n    \n  \n  \n    \n      0\n      40480218\n      2020-03-17T23:41:15.000Z\n      2020-04-25T13:00:00.000Z\n      Transmission Coordinator - Broadcast TV Playout - Sydney\n      Immediate, full-time TV media operations job in Sydney's North. Coordinate live multi-channel TV content to air in a digital TV playout facility.\n      {'id': 30979201, 'description': 'Lang Deacon', 'searchParams': {'keywords': 'Lang Deacon'}}\n      {'nation': 'Australia', 'state': 'New South Wales', 'city': 'Sydney', 'area': 'North Shore & Northern Beaches', 'suburb': 'northsydney'}\n      1000\n      3101\n      Full Time\n      ...\n      [Which of the following statements best describes your right to work in Australia?, What's your expected annual base salary?, How much notice are ...\n      <ul> <li><strong>Live media TV operations; </strong></li> <li><strong>Based Sydney North; </strong></li> <li><strong>Career development &amp; team...\n      None\n      []\n      False\n      coordinator\n      False\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      40673486\n      2020-03-04T21:38:36.000Z\n      2020-04-06T02:37:47.000Z\n      Caseworker - Aboriginal Identified\n      Do you want a job that will give you the opportunity to make a real difference in the lives of Aboriginal children who are in need of support?\n      {'id': 24524763, 'description': 'KARI', 'searchParams': {'keywords': 'KARI'}}\n      {'nation': 'Australia', 'state': 'New South Wales', 'city': 'Sydney', 'area': 'South West & M5 Corridor', 'suburb': 'liverpool'}\n      1000\n      3101\n      Full Time\n      ...\n      [Which of the following statements best describes your right to work in Australia?, Do you have a current Australian driver's licence?]\n      <p><strong>Caseworker - Aboriginal identified role</strong></p> <p>Salary Package up to $82,162</p> <p><em>(Inclusive of $70,000 base salary, leav...\n      None\n      [{'type': 'Email', 'value': 'ashley.crooks@kari.org.au'}, {'type': 'Phone', 'value': '(02) 8782 0300'}]\n      False\n      caseworker\n      False\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      40778851\n      2020-03-29T22:35:20.000Z\n      2020-05-18T13:59:59.000Z\n      Financial Counsellor\n      Outstanding opportunity for an experienced Financial Counsellor to join our multidisciplinary team.\n      {'id': 24132249, 'description': 'Better Place Australia', 'searchParams': {'keywords': 'Better Place Australia'}}\n      {'nation': 'Australia', 'state': 'Victoria', 'city': 'Melbourne', 'area': 'CBD & Inner Suburbs', 'suburb': 'melbourne'}\n      1002\n      3106\n      Full Time\n      ...\n      []\n      <p>Better Place Australia has a vision of <em>“An Australia where all people experience positive relationships, truly value each and live safer, m...\n      None\n      [{'type': 'Phone', 'value': '9556 5333'}]\n      True\n      financial-counsellor,counsellor\n      False\n      <meta charset=\"utf-8\" />\\n<style type=\"text/css\"><!--#VideoJobAd,.videoembed{display:block;height:310px;padding:5px 0;text-align:center;width:100%...\n      NaN\n      NaN\n    \n    \n      3\n      40832664\n      2020-03-24T02:12:22.000Z\n      2020-04-26T13:00:00.000Z\n      Apprentice or Trainee Hairdresser\n      *Maurice Meade are currently looking for talented apprentice or trainee hairdressers to be placed within our salons*\n      {'id': 25844154, 'description': 'Maurice Meade', 'searchParams': {'keywords': 'Maurice Meade'}}\n      {'nation': 'Australia', 'state': 'Western Australia', 'city': 'Perth', 'area': '', 'suburb': ''}\n      1009\n      3107\n      Full Time\n      ...\n      [Which of the following statements best describes your right to work in Australia?, How many years' experience do you have as a hairdresser?, How ...\n      <p>As Perth's leading hair salon and one of Australia's most recognised names in the industry, an <strong>apprenticeship/traineeship</strong> at <...\n      {'companyOverallRating': 3.2, 'companyTotalReviews': 10, 'companyProfileUrl': '/companies/maurice-meade-935469/reviews?jobId=40832664', 'companyNa...\n      []\n      False\n      hairdresser\n      False\n      NaN\n      {'link': 'https://www.youtube.com/embed/TZ2kBC90E1k?rel=0', 'position': 'Below'}\n      {'id': '0f3b1437-a64e-8659-fe7a-d7eef2e23552', 'isDefault': False, 'logo': {'id': '087aa0e914db2d98d3ccaaed48971da2341e24ac', 'url': 'https://imag...\n    \n    \n      4\n      40878691\n      2020-03-04T21:56:03.000Z\n      2020-04-04T13:00:00.000Z\n      Practice Nurse (RN)\n      2 X part-time practice nurses (RN) in Hobart CBD\n      {'id': 44061230, 'description': 'HEALTHPLUS MEDICAL CENTRE', 'searchParams': {'advertiserid': 44061230}}\n      {'nation': 'Australia', 'state': 'Tasmania', 'city': 'Hobart', 'area': '', 'suburb': 'hobart'}\n      1011\n      3105\n      Part Time\n      ...\n      [Which of the following statements best describes your right to work in Australia?, How many years' experience do you have as a registered nurse?]\n      <p>A well established general practice located in Hobart CBD, is seeking to employ two experienced general practice nurses (RN) to join their frie...\n      None\n      []\n      False\n      practice-registered-nurse,practice-nurse,registered-nurse,nurse\n      False\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows × 31 columns\n\n\n\n714 ads in a month; not bad\n\npd.DataFrame(objs).url.str.replace(r'\\?.*', '').nunique()\n\n714"
  },
  {
    "objectID": "notebooks/Extracting Jobs with Common Crawl.html#jora",
    "href": "notebooks/Extracting Jobs with Common Crawl.html#jora",
    "title": "skeptric",
    "section": "Jora",
    "text": "Jora\nNot captured, just search results.\nSearch results could be used to track job volume over time.\n\nobjs = list(cdx.iter('au.jora.com/job/*',\n                     from_ts='202004', to='202005',\n                     filter=['status:200']))\npd.DataFrame(objs)"
  },
  {
    "objectID": "notebooks/Searching Common Crawl Index.html",
    "href": "notebooks/Searching Common Crawl Index.html",
    "title": "skeptric",
    "section": "",
    "text": "This explores different ways of using the common crawl index\n\nComcrawl library\nCDX Toolkit\nQuerying HTTP Endpoint directly\n\nSee the related article and Jupyter notebook.\n\nimport requests\nimport warcio\nfrom contextlib import closing\nfrom bs4 import BeautifulSoup\nimport json\n\nimport logging\nfrom IPython.display import HTML\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/Searching Common Crawl Index.html#basic-usage",
    "href": "notebooks/Searching Common Crawl Index.html#basic-usage",
    "title": "skeptric",
    "section": "Basic usage",
    "text": "Basic usage\n\nr = requests.get(api_url,\n                 params = {\n                     'url': 'reddit.com',\n                     'limit': 10,\n                     'output': 'json'\n                 })\n\n\nrecords = [json.loads(line) for line in r.text.split('\\n') if line]\n\n\npd.DataFrame(records)\n\n\n\n\n\n  \n    \n      \n      urlkey\n      timestamp\n      offset\n      status\n      languages\n      digest\n      length\n      mime-detected\n      filename\n      charset\n      mime\n      url\n      redirect\n    \n  \n  \n    \n      0\n      com,reddit)/\n      20200525024432\n      873986269\n      200\n      eng\n      C6Y4VCGYLE3NGEWLJNONES6JMNA74IA3\n      40851\n      text/html\n      crawl-data/CC-MAIN-2020-24/segments/1590347387155.10/warc/CC-MAIN-20200525001747-20200525031747-00335.warc.gz\n      UTF-8\n      text/html\n      https://www.reddit.com/\n      NaN\n    \n    \n      1\n      com,reddit)/\n      20200526071834\n      787273867\n      200\n      eng\n      PHMHCKU365PLDN5UQETZVR4UGMSPDXQJ\n      42855\n      text/html\n      crawl-data/CC-MAIN-2020-24/segments/1590347390448.11/warc/CC-MAIN-20200526050333-20200526080333-00335.warc.gz\n      UTF-8\n      text/html\n      https://www.reddit.com/\n      NaN\n    \n    \n      2\n      com,reddit)/\n      20200526163829\n      3815970\n      200\n      NaN\n      X67YXUXXE5GQPMJKMEE6555BNFPIER7L\n      35345\n      text/html\n      crawl-data/CC-MAIN-2020-24/segments/1590347391277.13/robotstxt/CC-MAIN-20200526160400-20200526190400-00048.warc.gz\n      NaN\n      text/html\n      https://www.reddit.com\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      com,reddit)/\n      20200528125122\n      12374752\n      301\n      NaN\n      3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\n      616\n      application/octet-stream\n      crawl-data/CC-MAIN-2020-24/segments/1590347396089.30/crawldiagnostics/CC-MAIN-20200528104652-20200528134652-00582.warc.gz\n      NaN\n      unk\n      http://www.reddit.com/\n      https://www.reddit.com/\n    \n    \n      8\n      com,reddit)/\n      20200528125122\n      889368118\n      200\n      eng\n      7CF6J2D6SHWFD35MEQI43NNGR2W4SHHR\n      41402\n      text/html\n      crawl-data/CC-MAIN-2020-24/segments/1590347396089.30/warc/CC-MAIN-20200528104652-20200528134652-00335.warc.gz\n      UTF-8\n      text/html\n      https://www.reddit.com/\n      NaN\n    \n    \n      9\n      com,reddit)/\n      20200528192150\n      13537156\n      301\n      NaN\n      3I42H3S6NNFQ2MSVX7XZKYAYSCX5QBYJ\n      618\n      application/octet-stream\n      crawl-data/CC-MAIN-2020-24/segments/1590347399830.24/crawldiagnostics/CC-MAIN-20200528170840-20200528200840-00582.warc.gz\n      NaN\n      unk\n      http://www.reddit.com/\n      https://www.reddit.com/\n    \n  \n\n10 rows × 13 columns\n\n\n\n\nprint(pd.DataFrame(records).head().to_markdown())\n\n|    | urlkey       |      timestamp |    offset |   status | languages   | digest                           |   length | mime-detected   | filename                                                                                                           | charset   | mime      | url                     |   redirect |\n|---:|:-------------|---------------:|----------:|---------:|:------------|:---------------------------------|---------:|:----------------|:-------------------------------------------------------------------------------------------------------------------|:----------|:----------|:------------------------|-----------:|\n|  0 | com,reddit)/ | 20200525024432 | 873986269 |      200 | eng         | C6Y4VCGYLE3NGEWLJNONES6JMNA74IA3 |    40851 | text/html       | crawl-data/CC-MAIN-2020-24/segments/1590347387155.10/warc/CC-MAIN-20200525001747-20200525031747-00335.warc.gz      | UTF-8     | text/html | https://www.reddit.com/ |        nan |\n|  1 | com,reddit)/ | 20200526071834 | 787273867 |      200 | eng         | PHMHCKU365PLDN5UQETZVR4UGMSPDXQJ |    42855 | text/html       | crawl-data/CC-MAIN-2020-24/segments/1590347390448.11/warc/CC-MAIN-20200526050333-20200526080333-00335.warc.gz      | UTF-8     | text/html | https://www.reddit.com/ |        nan |\n|  2 | com,reddit)/ | 20200526163829 |   3815970 |      200 | nan         | X67YXUXXE5GQPMJKMEE6555BNFPIER7L |    35345 | text/html       | crawl-data/CC-MAIN-2020-24/segments/1590347391277.13/robotstxt/CC-MAIN-20200526160400-20200526190400-00048.warc.gz | nan       | text/html | https://www.reddit.com  |        nan |\n|  3 | com,reddit)/ | 20200526165552 | 879974740 |      200 | eng         | OSGHIVCFBI47ZSNMLG574K6SBZJ3LTBC |    39146 | text/html       | crawl-data/CC-MAIN-2020-24/segments/1590347391277.13/warc/CC-MAIN-20200526160400-20200526190400-00335.warc.gz      | UTF-8     | text/html | https://www.reddit.com/ |        nan |\n|  4 | com,reddit)/ | 20200527211917 | 858583595 |      200 | eng         | UHM2VERG5OUOELJFD7O25JVUBZVDPDLU |    35751 | text/html       | crawl-data/CC-MAIN-2020-24/segments/1590347396163.18/warc/CC-MAIN-20200527204212-20200527234212-00335.warc.gz      | UTF-8     | text/html | https://www.reddit.com/ |        nan |"
  },
  {
    "objectID": "notebooks/Searching Common Crawl Index.html#filters-and-fields",
    "href": "notebooks/Searching Common Crawl Index.html#filters-and-fields",
    "title": "skeptric",
    "section": "Filters and fields",
    "text": "Filters and fields\nLet’s use a few of the bells and whistles form the API.\nParticularly interesting are the filters which let us to only get rows that we need.\n\nr = requests.get(api_url,\n                 params = {\n                     'url': 'https://www.reddit.com/r/',\n                     'matchType': 'prefix',\n                     'limit': 10,\n                     'output': 'json',\n                     'fl': 'url,filename,offset,length',\n                     'filter': ['=status:200', '=mime-detected:text/html', '~url:.*/comments/']\n                 })\n\n\nr.raise_for_status()\n\n\npd.DataFrame([json.loads(line) for line in r.text.split('\\n') if line])\n\n\n\n\n\n  \n    \n      \n      url\n      filename\n      offset\n      length\n    \n  \n  \n    \n      0\n      https://www.reddit.com/r/0xbitcoin/comments/8o06dk/links_to_the_newestbest_miners_for_nvidia_amd/\n      crawl-data/CC-MAIN-2020-24/segments/1590347401260.16/warc/CC-MAIN-20200529023731-20200529053731-00112.warc.gz\n      873475618\n      30260\n    \n    \n      1\n      https://www.reddit.com/r/100yearsago/comments/ghkkz1/may_11th_1920_first_nsdap_advertising_posters_in/?ref_source=embed&ref=share\n      crawl-data/CC-MAIN-2020-24/segments/1590347392142.20/warc/CC-MAIN-20200527075559-20200527105559-00198.warc.gz\n      880229230\n      32606\n    \n    \n      2\n      https://www.reddit.com/r/2007scape/comments/6250um/thinking_about_returning_to_osrs/\n      crawl-data/CC-MAIN-2020-24/segments/1590347391923.3/warc/CC-MAIN-20200526222359-20200527012359-00533.warc.gz\n      895963534\n      24631\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      https://www.reddit.com/r/2darkpark/comments/frm60b/so_how_is_everyone_doing/\n      crawl-data/CC-MAIN-2020-24/segments/1590348492295.88/warc/CC-MAIN-20200604223445-20200605013445-00195.warc.gz\n      851246292\n      17847\n    \n    \n      8\n      https://www.reddit.com/r/2healthbars/comments/8tg1y7/the_heel_of_these_heels_are_heels/\n      crawl-data/CC-MAIN-2020-24/segments/1590347415315.43/warc/CC-MAIN-20200601071242-20200601101242-00439.warc.gz\n      855249693\n      31284\n    \n    \n      9\n      https://www.reddit.com/r/3Dprinting/comments/3pf96w/troubleshooting_proximity_sensor/\n      crawl-data/CC-MAIN-2020-24/segments/1590347445880.79/warc/CC-MAIN-20200604161214-20200604191214-00560.warc.gz\n      842910073\n      22084\n    \n  \n\n10 rows × 4 columns"
  },
  {
    "objectID": "notebooks/Searching Common Crawl Index.html#pagination",
    "href": "notebooks/Searching Common Crawl Index.html#pagination",
    "title": "skeptric",
    "section": "Pagination",
    "text": "Pagination\nThe introductory blog post to CDX on Common Crawl mentions it’s paginated to 15,000 results by default.\nLet’s test that\n\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'showNumPages': True,\n                 })\n\n\npageSize is number of results in (compressed) blocks\nblocks is total number of compressed blocks\npages = (blocks // page_size)\n\n\nnum_pages = r.json()\nnum_pages\n\n{'pageSize': 5, 'blocks': 2044, 'pages': 409}\n\n\n\nimport math\n\n\nmath.ceil(num_pages['blocks'] / num_pages['pageSize']) == num_pages['pages']\n\nTrue\n\n\n\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                 })\n\n\nresults = [json.loads(line) for line in r.text.split('\\n') if line]\n\nThe history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n\n\n\nlen(results)\n\n14735\n\n\n\nresults[-1]\n\n{'urlkey': 'org,wikipedia,ace)/wiki/geurija_katolik_roma',\n 'timestamp': '20200606214423',\n 'offset': '227178899',\n 'status': '200',\n 'languages': 'nno,roh,srp',\n 'digest': 'SH3WZL442PB2DKYVIFADVHJU6JC2THSA',\n 'length': '18538',\n 'mime-detected': 'text/html',\n 'filename': 'crawl-data/CC-MAIN-2020-24/segments/1590348519531.94/warc/CC-MAIN-20200606190934-20200606220934-00311.warc.gz',\n 'charset': 'UTF-8',\n 'mime': 'text/html',\n 'url': 'https://ace.wikipedia.org/wiki/Geurija_Katolik_Roma'}\n\n\nWe can adjust the pageSize (in blocks) as well\n\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'page': 3,\n                     'pageSize': 1,\n                 })\n\n\nresults2 = [json.loads(line) for line in r.text.split('\\n') if line]\n\nAbout 3,000 results per page\n\nlen(results2)\n\n3000\n\n\n\nresults[0]\n\n{'urlkey': 'org,wikipedia)/',\n 'timestamp': '20200524210621',\n 'offset': '3147602',\n 'status': '301',\n 'digest': 'C4WTJB6KZKE6XGJGU4MBB2U4ON7YIZTW',\n 'redirect': 'https://www.wikipedia.org/',\n 'length': '938',\n 'mime-detected': 'text/html',\n 'filename': 'crawl-data/CC-MAIN-2020-24/segments/1590347385193.5/robotstxt/CC-MAIN-20200524210325-20200525000325-00349.warc.gz',\n 'mime': 'text/html',\n 'url': 'https://wikipedia.org'}\n\n\nThis should correspond to the 3rd fifth of results\n\n[r for r in results2 if r not in results]\n\n[]\n\n\nGoing past the last page\n\nr = requests.get(api_url,\n                 params = {\n                     'url': '*.wikipedia.org',\n                     'output': 'json',\n                     'page': 409,\n                 })\n\n\nr.status_code\n\n400\n\n\n\nprint(r.text)\n\n<!DOCTYPE html>\n<html>\n<head>\n<link rel=\"stylesheet\" href=\"/static/__shared/shared.css\"/>\n</head>\n<body>\n<h2>Common Crawl Index Server Error</h2>\n<b>Page 409 invalid: First Page is 0, Last Page is 408</b>\n\n</body>\n</html>"
  },
  {
    "objectID": "notebooks/Searching Common Crawl Index.html#an-empty-request",
    "href": "notebooks/Searching Common Crawl Index.html#an-empty-request",
    "title": "skeptric",
    "section": "An empty request",
    "text": "An empty request\n\nr = requests.get(api_url,\n                 params = {\n                     'url': 'skeptric.com/*',\n                     'output': 'json',\n                 })\n\n\nr.status_code\n\n404\n\n\n\nr.json()\n\n{'error': 'No Captures found for: skeptric.com/*'}"
  },
  {
    "objectID": "notebooks/building-layered-api-with-fashion-mnist.html",
    "href": "notebooks/building-layered-api-with-fashion-mnist.html",
    "title": "skeptric",
    "section": "",
    "text": "Tags: python, data, fastai\ndate: 2022-05-23T19:05:13+10:00\nfeature_image: /images/fashion_mnist_training_loop.jpg\n\nWe’re going to build up a simple Deep Learning API inspired by fastai on Fashion MNIST from scratch. Humans can only fit so many things in their head at once (somewhere between 3 and 7); trying to grasp all the details of the training loop at once is difficult, especially as we add more features to it. The right abstractions can make this much easier by only having to think about what we’re changing in the interface. Coming up with a good abstraction that generalises across many usecases is hard, so we’re going to use the fast.ai interface.\nThen to show how it’s useful once we have our training loop we’ll see how we can change the model and retrain.\nThis post was generated with a Jupyter notebook. You can also view this notebook on Kaggle or download the Jupyter notebook."
  },
  {
    "objectID": "notebooks/building-layered-api-with-fashion-mnist.html#running-metrics",
    "href": "notebooks/building-layered-api-with-fashion-mnist.html#running-metrics",
    "title": "skeptric",
    "section": "Running metrics",
    "text": "Running metrics\nIf we want to evaluate on large datasets we need a way to accumulate the metric over minibatches.\nHow this is framed is surprisingly non-standard; we’ll keep to the spirit of fastai (but not the implementation which uses callbacks), but there’s also an external library torchmetrics, and huggingface have a different concept of Metric.\n\nds_valid = list(zip(X_valid, y_valid))\ndl_valid = DataLoader(ds_valid, batch_size=2048, shuffle=False)\n\n\naccuracy(model(X_valid), y_valid)\n\ntensor(0.6303)\n\n\nTo calculate a running metric we’ll just add the accuracy, weighted by the size of the sample, divided by the length of the dataset.\n\ndef running(metrics, dl, model):\n    values = [0.] * len(metrics)\n    N = len(dl)\n    \n    for X, y in dl:\n        pred = model(X)\n        for idx, metric in enumerate(metrics):\n            values[idx] += metric(pred, y) * len(X) / N\n    return [v.item() if hasattr(v, 'item') else v for v in values]\n\nThis gives a similar result to before.\nHere we track:\n\nloss on the training set\nloss on validation set\naccuracy\n\nIdeally we’d calculate a running total of loss on the training set, but this is good enough for now.\n\nmodel.reset_parameters()\n\n# Note: reset creates new parameters. Could we update them instead?\noptim = SGD(model.parameters(), lr=0.2)\n\nds_train = list(zip(X_train, y_train))\ndl_train = DataLoader(ds_train, batch_size=2048, shuffle=True)\n\nmetrics = [cross_entropy, accuracy]\n\nfor epoch in range(5):\n    for X, y in dl_train:\n        pred = model(X)\n        loss = cross_entropy(pred, y)\n        loss.backward()\n        with no_grad():\n            optim.step()\n            optim.zero_grad()\n    \n    print(epoch, *running([cross_entropy], dl_train, model), *running(metrics, dl_valid, model))\n\n0 2.9654173851013184 2.9218735694885254 0.5206032991409302\n1 2.110874652862549 2.095884323120117 0.5787869095802307\n2 1.7752487659454346 1.7723487615585327 0.6119169592857361\n3 1.5830714702606201 1.5869991779327393 0.6269984841346741\n4 1.4491684436798096 1.4574403762817383 0.6405142545700073"
  },
  {
    "objectID": "notebooks/building-layered-api-with-fashion-mnist.html#learner",
    "href": "notebooks/building-layered-api-with-fashion-mnist.html#learner",
    "title": "skeptric",
    "section": "Learner",
    "text": "Learner\nWe can now package up all our objects into a single class, our Learner.\n\nfrom dataclasses import dataclass\nfrom typing import Callable, List\n\nclass Learner:\n    def __init__(self,\n                 dl_train: DataLoader,\n                 dl_valid: DataLoader,\n                 loss_func: Callable,\n                 model: Callable,\n                 lr: float,\n                 opt_func: Callable = SGD,\n                 metrics: List[Callable] = None):\n        self.dl_train = dl_train\n        self.dl_valid = dl_valid\n        self.loss_func = loss_func\n        self.model = model\n        self.lr = lr\n        self.optim = opt_func(model.parameters(), lr)\n        self.metrics = metrics if metrics is not None else [loss_func]\n        \n    def reset(self):\n        self.model.reset_parameters()\n    \n    \n    def fit(self, n_epoch):\n        for epoch in range(n_epoch):\n            for X, y in self.dl_train:\n                pred = self.model(X)\n                loss = self.loss_func(pred, y)\n                loss.backward()\n                with no_grad():\n                    self.optim.step()\n                    self.optim.zero_grad()\n                    \n            print(epoch, *running([self.loss_func], self.dl_train, self.model), *running(self.metrics, self.dl_valid, self.model))\n\n\nmetrics = [cross_entropy, accuracy]\n\nlearn = Learner(dl_train, dl_valid, cross_entropy, model, 0.2, SGD, metrics)\n\nWe can now train this for a bunch of epochs, getting around 74% accuracy.\n\nmodel.reset_parameters()\n\nlearn.fit(40)\n\n0 3.2126216888427734 3.283143997192383 0.5618098378181458\n1 2.278181314468384 2.346566915512085 0.5942805409431458\n2 1.8557775020599365 1.910788655281067 0.6112576127052307\n3 1.6559522151947021 1.710391640663147 0.6208175420761108\n4 1.5421751737594604 1.5935988426208496 0.634827733039856\n5 1.3805128335952759 1.4289369583129883 0.6319432854652405\n6 1.3306803703308105 1.3788357973098755 0.6361463665962219\n7 1.2377548217773438 1.2862215042114258 0.6534531116485596\n8 1.2100512981414795 1.2579960823059082 0.6574913263320923\n9 1.152338981628418 1.19868803024292 0.6574089527130127\n10 1.1125177145004272 1.159442663192749 0.6635899543762207\n11 1.083337426185608 1.1281803846359253 0.6678754091262817\n12 1.0920311212539673 1.137237787246704 0.6494148373603821\n13 1.0610631704330444 1.1104097366333008 0.6719960570335388\n14 1.0184332132339478 1.0690116882324219 0.6782594323158264\n15 1.0005489587783813 1.0493626594543457 0.668452262878418\n16 1.0299843549728394 1.076596736907959 0.6588099598884583\n17 0.9529225826263428 1.0024800300598145 0.6869127750396729\n18 0.9454663991928101 0.9934449195861816 0.6875721216201782\n19 0.9428024291992188 0.9916077852249146 0.6855117678642273\n20 1.0193240642547607 1.0734479427337646 0.6656501889228821\n21 0.9028168320655823 0.9518789052963257 0.6953189373016357\n22 0.8912039399147034 0.9412692785263062 0.6947420239448547\n23 0.8915138244628906 0.9431529641151428 0.6955661773681641\n24 0.9840039014816284 1.0355738401412964 0.6888906955718994\n25 0.8699730634689331 0.9195016622543335 0.6972144246101379\n26 0.8723984956741333 0.9201793670654297 0.7049612998962402\n27 0.8572273254394531 0.9069074392318726 0.7002636790275574\n28 0.8710482716560364 0.9210841655731201 0.6866655945777893\n29 0.9071570634841919 0.956194281578064 0.6764463186264038\n30 0.8600081205368042 0.9094617366790771 0.6917752027511597\n31 0.825920045375824 0.8766304850578308 0.7071864008903503\n32 0.8200230598449707 0.8686937093734741 0.7075160145759583\n33 0.8196104168891907 0.8700741529464722 0.7092467546463013\n34 0.8026244044303894 0.8533946871757507 0.7150980234146118\n35 0.8220050930976868 0.8715838193893433 0.7010878324508667\n36 0.7956870794296265 0.8447741270065308 0.7160870432853699\n37 0.7948266863822937 0.8461573123931885 0.7127904891967773\n38 0.8521597981452942 0.899774432182312 0.7057029604911804\n39 0.7868410348892212 0.8378427624702454 0.7184770107269287"
  },
  {
    "objectID": "notebooks/reading_parquet_metadata.html",
    "href": "notebooks/reading_parquet_metadata.html",
    "title": "skeptric",
    "section": "",
    "text": "Apache Parquet is a common output format for distributed data pipelines, for example Spark and Presto. The files contain a lot of metadata about the contents of the file which can be useful to understand the data before querying it.\nIn our usecase we’re going to get metadata from the Common Crawl Columnar Index to help find which Parquet files contain an index to specific URLs.\n\n\nPyarrow provides excellent support for Parquet and makes it easy to read the metadata.\n\nimport pyarrow.dataset as ds\nimport pyarrow as pa\n\nIt only takes 16s to find all the partition data. (Note that you will need an Amazon Web Services account configured to run this because it requires authentication).\n\n%%time\ncc_index_s3_path = 's3://commoncrawl/cc-index/table/cc-main/warc/'\n\ncc_index = ds.dataset(cc_index_s3_path, format='parquet', partitioning='hive')\n\nCPU times: user 3.14 s, sys: 512 ms, total: 3.65 s\nWall time: 16.9 s\n\n\nWe can access individual files.\n\ncc_index.files[-1]\n\n'commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-05/subset=warc/part-00299-1e2959d8-5649-433a-b76e-f1b876a6479d.c000.gz.parquet'\n\n\nThere’s also the get_fragments interface which returns pointers to the original files, but has additional methods and can be filtered.\nFor example we can list all the WARC files.\n\n%%time\nfragments = list(cc_index.get_fragments(filter=ds.field('subset') == 'warc'))\nn_warc = len(fragments)\nn_warc\n\nCPU times: user 138 ms, sys: 3.61 ms, total: 142 ms\nWall time: 145 ms\n\n\n25193\n\n\nHere we just get the fragments for the WARC data from the 2022-05 crawl; which is 300 Parquet files.\n\n%%time\nfragments = list(cc_index.get_fragments(filter=(ds.field('crawl') == 'CC-MAIN-2022-05') &\n                                        (ds.field('subset') == 'warc')))\nlen(fragments)\n\nCPU times: user 7.51 ms, sys: 716 µs, total: 8.23 ms\nWall time: 8.22 ms\n\n\n300\n\n\nEach fragment consists of an individual parquet file.\n\nfragments[0].path\n\n'commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2022-05/subset=warc/part-00000-1e2959d8-5649-433a-b76e-f1b876a6479d.c000.gz.parquet'\n\n\nA Parquet file is split into row groups.\n\nfragments[0].row_groups\n\n[RowGroupInfo(0),\n RowGroupInfo(1),\n RowGroupInfo(2),\n RowGroupInfo(3),\n RowGroupInfo(4),\n RowGroupInfo(5),\n RowGroupInfo(6),\n RowGroupInfo(7),\n RowGroupInfo(8),\n RowGroupInfo(9),\n RowGroupInfo(10)]\n\n\nThese row groups contain some statistics. In particular it’s approximately sorted by url_surtkey; if we are looking for a particular URL we can exclude row groups where the URL isn’t between the min and max values.\n\nfragments[0].row_groups[0].statistics\n\n{'url_surtkey': {'min': 'com,wordpress,freefall852)/2016/03/29/billy-guy',\n  'max': 'com,worldpackers)/search/skill_hospitality_entertainment/type_hotel?location_categories[]=nature&location_types[]=hotel&min_meals_count[]=3&months[]=11&skills[]=music'},\n 'url': {'min': 'http://03.worldchefsbible.com/',\n  'max': 'https://zh.worldallianceofdramatherapy.com/he-mission'},\n 'url_host_name': {'min': '03.worldchefsbible.com',\n  'max': 'zr1.worldblast.com'},\n 'url_host_tld': {'min': 'com', 'max': 'com'},\n 'url_host_2nd_last_part': {'min': 'wordpress', 'max': 'worldpackers'},\n 'url_host_3rd_last_part': {'min': '03', 'max': 'zr1'},\n 'url_host_4th_last_part': {'min': 'bbbfoundation', 'max': 'www'},\n 'url_host_5th_last_part': {'min': 'http', 'max': 'toolbox'},\n 'url_host_registry_suffix': {'min': 'com', 'max': 'com'},\n 'url_host_registered_domain': {'min': 'wordpress.com',\n  'max': 'worldpackers.com'},\n 'url_host_private_suffix': {'min': 'com', 'max': 'com'},\n 'url_host_private_domain': {'min': 'wordpress.com',\n  'max': 'worldpackers.com'},\n 'url_host_name_reversed': {'min': 'com.wordpress.freefall852',\n  'max': 'com.worldpackers.www'},\n 'url_protocol': {'min': 'http', 'max': 'https'},\n 'url_port': {'min': 443, 'max': 2000},\n 'url_path': {'min': '',\n  'max': '/▶-if-i-unblock-someone-on-whatsapp-will-they-find-out/'},\n 'url_query': {'min': '', 'max': 'zh-cn'},\n 'fetch_time': {'min': datetime.datetime(2022, 1, 16, 9, 32, 20, tzinfo=<UTC>),\n  'max': datetime.datetime(2022, 1, 29, 15, 10, 45, tzinfo=<UTC>)},\n 'fetch_status': {'min': 200, 'max': 200},\n 'content_digest': {'min': '2223CG5SRUGFII5CRWMUD2766UWOL7MU',\n  'max': 'ZZZZKJBCLHGPU4P2LIHR3X4PZOZSJ4SV'},\n 'content_mime_type': {'min': 'Application/Unknown', 'max': 'video/x-ms-asf'},\n 'content_mime_detected': {'min': 'application/atom+xml',\n  'max': 'video/x-ms-asf'},\n 'content_charset': {'min': 'Big5', 'max': 'x-windows-949'},\n 'content_languages': {'min': 'afr', 'max': 'zho,xho,eng'},\n 'content_truncated': {'min': 'disconnect', 'max': 'length'},\n 'warc_filename': {'min': 'crawl-data/CC-MAIN-2022-05/segments/1642320299852.23/warc/CC-MAIN-20220116093137-20220116123137-00000.warc.gz',\n  'max': 'crawl-data/CC-MAIN-2022-05/segments/1642320306346.64/warc/CC-MAIN-20220128212503-20220129002503-00719.warc.gz'},\n 'warc_record_offset': {'min': 932, 'max': 1202361748},\n 'warc_record_length': {'min': 489, 'max': 1049774},\n 'warc_segment': {'min': '1642320299852.23', 'max': '1642320306346.64'}}\n\n\n\nfragments[0].row_groups[0].id\n\n0\n\n\n\nfragments[0].row_groups[0].num_rows\n\n1730100\n\n\n\n(25_000 /3) / 60\n\n138.88888888888889\n\n\nWe can go ahead an iterate through all the fragments and extract the row-group data.\nAround 2-3/second.\n\nfrom tqdm.auto import tqdm\nfrom time import time\n\nN = 20\nstart_time = time()\n\nrow_group_statistics = []\nfor i, f in tqdm(enumerate(fragments), total=len(fragments)):\n    for row_group in f.row_groups:\n        row_group_statistics.append(\n            {'bucket': f.path.split('/', maxsplit=1)[0],\n             'key': f.path.split('/', maxsplit=1)[1],\n             'id': row_group.id,\n             'num_rows': row_group.num_rows,\n             'min_url_surtkey': row_group.statistics['url_surtkey']['min'],\n             'max_url_surtkey': row_group.statistics['url_surtkey']['min'],\n            })\n    if i >= N:\n        break\n        \nelapsed_time = time() - start_time\nelapsed_time\n\n\n\n\n7.918716907501221\n\n\nProcessing all the files would take around this many minutes:\n\n(elapsed_time * n_warc / N) / 60\n\n166.2468625422319\n\n\nIt seems to take an unusually long time to read the row_group statistics.\n\n\n\nFastparquet is another system to read Parquet files creates by the Dask Project.\nFor remote files we need to pass the fsspec filesystem; in this case using s3fs.\n\nfrom fastparquet import ParquetFile\nimport s3fs\n\nfs = s3fs.S3FileSystem()\n\nIt’s quite slow and takes seconds to even access a single file (this just reads the metadata, no data is loaded). It seems like we can pass the whole cc_index_s3_path, but it would take prohibitively long to process. Perhaps this would be better if the compute was located closer to the data (this is being run on a laptop in Australia).\n\n%%time\npf = ParquetFile(fn=fragments[0].path, fs=fs)\n\nCPU times: user 176 ms, sys: 56.6 ms, total: 232 ms\nWall time: 2.33 s\n\n\nWe can access all the statistics through the fmd attribue (file meta data).\n\npf.fmd.row_groups[0].columns[0].meta_data._asdict()\n\n{'type': 6,\n 'encodings': [0, 4],\n 'path_in_schema': ['url_surtkey'],\n 'codec': 2,\n 'num_values': 1730100,\n 'total_uncompressed_size': 117917394,\n 'total_compressed_size': 23113472,\n 'key_value_metadata': None,\n 'data_page_offset': 4,\n 'index_page_offset': None,\n 'dictionary_page_offset': None,\n 'statistics': {'max': None,\n  'min': None,\n  'null_count': 0,\n  'distinct_count': None,\n  'max_value': \"b'com,worldpackers)/search/skill_hospitality_entertainment/type_hotel?location_categories[]=nature&location_types[]=hotel&min_meals_count[]=3&months[]=11&skills[]=music'\",\n  'min_value': \"b'com,wordpress,freefall852)/2016/03/29/billy-guy'\"},\n 'encoding_stats': [{'page_type': 0, 'encoding': 0, 'count': 122}],\n 'bloom_filter_offset': None}"
  },
  {
    "objectID": "notebooks/reading_parquet_metadata.html#getting-the-file-metadata",
    "href": "notebooks/reading_parquet_metadata.html#getting-the-file-metadata",
    "title": "skeptric",
    "section": "Getting the file metadata",
    "text": "Getting the file metadata\nReading in the whole file is going to be slow, so we need to work out where the end of the file is; that is we need to know the file’s length. For a start we’re going to need a way of working out where the end of the file is; that is it’s length.\nWe can use a HTTP HEAD request, which should return the Content-Length; the size of the file in bytes (8 bits).\n\n%time metadata = s3.head_object(Bucket=bucket, Key=key)\n\nCPU times: user 27.4 ms, sys: 7.98 ms, total: 35.4 ms\nWall time: 1.01 s\n\n\n\nmetadata\n\n{'ResponseMetadata': {'RequestId': '6AWKV1DJNRTEER3W',\n  'HostId': 'YtP3flNoav+Ht8NgTRWGNrbeXXvkPbpH2+C1Xo8m2gcD4e9gbZQBazEVnozbS0qEdMXZg4XvQ1g=',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amz-id-2': 'YtP3flNoav+Ht8NgTRWGNrbeXXvkPbpH2+C1Xo8m2gcD4e9gbZQBazEVnozbS0qEdMXZg4XvQ1g=',\n   'x-amz-request-id': '6AWKV1DJNRTEER3W',\n   'date': 'Mon, 18 Apr 2022 12:23:58 GMT',\n   'last-modified': 'Sun, 30 Jan 2022 05:01:16 GMT',\n   'etag': '\"45ca767a5b7c8226dd75be1b8bb525f0\"',\n   'x-amz-storage-class': 'INTELLIGENT_TIERING',\n   'accept-ranges': 'bytes',\n   'content-type': 'application/octet-stream',\n   'server': 'AmazonS3',\n   'content-length': '1344281529'},\n  'RetryAttempts': 0},\n 'AcceptRanges': 'bytes',\n 'LastModified': datetime.datetime(2022, 1, 30, 5, 1, 16, tzinfo=tzutc()),\n 'ContentLength': 1344281529,\n 'ETag': '\"45ca767a5b7c8226dd75be1b8bb525f0\"',\n 'ContentType': 'application/octet-stream',\n 'Metadata': {},\n 'StorageClass': 'INTELLIGENT_TIERING'}\n\n\nThe whole file is quite large to read in at once, and there are 300 of them!\n\ncontent_length = int(metadata['ContentLength'])\nf'{content_length / (1024**3):0.1f} GB'\n\n'1.3 GB'\n\n\nWe can just read in the last 8 bytes by passing Range to get_object, which under the hood is using a HTTP Range Requst.\n\nend_byte = content_length\nstart_byte = end_byte - 8\n\n\n%%time\nresponse = s3.get_object(Bucket=bucket, Key=key, Range=f'bytes={start_byte}-{end_byte}')\nend_content = response['Body'].read()\nend_content\n\nCPU times: user 11.3 ms, sys: 0 ns, total: 11.3 ms\nWall time: 258 ms\n\n\nb'o\\xe5\\x00\\x00PAR1'\n\n\nThe end is the magic number for the Parquet format.\n\nassert end_content[-4:] == b'PAR1'\n\nThis is preceeded by the length of the metadata in bytes.\n\nfile_meta_length = int.from_bytes(end_content[:4], byteorder='little')\nf'{file_meta_length / 1024:0.1f} kb'\n\n'57.4 kb'\n\n\nNow we know how long the metadata is we can read it from the file.\n\nend_byte = content_length - 8\nstart_byte = content_length - 8 - file_meta_length\n\nNote this takes just a little longer than reading 2 bytes; there’s a relatively high constant overhead per request.\n\n%%time\nresponse = s3.get_object(Bucket=bucket, Key=key, Range=f'bytes={start_byte}-{end_byte}')\nfile_meta_content = response['Body'].read()\n\nCPU times: user 5.33 ms, sys: 4.37 ms, total: 9.7 ms\nWall time: 496 ms"
  },
  {
    "objectID": "notebooks/reading_parquet_metadata.html#decoding-file-metadata",
    "href": "notebooks/reading_parquet_metadata.html#decoding-file-metadata",
    "title": "skeptric",
    "section": "Decoding file metadata",
    "text": "Decoding file metadata\nThe Apache Parquet metadata documentation shows the format of the file metadata, which we can parse.\n\n\n\nimage.png\n\n\nThis is pretty involved, so I’ll cheat and use the good work of fastparquet to read the metadata for me.\n\nfrom fastparquet.cencoding import from_buffer\n\nfmd = from_buffer(file_meta_content, \"FileMetaData\")\n\nIt’s using Apache Thrift for the file metadata.\n\ntype(fmd)\n\nfastparquet.cencoding.ThriftObject\n\n\nWe can then extract these fields as attributes; like the version\n\ndir(fmd)\n\n['column_orders',\n 'created_by',\n 'encryption_algorithm',\n 'footer_signing_key_metadata',\n 'key_value_metadata',\n 'num_rows',\n 'row_groups',\n 'schema',\n 'version']\n\n\n\nfmd.version\n\n1\n\n\nOr the schema\n\nfmd.schema[0]\n\n{'type': None, 'type_length': None, 'repetition_type': None, 'name': \"b'spark_schema'\", 'num_children': 30, 'converted_type': None, 'scale': None, 'precision': None, 'field_id': None, 'logicalType': None}\n\n\n\nfmd.schema[1]\n\n{'type': 6, 'type_length': None, 'repetition_type': 0, 'name': \"b'url_surtkey'\", 'num_children': None, 'converted_type': 0, 'scale': None, 'precision': None, 'field_id': None, 'logicalType': {'STRING': {}, 'MAP': None, 'LIST': None, 'ENUM': None, 'DECIMAL': None, 'DATE': None, 'TIME': None, 'TIMESTAMP': None, 'INTEGER': None, 'UNKNOWN': None, 'JSON': None, 'BSON': None, 'UUID': None}}\n\n\nAnd the row groups\n\nfmd.row_groups[0].total_byte_size\n\n452304596\n\n\n\nfmd.row_groups[0].columns[0].meta_data._asdict()\n\n{'type': 6,\n 'encodings': [0, 4],\n 'path_in_schema': ['url_surtkey'],\n 'codec': 2,\n 'num_values': 1730100,\n 'total_uncompressed_size': 117917394,\n 'total_compressed_size': 23113472,\n 'key_value_metadata': None,\n 'data_page_offset': 4,\n 'index_page_offset': None,\n 'dictionary_page_offset': None,\n 'statistics': {'max': None,\n  'min': None,\n  'null_count': 0,\n  'distinct_count': None,\n  'max_value': \"b'com,worldpackers)/search/skill_hospitality_entertainment/type_hotel?location_categories[]=nature&location_types[]=hotel&min_meals_count[]=3&months[]=11&skills[]=music'\",\n  'min_value': \"b'com,wordpress,freefall852)/2016/03/29/billy-guy'\"},\n 'encoding_stats': [{'page_type': 0, 'encoding': 0, 'count': 122}],\n 'bloom_filter_offset': None}\n\n\nAnd the location of each column\n\nfmd.row_groups[1].columns[0].file_offset\n\n134217728"
  },
  {
    "objectID": "notebooks/reading_parquet_metadata.html#putting-it-together",
    "href": "notebooks/reading_parquet_metadata.html#putting-it-together",
    "title": "skeptric",
    "section": "Putting it together",
    "text": "Putting it together\nWe can combine all of this into a single function.\n\ndef parquet_metadata_s3(path, s3):\n    metadata = s3.head_object(Bucket=bucket, Key=key)\n    content_length = int(metadata['ContentLength'])\n    \n    end_response = s3.get_object(Bucket=bucket, Key=key, Range=f'bytes={content_length-8}-{content_length}')\n    end_content = end_response['Body'].read()\n    \n    if end_content[-4:] != b'PAR1':\n        raise ValueError('File at %s does not look like a Parquet file; magic %s' % (path, end_content[-4:]))\n    file_meta_length = int.from_bytes(end_content[:4], byteorder='little')\n\n    file_meta_response = s3.get_object(Bucket=bucket, Key=key,\n                                       Range=f'bytes={content_length-8-file_meta_length}-{content_length-8}')\n    file_meta_content = file_meta_response['Body'].read()\n    \n    fmd = from_buffer(file_meta_content, \"FileMetaData\")\n    \n    return fmd\n\nThis is quicker than fastparquet (although perhaps doing less), and a little faster than pyarrow.\n\n%time fmd = parquet_metadata_s3(file, s3)\n\nCPU times: user 22 ms, sys: 258 µs, total: 22.2 ms\nWall time: 1.02 s"
  },
  {
    "objectID": "notebooks/reading_parquet_metadata.html#using-http",
    "href": "notebooks/reading_parquet_metadata.html#using-http",
    "title": "skeptric",
    "section": "Using HTTP",
    "text": "Using HTTP\nCommon Crawl also exposes a HTTP Interface to the S3 buckets hosted using AWS CloudFront. We can try to access those instead.\n\nUsing Pyarrow\nPyarrow can read the files from anything that supports fsspec, and so we can pass a HTTPFileSystem.\n\nfrom fsspec.implementations.http import HTTPFileSystem\n\nhttp = HTTPFileSystem()\n\nHowever it can’t discover the partitions and files because there is no way to list them over HTTP; only the individual files can be accessed. We can manually pass the files however.\n\nhttp_files = ['https://data.commoncrawl.org/' + x.split('/', 1)[1] for x in cc_index.files]\nhttp_files[0]\n\n'https://data.commoncrawl.org/cc-index/table/cc-main/warc/crawl=CC-MAIN-2013-20/subset=warc/part-00000-6ac52f25-05a1-4998-adf1-b8c830c08eec.c000.gz.parquet'\n\n\n\n%%time\ncc_index_http = ds.dataset(http_files, format='parquet', partitioning='hive', filesystem=http)\n\nCPU times: user 1.16 s, sys: 110 ms, total: 1.27 s\nWall time: 2.97 s\n\n\n\n%%time\nfragments_http = list(cc_index_http.get_fragments())\n\nCPU times: user 296 ms, sys: 0 ns, total: 296 ms\nWall time: 294 ms\n\n\nThis runs at a similar rate to S3.\n\nfrom tqdm.auto import tqdm\nfrom time import time\n\nN = 20\nstart_time = time()\n\nrow_group_statistics = []\nfor i, f in tqdm(enumerate(fragments_http), total=len(fragments_http)):\n    for row_group in f.row_groups:\n        row_group_statistics.append(\n            {'bucket': f.path.split('/', maxsplit=1)[0],\n             'key': f.path.split('/', maxsplit=1)[1],\n             'id': row_group.id,\n             'num_rows': row_group.num_rows,\n             'min_url_surtkey': row_group.statistics['url_surtkey']['min'],\n             'max_url_surtkey': row_group.statistics['url_surtkey']['min'],\n            })\n    if i >= N:\n        break\n        \nelapsed_time = time() - start_time\nelapsed_time\n\n\n\n\n31.016972541809082\n\n\n\n\nAccessing HTTP endpoint directly\nThis is essentially the same as the S3 Boto version.\n\nimport requests\n\ndef parquet_metadata_http(url, session=requests):\n    metadata = session.head(url)\n    metadata.raise_for_status()\n    content_length = int(metadata.headers['Content-Length'])\n    \n    end_response = requests.get(url, headers={\"Range\": f'bytes={content_length-8}-{content_length}'})\n    end_response.raise_for_status()\n    end_content = end_response.content\n    \n    if end_content[-4:] != b'PAR1':\n        raise ValueError('File at %s does not look like a Parquet file; magic %s' % (path, end_content[-4:]))\n    file_meta_length = int.from_bytes(end_content[:4], byteorder='little')\n\n    file_meta_response = requests.get(url, headers={\"Range\": f'bytes={content_length-8-file_meta_length}-{content_length-8}'})\n    file_meta_response.raise_for_status()\n    file_meta_content = file_meta_response.content\n    \n    fmd = from_buffer(file_meta_content, \"FileMetaData\")\n    \n    return fmd\n\nThe result is a similar speed to the S3 version.\n\n%time fmd = parquet_metadata_http(http_files[0])\n\nCPU times: user 91.9 ms, sys: 0 ns, total: 91.9 ms\nWall time: 1.59 s"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html",
    "title": "skeptric",
    "section": "",
    "text": "Chapter 4 of the fastai book covers how to build a Neural Network for distinguishing 3s and 7s on MNIST from scratch. We’re going to do a similar thing but instead of building the neural network from the ground up we’re going to use fastai’s layered API to build it top down. We’ll start with the high level API to train a dense neural network in a few lines. Then we’ll redo the problem going deeper and deeper into the API. At the very core it’s mainly PyTorch, and we’ll have a pure PyTorch implementation like in the book. Then we’ll start rebuilding the abstractions from scratch to get a high level API like we started with.\nInstead of using the MNIST digits we’ll use Fashion MNIST, which contains little black and white images of different types of clothing. This is a bit harder and a convolutional neural network would perform better here (as demonstrated in v3 of fastai course). But to keep things simple we’ll use a dense neural network."
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#what-did-we-just-do",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#what-did-we-just-do",
    "title": "skeptric",
    "section": "What did we just do?",
    "text": "What did we just do?\nLet’s go back through those 5 lines slowly to see what was going on.\n\n1. Import\nThe first line imports all the libraries we need for tabular analysis.\nThis includes specific fastai libraries, as well as general utilities such as Pandas, numpy and PyTorch, and much more\n\nfrom fastai.tabular.all import *\n\nIf you want to see exactly what was imported you can look into the module or the source code.\n\nimport fastai.tabular.all\nL(dir(fastai.tabular.all))\n\n(#846) ['APScoreBinary','APScoreMulti','AccumMetric','ActivationStats','Adam','AdaptiveAvgPool','AdaptiveConcatPool1d','AdaptiveConcatPool2d','ArrayBase','ArrayImage'...]\n\n\nThis includes standard imports like “pandas as pd”\n\nfastai.tabular.all.pd\n\n<module 'pandas' from '/opt/conda/lib/python3.7/site-packages/pandas/__init__.py'>\n\n\n\n\n2. Data\nWe read in the data from Pandas as a CSV, letting Pandas know that the label column is categorical.\n\ndf = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv', dtype={'label':'category'})\n\nThe dataframe contains a label column giving the kind of image, and then 784 columns for the pixel value from 0-255.\n\ndf\n\n\n\n\n\n  \n    \n      \n      label\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      pixel9\n      ...\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n      pixel784\n    \n  \n  \n    \n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      5\n      0\n      ...\n      0\n      0\n      0\n      30\n      43\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      1\n      2\n      0\n      0\n      0\n      0\n      ...\n      3\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      59995\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59996\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      73\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59997\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      160\n      162\n      163\n      135\n      94\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59998\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59999\n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n60000 rows × 785 columns\n\n\n\nA histogram of the pixels shows they are mostly 0, with values up to 255.\n\n_ = plt.hist(df.filter(like='pixel', axis=1).to_numpy().reshape(-1))\n\n\n\n\nFrom a singe row of the dataframe we can read the label, and the pixels\n\nlabel, *pixels = df.iloc[0]\n\nlabel, len(pixels)\n\n('2', 784)\n\n\nThe 784 pixels are actually 28 rows of the image, each containing 28 columns. If we rearrange them we can plot it as an image.\n\nimage_array = np.array(pixels).reshape(28, 28)\n_ = plt.imshow(image_array, cmap='Greys')\n\n\n\n\nAll we are seeing here are the pixel intensities from 0 (white) to 255 (black) on a grid.\n\nfig, ax = plt.subplots(figsize=(12,12))\nim = ax.imshow(image_array, cmap=\"Greys\")\n\nfor i in range(image_array.shape[0]):\n    for j in range(image_array.shape[1]):\n        text = ax.text(j, i, image_array[i, j], ha=\"center\", va=\"center\", color=\"magenta\")\n\n\n\n\nThe labels are categorical codes for different types of clothing.\nWe can copy the label description and convert it into a Python dictionary.\n\nlabels_txt = \"\"\"\nLabel   Description\n0   T-shirt/top\n1   Trouser\n2   Pullover\n3   Dress\n4   Coat\n5   Sandal\n6   Shirt\n7   Sneaker\n8   Bag\n9   Ankle boot\n\"\"\".strip()\n\nlabels = dict([row.split('\\t') for row in labels_txt.split('\\n')[1:]])\nlabels\n\n{'0': 'T-shirt/top',\n '1': 'Trouser',\n '2': 'Pullover',\n '3': 'Dress',\n '4': 'Coat',\n '5': 'Sandal',\n '6': 'Shirt',\n '7': 'Sneaker',\n '8': 'Bag',\n '9': 'Ankle boot'}\n\n\nThe image above is of a Pullover\n\nlabel, labels[label]\n\n('2', 'Pullover')\n\n\nWe’ve got 6000 images of each type.\n\ndf.label.map(labels).value_counts()\n\nT-shirt/top    6000\nTrouser        6000\nPullover       6000\nDress          6000\nCoat           6000\nSandal         6000\nShirt          6000\nSneaker        6000\nBag            6000\nAnkle boot     6000\nName: label, dtype: int64\n\n\n\n\n3. Dataloader\nNow we have our raw data we need a way to pass that into the model in a way it understands. We do this with a DataLoader reading from the dataframe. We need to tell it:\n\ndf: the dataframe to read from\ny_names: the name of the column containing the outcome variable, here label\nbs: the batch size, how many rows to feed to the model each time. We use 4096 because the data and models are small\nprocs: any preprocessing steps to do, here we use Normalize to map them from 0-255 to a more reasonable range.\ncont_names: The name of the continuous columns\n\nNote that before we didn’t pass cont_names and it automatically detected them; however it can reorder the columns so we specify it here for clarity.\n\ndls = TabularDataLoaders.from_df(df, y_names='label', bs=4096, procs=[Normalize], cont_names=list(df.columns[1:]))\n\nThis data loader can then produce the pixel arrays for a subset of rows, and the outcome labels on demand. Note these are the values before using procs.\n\ndls.show_batch()\n\n\n\n  \n    \n      \n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      pixel9\n      pixel10\n      pixel11\n      pixel12\n      pixel13\n      pixel14\n      pixel15\n      pixel16\n      pixel17\n      pixel18\n      pixel19\n      pixel20\n      pixel21\n      pixel22\n      pixel23\n      pixel24\n      pixel25\n      pixel26\n      pixel27\n      pixel28\n      pixel29\n      pixel30\n      pixel31\n      pixel32\n      pixel33\n      pixel34\n      pixel35\n      pixel36\n      pixel37\n      pixel38\n      pixel39\n      pixel40\n      pixel41\n      pixel42\n      pixel43\n      pixel44\n      pixel45\n      pixel46\n      pixel47\n      pixel48\n      pixel49\n      pixel50\n      pixel51\n      pixel52\n      pixel53\n      pixel54\n      pixel55\n      pixel56\n      pixel57\n      pixel58\n      pixel59\n      pixel60\n      pixel61\n      pixel62\n      pixel63\n      pixel64\n      pixel65\n      pixel66\n      pixel67\n      pixel68\n      pixel69\n      pixel70\n      pixel71\n      pixel72\n      pixel73\n      pixel74\n      pixel75\n      pixel76\n      pixel77\n      pixel78\n      pixel79\n      pixel80\n      pixel81\n      pixel82\n      pixel83\n      pixel84\n      pixel85\n      pixel86\n      pixel87\n      pixel88\n      pixel89\n      pixel90\n      pixel91\n      pixel92\n      pixel93\n      pixel94\n      pixel95\n      pixel96\n      pixel97\n      pixel98\n      pixel99\n      pixel100\n      pixel101\n      pixel102\n      pixel103\n      pixel104\n      pixel105\n      pixel106\n      pixel107\n      pixel108\n      pixel109\n      pixel110\n      pixel111\n      pixel112\n      pixel113\n      pixel114\n      pixel115\n      pixel116\n      pixel117\n      pixel118\n      pixel119\n      pixel120\n      pixel121\n      pixel122\n      pixel123\n      pixel124\n      pixel125\n      pixel126\n      pixel127\n      pixel128\n      pixel129\n      pixel130\n      pixel131\n      pixel132\n      pixel133\n      pixel134\n      pixel135\n      pixel136\n      pixel137\n      pixel138\n      pixel139\n      pixel140\n      pixel141\n      pixel142\n      pixel143\n      pixel144\n      pixel145\n      pixel146\n      pixel147\n      pixel148\n      pixel149\n      pixel150\n      pixel151\n      pixel152\n      pixel153\n      pixel154\n      pixel155\n      pixel156\n      pixel157\n      pixel158\n      pixel159\n      pixel160\n      pixel161\n      pixel162\n      pixel163\n      pixel164\n      pixel165\n      pixel166\n      pixel167\n      pixel168\n      pixel169\n      pixel170\n      pixel171\n      pixel172\n      pixel173\n      pixel174\n      pixel175\n      pixel176\n      pixel177\n      pixel178\n      pixel179\n      pixel180\n      pixel181\n      pixel182\n      pixel183\n      pixel184\n      pixel185\n      pixel186\n      pixel187\n      pixel188\n      pixel189\n      pixel190\n      pixel191\n      pixel192\n      pixel193\n      pixel194\n      pixel195\n      pixel196\n      pixel197\n      pixel198\n      pixel199\n      pixel200\n      pixel201\n      pixel202\n      pixel203\n      pixel204\n      pixel205\n      pixel206\n      pixel207\n      pixel208\n      pixel209\n      pixel210\n      pixel211\n      pixel212\n      pixel213\n      pixel214\n      pixel215\n      pixel216\n      pixel217\n      pixel218\n      pixel219\n      pixel220\n      pixel221\n      pixel222\n      pixel223\n      pixel224\n      pixel225\n      pixel226\n      pixel227\n      pixel228\n      pixel229\n      pixel230\n      pixel231\n      pixel232\n      pixel233\n      pixel234\n      pixel235\n      pixel236\n      pixel237\n      pixel238\n      pixel239\n      pixel240\n      pixel241\n      pixel242\n      pixel243\n      pixel244\n      pixel245\n      pixel246\n      pixel247\n      pixel248\n      pixel249\n      pixel250\n      pixel251\n      pixel252\n      pixel253\n      pixel254\n      pixel255\n      pixel256\n      pixel257\n      pixel258\n      pixel259\n      pixel260\n      pixel261\n      pixel262\n      pixel263\n      pixel264\n      pixel265\n      pixel266\n      pixel267\n      pixel268\n      pixel269\n      pixel270\n      pixel271\n      pixel272\n      pixel273\n      pixel274\n      pixel275\n      pixel276\n      pixel277\n      pixel278\n      pixel279\n      pixel280\n      pixel281\n      pixel282\n      pixel283\n      pixel284\n      pixel285\n      pixel286\n      pixel287\n      pixel288\n      pixel289\n      pixel290\n      pixel291\n      pixel292\n      pixel293\n      pixel294\n      pixel295\n      pixel296\n      pixel297\n      pixel298\n      pixel299\n      pixel300\n      pixel301\n      pixel302\n      pixel303\n      pixel304\n      pixel305\n      pixel306\n      pixel307\n      pixel308\n      pixel309\n      pixel310\n      pixel311\n      pixel312\n      pixel313\n      pixel314\n      pixel315\n      pixel316\n      pixel317\n      pixel318\n      pixel319\n      pixel320\n      pixel321\n      pixel322\n      pixel323\n      pixel324\n      pixel325\n      pixel326\n      pixel327\n      pixel328\n      pixel329\n      pixel330\n      pixel331\n      pixel332\n      pixel333\n      pixel334\n      pixel335\n      pixel336\n      pixel337\n      pixel338\n      pixel339\n      pixel340\n      pixel341\n      pixel342\n      pixel343\n      pixel344\n      pixel345\n      pixel346\n      pixel347\n      pixel348\n      pixel349\n      pixel350\n      pixel351\n      pixel352\n      pixel353\n      pixel354\n      pixel355\n      pixel356\n      pixel357\n      pixel358\n      pixel359\n      pixel360\n      pixel361\n      pixel362\n      pixel363\n      pixel364\n      pixel365\n      pixel366\n      pixel367\n      pixel368\n      pixel369\n      pixel370\n      pixel371\n      pixel372\n      pixel373\n      pixel374\n      pixel375\n      pixel376\n      pixel377\n      pixel378\n      pixel379\n      pixel380\n      pixel381\n      pixel382\n      pixel383\n      pixel384\n      pixel385\n      pixel386\n      pixel387\n      pixel388\n      pixel389\n      pixel390\n      pixel391\n      pixel392\n      pixel393\n      pixel394\n      pixel395\n      pixel396\n      pixel397\n      pixel398\n      pixel399\n      pixel400\n      pixel401\n      pixel402\n      pixel403\n      pixel404\n      pixel405\n      pixel406\n      pixel407\n      pixel408\n      pixel409\n      pixel410\n      pixel411\n      pixel412\n      pixel413\n      pixel414\n      pixel415\n      pixel416\n      pixel417\n      pixel418\n      pixel419\n      pixel420\n      pixel421\n      pixel422\n      pixel423\n      pixel424\n      pixel425\n      pixel426\n      pixel427\n      pixel428\n      pixel429\n      pixel430\n      pixel431\n      pixel432\n      pixel433\n      pixel434\n      pixel435\n      pixel436\n      pixel437\n      pixel438\n      pixel439\n      pixel440\n      pixel441\n      pixel442\n      pixel443\n      pixel444\n      pixel445\n      pixel446\n      pixel447\n      pixel448\n      pixel449\n      pixel450\n      pixel451\n      pixel452\n      pixel453\n      pixel454\n      pixel455\n      pixel456\n      pixel457\n      pixel458\n      pixel459\n      pixel460\n      pixel461\n      pixel462\n      pixel463\n      pixel464\n      pixel465\n      pixel466\n      pixel467\n      pixel468\n      pixel469\n      pixel470\n      pixel471\n      pixel472\n      pixel473\n      pixel474\n      pixel475\n      pixel476\n      pixel477\n      pixel478\n      pixel479\n      pixel480\n      pixel481\n      pixel482\n      pixel483\n      pixel484\n      pixel485\n      pixel486\n      pixel487\n      pixel488\n      pixel489\n      pixel490\n      pixel491\n      pixel492\n      pixel493\n      pixel494\n      pixel495\n      pixel496\n      pixel497\n      pixel498\n      pixel499\n      pixel500\n      pixel501\n      pixel502\n      pixel503\n      pixel504\n      pixel505\n      pixel506\n      pixel507\n      pixel508\n      pixel509\n      pixel510\n      pixel511\n      pixel512\n      pixel513\n      pixel514\n      pixel515\n      pixel516\n      pixel517\n      pixel518\n      pixel519\n      pixel520\n      pixel521\n      pixel522\n      pixel523\n      pixel524\n      pixel525\n      pixel526\n      pixel527\n      pixel528\n      pixel529\n      pixel530\n      pixel531\n      pixel532\n      pixel533\n      pixel534\n      pixel535\n      pixel536\n      pixel537\n      pixel538\n      pixel539\n      pixel540\n      pixel541\n      pixel542\n      pixel543\n      pixel544\n      pixel545\n      pixel546\n      pixel547\n      pixel548\n      pixel549\n      pixel550\n      pixel551\n      pixel552\n      pixel553\n      pixel554\n      pixel555\n      pixel556\n      pixel557\n      pixel558\n      pixel559\n      pixel560\n      pixel561\n      pixel562\n      pixel563\n      pixel564\n      pixel565\n      pixel566\n      pixel567\n      pixel568\n      pixel569\n      pixel570\n      pixel571\n      pixel572\n      pixel573\n      pixel574\n      pixel575\n      pixel576\n      pixel577\n      pixel578\n      pixel579\n      pixel580\n      pixel581\n      pixel582\n      pixel583\n      pixel584\n      pixel585\n      pixel586\n      pixel587\n      pixel588\n      pixel589\n      pixel590\n      pixel591\n      pixel592\n      pixel593\n      pixel594\n      pixel595\n      pixel596\n      pixel597\n      pixel598\n      pixel599\n      pixel600\n      pixel601\n      pixel602\n      pixel603\n      pixel604\n      pixel605\n      pixel606\n      pixel607\n      pixel608\n      pixel609\n      pixel610\n      pixel611\n      pixel612\n      pixel613\n      pixel614\n      pixel615\n      pixel616\n      pixel617\n      pixel618\n      pixel619\n      pixel620\n      pixel621\n      pixel622\n      pixel623\n      pixel624\n      pixel625\n      pixel626\n      pixel627\n      pixel628\n      pixel629\n      pixel630\n      pixel631\n      pixel632\n      pixel633\n      pixel634\n      pixel635\n      pixel636\n      pixel637\n      pixel638\n      pixel639\n      pixel640\n      pixel641\n      pixel642\n      pixel643\n      pixel644\n      pixel645\n      pixel646\n      pixel647\n      pixel648\n      pixel649\n      pixel650\n      pixel651\n      pixel652\n      pixel653\n      pixel654\n      pixel655\n      pixel656\n      pixel657\n      pixel658\n      pixel659\n      pixel660\n      pixel661\n      pixel662\n      pixel663\n      pixel664\n      pixel665\n      pixel666\n      pixel667\n      pixel668\n      pixel669\n      pixel670\n      pixel671\n      pixel672\n      pixel673\n      pixel674\n      pixel675\n      pixel676\n      pixel677\n      pixel678\n      pixel679\n      pixel680\n      pixel681\n      pixel682\n      pixel683\n      pixel684\n      pixel685\n      pixel686\n      pixel687\n      pixel688\n      pixel689\n      pixel690\n      pixel691\n      pixel692\n      pixel693\n      pixel694\n      pixel695\n      pixel696\n      pixel697\n      pixel698\n      pixel699\n      pixel700\n      pixel701\n      pixel702\n      pixel703\n      pixel704\n      pixel705\n      pixel706\n      pixel707\n      pixel708\n      pixel709\n      pixel710\n      pixel711\n      pixel712\n      pixel713\n      pixel714\n      pixel715\n      pixel716\n      pixel717\n      pixel718\n      pixel719\n      pixel720\n      pixel721\n      pixel722\n      pixel723\n      pixel724\n      pixel725\n      pixel726\n      pixel727\n      pixel728\n      pixel729\n      pixel730\n      pixel731\n      pixel732\n      pixel733\n      pixel734\n      pixel735\n      pixel736\n      pixel737\n      pixel738\n      pixel739\n      pixel740\n      pixel741\n      pixel742\n      pixel743\n      pixel744\n      pixel745\n      pixel746\n      pixel747\n      pixel748\n      pixel749\n      pixel750\n      pixel751\n      pixel752\n      pixel753\n      pixel754\n      pixel755\n      pixel756\n      pixel757\n      pixel758\n      pixel759\n      pixel760\n      pixel761\n      pixel762\n      pixel763\n      pixel764\n      pixel765\n      pixel766\n      pixel767\n      pixel768\n      pixel769\n      pixel770\n      pixel771\n      pixel772\n      pixel773\n      pixel774\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n      pixel784\n      label\n    \n  \n  \n    \n      0\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      9.682471e-07\n      -0.000002\n      -5.505288e-07\n      4.053924e-07\n      4.035032e-07\n      2.956210e-07\n      4.005417e-07\n      -0.000001\n      5.516976e-08\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      -0.000001\n      7.044741e-07\n      -0.000002\n      -0.000002\n      -7.585758e-07\n      -0.000006\n      0.000006\n      0.000002\n      -1.279166e-07\n      0.000001\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      0.000002\n      -0.000002\n      0.000002\n      0.000001\n      -0.000003\n      0.000003\n      -4.594104e-07\n      0.000004\n      0.000005\n      0.000001\n      -4.233165e-07\n      0.000002\n      -0.000002\n      7.057845e-07\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      8.386749e-07\n      0.000002\n      0.000005\n      0.000004\n      -0.000003\n      -0.000003\n      -0.000003\n      -0.000004\n      -6.836428e-07\n      -3.299791e-07\n      -0.000005\n      0.000002\n      5.257434e-07\n      8.999998e+00\n      6.600000e+01\n      9.500000e+01\n      9.700000e+01\n      1.000000e+01\n      -4.949160e-09\n      1.000000e+00\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      -0.000002\n      0.000002\n      2.000001\n      0.000003\n      10.000000\n      1.880000e+02\n      2.170000e+02\n      238.000000\n      250.000003\n      251.000002\n      2.550000e+02\n      218.000003\n      2.290000e+02\n      2.550000e+02\n      2.550000e+02\n      2.360000e+02\n      2.550000e+02\n      1.420000e+02\n      -1.021340e-08\n      2.000000e+00\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      -0.000002\n      -0.000005\n      9.999999\n      6.513797e-07\n      70.999999\n      255.000005\n      236.999996\n      236.000001\n      236.999995\n      229.000004\n      225.000003\n      234.999999\n      2.310000e+02\n      224.000001\n      2.240000e+02\n      2.190000e+02\n      2.440000e+02\n      1.430000e+02\n      1.621610e-07\n      1.000000e+00\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      -0.000002\n      0.000002\n      0.000001\n      -8.460442e-08\n      0.000003\n      6.000000e+00\n      -0.000002\n      58.000001\n      246.999997\n      228.000003\n      232.999999\n      233.999998\n      227.999995\n      227.000000\n      232.000000\n      220.999996\n      2.280000e+02\n      239.000001\n      2.310000e+02\n      2.450000e+02\n      1.570000e+02\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      -0.000001\n      0.000001\n      -0.000004\n      3.999998e+00\n      -0.000004\n      1.440000e+02\n      248.999999\n      228.000004\n      241.999999\n      240.999997\n      240.000003\n      237.999997\n      242.999999\n      238.000004\n      234.000004\n      240.000001\n      230.999996\n      2.460000e+02\n      2.150000e+02\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      7.406876e-07\n      -0.000003\n      -9.455209e-07\n      0.000004\n      0.000004\n      -0.000002\n      154.999998\n      246.999996\n      217.000003\n      230.000003\n      228.000001\n      228.000003\n      229.000003\n      223.999996\n      2.300000e+02\n      229.999996\n      237.999998\n      220.000002\n      244.999993\n      2.220000e+02\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      9.804652e-07\n      0.000001\n      -0.000003\n      -0.000005\n      -0.000004\n      -0.000004\n      183.999997\n      250.000004\n      2.260000e+02\n      232.000004\n      228.999998\n      228.000002\n      226.999997\n      221.999995\n      229.000000\n      234.000001\n      231.999997\n      228.000004\n      2.410000e+02\n      2.440000e+02\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      0.000002\n      0.000003\n      -0.000002\n      0.000003\n      -0.000003\n      0.000002\n      254.999999\n      241.000002\n      229.000004\n      233.999998\n      235.999997\n      236.000002\n      233.999998\n      231.000000\n      2.350000e+02\n      239.000004\n      234.999997\n      2.370000e+02\n      230.999995\n      255.000001\n      1.000000e+02\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      0.000002\n      -0.000002\n      1.999999\n      0.000005\n      29.000004\n      254.999996\n      229.999997\n      237.000003\n      234.000000\n      236.999997\n      238.000002\n      236.000000\n      235.000003\n      235.999998\n      232.000001\n      236.999995\n      237.000003\n      229.000001\n      2.390000e+02\n      226.999998\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000003\n      0.000002\n      0.000002\n      -0.000004\n      176.999998\n      249.999996\n      225.999996\n      2.370000e+02\n      233.000003\n      234.999999\n      237.000003\n      234.999996\n      233.000005\n      236.000004\n      235.999998\n      235.000000\n      233.000000\n      230.000003\n      226.000004\n      255.000000\n      2.700000e+01\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      0.000001\n      3.000000\n      0.000001\n      24.999996\n      254.999996\n      230.999999\n      245.000004\n      234.999997\n      234.000001\n      232.000003\n      235.999999\n      235.999998\n      233.999996\n      2.370000e+02\n      235.999995\n      235.999998\n      230.000004\n      227.999997\n      2.210000e+02\n      254.999991\n      9.300000e+01\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      0.000002\n      0.000005\n      -0.000001\n      0.000003\n      201.999999\n      243.999999\n      226.000003\n      2.350000e+02\n      237.000004\n      234.000001\n      230.999999\n      233.999997\n      235.000000\n      232.000001\n      234.999997\n      2.350000e+02\n      2.300000e+02\n      226.000000\n      224.999997\n      2.210000e+02\n      255.000004\n      1.510000e+02\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      1.000003\n      1.999999e+00\n      0.000004\n      0.000005\n      1.070000e+02\n      254.999997\n      231.999999\n      232.999999\n      231.000002\n      233.999996\n      236.000002\n      232.999996\n      233.000004\n      231.000002\n      229.999999\n      234.000000\n      232.999996\n      2.280000e+02\n      229.999995\n      231.999996\n      2.230000e+02\n      2.550000e+02\n      1.730000e+02\n      -2.135472e-07\n      1.000000e+00\n      5.000000e+00\n      8.000000\n      -5.332055e-07\n      5.139264e-07\n      -0.000002\n      -0.000004\n      74.000001\n      254.999999\n      231.999998\n      229.000002\n      233.000003\n      232.999999\n      232.999998\n      234.999997\n      232.999999\n      232.000003\n      232.999998\n      2.280000e+02\n      223.000004\n      2.330000e+02\n      228.000000\n      227.000001\n      229.000000\n      220.000000\n      2.550000e+02\n      1.340000e+02\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      0.000002\n      68.999999\n      197.999998\n      254.999996\n      237.000001\n      229.000001\n      231.000004\n      233.000000\n      231.999996\n      233.999999\n      2.340000e+02\n      234.000004\n      230.999996\n      231.000002\n      2.280000e+02\n      2.310000e+02\n      2.230000e+02\n      227.999996\n      231.999995\n      2.380000e+02\n      2.330000e+02\n      255.000009\n      2.000000e+00\n      -5.947502e-08\n      -2.880678e-07\n      74.999998\n      1.330000e+02\n      196.000003\n      245.999997\n      254.999999\n      244.000001\n      227.999995\n      230.999998\n      231.999996\n      233.000002\n      237.000000\n      232.999998\n      234.000001\n      233.999996\n      232.999998\n      233.000000\n      230.000000\n      222.000002\n      227.000002\n      250.999998\n      246.000005\n      2.320000e+02\n      2.200000e+02\n      232.999997\n      1.860000e+02\n      -3.012789e-07\n      6.600000e+01\n      2.280000e+02\n      2.410000e+02\n      2.420000e+02\n      2.370000e+02\n      2.350000e+02\n      224.999999\n      211.999995\n      2.300000e+02\n      232.000002\n      233.000002\n      235.000004\n      235.999999\n      232.000002\n      234.000000\n      2.340000e+02\n      235.999999\n      226.000000\n      227.000003\n      254.999995\n      254.999998\n      197.999995\n      215.000005\n      202.000000\n      186.999999\n      2.360000e+02\n      140.000000\n      4.011770e-07\n      1.750000e+02\n      2.470000e+02\n      2.140000e+02\n      224.000000\n      222.000002\n      221.000005\n      2.280000e+02\n      233.000004\n      2.310000e+02\n      2.330000e+02\n      231.000003\n      231.000000\n      229.999996\n      230.999999\n      234.999996\n      236.000004\n      227.000001\n      235.999997\n      255.000001\n      217.000005\n      17.000001\n      -2.610268e-07\n      204.999999\n      220.000001\n      204.999997\n      218.000003\n      7.500000e+01\n      -1.202886e-08\n      9.600000e+01\n      2.540000e+02\n      2.380000e+02\n      2.210000e+02\n      2.290000e+02\n      222.999997\n      225.999995\n      2.320000e+02\n      2.300000e+02\n      231.000001\n      2.310000e+02\n      2.350000e+02\n      236.000004\n      237.999995\n      224.000005\n      230.000000\n      2.410000e+02\n      254.999998\n      78.000001\n      0.000004\n      6.923674e-07\n      0.000001\n      2.170000e+02\n      2.070000e+02\n      1.980000e+02\n      211.999995\n      2.700000e+01\n      -2.188508e-07\n      1.379810e-07\n      9.100000e+01\n      2.550000e+02\n      2.520000e+02\n      243.999992\n      235.000002\n      2.190000e+02\n      2.200000e+02\n      224.000004\n      2.270000e+02\n      232.000003\n      235.000003\n      224.999995\n      2.230000e+02\n      234.000001\n      254.999996\n      2.000000e+02\n      0.000005\n      -0.000004\n      -0.000004\n      4.021377e-07\n      0.000002\n      187.000001\n      216.000000\n      2.200000e+02\n      2.150000e+02\n      5.000000e+00\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      9.100000e+01\n      199.999998\n      2.550000e+02\n      255.000003\n      2.550000e+02\n      253.999999\n      245.000000\n      233.999998\n      235.999999\n      243.999996\n      254.999996\n      255.000000\n      1.170000e+02\n      0.000003\n      -0.000003\n      0.999999\n      0.000002\n      -0.000002\n      0.000001\n      209.999999\n      2.060000e+02\n      1.810000e+02\n      1.470000e+02\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      7.000000e+00\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      0.000002\n      49.000000\n      1.160000e+02\n      1.810000e+02\n      208.999997\n      2.260000e+02\n      223.000000\n      205.999999\n      139.999999\n      -0.000001\n      8.076585e-07\n      -0.000005\n      3.000001\n      0.000005\n      -1.852046e-07\n      6.976350e-08\n      0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      -0.000002\n      3.933536e-07\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      0.000002\n      -0.000005\n      -5.581073e-07\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      -0.000002\n      0.000001\n      0.000003\n      0.000002\n      -0.000002\n      0.000002\n      -0.000003\n      0.000001\n      -0.000003\n      0.000001\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      9.522988e-07\n      -3.426212e-07\n      -6.002562e-07\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      -3.593068e-07\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      9\n    \n    \n      1\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      3.400000e+01\n      4.500000e+01\n      1.650000e+02\n      1.480000e+02\n      34.000000\n      -5.505288e-07\n      4.053924e-07\n      4.035032e-07\n      2.956210e-07\n      6.300000e+01\n      190.000002\n      1.390000e+02\n      2.600000e+01\n      2.800000e+01\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      2.000000e+00\n      -4.694904e-09\n      8.493365e-08\n      1.070000e+02\n      1.310000e+02\n      6.400000e+01\n      151.000002\n      2.290000e+02\n      226.000002\n      219.999994\n      2.410000e+02\n      236.999998\n      243.000004\n      255.000000\n      1.950000e+02\n      92.000000\n      9.000000e+01\n      1.030000e+02\n      9.000000e+01\n      8.000000e+00\n      -8.071513e-09\n      1.000000e+00\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      1.040000e+02\n      9.000000e+01\n      5.200000e+01\n      61.000000\n      76.000000\n      172.000000\n      183.999998\n      197.000003\n      191.000000\n      1.880000e+02\n      169.000002\n      160.000000\n      128.999999\n      7.000000e+01\n      88.000001\n      52.000000\n      8.500000e+01\n      7.700000e+01\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.700000e+01\n      8.900000e+01\n      5.700000e+01\n      5.700000e+01\n      7.000000e+01\n      59.000001\n      127.000000\n      184.999999\n      182.000001\n      179.000002\n      168.999998\n      163.000000\n      1.710000e+02\n      8.900000e+01\n      85.000000\n      77.000000\n      5.900000e+01\n      5.700000e+01\n      8.100000e+01\n      8.000000e+00\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      5.000000e+01\n      6.700000e+01\n      6.400000e+01\n      65.000000\n      65.000000\n      78.000000\n      69.000000\n      139.000001\n      215.000005\n      1.780000e+02\n      1.960000e+02\n      177.000000\n      81.999999\n      83.000001\n      8.200000e+01\n      65.999999\n      5.900000e+01\n      6.000000e+01\n      8.800000e+01\n      4.100000e+01\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      7.200000e+01\n      69.000000\n      67.000001\n      64.000000\n      67.000000\n      58.999999\n      66.999999\n      5.800000e+01\n      91.000000\n      177.000000\n      116.000000\n      35.000001\n      52.999998\n      71.000001\n      58.000002\n      63.000000\n      5.700000e+01\n      72.000000\n      6.400000e+01\n      7.300000e+01\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.000000e+00\n      8.400000e+01\n      60.000001\n      69.000000\n      60.000000\n      6.100000e+01\n      56.000001\n      5.200000e+01\n      68.999999\n      55.999999\n      76.000000\n      58.000000\n      40.000001\n      56.999998\n      54.000001\n      45.000001\n      51.000001\n      46.000000\n      6.500000e+01\n      61.000000\n      9.700000e+01\n      2.000000e+00\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      7.100000e+01\n      5.400000e+01\n      56.000000\n      72.000000\n      82.000000\n      66.000000\n      48.000000\n      5.200000e+01\n      52.999997\n      7.600000e+01\n      106.000000\n      48.000000\n      60.999999\n      73.000000\n      73.000000\n      61.000000\n      78.000000\n      71.999999\n      63.999999\n      56.999999\n      82.000000\n      1.290000e+02\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      1.540000e+02\n      1.260000e+02\n      16.000000\n      7.200000e+01\n      67.000000\n      4.100000e+01\n      73.000000\n      61.000001\n      57.000001\n      63.000002\n      97.000000\n      65.000002\n      60.999998\n      71.000000\n      61.000002\n      70.000001\n      77.999999\n      5.400000e+01\n      108.000000\n      83.000000\n      144.000001\n      117.000001\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      1.450000e+02\n      1.470000e+02\n      1.500000e+02\n      122.000001\n      106.000000\n      106.000000\n      51.999998\n      55.999999\n      59.999999\n      81.999999\n      6.500000e+01\n      55.999997\n      69.000001\n      52.999997\n      70.000001\n      61.000002\n      94.000000\n      205.999996\n      168.000003\n      127.000002\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      9.500000e+01\n      188.000003\n      190.999996\n      98.000000\n      70.000000\n      49.999998\n      52.999998\n      57.000000\n      58.000001\n      52.000002\n      52.000001\n      76.000000\n      52.999998\n      53.999996\n      64.000001\n      7.700000e+01\n      161.999997\n      85.000000\n      1.135173e-07\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      31.999998\n      60.000001\n      63.999999\n      51.999997\n      53.000000\n      50.000003\n      61.000000\n      51.000002\n      50.999997\n      75.000002\n      69.999999\n      60.999997\n      86.000000\n      19.999996\n      -0.000002\n      0.000005\n      0.000002\n      1.000002\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      2.000000\n      0.000001\n      40.999999\n      73.000000\n      64.000000\n      53.000000\n      47.999999\n      52.000004\n      58.000004\n      5.200000e+01\n      52.999999\n      56.999997\n      64.000002\n      53.000000\n      77.000002\n      43.999999\n      -0.000005\n      3.000005\n      0.000003\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      32.000000\n      69.000001\n      47.000001\n      58.000002\n      51.999996\n      52.999996\n      53.999996\n      52.000001\n      56.000004\n      56.999997\n      58.999996\n      51.000000\n      66.000002\n      3.900000e+01\n      -0.000001\n      1.000001\n      0.000004\n      0.000002\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      1.999999e+00\n      0.000002\n      26.000003\n      59.999999\n      47.999997\n      58.000000\n      52.999997\n      49.999999\n      5.200000e+01\n      50.999996\n      56.999999\n      62.999997\n      57.000003\n      57.999998\n      73.000001\n      56.999999\n      7.423447e-07\n      9.999991e-01\n      0.000005\n      0.000002\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      1.999998\n      -5.223541e-09\n      26.000001\n      53.000002\n      5.000000e+01\n      63.000003\n      57.000000\n      53.000003\n      60.999999\n      56.999998\n      55.999997\n      64.000004\n      56.999997\n      60.999998\n      69.000000\n      64.999998\n      0.000005\n      1.000005e+00\n      -0.000005\n      0.000002\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      -5.332055e-07\n      2.000003e+00\n      -0.000002\n      32.000000\n      50.000000\n      66.000002\n      76.000002\n      60.999997\n      55.999997\n      58.999998\n      57.000004\n      59.999999\n      64.999999\n      51.000002\n      63.999998\n      6.100000e+01\n      59.000000\n      -4.653743e-07\n      -0.000005\n      0.000002\n      -0.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      0.000002\n      0.000002\n      51.000002\n      47.000002\n      69.000002\n      66.999997\n      58.999998\n      56.000002\n      59.999997\n      58.000000\n      6.100000e+01\n      63.000000\n      52.999998\n      66.000002\n      6.400000e+01\n      5.800000e+01\n      -6.241514e-07\n      -0.000005\n      1.000003\n      5.651978e-07\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.352320e-07\n      0.000002\n      -0.000002\n      -0.000002\n      59.999999\n      61.000001\n      77.000000\n      71.999997\n      64.000003\n      57.999997\n      62.999996\n      58.999998\n      65.999999\n      63.000000\n      58.000000\n      67.000003\n      72.000000\n      71.999999\n      11.000003\n      -0.000005\n      9.999988e-01\n      -3.174695e-07\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      -9.513843e-07\n      1.000001e+00\n      2.894738e-08\n      0.000004\n      76.000000\n      5.800000e+01\n      81.999998\n      82.999998\n      68.999996\n      63.000002\n      66.000002\n      64.000001\n      7.200000e+01\n      66.000002\n      64.000004\n      65.000000\n      76.000001\n      87.999999\n      32.000002\n      -0.000002\n      2.999998\n      -0.000002\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      -0.000002\n      2.000001\n      -0.000003\n      1.100000e+01\n      92.000000\n      6.400000e+01\n      8.500000e+01\n      84.999998\n      78.999999\n      71.999999\n      69.000002\n      72.999999\n      85.999999\n      73.000002\n      70.999998\n      67.000000\n      83.999999\n      98.000000\n      2.800000e+01\n      0.000003\n      3.000001\n      -0.000002\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      -2.873870e-07\n      1.999999e+00\n      0.000001\n      15.999998\n      9.800000e+01\n      8.500000e+01\n      88.000001\n      8.100000e+01\n      8.800000e+01\n      79.000001\n      72.999999\n      83.000000\n      91.000001\n      7.800000e+01\n      81.000000\n      84.000001\n      88.000000\n      1.220000e+02\n      35.000001\n      -8.325330e-07\n      2.000000e+00\n      -1.849482e-07\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      1.000002\n      -0.000001\n      3.300000e+01\n      1.350000e+02\n      96.000000\n      8.900000e+01\n      89.999999\n      96.000000\n      83.999999\n      9.000000e+01\n      97.999999\n      99.999999\n      9.000000e+01\n      90.000000\n      102.000000\n      72.999999\n      1.350000e+02\n      52.000001\n      0.000002\n      2.000001\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      2.000002\n      9.323434e-07\n      101.000000\n      1.290000e+02\n      110.000000\n      115.000000\n      97.999999\n      96.000001\n      91.000000\n      95.000001\n      101.000000\n      1.030000e+02\n      97.000000\n      92.000000\n      104.000000\n      101.000000\n      113.000001\n      59.000000\n      -0.000001\n      3.000000e+00\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      2.999998e+00\n      0.000002\n      46.000000\n      1.560000e+02\n      9.400000e+01\n      117.000000\n      1.170000e+02\n      127.000000\n      112.000000\n      112.000000\n      128.000000\n      1.230000e+02\n      133.000000\n      121.000001\n      102.000000\n      9.200000e+01\n      1.500000e+02\n      51.000000\n      -0.000002\n      3.000002\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      1.270000e+02\n      160.000000\n      1.040000e+02\n      58.999999\n      7.300000e+01\n      85.000000\n      82.000000\n      95.000000\n      92.000000\n      86.000000\n      90.000000\n      1.060000e+02\n      165.999997\n      112.999998\n      -0.000001\n      2.139088e-07\n      1.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      109.999998\n      222.000005\n      213.999997\n      181.000000\n      163.000000\n      169.000001\n      175.999999\n      169.000001\n      171.999999\n      188.000002\n      187.000002\n      82.999999\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      6.100000e+01\n      1.260000e+02\n      1.420000e+02\n      1.620000e+02\n      1.830000e+02\n      171.000000\n      1.450000e+02\n      5.400000e+01\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      0\n    \n    \n      2\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      6.900000e+01\n      4.800000e+01\n      9.682471e-07\n      3.000002\n      -5.505288e-07\n      4.053924e-07\n      4.035032e-07\n      2.956210e-07\n      4.005417e-07\n      -0.000001\n      5.516976e-08\n      1.300000e+02\n      3.200000e+01\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      1.770000e+02\n      173.000001\n      7.044741e-07\n      -0.000002\n      -0.000002\n      -7.585758e-07\n      0.999997\n      0.000006\n      1.999996\n      -1.279166e-07\n      0.000001\n      2.040000e+02\n      7.800000e+01\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      127.999997\n      184.999995\n      0.000002\n      0.000001\n      0.999996\n      1.000004\n      -4.594104e-07\n      0.000004\n      5.000005\n      0.000001\n      5.100000e+01\n      200.999995\n      -0.000002\n      7.057845e-07\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      7.300000e+01\n      212.000004\n      17.999997\n      0.000004\n      0.999997\n      -0.000003\n      -0.000003\n      -0.000004\n      -6.836428e-07\n      -3.299791e-07\n      121.999999\n      189.000002\n      5.257434e-07\n      -3.949514e-07\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      44.000000\n      219.999997\n      97.000000\n      0.000003\n      2.000004\n      9.626116e-07\n      6.564152e-07\n      1.000003\n      0.999998\n      -0.000005\n      1.770000e+02\n      166.000002\n      5.942894e-07\n      9.570876e-07\n      7.753735e-07\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      41.999999\n      206.000005\n      197.999999\n      6.513797e-07\n      -0.000001\n      0.000005\n      -0.000004\n      0.000004\n      -0.000001\n      45.999999\n      209.000003\n      170.999998\n      2.016850e-07\n      -0.000002\n      -4.557509e-07\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      -0.000002\n      0.000002\n      0.000001\n      4.900000e+01\n      224.000005\n      1.940000e+02\n      162.000001\n      20.000002\n      -0.000001\n      -0.000004\n      -0.000005\n      47.999998\n      203.000001\n      195.999999\n      194.999998\n      0.000005\n      -7.583148e-07\n      -0.000002\n      8.919840e-07\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      -0.000001\n      57.000000\n      221.999997\n      1.800000e+02\n      202.999997\n      2.150000e+02\n      194.999999\n      194.999999\n      207.000000\n      213.000002\n      190.000000\n      183.000001\n      222.999998\n      1.999996\n      -0.000002\n      0.000002\n      0.000001\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      7.406876e-07\n      -0.000003\n      1.130000e+02\n      223.000002\n      166.000000\n      165.000001\n      173.999998\n      185.000001\n      186.999998\n      181.000001\n      176.000001\n      168.999999\n      182.000001\n      222.000000\n      8.100000e+01\n      0.000001\n      0.000002\n      0.000001\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      9.804652e-07\n      0.000001\n      165.999999\n      243.000000\n      233.000002\n      223.999995\n      183.000001\n      186.000002\n      1.800000e+02\n      191.999998\n      187.000002\n      191.000000\n      189.000001\n      232.000000\n      119.000000\n      -0.000006\n      0.000005\n      0.000002\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      0.000002\n      0.000003\n      129.000000\n      230.000005\n      231.000002\n      233.999995\n      222.000001\n      215.000002\n      223.999995\n      215.000001\n      216.000002\n      225.000002\n      221.000001\n      255.000004\n      4.500000e+01\n      -0.000003\n      -0.000002\n      1.135173e-07\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      0.000002\n      108.000000\n      221.000000\n      213.000004\n      223.000003\n      215.000002\n      220.999997\n      230.999996\n      216.999998\n      230.000001\n      188.000000\n      222.000000\n      239.000005\n      15.000004\n      -0.000002\n      0.000005\n      0.000002\n      -0.000001\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000003\n      71.000000\n      250.999996\n      229.000000\n      189.999998\n      192.000001\n      200.999998\n      1.990000e+02\n      195.000001\n      199.999999\n      200.000002\n      214.000000\n      216.000002\n      -0.000003\n      -0.000005\n      -0.000002\n      0.000003\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      0.000001\n      36.000001\n      235.999998\n      191.000001\n      186.999999\n      182.999999\n      176.000000\n      174.000000\n      174.000001\n      176.999999\n      179.999999\n      215.999998\n      194.000001\n      5.151848e-07\n      -0.000001\n      0.000001\n      0.000004\n      0.000002\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      0.000002\n      0.000005\n      10.999995\n      231.999995\n      191.999999\n      189.000001\n      185.999999\n      1.880000e+02\n      185.999999\n      183.000000\n      186.000000\n      182.999999\n      213.000002\n      191.999998\n      0.000001\n      7.423447e-07\n      1.327465e-07\n      0.000005\n      0.000002\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      0.000002\n      -5.223541e-09\n      0.000004\n      1.999999\n      2.340000e+02\n      195.999998\n      187.000000\n      185.000001\n      180.999999\n      180.999998\n      181.000001\n      182.000001\n      183.000001\n      206.000000\n      191.000001\n      0.000002\n      0.000005\n      4.826824e-07\n      -0.000005\n      0.000002\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      -5.332055e-07\n      5.139264e-07\n      -0.000002\n      -0.000004\n      40.999999\n      225.999998\n      183.000001\n      189.000000\n      184.999999\n      181.000000\n      182.000001\n      181.000001\n      181.000001\n      185.000000\n      188.999999\n      2.230000e+02\n      11.000000\n      -4.653743e-07\n      -0.000005\n      0.000002\n      -0.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      0.000002\n      0.000002\n      0.000005\n      186.000001\n      207.999998\n      181.000001\n      186.000000\n      181.999999\n      181.000000\n      181.000001\n      1.800000e+02\n      182.000000\n      179.000000\n      180.000000\n      2.120000e+02\n      1.350000e+02\n      -6.241514e-07\n      -0.000005\n      0.000002\n      5.651978e-07\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.352320e-07\n      0.000002\n      -0.000002\n      -0.000002\n      -0.000005\n      144.000000\n      220.999998\n      182.000001\n      183.000000\n      180.000001\n      181.999999\n      178.999999\n      179.999999\n      176.999999\n      188.000001\n      222.999998\n      151.999999\n      0.999998\n      0.000004\n      -0.000005\n      -2.677840e-07\n      -3.174695e-07\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      -9.513843e-07\n      -2.911069e-07\n      2.894738e-08\n      0.000004\n      0.000005\n      -3.351748e-07\n      93.000001\n      225.999998\n      185.000000\n      181.000001\n      180.999999\n      180.000001\n      1.770000e+02\n      191.000000\n      213.000000\n      67.000001\n      0.000005\n      0.000003\n      0.000005\n      -0.000002\n      -0.000001\n      -0.000002\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      -0.000002\n      -0.000002\n      -0.000003\n      3.360567e-07\n      0.000004\n      9.999977e-01\n      2.258955e-07\n      84.000001\n      226.000001\n      180.000000\n      182.000001\n      181.999998\n      177.999999\n      226.000001\n      43.999998\n      -0.000004\n      1.000000\n      -0.000001\n      -2.610268e-07\n      0.000003\n      -0.000002\n      -0.000002\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      -2.873870e-07\n      5.500636e-07\n      0.000001\n      -0.000002\n      6.275720e-07\n      4.274649e-07\n      0.000001\n      5.958969e-07\n      1.540000e+02\n      213.000001\n      178.000001\n      172.999998\n      216.000001\n      1.180000e+02\n      0.000001\n      0.000001\n      0.999996\n      6.923674e-07\n      0.000001\n      -8.325330e-07\n      -5.767398e-07\n      -1.849482e-07\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      0.000001\n      -0.000001\n      -4.551377e-07\n      7.378563e-07\n      -0.000002\n      1.000002e+00\n      0.000003\n      29.000002\n      221.999998\n      1.800000e+02\n      182.000000\n      223.000000\n      1.000002e+00\n      0.000005\n      -0.000004\n      -0.000004\n      1.000000e+00\n      0.000002\n      0.000002\n      0.000001\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      0.000001\n      9.323434e-07\n      0.000002\n      8.744090e-07\n      0.000002\n      1.000005\n      0.000003\n      0.000005\n      197.999999\n      187.000003\n      194.000002\n      1.530000e+02\n      0.000003\n      -0.000003\n      0.000005\n      0.000002\n      1.000002\n      0.000001\n      -0.000001\n      3.297868e-07\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      0.000002\n      -0.000001\n      8.809801e-07\n      1.723858e-07\n      0.000001\n      -7.570235e-07\n      0.000001\n      135.000000\n      196.999999\n      199.000002\n      8.800000e+01\n      -0.000005\n      1.000000\n      0.000005\n      -1.852046e-07\n      1.000000e+00\n      0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      0.999999\n      3.933536e-07\n      58.999999\n      199.000005\n      197.000002\n      51.000001\n      0.000002\n      0.999998\n      -5.581073e-07\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      -0.000002\n      0.000001\n      2.000002\n      0.000002\n      -0.000002\n      201.000001\n      223.999998\n      26.999997\n      -0.000003\n      2.999998\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      1.000000e+00\n      -3.426212e-07\n      -6.002562e-07\n      9.900000e+01\n      1.280000e+02\n      -0.000002\n      6.155732e-07\n      1.000001e+00\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      0\n    \n    \n      3\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      9.682471e-07\n      -0.000002\n      -5.505288e-07\n      4.053924e-07\n      4.035032e-07\n      2.956210e-07\n      4.005417e-07\n      -0.000001\n      5.516976e-08\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      -0.000001\n      7.044741e-07\n      -0.000002\n      -0.000002\n      -7.585758e-07\n      -0.000006\n      0.000006\n      0.000002\n      -1.279166e-07\n      0.000001\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      0.000002\n      -0.000002\n      0.000002\n      1.000000\n      -0.000003\n      0.000003\n      1.290000e+02\n      197.000002\n      181.000000\n      207.000005\n      2.370000e+02\n      219.000005\n      180.999997\n      1.610000e+02\n      1.110000e+02\n      9.700000e+01\n      7.500000e+01\n      4.400000e+01\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      8.386749e-07\n      0.000002\n      0.000005\n      2.000005\n      -0.000003\n      -0.000003\n      203.999995\n      214.000003\n      1.880000e+02\n      2.080000e+02\n      247.000002\n      217.000003\n      2.300000e+02\n      2.220000e+02\n      2.220000e+02\n      2.400000e+02\n      2.170000e+02\n      2.520000e+02\n      9.000000e+00\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      -0.000002\n      0.000002\n      0.000005\n      2.999999\n      0.000002\n      9.626116e-07\n      2.170000e+02\n      206.000004\n      209.000005\n      213.000000\n      2.010000e+02\n      174.000002\n      2.000000e+02\n      1.990000e+02\n      1.940000e+02\n      2.030000e+02\n      2.010000e+02\n      1.860000e+02\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      -0.000002\n      -0.000005\n      -0.000001\n      3.999999e+00\n      -0.000001\n      3.999997\n      193.000001\n      200.000002\n      201.000002\n      197.000000\n      199.000002\n      185.000003\n      2.300000e+02\n      189.999995\n      1.960000e+02\n      1.980000e+02\n      2.100000e+02\n      2.060000e+02\n      1.880000e+02\n      2.200000e+01\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      -0.000002\n      0.000002\n      0.000001\n      -8.460442e-08\n      0.000003\n      -9.104431e-07\n      4.999997\n      0.000005\n      13.000001\n      194.999999\n      198.999998\n      202.000002\n      219.000000\n      229.999995\n      197.000001\n      198.000001\n      1.650000e+02\n      218.000001\n      2.160000e+02\n      2.120000e+02\n      2.210000e+02\n      2.250000e+02\n      8.100000e+01\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      -0.000001\n      0.000001\n      -0.000004\n      1.262333e-07\n      4.999995\n      -4.162347e-07\n      14.999996\n      237.000002\n      200.000002\n      196.000000\n      190.000000\n      192.999998\n      253.000005\n      223.999996\n      223.000005\n      226.000000\n      216.999997\n      2.100000e+02\n      1.840000e+02\n      1.900000e+01\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      7.406876e-07\n      -0.000003\n      -9.455209e-07\n      0.000004\n      0.000004\n      5.000001\n      -0.000002\n      14.000000\n      223.999995\n      201.000001\n      205.999998\n      215.999998\n      186.000002\n      188.000000\n      2.120000e+02\n      192.999997\n      194.000003\n      191.999997\n      219.999991\n      1.210000e+02\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      9.804652e-07\n      0.000001\n      -0.000003\n      -0.000005\n      -0.000004\n      2.999995\n      -0.000002\n      14.000003\n      2.260000e+02\n      199.000000\n      209.000001\n      214.999999\n      213.000000\n      199.000001\n      200.000000\n      202.999995\n      203.000000\n      189.999996\n      2.170000e+02\n      1.420000e+02\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      0.000002\n      0.000003\n      -0.000002\n      0.000003\n      -0.000003\n      1.000002\n      -0.000004\n      4.999998\n      211.000002\n      210.999999\n      211.999998\n      197.000001\n      204.000001\n      203.000002\n      2.000000e+02\n      201.999999\n      200.000002\n      1.920000e+02\n      202.000001\n      206.000000\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      0.000002\n      -0.000002\n      0.000004\n      0.000005\n      -0.000002\n      -0.000002\n      144.000000\n      210.000002\n      189.000001\n      233.000002\n      252.999998\n      228.999998\n      222.999998\n      214.999998\n      212.000005\n      204.999999\n      210.000003\n      196.000004\n      2.090000e+02\n      62.000001\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000003\n      0.000002\n      0.000002\n      1.999995\n      0.000001\n      0.000002\n      222.999996\n      1.890000e+02\n      196.000002\n      164.000000\n      146.000000\n      221.000000\n      180.000002\n      199.999999\n      207.999997\n      207.000001\n      201.999999\n      196.000002\n      205.000000\n      163.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      0.000001\n      0.000003\n      0.000001\n      4.000004\n      0.000002\n      49.999998\n      233.000001\n      178.999998\n      218.000001\n      190.999999\n      155.000000\n      205.000002\n      217.000000\n      2.270000e+02\n      185.000000\n      201.000000\n      203.000004\n      200.000001\n      2.000000e+02\n      200.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      0.000002\n      0.000005\n      0.999998\n      0.000003\n      -0.000001\n      -0.000003\n      154.000000\n      2.110000e+02\n      199.000000\n      216.999998\n      190.000001\n      228.000001\n      208.000000\n      201.999998\n      231.000001\n      2.050000e+02\n      1.880000e+02\n      193.999997\n      200.000003\n      2.000000e+02\n      212.000002\n      9.000000e+00\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      9.999992e-01\n      1.999999\n      1.999998\n      2.999998e+00\n      0.000004\n      0.000005\n      -1.976124e-07\n      44.999999\n      161.000000\n      177.000000\n      145.000000\n      220.000000\n      222.999999\n      141.000000\n      223.999999\n      218.999999\n      223.999996\n      215.000002\n      205.000002\n      2.250000e+02\n      206.999995\n      207.999997\n      1.970000e+02\n      2.240000e+02\n      3.900000e+01\n      -2.135472e-07\n      2.000001e+00\n      2.999999e+00\n      0.000001\n      -5.332055e-07\n      5.139264e-07\n      -0.000002\n      -0.000004\n      -0.000002\n      101.000001\n      174.000000\n      165.000000\n      171.000001\n      174.999999\n      195.999998\n      232.000004\n      237.000001\n      222.999999\n      174.999999\n      1.800000e+02\n      171.000001\n      1.960000e+02\n      192.000003\n      186.999997\n      189.999995\n      191.000004\n      2.160000e+02\n      1.700000e+01\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      6.000001\n      2.000002\n      29.000001\n      84.000000\n      150.999999\n      147.000000\n      113.999999\n      164.000000\n      196.000001\n      211.999998\n      207.000000\n      2.040000e+02\n      208.000000\n      204.999999\n      190.000001\n      1.930000e+02\n      1.960000e+02\n      1.850000e+02\n      180.000000\n      182.000000\n      1.840000e+02\n      1.840000e+02\n      216.000002\n      1.400000e+01\n      -5.947502e-08\n      -2.880678e-07\n      101.000000\n      1.340000e+02\n      144.000002\n      163.999999\n      161.999999\n      144.000000\n      109.000000\n      113.000000\n      169.000000\n      193.999998\n      204.000002\n      209.999999\n      212.000002\n      203.000001\n      199.000001\n      188.999999\n      176.999999\n      178.999998\n      188.000002\n      204.000003\n      205.000005\n      2.010000e+02\n      1.990000e+02\n      190.000001\n      2.190000e+02\n      1.600000e+01\n      -1.447285e-07\n      9.400000e+01\n      1.930000e+02\n      1.260000e+02\n      6.400000e+01\n      3.200000e+01\n      11.000002\n      90.000000\n      1.520000e+02\n      170.999999\n      191.000000\n      199.000000\n      204.000002\n      209.000002\n      207.999998\n      1.900000e+02\n      186.999999\n      180.000001\n      214.000002\n      233.000001\n      204.000005\n      182.000002\n      176.999998\n      175.000000\n      178.000005\n      1.760000e+02\n      215.000007\n      1.700000e+01\n      2.200000e+01\n      1.870000e+02\n      2.160000e+02\n      215.999999\n      201.999998\n      199.999997\n      1.890000e+02\n      195.000001\n      1.960000e+02\n      1.960000e+02\n      195.999998\n      199.999999\n      204.000001\n      199.999998\n      204.999999\n      196.000001\n      215.999998\n      255.000000\n      143.000000\n      90.000000\n      190.000005\n      1.630000e+02\n      171.000000\n      175.999995\n      179.999997\n      180.000004\n      2.110000e+02\n      1.900000e+01\n      1.090000e+02\n      1.960000e+02\n      1.720000e+02\n      1.960000e+02\n      2.080000e+02\n      214.999998\n      222.999998\n      2.100000e+02\n      2.050000e+02\n      202.999998\n      2.010000e+02\n      2.060000e+02\n      210.999999\n      214.000003\n      200.000002\n      219.000001\n      1.870000e+02\n      31.000004\n      0.000001\n      27.000001\n      2.280000e+02\n      165.000001\n      1.790000e+02\n      1.810000e+02\n      1.800000e+02\n      179.000000\n      2.060000e+02\n      2.900000e+01\n      4.600000e+01\n      1.900000e+02\n      2.140000e+02\n      1.950000e+02\n      178.999996\n      187.999999\n      1.930000e+02\n      2.020000e+02\n      205.000001\n      2.120000e+02\n      210.000000\n      209.000001\n      199.999999\n      1.830000e+02\n      197.999999\n      109.000000\n      9.842208e-07\n      0.000005\n      -0.000004\n      44.000002\n      1.880000e+02\n      168.000004\n      176.999996\n      164.999996\n      1.750000e+02\n      1.760000e+02\n      1.760000e+02\n      2.500000e+01\n      -1.867194e-08\n      -2.868018e-07\n      9.100000e+01\n      1.930000e+02\n      210.999997\n      2.050000e+02\n      202.000003\n      2.010000e+02\n      199.000001\n      188.000001\n      189.999997\n      191.000001\n      183.000000\n      211.000004\n      152.000000\n      8.867231e-07\n      0.000003\n      4.999995\n      0.000005\n      12.999999\n      198.999999\n      190.000003\n      192.999998\n      1.970000e+02\n      1.990000e+02\n      2.030000e+02\n      2.120000e+02\n      5.900000e+01\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      2.200000e+01\n      62.000000\n      152.000000\n      2.050000e+02\n      1.860000e+02\n      232.000002\n      2.290000e+02\n      182.999998\n      214.999995\n      164.000003\n      7.999998\n      8.076585e-07\n      2.000003\n      -0.000006\n      0.000005\n      9.999992e-01\n      1.840000e+02\n      137.000002\n      155.000001\n      165.000005\n      136.000003\n      1.290000e+02\n      1.130000e+02\n      1.800000e+01\n      3.033599e-08\n      5.283691e-08\n      1.000000e+00\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      20.000000\n      3.933536e-07\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      0.000002\n      -0.000005\n      -5.581073e-07\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      -0.000002\n      0.000001\n      0.000003\n      0.000002\n      -0.000002\n      0.000002\n      -0.000003\n      0.000001\n      -0.000003\n      0.000001\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      9.522988e-07\n      -3.426212e-07\n      -6.002562e-07\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      -3.593068e-07\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      9\n    \n    \n      4\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      1.930000e+02\n      222.000000\n      2.050000e+02\n      1.790000e+02\n      1.970000e+02\n      1.700000e+02\n      1.770000e+02\n      201.000007\n      1.480000e+02\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      -0.000001\n      2.550000e+02\n      236.999996\n      240.999995\n      2.390000e+02\n      219.000004\n      206.000000\n      207.000000\n      2.100000e+02\n      221.000003\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      0.000002\n      24.000000\n      255.000005\n      227.000002\n      227.000003\n      232.000001\n      2.140000e+02\n      202.999998\n      204.999995\n      181.000003\n      2.250000e+02\n      0.000002\n      -0.000002\n      7.057845e-07\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      8.386749e-07\n      93.000000\n      254.999997\n      214.000001\n      212.999998\n      211.000002\n      219.999995\n      214.000003\n      2.320000e+02\n      2.150000e+02\n      235.999998\n      66.000000\n      5.257434e-07\n      -3.949514e-07\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      -0.000002\n      153.000000\n      254.999998\n      205.000000\n      195.000002\n      1.950000e+02\n      2.180000e+02\n      227.000002\n      233.999997\n      197.999999\n      1.900000e+02\n      162.000002\n      5.942894e-07\n      9.570876e-07\n      7.753735e-07\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      -0.000002\n      178.000001\n      253.000005\n      1.980000e+02\n      161.999999\n      215.000002\n      241.000000\n      228.000000\n      237.999996\n      207.000001\n      166.000002\n      194.000003\n      2.016850e-07\n      -0.000002\n      -4.557509e-07\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      -0.000002\n      0.000002\n      0.000001\n      -8.460442e-08\n      188.999996\n      2.510000e+02\n      196.999998\n      170.000000\n      232.000002\n      205.000002\n      251.999998\n      238.999999\n      211.999997\n      166.999999\n      206.000000\n      0.000005\n      -7.583148e-07\n      -0.000002\n      8.919840e-07\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      -0.000001\n      0.000001\n      188.000003\n      2.490000e+02\n      199.000002\n      1.860000e+02\n      255.000003\n      79.000000\n      254.999997\n      240.000002\n      214.999997\n      177.000001\n      201.000002\n      0.000002\n      -0.000002\n      0.000002\n      0.000001\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      7.406876e-07\n      -0.000003\n      -9.455209e-07\n      184.999998\n      246.999997\n      204.000001\n      197.999999\n      255.000000\n      27.000001\n      246.999998\n      245.999998\n      223.000005\n      192.999999\n      188.999998\n      3.125923e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      9.804652e-07\n      0.000001\n      -0.000003\n      155.000002\n      253.999999\n      206.000004\n      209.000003\n      255.000000\n      -6.622733e-07\n      230.999997\n      255.000000\n      221.000004\n      213.000000\n      171.999998\n      -0.000002\n      -0.000006\n      0.000005\n      0.000002\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      0.000002\n      0.000003\n      -0.000002\n      99.000000\n      254.999997\n      205.999997\n      218.999996\n      252.000002\n      -0.000003\n      179.000000\n      255.000001\n      219.000002\n      227.000003\n      148.000000\n      8.404825e-07\n      -0.000003\n      -0.000002\n      1.135173e-07\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      0.000002\n      -0.000002\n      43.000000\n      254.999995\n      209.000002\n      217.999997\n      227.999996\n      0.000004\n      96.000000\n      254.999999\n      220.000001\n      243.000002\n      127.000000\n      -0.000005\n      -0.000002\n      0.000005\n      0.000002\n      -0.000001\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000003\n      0.000002\n      0.000002\n      251.000003\n      217.000004\n      221.000001\n      187.999999\n      -5.688553e-07\n      47.000001\n      255.000002\n      220.000001\n      251.999996\n      103.999999\n      -0.000003\n      -0.000005\n      -0.000002\n      0.000003\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      0.000001\n      0.000003\n      0.000001\n      218.000002\n      222.999999\n      225.000003\n      150.000000\n      0.000002\n      26.999997\n      255.000003\n      216.000001\n      253.999997\n      76.000002\n      5.151848e-07\n      -0.000001\n      0.000001\n      0.000004\n      0.000002\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      0.000002\n      0.000005\n      -0.000001\n      0.000003\n      169.000001\n      229.000002\n      230.000000\n      1.200000e+02\n      -0.000004\n      5.999993\n      255.000003\n      218.000002\n      250.999996\n      59.000000\n      0.000001\n      7.423447e-07\n      1.327465e-07\n      0.000005\n      0.000002\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      0.000002\n      -5.223541e-09\n      0.000004\n      0.000005\n      -1.976124e-07\n      138.000000\n      233.999999\n      228.999997\n      94.999998\n      0.000001\n      0.000002\n      255.000000\n      221.000001\n      221.999998\n      63.000001\n      0.000002\n      0.000005\n      4.826824e-07\n      -0.000005\n      0.000002\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      -5.332055e-07\n      5.139264e-07\n      -0.000002\n      -0.000004\n      -0.000002\n      -0.000003\n      146.000000\n      232.000000\n      232.000002\n      96.000000\n      -0.000004\n      -0.000006\n      242.000001\n      226.999999\n      222.000000\n      6.300000e+01\n      -0.000005\n      -4.653743e-07\n      -0.000005\n      0.000002\n      -0.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      0.000002\n      0.000002\n      0.000005\n      0.000002\n      -0.000005\n      119.000000\n      232.000000\n      227.999997\n      75.000000\n      -0.000001\n      -5.077659e-07\n      208.000000\n      232.000004\n      212.999999\n      2.900000e+01\n      3.296967e-07\n      -6.241514e-07\n      -0.000005\n      0.000002\n      5.651978e-07\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.352320e-07\n      0.000002\n      -0.000002\n      -0.000002\n      -0.000005\n      -0.000004\n      0.000001\n      139.000000\n      235.000003\n      214.999998\n      18.000001\n      0.000003\n      -0.000008\n      169.000000\n      234.999997\n      211.999998\n      12.000004\n      -0.000003\n      0.000004\n      -0.000005\n      -2.677840e-07\n      -3.174695e-07\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      -9.513843e-07\n      -2.911069e-07\n      2.894738e-08\n      0.000004\n      0.000005\n      -3.351748e-07\n      -0.000003\n      158.000000\n      236.000003\n      213.999998\n      14.000003\n      -0.000002\n      -9.949699e-07\n      176.000001\n      234.999997\n      215.000001\n      32.000002\n      0.000003\n      0.000005\n      -0.000002\n      -0.000001\n      -0.000002\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      -0.000002\n      -0.000002\n      -0.000003\n      3.360567e-07\n      0.000004\n      1.282302e-07\n      2.258955e-07\n      152.000000\n      232.000001\n      227.000003\n      66.000000\n      -0.000002\n      0.000002\n      203.000000\n      234.999997\n      217.000002\n      35.000002\n      -0.000001\n      -2.610268e-07\n      0.000003\n      -0.000002\n      -0.000002\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      -2.873870e-07\n      5.500636e-07\n      0.000001\n      -0.000002\n      6.275720e-07\n      4.274649e-07\n      0.000001\n      1.450000e+02\n      2.340000e+02\n      229.999998\n      90.000000\n      -0.000002\n      0.000002\n      2.080000e+02\n      235.000001\n      212.999995\n      8.999999\n      6.923674e-07\n      0.000001\n      -8.325330e-07\n      -5.767398e-07\n      -1.849482e-07\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      0.000001\n      -0.000001\n      -4.551377e-07\n      7.378563e-07\n      -0.000002\n      3.950150e-07\n      119.000000\n      236.999998\n      229.000001\n      7.600000e+01\n      0.000003\n      -0.000005\n      1.670000e+02\n      241.000000\n      208.000003\n      -0.000004\n      4.021377e-07\n      0.000002\n      0.000002\n      0.000001\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      0.000001\n      9.323434e-07\n      0.000002\n      8.744090e-07\n      0.000002\n      0.000005\n      76.000002\n      239.000004\n      218.999999\n      19.000002\n      0.000005\n      8.867231e-07\n      93.000000\n      244.999997\n      206.000001\n      0.000002\n      -0.000002\n      0.000001\n      -0.000001\n      3.297868e-07\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      0.000002\n      -0.000001\n      8.809801e-07\n      1.723858e-07\n      0.000001\n      4.900000e+01\n      239.000004\n      203.000003\n      -0.000001\n      -0.000001\n      8.076585e-07\n      31.000002\n      238.000000\n      205.000002\n      -1.852046e-07\n      6.976350e-08\n      0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      40.000001\n      2.390000e+02\n      191.000002\n      -0.000002\n      -0.000001\n      0.000002\n      18.000001\n      228.000000\n      2.010000e+02\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      -0.000002\n      0.000001\n      29.000000\n      245.999998\n      198.999995\n      0.000002\n      -0.000003\n      0.000001\n      9.000003\n      239.000002\n      219.999997\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      9.522988e-07\n      1.600000e+02\n      1.340000e+02\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      1.380000e+02\n      128.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      1\n    \n    \n      5\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      9.682471e-07\n      38.000000\n      3.000000e+01\n      6.000002e+00\n      1.600000e+01\n      3.900000e+01\n      2.500000e+01\n      -0.000001\n      5.516976e-08\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      7.000000\n      3.000000e+01\n      68.000001\n      64.999999\n      7.500000e+01\n      39.999998\n      67.000000\n      54.999999\n      3.000000e+00\n      0.000001\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      3.000000e+00\n      3.900000e+01\n      75.999999\n      56.000000\n      9.000000\n      132.999999\n      161.999998\n      151.000001\n      1.520000e+02\n      164.000000\n      72.999999\n      22.999999\n      4.800000e+01\n      61.000000\n      12.000000\n      7.057845e-07\n      -1.318873e-07\n      1.000000e+00\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      1.600000e+01\n      6.800000e+01\n      6.300000e+01\n      3.800000e+01\n      24.999998\n      0.000005\n      109.000000\n      241.000002\n      240.000000\n      255.000003\n      117.000000\n      2.000000e+01\n      1.880000e+02\n      47.000001\n      30.000001\n      5.400000e+01\n      5.500000e+01\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.400000e+01\n      3.800000e+01\n      20.000000\n      54.000000\n      58.999999\n      100.000000\n      0.000003\n      138.000000\n      1.840000e+02\n      1.860000e+02\n      0.000004\n      0.000004\n      10.999999\n      1.050000e+02\n      42.000001\n      1.900000e+01\n      4.100000e+01\n      3.000001e+00\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      41.000000\n      33.000000\n      35.000001\n      59.000000\n      41.999999\n      22.999999\n      9.999996e+00\n      -0.000001\n      101.000000\n      56.000002\n      0.000004\n      19.000002\n      8.000001\n      34.999998\n      41.000000\n      1.600000e+01\n      41.000001\n      2.400000e+01\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      1.000000e+00\n      45.000000\n      36.999999\n      34.000001\n      2.200000e+01\n      24.000000\n      2.600000e+01\n      -0.000002\n      0.000005\n      -0.000001\n      25.000001\n      15.999998\n      7.000003\n      14.000005\n      95.000000\n      12.999997\n      16.999999\n      3.200000e+01\n      34.000001\n      8.919840e-07\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.000000e+00\n      44.000000\n      34.000000\n      42.000000\n      28.999999\n      51.999999\n      1.370000e+02\n      136.999999\n      1.110000e+02\n      49.999997\n      17.000002\n      26.999997\n      23.999996\n      72.000000\n      124.000000\n      5.000002\n      28.000002\n      30.999998\n      37.000000\n      1.000000\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      1.400000e+01\n      49.000000\n      2.500000e+01\n      48.999999\n      3.900000e+01\n      26.000002\n      39.999999\n      81.000000\n      117.000000\n      125.000000\n      95.000000\n      54.000002\n      17.999999\n      42.999996\n      55.000001\n      2.999996\n      3.000000e+01\n      26.999999\n      33.000001\n      9.000000\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      2.600000e+01\n      4.900000e+01\n      8.000000e+00\n      141.000003\n      75.000000\n      11.000000\n      14.000002\n      6.000004\n      14.000001\n      35.000000\n      7.200000e+01\n      93.999999\n      13.000003\n      72.999998\n      8.999999\n      81.000001\n      126.000000\n      4.999995\n      34.000002\n      19.000001\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      3.100000e+01\n      5.100000e+01\n      13.999998\n      159.999997\n      88.000000\n      13.000001\n      20.000000\n      25.000000\n      17.000003\n      19.000002\n      11.000003\n      38.000001\n      133.000000\n      69.000003\n      0.000005\n      159.000000\n      1.380000e+02\n      1.000002\n      33.000002\n      2.600000e+01\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      3.400000e+01\n      4.700000e+01\n      45.000000\n      99.999999\n      23.000000\n      71.000000\n      31.000002\n      17.999999\n      41.999996\n      75.999998\n      13.000002\n      23.999999\n      82.000002\n      59.000001\n      22.000000\n      37.999999\n      82.999999\n      12.999999\n      32.999999\n      28.999998\n      -0.000001\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      41.000000\n      39.999999\n      72.000000\n      68.000000\n      5.000001\n      100.000000\n      50.000000\n      23.000004\n      32.999996\n      95.000000\n      9.999981e-01\n      67.999997\n      62.999997\n      66.999996\n      24.999999\n      16.000001\n      93.999999\n      22.000005\n      33.999999\n      33.000002\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      42.000000\n      42.000001\n      8.500000e+01\n      57.000001\n      72.000001\n      49.000002\n      33.999996\n      27.999999\n      37.999997\n      122.000000\n      0.000002\n      105.999999\n      82.000001\n      12.999995\n      94.999998\n      0.000004\n      8.900000e+01\n      35.999998\n      38.000000\n      32.999998\n      3.999999\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      7.000000e+00\n      40.000000\n      3.600000e+01\n      97.000000\n      54.000000\n      45.999999\n      27.000003\n      36.000000\n      29.999999\n      40.999997\n      1.110000e+02\n      -0.000004\n      119.000000\n      56.000002\n      3.000001\n      79.000004\n      46.000002\n      62.999998\n      6.800000e+01\n      3.800000e+01\n      24.999999\n      9.000003\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      1.600000e+01\n      36.999999\n      35.000001\n      1.140000e+02\n      30.000001\n      26.000002\n      3.000000e+01\n      32.999997\n      22.000002\n      37.000002\n      110.000000\n      0.000001\n      125.999999\n      32.999996\n      16.000001\n      29.000002\n      53.000004\n      55.999998\n      90.999999\n      3.700000e+01\n      22.000000\n      16.000000\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      22.000000\n      3.500000e+01\n      3.900000e+01\n      131.000000\n      2.000004\n      40.999999\n      52.000000\n      50.999997\n      33.999998\n      40.999997\n      104.999999\n      0.999995\n      136.000000\n      19.000007\n      28.000002\n      34.000002\n      1.800000e+01\n      35.999998\n      1.160000e+02\n      31.000001\n      23.000002\n      18.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      2.200000e+01\n      33.000000\n      54.000001\n      137.999999\n      0.000005\n      47.000002\n      63.999998\n      41.000000\n      28.000001\n      56.999998\n      94.999998\n      43.999996\n      1.070000e+02\n      38.000001\n      45.000003\n      42.999999\n      2.600000e+01\n      8.000003e+00\n      1.440000e+02\n      30.000003\n      22.000002\n      1.900000e+01\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      2.700000e+01\n      23.999998\n      62.000001\n      130.000001\n      -0.000005\n      22.999996\n      50.999996\n      43.000002\n      35.000004\n      42.000002\n      19.999999\n      58.999998\n      18.000000\n      39.999998\n      42.000000\n      24.999998\n      26.000004\n      -0.000003\n      142.999999\n      34.000001\n      2.200000e+01\n      2.400000e+01\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      2.900000e+01\n      2.400000e+01\n      7.200000e+01\n      109.999999\n      2.000004\n      5.800000e+01\n      59.000000\n      59.999996\n      58.000004\n      73.999996\n      52.999996\n      83.999998\n      4.500000e+01\n      52.000000\n      55.999999\n      37.000002\n      39.999998\n      6.000001\n      120.000001\n      41.000001\n      17.999997\n      20.000000\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      27.000000\n      18.999999\n      75.000000\n      9.700000e+01\n      0.000004\n      4.500000e+01\n      4.300000e+01\n      60.000004\n      62.999998\n      77.000001\n      58.000001\n      120.000001\n      98.999999\n      29.000004\n      47.999997\n      36.000001\n      29.000001\n      6.999998\n      1.180000e+02\n      42.000001\n      20.000002\n      24.000000\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      6.200000e+01\n      3.600000e+01\n      64.000000\n      81.000000\n      6.275720e-07\n      4.274649e-07\n      0.000001\n      5.958969e-07\n      6.911157e-07\n      -0.000003\n      -0.000004\n      9.999997\n      1.999998\n      -3.494023e-08\n      0.000001\n      0.000001\n      0.000004\n      6.923674e-07\n      100.000000\n      3.700000e+01\n      5.800000e+01\n      5.100000e+01\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      8.300000e+01\n      35.000001\n      58.000000\n      9.100000e+01\n      7.378563e-07\n      -0.000002\n      3.950150e-07\n      0.000003\n      -0.000003\n      0.000004\n      -4.699235e-07\n      0.000003\n      -0.000005\n      9.842208e-07\n      0.000005\n      -0.000004\n      -0.000004\n      4.021377e-07\n      100.000000\n      27.999998\n      70.000000\n      6.000000e+01\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      8.100000e+01\n      36.000000\n      5.500000e+01\n      95.000000\n      8.744090e-07\n      0.000002\n      0.000005\n      0.000003\n      0.000005\n      -0.000001\n      0.000004\n      1.000001\n      8.867231e-07\n      0.000003\n      -0.000003\n      0.000005\n      0.000002\n      -0.000002\n      96.000000\n      29.999999\n      7.500000e+01\n      5.800000e+01\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      6.900000e+01\n      4.100000e+01\n      51.000000\n      102.000001\n      8.809801e-07\n      1.723858e-07\n      0.000001\n      -7.570235e-07\n      0.000001\n      0.000002\n      -0.000001\n      -0.000001\n      8.076585e-07\n      -0.000005\n      -0.000006\n      0.000005\n      -1.852046e-07\n      6.976350e-08\n      88.000001\n      28.000000\n      73.999999\n      52.999999\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      6.900000e+01\n      38.000000\n      46.000000\n      106.000000\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      -0.000002\n      3.933536e-07\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      0.000002\n      -0.000005\n      1.000001e+00\n      0.000002\n      -0.000002\n      90.999999\n      1.900000e+01\n      79.999999\n      5.300000e+01\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      7.600000e+01\n      5.000000e+01\n      6.000000e+01\n      106.000001\n      6.000001e+00\n      -0.000002\n      0.000001\n      0.000003\n      0.000002\n      -0.000002\n      0.000002\n      -0.000003\n      0.000001\n      -0.000003\n      0.000001\n      1.000001\n      -0.000002\n      3.000000\n      99.000002\n      43.000000\n      99.999998\n      6.000000e+01\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      3.800000e+01\n      1.600000e+01\n      3.743793e-07\n      1.100000e+01\n      6.560168e-08\n      2.924545e-07\n      9.999998e-01\n      9.522988e-07\n      -3.426212e-07\n      -6.002562e-07\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      -3.593068e-07\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      3.000001e+00\n      -6.984834e-07\n      2.100000e+01\n      9.000000e+00\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      4\n    \n    \n      6\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      1.250000e+02\n      114.000003\n      1.090000e+02\n      1.060000e+02\n      1.110000e+02\n      1.200000e+02\n      1.130000e+02\n      131.999999\n      9.900000e+01\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      4.000002\n      2.550000e+02\n      237.999997\n      254.999999\n      2.550000e+02\n      255.000004\n      255.000004\n      255.000000\n      2.460000e+02\n      255.000007\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      0.000002\n      110.000001\n      236.999999\n      217.999997\n      218.000003\n      219.999999\n      2.190000e+02\n      215.999998\n      218.000001\n      216.999996\n      2.550000e+02\n      21.999999\n      -0.000002\n      7.057845e-07\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      8.386749e-07\n      219.000000\n      232.999996\n      218.999999\n      224.999995\n      224.999998\n      224.000003\n      223.000003\n      2.240000e+02\n      2.150000e+02\n      255.000000\n      110.000001\n      5.257434e-07\n      -3.949514e-07\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      -0.000002\n      253.999995\n      225.000000\n      217.000005\n      220.000004\n      2.220000e+02\n      2.230000e+02\n      220.999998\n      223.000002\n      211.000002\n      2.520000e+02\n      146.000002\n      5.942894e-07\n      9.570876e-07\n      7.753735e-07\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      -0.000002\n      250.000000\n      223.000005\n      2.180000e+02\n      220.000001\n      220.999995\n      218.999997\n      221.000002\n      219.000003\n      216.000004\n      246.000001\n      133.000000\n      2.016850e-07\n      -0.000002\n      -4.557509e-07\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      -0.000002\n      0.000002\n      0.000001\n      -8.460442e-08\n      240.000000\n      2.240000e+02\n      220.000000\n      221.999997\n      226.000002\n      227.000001\n      218.999998\n      221.000000\n      215.000002\n      243.000003\n      140.999999\n      0.000005\n      -7.583148e-07\n      -0.000002\n      8.919840e-07\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      -0.000001\n      0.000001\n      251.000000\n      2.270000e+02\n      217.000000\n      2.250000e+02\n      240.000004\n      232.999997\n      225.999998\n      218.999998\n      214.999997\n      239.000004\n      160.000002\n      0.000002\n      -0.000002\n      0.000002\n      0.000001\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      7.406876e-07\n      -0.000003\n      -9.455209e-07\n      254.000002\n      220.000004\n      215.999999\n      230.000000\n      220.999998\n      130.000000\n      255.000002\n      215.999997\n      216.999998\n      248.999996\n      131.000000\n      3.125923e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      9.804652e-07\n      0.000001\n      22.999998\n      255.000003\n      215.999997\n      211.000002\n      252.000001\n      175.999998\n      2.300000e+01\n      254.999998\n      217.999999\n      214.000001\n      255.000002\n      102.000000\n      -0.000002\n      -0.000006\n      0.000005\n      0.000002\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      0.000002\n      0.000003\n      37.999998\n      254.999999\n      216.000001\n      208.999997\n      254.999999\n      86.000000\n      -0.000003\n      255.000002\n      222.000002\n      217.999998\n      255.000001\n      74.999998\n      8.404825e-07\n      -0.000003\n      -0.000002\n      1.135173e-07\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      -0.000002\n      0.000002\n      37.000002\n      255.000002\n      214.000003\n      211.999997\n      254.999996\n      19.000001\n      0.000004\n      254.000001\n      228.000004\n      217.000001\n      254.999998\n      54.000001\n      -0.000005\n      -0.000002\n      0.000005\n      0.000002\n      -0.000001\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      0.000001\n      0.000003\n      33.000001\n      255.000001\n      212.999995\n      214.000002\n      254.999999\n      -0.000002\n      -5.688553e-07\n      220.000002\n      233.999997\n      218.000000\n      255.000004\n      12.999999\n      -0.000003\n      -0.000005\n      -0.000002\n      0.000003\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      -7.203892e-07\n      0.000001\n      37.000002\n      255.000000\n      212.000002\n      222.000002\n      214.000000\n      -0.000002\n      0.000002\n      186.999999\n      236.000002\n      219.999998\n      246.000000\n      0.000004\n      5.151848e-07\n      -0.000001\n      0.000001\n      0.000004\n      0.000002\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      0.000002\n      0.000005\n      55.000001\n      255.000002\n      209.000000\n      227.000000\n      193.000001\n      4.167304e-07\n      -0.000004\n      160.000000\n      239.000001\n      220.999999\n      238.000001\n      0.000005\n      0.000001\n      7.423447e-07\n      1.327465e-07\n      0.000005\n      0.000002\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      0.000002\n      -5.223541e-09\n      0.000004\n      41.000002\n      2.550000e+02\n      211.000001\n      229.000003\n      189.000001\n      0.000002\n      0.000001\n      140.000000\n      239.999997\n      222.999999\n      233.000001\n      -0.000001\n      0.000002\n      0.000005\n      4.826824e-07\n      -0.000005\n      0.000002\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      -5.332055e-07\n      5.139264e-07\n      -0.000002\n      -0.000004\n      -0.000002\n      254.000003\n      216.000002\n      227.999997\n      215.000000\n      -0.000004\n      -0.000004\n      113.000001\n      241.000002\n      223.999999\n      221.000002\n      -1.050891e-07\n      -0.000005\n      -4.653743e-07\n      -0.000005\n      0.000002\n      -0.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      0.000002\n      0.000002\n      0.000005\n      0.000002\n      240.000004\n      224.000001\n      235.000004\n      196.000001\n      0.000004\n      -0.000001\n      7.200000e+01\n      238.999997\n      225.000001\n      221.999998\n      8.073550e-07\n      3.296967e-07\n      -6.241514e-07\n      -0.000005\n      0.000002\n      5.651978e-07\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.352320e-07\n      0.000002\n      -0.000002\n      -0.000002\n      -0.000005\n      -0.000004\n      176.999999\n      231.999996\n      229.000000\n      200.999998\n      -0.000004\n      0.000003\n      48.999999\n      234.000003\n      228.000003\n      226.000003\n      -0.000003\n      -0.000003\n      0.000004\n      -0.000005\n      -2.677840e-07\n      -3.174695e-07\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      -9.513843e-07\n      -2.911069e-07\n      2.894738e-08\n      0.000004\n      0.000005\n      -3.351748e-07\n      112.000001\n      236.999999\n      217.000000\n      255.000002\n      -0.000003\n      -0.000002\n      5.000000e+01\n      233.999996\n      229.999998\n      226.000004\n      0.000005\n      0.000003\n      0.000005\n      -0.000002\n      -0.000001\n      -0.000002\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      -0.000002\n      -0.000002\n      -0.000003\n      3.360567e-07\n      0.000004\n      1.282302e-07\n      6.400000e+01\n      238.000001\n      213.000000\n      232.000001\n      31.000001\n      -0.000002\n      62.000000\n      240.000004\n      227.000000\n      213.000000\n      -0.000004\n      -0.000001\n      -2.610268e-07\n      0.000003\n      -0.000002\n      -0.000002\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      -2.873870e-07\n      5.500636e-07\n      0.000001\n      -0.000002\n      6.275720e-07\n      4.274649e-07\n      28.000004\n      2.290000e+02\n      2.160000e+02\n      236.000004\n      71.000000\n      -0.000002\n      59.000001\n      2.390000e+02\n      227.999996\n      211.000002\n      0.000004\n      6.923674e-07\n      0.000001\n      -8.325330e-07\n      -5.767398e-07\n      -1.849482e-07\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      0.000001\n      -0.000001\n      -4.551377e-07\n      7.378563e-07\n      -0.000002\n      3.950150e-07\n      212.000002\n      222.000000\n      237.000004\n      1.090000e+02\n      0.000003\n      51.000001\n      2.370000e+02\n      229.000001\n      219.999996\n      -0.000004\n      4.021377e-07\n      0.000002\n      0.000002\n      0.000001\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      0.000001\n      9.323434e-07\n      0.000002\n      8.744090e-07\n      0.000002\n      0.000005\n      207.999998\n      227.999997\n      236.000003\n      158.000001\n      0.000005\n      5.600000e+01\n      239.000004\n      229.000003\n      218.000000\n      0.000002\n      -0.000002\n      0.000001\n      -0.000001\n      3.297868e-07\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      0.000002\n      -0.000001\n      8.809801e-07\n      1.000002e+00\n      0.000001\n      1.510000e+02\n      238.000000\n      230.999997\n      206.999996\n      -0.000001\n      5.100000e+01\n      239.999999\n      231.000004\n      214.000003\n      -1.852046e-07\n      6.976350e-08\n      0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      3.000001\n      7.009453e-07\n      103.000000\n      2.360000e+02\n      225.000002\n      238.000004\n      -0.000001\n      41.000001\n      234.000001\n      230.999997\n      2.050000e+02\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      3.000000\n      0.000001\n      40.000002\n      242.999999\n      223.999995\n      254.999998\n      -0.000003\n      34.999997\n      247.000003\n      245.999994\n      200.000003\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      3.000001e+00\n      -2.304249e-08\n      9.522988e-07\n      1.720000e+02\n      2.240000e+02\n      1.390000e+02\n      8.776177e-07\n      -0.000002\n      1.050000e+02\n      1.670000e+02\n      27.000000\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      1\n    \n    \n      7\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      -4.371375e-07\n      9.682471e-07\n      -0.000002\n      -5.505288e-07\n      4.053924e-07\n      4.035032e-07\n      2.956210e-07\n      4.005417e-07\n      -0.000001\n      5.516976e-08\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      -8.031146e-08\n      1.985495e-07\n      -1.276844e-07\n      -0.000001\n      7.044741e-07\n      -0.000002\n      -0.000002\n      -7.585758e-07\n      -0.000006\n      0.000006\n      0.000002\n      -1.279166e-07\n      0.000001\n      -9.767781e-07\n      -6.219337e-07\n      1.444539e-07\n      -1.736492e-07\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      2.055711e-07\n      4.611180e-07\n      3.908779e-07\n      0.000002\n      -0.000002\n      0.000002\n      0.000001\n      -0.000003\n      0.000003\n      -4.594104e-07\n      0.000004\n      0.000005\n      0.000001\n      -4.233165e-07\n      0.000002\n      -0.000002\n      7.057845e-07\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      7.656082e-07\n      -3.514349e-07\n      -9.688781e-07\n      8.386749e-07\n      0.000002\n      0.000005\n      0.000004\n      -0.000003\n      -0.000003\n      -0.000003\n      -0.000004\n      -6.836428e-07\n      -3.299791e-07\n      -0.000005\n      0.000002\n      5.257434e-07\n      -3.949514e-07\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      2.273738e-07\n      6.555034e-07\n      -0.000001\n      -0.000002\n      0.000002\n      0.000005\n      0.000003\n      0.000002\n      9.626116e-07\n      6.564152e-07\n      0.000004\n      0.000004\n      -0.000005\n      2.960771e-07\n      -0.000002\n      5.942894e-07\n      9.570876e-07\n      7.753735e-07\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      -0.000002\n      0.000001\n      -0.000002\n      -0.000005\n      -0.000001\n      6.513797e-07\n      -0.000001\n      0.000005\n      -0.000004\n      0.000004\n      -0.000001\n      0.000004\n      0.000002\n      0.000004\n      2.016850e-07\n      -0.000002\n      -4.557509e-07\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      1.030000e+02\n      6.700000e+01\n      6.900000e+01\n      68.000000\n      60.000000\n      56.000000\n      5.400000e+01\n      49.999999\n      5.100000e+01\n      51.000002\n      49.000003\n      55.000002\n      59.000002\n      55.999998\n      58.000001\n      60.000001\n      71.000001\n      72.000001\n      74.000000\n      7.500000e+01\n      81.000000\n      8.300000e+01\n      7.900000e+01\n      8.100000e+01\n      1.220000e+02\n      1.100000e+01\n      -2.890140e-08\n      5.900000e+01\n      2.370000e+02\n      2.280000e+02\n      2.270000e+02\n      254.999999\n      255.000005\n      254.999999\n      255.000005\n      252.999998\n      2.530000e+02\n      252.999997\n      2.550000e+02\n      255.000003\n      254.999997\n      254.999997\n      254.999999\n      254.999995\n      255.000000\n      254.999999\n      254.999996\n      254.999998\n      254.999999\n      255.000001\n      2.550000e+02\n      2.290000e+02\n      2.480000e+02\n      1.250000e+02\n      2.976238e-08\n      1.230000e+02\n      2.380000e+02\n      2.120000e+02\n      2.140000e+02\n      213.999998\n      2.150000e+02\n      214.999996\n      2.130000e+02\n      209.000005\n      209.000001\n      209.000004\n      210.000003\n      213.000005\n      213.999999\n      215.000003\n      215.999997\n      216.999998\n      217.000003\n      217.000002\n      2.180000e+02\n      217.999999\n      217.999995\n      218.999997\n      218.000006\n      2.180000e+02\n      2.330000e+02\n      1.480000e+02\n      -2.152347e-08\n      1.610000e+02\n      2.370000e+02\n      2.110000e+02\n      2.180000e+02\n      2.170000e+02\n      2.190000e+02\n      217.999998\n      217.000003\n      215.000001\n      212.999998\n      213.000001\n      214.000000\n      214.999997\n      2.160000e+02\n      215.999999\n      217.999999\n      217.999999\n      220.000001\n      219.999999\n      221.000005\n      221.999999\n      220.999998\n      222.000002\n      2.250000e+02\n      2.240000e+02\n      2.350000e+02\n      1.770000e+02\n      3.391509e-08\n      1.520000e+02\n      2.400000e+02\n      2.150000e+02\n      2.190000e+02\n      2.150000e+02\n      217.999996\n      217.999999\n      217.000001\n      214.000000\n      211.999998\n      210.999997\n      210.999996\n      213.000003\n      216.000002\n      216.000001\n      218.000002\n      217.999998\n      219.000002\n      219.000003\n      2.200000e+02\n      220.000005\n      221.000002\n      2.250000e+02\n      226.000000\n      221.000008\n      2.320000e+02\n      1.970000e+02\n      -3.134657e-08\n      1.000000e+02\n      2.410000e+02\n      2.210000e+02\n      2.150000e+02\n      2.190000e+02\n      217.999998\n      221.000004\n      219.000001\n      218.000002\n      217.000002\n      215.999997\n      215.000002\n      218.000000\n      217.999998\n      217.999998\n      221.000000\n      220.999999\n      218.999999\n      220.999997\n      220.000001\n      221.000002\n      220.000004\n      222.999999\n      224.000002\n      2.180000e+02\n      255.000002\n      1.500000e+02\n      -1.048992e-07\n      1.610000e+02\n      2.290000e+02\n      2.080000e+02\n      231.999995\n      212.000005\n      219.000000\n      219.000001\n      217.999997\n      218.999999\n      217.999998\n      218.000000\n      217.000002\n      219.000000\n      2.190000e+02\n      218.000000\n      222.000001\n      223.000000\n      223.999999\n      224.000001\n      218.999998\n      218.999997\n      224.999997\n      223.999995\n      227.999995\n      223.000005\n      255.000000\n      1.500000e+01\n      -1.249524e-07\n      2.550000e+02\n      2.420000e+02\n      2.040000e+02\n      205.999994\n      222.999998\n      2.180000e+02\n      222.000004\n      220.999997\n      217.999997\n      218.000002\n      218.000002\n      216.000000\n      217.000002\n      218.000005\n      218.000001\n      218.999999\n      222.000001\n      220.000001\n      216.000001\n      2.170000e+02\n      219.999997\n      222.999995\n      227.000001\n      225.999996\n      2.150000e+02\n      254.999991\n      3.500000e+01\n      5.450883e-08\n      2.250000e+02\n      2.490000e+02\n      2.380000e+02\n      209.999997\n      1.990000e+02\n      220.000004\n      225.999999\n      222.999996\n      222.000002\n      221.000000\n      220.000000\n      219.999999\n      2.180000e+02\n      226.999999\n      221.000001\n      215.999999\n      222.000002\n      220.000000\n      217.000000\n      225.999998\n      2.280000e+02\n      2.230000e+02\n      208.000003\n      218.000004\n      2.320000e+02\n      255.000004\n      6.200000e+01\n      1.800000e+01\n      2.370000e+02\n      2.400000e+02\n      2.430000e+02\n      254.999998\n      237.000000\n      2.050000e+02\n      206.999999\n      219.000000\n      2.250000e+02\n      225.000000\n      223.999998\n      221.000000\n      227.000003\n      213.000002\n      226.000002\n      218.999998\n      223.999999\n      222.999998\n      222.999998\n      225.000005\n      211.999997\n      2.050000e+02\n      230.999996\n      251.999997\n      2.400000e+02\n      2.530000e+02\n      7.000000e+01\n      3.300000e+01\n      2.410000e+02\n      2.380000e+02\n      237.999997\n      2.400000e+02\n      2.500000e+02\n      252.999996\n      234.000005\n      214.999999\n      213.000002\n      216.000002\n      219.999999\n      221.999999\n      235.999996\n      99.000001\n      210.999999\n      226.000002\n      223.999999\n      224.000000\n      2.150000e+02\n      205.000002\n      2.220000e+02\n      246.999999\n      254.999999\n      244.000005\n      236.000001\n      2.550000e+02\n      1.000000e+02\n      2.900000e+01\n      2.410000e+02\n      2.340000e+02\n      2.410000e+02\n      239.000008\n      238.999999\n      237.999999\n      243.999999\n      250.000003\n      236.000004\n      221.000002\n      210.999999\n      213.000001\n      225.999997\n      223.999998\n      2.240000e+02\n      217.000000\n      214.000001\n      209.000001\n      2.170000e+02\n      2.430000e+02\n      2.550000e+02\n      245.000001\n      237.999995\n      2.430000e+02\n      2.350000e+02\n      252.999993\n      1.120000e+02\n      2.000000e+00\n      2.300000e+02\n      242.000000\n      2.390000e+02\n      235.999996\n      242.000004\n      238.000000\n      237.000001\n      238.000002\n      245.000004\n      242.000002\n      236.000004\n      227.000003\n      215.000002\n      215.999998\n      208.000001\n      216.999998\n      222.999998\n      242.000002\n      249.000001\n      248.000001\n      243.999999\n      240.999996\n      2.350000e+02\n      2.410000e+02\n      233.999990\n      2.530000e+02\n      1.190000e+02\n      -1.447285e-07\n      1.890000e+02\n      2.550000e+02\n      2.320000e+02\n      2.350000e+02\n      2.340000e+02\n      236.999997\n      235.999997\n      2.340000e+02\n      234.999998\n      238.999998\n      241.999999\n      242.000004\n      241.000002\n      236.999999\n      2.410000e+02\n      243.000004\n      240.999998\n      237.999996\n      239.999997\n      234.999998\n      235.999999\n      237.999996\n      236.999996\n      237.000008\n      2.340000e+02\n      252.999998\n      5.000000e+01\n      1.363091e-07\n      3.289930e-07\n      2.060000e+02\n      244.000003\n      252.999992\n      254.999998\n      2.550000e+02\n      254.999996\n      2.550000e+02\n      2.500000e+02\n      243.999996\n      238.000001\n      235.000004\n      232.000003\n      230.999998\n      230.000005\n      235.000000\n      238.999996\n      248.999995\n      254.000005\n      254.999995\n      2.550000e+02\n      252.999996\n      248.999999\n      252.999990\n      253.999994\n      1.850000e+02\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      2.500000e+01\n      7.700000e+01\n      78.000000\n      76.000000\n      7.100000e+01\n      7.000000e+01\n      38.999999\n      2.800000e+01\n      1.500000e+01\n      4.000001\n      -0.000004\n      -0.000002\n      0.000002\n      1.999997e+00\n      14.000001\n      51.000002\n      55.000002\n      7.700000e+01\n      108.000000\n      1.340000e+02\n      1.530000e+02\n      1.380000e+02\n      76.999998\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      0.000001\n      -0.000001\n      -4.551377e-07\n      7.378563e-07\n      -0.000002\n      3.950150e-07\n      0.000003\n      -0.000003\n      0.000004\n      -4.699235e-07\n      0.000003\n      -0.000005\n      9.842208e-07\n      0.000005\n      -0.000004\n      -0.000004\n      4.021377e-07\n      0.000002\n      0.000002\n      0.000001\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      0.000001\n      9.323434e-07\n      0.000002\n      8.744090e-07\n      0.000002\n      0.000005\n      0.000003\n      0.000005\n      -0.000001\n      0.000004\n      0.000005\n      8.867231e-07\n      0.000003\n      -0.000003\n      0.000005\n      0.000002\n      -0.000002\n      0.000001\n      -0.000001\n      3.297868e-07\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      0.000002\n      -0.000001\n      8.809801e-07\n      1.723858e-07\n      0.000001\n      -7.570235e-07\n      0.000001\n      0.000002\n      -0.000001\n      -0.000001\n      8.076585e-07\n      -0.000005\n      -0.000006\n      0.000005\n      -1.852046e-07\n      6.976350e-08\n      0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      -0.000002\n      0.000003\n      -3.723668e-07\n      -0.000002\n      7.009453e-07\n      -0.000002\n      3.933536e-07\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      0.000002\n      -0.000005\n      -5.581073e-07\n      0.000002\n      -0.000002\n      -0.000001\n      2.139088e-07\n      0.000002\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      -8.592667e-07\n      0.000002\n      -7.577064e-07\n      -0.000002\n      0.000001\n      0.000003\n      0.000002\n      -0.000002\n      0.000002\n      -0.000003\n      0.000001\n      -0.000003\n      0.000001\n      -0.000002\n      -0.000002\n      -0.000001\n      0.000002\n      -0.000002\n      -0.000002\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      3.743793e-07\n      -6.565379e-07\n      6.560168e-08\n      2.924545e-07\n      -2.304249e-08\n      9.522988e-07\n      -3.426212e-07\n      -6.002562e-07\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      -3.593068e-07\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      8\n    \n    \n      8\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      -2.594887e-09\n      -2.792910e-08\n      -3.333367e-08\n      8.642878e-08\n      2.900000e+01\n      1.570000e+02\n      168.000001\n      1.850000e+02\n      1.820000e+02\n      1.770000e+02\n      1.740000e+02\n      1.690000e+02\n      181.000005\n      1.640000e+02\n      -2.068072e-07\n      -9.467687e-08\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      4.300000e+01\n      1.630000e+02\n      1.880000e+02\n      2.160000e+02\n      255.000010\n      2.550000e+02\n      255.000006\n      250.999996\n      2.540000e+02\n      255.000004\n      255.000004\n      255.000000\n      2.500000e+02\n      255.000007\n      2.550000e+02\n      1.750000e+02\n      1.270000e+02\n      1.600000e+01\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.690000e+02\n      2.500000e+02\n      2.550000e+02\n      2.550000e+02\n      252.999992\n      240.000004\n      234.999995\n      230.999999\n      231.999996\n      228.999997\n      2.300000e+02\n      228.000002\n      230.000004\n      229.000001\n      2.370000e+02\n      245.999994\n      255.000008\n      2.550000e+02\n      2.390000e+02\n      2.110000e+02\n      1.300000e+01\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      8.000000e+00\n      2.160000e+02\n      2.290000e+02\n      2.420000e+02\n      2.430000e+02\n      2.410000e+02\n      224.999998\n      231.000003\n      246.999995\n      240.000002\n      249.000000\n      253.000005\n      236.999995\n      2.300000e+02\n      2.370000e+02\n      241.999999\n      241.999997\n      2.350000e+02\n      2.330000e+02\n      2.270000e+02\n      2.500000e+02\n      1.280000e+02\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      4.000000e+00\n      2.520000e+02\n      2.340000e+02\n      2.300000e+02\n      234.999991\n      225.000000\n      236.000002\n      223.000004\n      237.999998\n      245.000000\n      2.380000e+02\n      2.330000e+02\n      210.000000\n      201.000002\n      229.000004\n      2.260000e+02\n      232.999996\n      2.450000e+02\n      2.300000e+02\n      2.260000e+02\n      2.520000e+02\n      1.080000e+02\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      3.100000e+01\n      249.000004\n      237.000009\n      227.999999\n      230.999999\n      248.999997\n      245.000005\n      2.280000e+02\n      241.000002\n      246.000005\n      230.999996\n      194.999999\n      222.999998\n      254.999996\n      227.000004\n      225.000001\n      2.230000e+02\n      232.999995\n      2.390000e+02\n      1.890000e+02\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      181.000000\n      238.999991\n      204.999995\n      2.160000e+02\n      216.000002\n      2.210000e+02\n      225.000000\n      214.000002\n      225.000000\n      229.999996\n      242.999996\n      238.999999\n      223.999998\n      208.000001\n      206.000000\n      204.000002\n      2.160000e+02\n      252.999997\n      8.919840e-07\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      254.999999\n      219.000005\n      207.000000\n      199.999999\n      205.000005\n      2.040000e+02\n      202.000003\n      2.210000e+02\n      161.000000\n      157.000001\n      220.000003\n      197.000000\n      207.999998\n      207.000001\n      205.999999\n      209.999999\n      203.000005\n      254.999999\n      55.000000\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      5.900000e+01\n      233.000005\n      2.050000e+02\n      206.000005\n      2.040000e+02\n      204.000003\n      206.999997\n      200.000005\n      219.999996\n      173.000000\n      156.000000\n      222.999996\n      204.000002\n      205.999998\n      207.000002\n      209.000001\n      2.060000e+02\n      207.000001\n      233.000003\n      137.000002\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      9.100000e+01\n      2.340000e+02\n      2.080000e+02\n      202.000002\n      207.999999\n      207.999995\n      209.000005\n      206.000004\n      218.999998\n      196.000000\n      1.910000e+02\n      221.999999\n      210.999998\n      211.999999\n      211.999998\n      211.000003\n      209.000002\n      225.999999\n      221.999998\n      199.999996\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      1.210000e+02\n      2.350000e+02\n      226.999995\n      224.999996\n      205.999999\n      211.000003\n      210.000002\n      209.999997\n      220.999996\n      197.000000\n      188.999999\n      224.000000\n      214.000002\n      213.000002\n      214.000001\n      214.000004\n      2.180000e+02\n      237.999999\n      219.999995\n      2.380000e+02\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      1.400000e+02\n      2.390000e+02\n      234.999997\n      215.999995\n      205.999996\n      214.000001\n      211.000005\n      209.999997\n      220.999996\n      196.999998\n      182.000002\n      223.000000\n      214.000001\n      217.000001\n      218.000000\n      216.000005\n      220.000001\n      248.999995\n      215.000003\n      254.999999\n      19.000002\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      182.999996\n      237.000004\n      229.000002\n      211.000002\n      215.999995\n      213.999998\n      212.000000\n      212.000000\n      219.000001\n      201.999999\n      1.810000e+02\n      226.999997\n      213.000001\n      218.000000\n      219.999999\n      216.000002\n      218.999998\n      255.000001\n      207.000001\n      254.999996\n      77.000000\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      221.000003\n      227.000005\n      2.310000e+02\n      212.000006\n      215.000003\n      213.000004\n      212.999998\n      213.000001\n      218.000000\n      209.999998\n      192.999998\n      225.999999\n      212.000000\n      216.000001\n      220.999999\n      221.999999\n      2.120000e+02\n      251.000001\n      211.000003\n      250.999999\n      112.000001\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      243.999994\n      2.300000e+02\n      232.000000\n      212.999997\n      219.999997\n      211.999998\n      214.000000\n      214.000001\n      217.000001\n      2.170000e+02\n      209.000000\n      221.000001\n      213.999999\n      217.000000\n      217.999999\n      224.000000\n      212.000002\n      2.490000e+02\n      2.170000e+02\n      241.999995\n      153.999998\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      236.999994\n      234.000003\n      2.210000e+02\n      214.999999\n      225.999999\n      2.090000e+02\n      215.000002\n      214.000002\n      220.000002\n      205.000002\n      190.000000\n      226.999997\n      212.000001\n      217.999999\n      213.000000\n      219.999997\n      216.999996\n      246.999999\n      2.200000e+02\n      230.999996\n      182.000004\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      2.490000e+02\n      2.270000e+02\n      235.999999\n      217.999998\n      230.999997\n      210.999999\n      216.000002\n      215.000000\n      220.000001\n      209.999998\n      184.000000\n      227.000002\n      214.999999\n      216.999999\n      216.000000\n      2.230000e+02\n      222.000000\n      2.550000e+02\n      225.000005\n      217.000005\n      200.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      3.200000e+01\n      255.000007\n      223.000000\n      241.999997\n      220.999998\n      229.000005\n      215.000002\n      214.999998\n      215.999999\n      218.999999\n      217.000002\n      199.999998\n      2.250000e+02\n      213.999999\n      214.000001\n      214.999999\n      2.270000e+02\n      2.220000e+02\n      2.480000e+02\n      234.000003\n      213.999996\n      2.090000e+02\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.700000e+01\n      254.999993\n      221.999995\n      240.000002\n      221.999997\n      227.000000\n      215.999998\n      216.000001\n      217.000002\n      220.000001\n      215.000002\n      201.000000\n      225.999996\n      213.999999\n      216.999999\n      213.000000\n      222.000002\n      223.999999\n      240.999999\n      233.999996\n      2.190000e+02\n      2.150000e+02\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      8.300000e+01\n      2.550000e+02\n      2.170000e+02\n      236.999997\n      221.000004\n      2.250000e+02\n      213.999999\n      216.999999\n      217.999999\n      221.000000\n      217.000001\n      202.999998\n      2.290000e+02\n      217.999999\n      219.000002\n      218.000001\n      218.000003\n      224.000005\n      234.999994\n      229.999998\n      215.999999\n      220.999998\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      97.999999\n      255.000009\n      217.000004\n      2.370000e+02\n      219.000001\n      2.220000e+02\n      2.140000e+02\n      215.999999\n      217.000002\n      222.000001\n      214.000002\n      196.999999\n      226.000005\n      214.999998\n      215.999999\n      219.999995\n      217.000005\n      217.000004\n      2.370000e+02\n      231.000005\n      216.000003\n      224.000004\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      9.800000e+01\n      2.550000e+02\n      219.000003\n      238.999997\n      2.140000e+02\n      2.190000e+02\n      218.000000\n      2.200000e+02\n      2.190000e+02\n      223.000001\n      216.000002\n      200.000002\n      231.999996\n      2.180000e+02\n      221.000002\n      219.000000\n      219.000004\n      2.080000e+02\n      239.999997\n      2.320000e+02\n      2.160000e+02\n      2.240000e+02\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      1.020000e+02\n      255.000008\n      220.000000\n      2.380000e+02\n      2.120000e+02\n      220.999996\n      2.140000e+02\n      216.999998\n      219.000002\n      224.999995\n      2.170000e+02\n      202.000000\n      228.999995\n      2.170000e+02\n      219.999997\n      219.000000\n      219.000002\n      2.090000e+02\n      233.999996\n      234.000003\n      216.999995\n      2.240000e+02\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      1.090000e+02\n      254.999991\n      2.210000e+02\n      243.999996\n      2.220000e+02\n      215.000000\n      222.000005\n      227.000004\n      226.000004\n      225.999999\n      212.999999\n      201.000003\n      2.210000e+02\n      221.999999\n      224.000005\n      224.000005\n      215.999996\n      215.999996\n      241.999996\n      224.000002\n      2.170000e+02\n      2.200000e+02\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      1.040000e+02\n      2.550000e+02\n      220.000005\n      241.000000\n      2.430000e+02\n      2.550000e+02\n      251.000000\n      2.540000e+02\n      248.999996\n      245.999998\n      227.000003\n      215.999995\n      2.450000e+02\n      249.000004\n      252.000004\n      249.999997\n      2.510000e+02\n      2.440000e+02\n      245.999993\n      224.999996\n      217.000005\n      226.999995\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      9.800000e+01\n      236.999994\n      211.000003\n      244.999999\n      2.050000e+02\n      104.000001\n      9.900000e+01\n      87.000000\n      8.600000e+01\n      82.000001\n      89.000000\n      95.000000\n      71.000000\n      86.000000\n      99.000000\n      1.090000e+02\n      91.000000\n      179.999999\n      251.000002\n      2.210000e+02\n      226.000007\n      2.140000e+02\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      2.350000e+02\n      2.430000e+02\n      246.999997\n      -7.577064e-07\n      -0.000002\n      0.000001\n      0.000003\n      0.000002\n      -0.000002\n      0.000002\n      28.000000\n      0.000001\n      -0.000003\n      0.000001\n      -0.000002\n      -0.000002\n      -0.000001\n      250.999997\n      242.999990\n      255.000007\n      3.700000e+01\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      1.560000e+02\n      2.370000e+02\n      2.210000e+02\n      6.560168e-08\n      2.924545e-07\n      3.999999e+00\n      2.000000e+00\n      1.000002e+00\n      -6.002562e-07\n      9.999983e-01\n      3.000000e+00\n      -0.000002\n      1.999999e+00\n      3.000001e+00\n      0.999999\n      3.771737e-07\n      5.207580e-07\n      1.710000e+02\n      2.410000e+02\n      2.270000e+02\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      4\n    \n    \n      9\n      -1.628456e-11\n      2.116344e-10\n      -1.459979e-10\n      2.426657e-09\n      -2.665179e-11\n      1.000000e+00\n      1.000000e+00\n      2.000000e+00\n      8.642878e-08\n      -4.371375e-07\n      3.400000e+01\n      89.999999\n      1.280000e+02\n      1.460000e+02\n      1.560000e+02\n      1.600000e+02\n      1.310000e+02\n      54.000000\n      5.516976e-08\n      -2.068072e-07\n      4.000000e+00\n      9.006170e-08\n      -2.751660e-08\n      -2.676011e-08\n      1.534132e-09\n      3.965276e-09\n      4.171343e-09\n      5.597892e-10\n      -2.459002e-10\n      1.018412e-09\n      4.727461e-09\n      4.497033e-09\n      -4.694904e-09\n      8.493365e-08\n      2.000000e+00\n      1.985495e-07\n      4.999999e+00\n      140.999998\n      1.740000e+02\n      213.999996\n      211.000002\n      2.240000e+02\n      236.999998\n      240.000004\n      225.999999\n      2.270000e+02\n      177.000001\n      -9.767781e-07\n      -6.219337e-07\n      9.999995e-01\n      9.999998e-01\n      -8.071513e-09\n      -1.345314e-08\n      -3.650504e-08\n      3.064484e-09\n      1.395653e-09\n      -5.678459e-10\n      -8.652231e-10\n      6.095701e-09\n      -2.268324e-08\n      1.082306e-07\n      1.000000e+00\n      7.000001e+00\n      3.908779e-07\n      112.000002\n      237.000000\n      219.999999\n      197.999999\n      199.000000\n      191.000000\n      1.910000e+02\n      180.000001\n      205.999997\n      231.000004\n      2.500000e+02\n      124.000000\n      -0.000002\n      5.000000e+00\n      -1.318873e-07\n      -1.672773e-07\n      2.125031e-09\n      -8.742525e-08\n      -1.006425e-08\n      -5.139340e-09\n      2.001768e-09\n      -9.364624e-10\n      -1.518954e-08\n      -4.168295e-08\n      1.493545e-07\n      9.999997e-01\n      5.000001e+00\n      -9.688781e-07\n      2.600000e+01\n      214.000000\n      197.000001\n      198.999997\n      247.000001\n      246.999999\n      244.999999\n      239.000005\n      2.090000e+02\n      2.010000e+02\n      241.999999\n      111.999999\n      5.257434e-07\n      5.000000e+00\n      -1.805364e-07\n      4.216997e-07\n      -1.558985e-07\n      -1.001893e-07\n      -4.949160e-09\n      1.981593e-08\n      2.329049e-09\n      -5.676910e-09\n      -1.800060e-08\n      -3.427830e-08\n      -5.326159e-07\n      9.999991e-01\n      6.555034e-07\n      -0.000001\n      -0.000002\n      0.000002\n      199.000006\n      217.000005\n      247.999998\n      2.120000e+02\n      2.080000e+02\n      248.999998\n      225.000000\n      177.999999\n      6.400000e+01\n      -0.000002\n      5.942894e-07\n      9.570876e-07\n      7.753735e-07\n      -8.642634e-07\n      -4.548926e-07\n      1.847391e-08\n      -1.021340e-08\n      1.077761e-08\n      -2.561862e-09\n      -4.695927e-08\n      4.582200e-08\n      -3.727119e-08\n      1.100016e-07\n      0.000002\n      1.000002\n      0.000001\n      4.999999\n      125.000001\n      215.000005\n      1.550000e+02\n      219.000005\n      235.999999\n      242.999996\n      255.000001\n      188.000001\n      161.000001\n      148.000001\n      91.000000\n      2.100000e+01\n      -0.000002\n      9.999988e-01\n      5.144796e-07\n      -1.955981e-07\n      1.345243e-07\n      1.621610e-07\n      1.430707e-08\n      -4.618929e-09\n      4.971934e-09\n      6.497929e-08\n      2.643760e-07\n      -2.410009e-07\n      2.999998\n      0.000002\n      30.000000\n      1.950000e+02\n      175.999999\n      1.800000e+02\n      205.999999\n      199.999998\n      227.000004\n      253.999999\n      185.000002\n      168.000000\n      138.000000\n      181.000001\n      200.000000\n      204.000002\n      3.800000e+01\n      -0.000002\n      2.999998e+00\n      -6.656718e-07\n      -5.450623e-07\n      -4.032686e-07\n      -1.347659e-08\n      -2.890140e-08\n      1.131168e-07\n      -4.175154e-08\n      -5.971974e-07\n      3.715096e-07\n      0.000001\n      0.000001\n      170.999996\n      159.000002\n      144.000002\n      1.490000e+02\n      155.000000\n      1.530000e+02\n      190.000003\n      172.999999\n      135.000001\n      159.000000\n      153.000000\n      146.000000\n      145.999999\n      155.999999\n      140.000002\n      0.000002\n      0.000001\n      -7.762443e-07\n      -6.404821e-07\n      -6.205741e-07\n      -9.954905e-08\n      2.976238e-08\n      -9.164984e-08\n      -1.933697e-07\n      -3.981918e-07\n      4.334051e-07\n      -0.000002\n      1.100000e+01\n      166.000000\n      1.530000e+02\n      156.999999\n      159.000001\n      152.000001\n      137.000001\n      178.999998\n      178.000002\n      143.000000\n      162.000001\n      152.000000\n      159.000000\n      158.000001\n      1.520000e+02\n      165.000002\n      40.000002\n      0.000001\n      0.000002\n      8.170825e-07\n      -8.944622e-08\n      9.377016e-08\n      -2.152347e-08\n      -9.805354e-08\n      1.100760e-07\n      -1.284166e-07\n      -8.785177e-07\n      -5.893633e-07\n      5.400000e+01\n      180.000005\n      161.999997\n      153.000000\n      155.000002\n      157.999998\n      145.000000\n      174.999998\n      1.750000e+02\n      143.000000\n      157.000000\n      153.000000\n      152.000000\n      155.000000\n      161.000001\n      168.000001\n      55.000000\n      0.000002\n      -1.907800e-08\n      -6.678754e-07\n      3.626715e-07\n      7.653776e-08\n      3.391509e-08\n      -1.837763e-07\n      -1.002178e-07\n      9.562319e-08\n      5.449854e-07\n      6.690875e-07\n      94.000000\n      184.000000\n      170.000002\n      162.999998\n      152.000001\n      156.000001\n      144.000001\n      166.999999\n      179.999998\n      145.000000\n      157.000000\n      153.000000\n      147.000000\n      178.999998\n      1.660000e+02\n      171.999999\n      116.000000\n      1.135173e-07\n      -0.000001\n      0.000001\n      -7.649599e-07\n      1.421396e-07\n      -3.134657e-08\n      -1.488990e-07\n      -2.217518e-07\n      -7.524475e-07\n      -8.247956e-07\n      2.124748e-07\n      131.999997\n      183.999999\n      181.999997\n      177.000001\n      164.999999\n      155.000000\n      150.000001\n      164.000001\n      186.000000\n      146.000000\n      158.000000\n      157.000000\n      145.000000\n      172.999999\n      169.000001\n      173.999998\n      180.000001\n      0.000002\n      -0.000001\n      -2.314611e-07\n      -0.000002\n      3.276677e-07\n      -1.048992e-07\n      -9.582211e-08\n      -1.728624e-07\n      4.348013e-07\n      0.000001\n      0.000002\n      158.000000\n      179.999998\n      196.999997\n      198.999996\n      178.000001\n      158.999999\n      151.000000\n      161.000001\n      1.880000e+02\n      146.000000\n      157.000000\n      157.000000\n      157.000000\n      187.000000\n      169.000000\n      169.999997\n      215.999999\n      0.000003\n      -0.000002\n      -0.000002\n      0.000001\n      -1.099796e-08\n      -1.249524e-07\n      8.047429e-10\n      1.957992e-07\n      -5.361049e-07\n      0.000001\n      0.000002\n      1.710000e+02\n      172.999998\n      196.000004\n      216.999997\n      183.000002\n      165.000001\n      152.000000\n      160.999999\n      188.999998\n      148.000000\n      159.000000\n      158.000000\n      162.000000\n      194.999999\n      1.680000e+02\n      164.000000\n      212.000003\n      14.999999\n      0.000002\n      3.101954e-07\n      -0.000002\n      -2.671779e-07\n      5.450883e-08\n      -1.799488e-07\n      -2.672337e-07\n      3.815783e-07\n      0.000002\n      -8.765240e-07\n      185.000004\n      167.999998\n      188.999997\n      164.999999\n      186.999999\n      160.000000\n      154.000000\n      1.560000e+02\n      192.999999\n      148.000000\n      159.000000\n      156.000000\n      162.000000\n      176.999999\n      173.000002\n      1.640000e+02\n      1.920000e+02\n      46.000002\n      0.000002\n      3.281200e-07\n      -0.000002\n      -1.307099e-07\n      3.196277e-07\n      5.418402e-08\n      -2.917852e-07\n      7.308653e-07\n      -0.000002\n      13.000001\n      1.930000e+02\n      167.999998\n      184.000000\n      1.170000e+02\n      191.000002\n      157.000000\n      158.000000\n      156.000000\n      194.999998\n      149.000000\n      163.000000\n      150.000000\n      160.000000\n      124.000001\n      169.000001\n      165.999999\n      1.870000e+02\n      70.000001\n      0.000002\n      3.892250e-07\n      6.972123e-07\n      -2.291508e-07\n      -2.135472e-07\n      -4.984975e-07\n      -8.258425e-07\n      0.000001\n      -5.332055e-07\n      3.300000e+01\n      191.999997\n      166.999997\n      195.000000\n      90.000001\n      185.999999\n      156.000000\n      158.000000\n      157.000000\n      193.999999\n      151.000000\n      166.000000\n      153.000000\n      174.000000\n      9.300000e+01\n      158.000000\n      1.880000e+02\n      184.000000\n      96.000000\n      -0.000002\n      0.000001\n      -7.986391e-07\n      -4.286650e-07\n      -1.681014e-07\n      6.363970e-07\n      -6.975870e-07\n      -5.423990e-07\n      0.000001\n      56.000001\n      190.999996\n      168.000003\n      203.999996\n      126.000000\n      172.999999\n      161.000000\n      161.000000\n      155.000000\n      193.000001\n      1.550000e+02\n      163.000000\n      156.000000\n      170.000000\n      1.090000e+02\n      1.520000e+02\n      2.060000e+02\n      180.000000\n      114.999999\n      5.651978e-07\n      -5.163183e-07\n      0.000002\n      -1.174552e-07\n      -5.947502e-08\n      -2.880678e-07\n      -0.000001\n      5.352320e-07\n      0.000002\n      77.000000\n      189.000001\n      173.000001\n      208.000002\n      157.000000\n      174.000001\n      162.000000\n      163.000000\n      154.000000\n      192.999999\n      157.000000\n      163.000000\n      159.000000\n      172.000000\n      130.000000\n      130.000000\n      194.999999\n      177.000003\n      1.270000e+02\n      -3.174695e-07\n      -0.000002\n      -5.629809e-07\n      -3.012789e-07\n      -1.447285e-07\n      -6.046403e-07\n      1.118729e-07\n      -9.513843e-07\n      -2.911069e-07\n      8.100000e+01\n      186.000001\n      176.000003\n      1.970000e+02\n      148.000000\n      174.000000\n      162.999999\n      164.000000\n      156.000000\n      194.999999\n      1.530000e+02\n      164.000000\n      161.000000\n      161.000000\n      155.000001\n      87.999999\n      192.999999\n      176.000001\n      139.000002\n      -0.000002\n      9.265024e-07\n      0.000002\n      4.011770e-07\n      1.363091e-07\n      3.289930e-07\n      -2.624320e-07\n      -0.000002\n      -0.000002\n      90.000000\n      1.830000e+02\n      198.999996\n      1.550000e+02\n      1.290000e+02\n      183.000000\n      161.000000\n      165.000000\n      156.000000\n      195.000000\n      155.000000\n      159.000000\n      164.000001\n      158.999999\n      166.999998\n      100.000000\n      1.940000e+02\n      175.000002\n      149.999999\n      -0.000002\n      -0.000002\n      7.528400e-07\n      -1.202886e-08\n      -1.592361e-07\n      5.095788e-07\n      -4.052591e-08\n      -2.873870e-07\n      5.500636e-07\n      103.000001\n      181.000005\n      2.090000e+02\n      1.010000e+02\n      147.000000\n      1.780000e+02\n      1.630000e+02\n      165.000000\n      155.000000\n      197.000001\n      162.000000\n      1.570000e+02\n      165.000001\n      161.000001\n      161.000002\n      1.520000e+02\n      191.000005\n      1.700000e+02\n      1.600000e+02\n      -1.849482e-07\n      0.000002\n      2.427885e-08\n      -2.188508e-07\n      1.379810e-07\n      -1.391677e-07\n      1.906360e-07\n      4.654391e-07\n      0.000001\n      113.000001\n      1.810000e+02\n      2.100000e+02\n      79.000001\n      1.720000e+02\n      173.000000\n      166.000000\n      169.000001\n      1.570000e+02\n      197.999999\n      167.000000\n      1.570000e+02\n      161.999999\n      161.999998\n      157.000002\n      1.520000e+02\n      184.000001\n      173.000003\n      156.000003\n      7.859119e-07\n      2.466921e-07\n      1.444405e-07\n      5.295760e-08\n      -1.867194e-08\n      -2.868018e-07\n      -7.280360e-07\n      -6.257008e-07\n      0.000001\n      1.320000e+02\n      179.000002\n      2.110000e+02\n      101.000000\n      180.000001\n      170.000001\n      167.000000\n      171.000001\n      158.000001\n      193.000000\n      1.720000e+02\n      163.000001\n      155.000000\n      165.000000\n      159.999998\n      150.000003\n      165.000004\n      181.000002\n      1.570000e+02\n      7.765040e-07\n      -1.484936e-08\n      5.969346e-07\n      4.756745e-08\n      -9.525822e-08\n      -1.043257e-07\n      -3.057516e-08\n      -7.851076e-07\n      -8.448512e-07\n      148.000005\n      180.000002\n      2.020000e+02\n      1.130000e+02\n      179.000001\n      1.660000e+02\n      164.999998\n      166.000001\n      161.999999\n      180.999998\n      1.640000e+02\n      162.999997\n      160.000002\n      157.000003\n      1.560000e+02\n      1.540000e+02\n      162.000005\n      169.999998\n      156.000000\n      0.000002\n      5.335659e-07\n      -5.266864e-07\n      8.280237e-08\n      3.033599e-08\n      5.283691e-08\n      1.380712e-07\n      1.159922e-07\n      -0.000001\n      150.999996\n      182.000001\n      1.810000e+02\n      136.000000\n      2.090000e+02\n      177.999998\n      1.770000e+02\n      177.999999\n      171.999998\n      193.999998\n      176.999999\n      168.000000\n      171.000002\n      1.690000e+02\n      174.000004\n      167.000003\n      160.000002\n      1.870000e+02\n      166.999997\n      -5.421757e-07\n      6.163737e-07\n      -5.578727e-08\n      4.793912e-09\n      -4.510589e-09\n      -9.451520e-09\n      -1.198971e-07\n      -2.364927e-08\n      -9.384266e-07\n      1.520000e+02\n      186.999998\n      1.790000e+02\n      6.000001\n      92.000001\n      108.000001\n      119.000000\n      119.000001\n      117.000001\n      154.999999\n      147.999997\n      140.000001\n      144.000000\n      130.000002\n      77.999999\n      -0.000001\n      38.000000\n      209.999999\n      146.000004\n      4.965935e-07\n      -2.344399e-07\n      -1.488622e-08\n      1.608706e-08\n      -3.893886e-11\n      -3.305654e-09\n      -5.438611e-09\n      -4.320514e-08\n      -2.151095e-07\n      1.660000e+02\n      2.090000e+02\n      1.280000e+02\n      2.924545e-07\n      -2.304249e-08\n      9.522988e-07\n      -3.426212e-07\n      -6.002562e-07\n      -3.102456e-07\n      8.776177e-07\n      -0.000002\n      6.155732e-07\n      -3.593068e-07\n      0.000002\n      3.771737e-07\n      5.207580e-07\n      -1.732680e-07\n      -6.984834e-07\n      -2.398092e-07\n      2.374313e-07\n      -4.985816e-08\n      -2.682765e-10\n      -2.551856e-09\n      4\n    \n  \n\n\n\nBehind the scenes this also splits our data into a training dataset that we use to update the model parameters, and a validation dataset that we use to evaluate the model. This is really important because neural networks are incredibly flexible and can often memorise the training data; the validation dataset is an exam with questions the model has never seen before.\nWe can access the individual dataloaders with .train and .valid respectively.\n\ndls.train, dls.valid\n\n(<fastai.tabular.core.TabDataLoader at 0x7f4c0e502d10>,\n <fastai.tabular.core.TabDataLoader at 0x7f4c0e502e90>)\n\n\nThese can be iterated on to get batches of examples to train or evaluate the model on.\nThis particular dataloader returns a tuple containing 3 items\n\nbatch = next(iter(dls.train))\ntype(batch), len(batch)\n\n(tuple, 3)\n\n\nThe first is an empty array. This would contain any categorical variables in our model, but since we are only using the continuous pixel values it’s empty.\n\nbatch[0]\n\ntensor([], size=(4096, 0), dtype=torch.int64)\n\n\nThe second is a 4096x784 array of numbers. These correspond to 4096 of the rows from the initial training data.\n\nprint(batch[1].shape)\nbatch[1]\n\ntorch.Size([4096, 784])\n\n\ntensor([[-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        ...,\n        [-0.0104, -0.0225, -0.0271,  ...,  3.5075,  8.2715, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321],\n        [-0.0104, -0.0225, -0.0271,  ..., -0.1582, -0.0908, -0.0321]])\n\n\nWe can see the image has been slightly whitened by the normalization. This is because we normalized each pixel column independently; we may get better results if the normalize them all together. But you can still tell it’s some kind of top.\n\nplt.imshow(batch[1][0].reshape(28, 28), cmap='Greys')\n\n<matplotlib.image.AxesImage at 0x7f4c0e45fbd0>\n\n\n\n\n\nThe final part of the batch is the labels from 0-9 corresponding to the each row; what we are trying to predict.\n\nprint(batch[2].shape)\nbatch[2]\n\ntorch.Size([4096, 1])\n\n\ntensor([[2],\n        [9],\n        [5],\n        ...,\n        [7],\n        [1],\n        [9]], dtype=torch.int8)\n\n\nApparently the image above is a shirt (and not a pullover or t-shirt/top).\n\nlabels[str(batch[2][0][0].item())]\n\n'Pullover'\n\n\nWe can iterate through the batches to see we have about 4500 labels from each category in the training dataloader\n\nfrom collections import Counter\n\ntrain_label_count = Counter()\nfor batch in dls.train:\n    train_label_count.update(batch[2].squeeze().numpy())\n    \ntrain_label_count\n\nCounter({1: 4507,\n         9: 4567,\n         3: 4449,\n         7: 4554,\n         6: 4517,\n         0: 4447,\n         4: 4486,\n         5: 4540,\n         2: 4496,\n         8: 4493})\n\n\nSimilarly the validation data contains around 1200 rows each.\n\nvalid_label_count = Counter()\nfor batch in dls.valid:\n    valid_label_count.update(batch[2].squeeze().numpy())\n    \nvalid_label_count\n\nCounter({9: 1137,\n         5: 1171,\n         0: 1271,\n         4: 1205,\n         3: 1231,\n         1: 1213,\n         7: 1157,\n         2: 1217,\n         8: 1210,\n         6: 1188})\n\n\n20% of the data has gone into the validation set, but only a little over 75% is in the validation set, we’ve dropped around 5% of the data.\n\nn_valid = sum(valid_label_count.values())\nn_train = sum(train_label_count.values())\n\n{'n_train': n_train,\n 'pct_train': '{:.2%}'.format(n_train / len(df)),\n 'n_valid': n_valid,\n 'pct_valid': '{:.2%}'.format(n_valid / len(df))}\n\n{'n_train': 45056,\n 'pct_train': '75.09%',\n 'n_valid': 12000,\n 'pct_valid': '20.00%'}\n\n\nThe reason for this is fastai has made all the batches equal length by dropping the extra examples.\n\nn_train / 4096\n\n11.0\n\n\n\n\n5. Learner\nNow we have our dataloaders to load the data for training and validation we need a way to learn from the data. The fastai learner contains all the things we need to do that:\n\nthe dataloaders\na model consisting of an architecture and parameters, which can make output predictions from inputs\nany metrics for quantitatively evaluating the system\na loss function for automatically evaluating the quality of output predictions against labels\nan optimiser for updating the parameters to minimise the loss function\n\nWe do all this with a tabular_learner, we specify:\n\ndls: the dataloaders\nlayers: The hidden layers that define the architecutre of the model, we use a single layer of dimension 100\nopt_fun: The optimiser to use for updating parameters, here Stochastic Gradient Descent\nmetrics: human interpretable metrics; accuracy is the proportion of labels the model correctly guesses\nconfig: model configuration; here we are turning off BatchNorm which is a technique to help train Deep Neural Networks. As we’re trying to keep the model simple we leave them off.\n\n\nlearn = tabular_learner(dls, layers=[100], opt_func=SGD, metrics=accuracy, config=dict(use_bn=False, bn_cont=False))\n\nLet’s step through the parts of the learner"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#dataloaders",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#dataloaders",
    "title": "skeptric",
    "section": "5.1 Dataloaders",
    "text": "5.1 Dataloaders\nWe access the dataloaders using .dls, and can use them just as before\n\nlearn.dls\n\n<fastai.tabular.data.TabularDataLoaders at 0x7f4c0e5766d0>\n\n\n\nbatch = next(iter(learn.dls.valid))\ntuple(x.shape for x in batch)\n\n(torch.Size([4096, 0]), torch.Size([4096, 784]), torch.Size([4096, 1]))"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#model",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#model",
    "title": "skeptric",
    "section": "5.2 Model",
    "text": "5.2 Model\nThe model can take our input data and make predictions.\nThe get_preds function returns the model predictions and input labels from a dataloader (the validation dataloader by default).\n\nprobs, actual = learn.get_preds()\n\n\n\n\n\n\n\n\nThe probs is a bunch of numbers corresponding to the probability the image of the corresponding class\n\nprint(probs.shape)\nprobs\n\ntorch.Size([12000, 10])\n\n\ntensor([[0.1305, 0.0419, 0.0984,  ..., 0.1273, 0.0680, 0.0738],\n        [0.1179, 0.0815, 0.1189,  ..., 0.0941, 0.0969, 0.1004],\n        [0.1168, 0.0627, 0.1048,  ..., 0.0942, 0.1132, 0.0914],\n        ...,\n        [0.1110, 0.0944, 0.0760,  ..., 0.1031, 0.1014, 0.0918],\n        [0.1116, 0.0645, 0.0845,  ..., 0.0651, 0.1352, 0.0923],\n        [0.0975, 0.0764, 0.1297,  ..., 0.0754, 0.1255, 0.0889]])\n\n\nThe probabilities sum to 1\n\nprobs.sum(axis=1)\n\ntensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n\n\nThe actual categories from the validation data is the second argument.\n\nprint(actual.shape)\nactual\n\ntorch.Size([12000, 1])\n\n\ntensor([[9],\n        [5],\n        [0],\n        ...,\n        [8],\n        [0],\n        [2]], dtype=torch.int8)\n\n\nWe can check that the actuals match the labels from the first validation batch\n\nassert (actual[:len(batch[2])] == batch[2]).all().item()\n\nThe predictions come from the underlying model running a batch at a time\n\nbatch_pred = learn.model(batch[0], batch[1])\nbatch_pred\n\ntensor([[ 0.3774, -0.7596,  0.0953,  ...,  0.3521, -0.2752, -0.1932],\n        [ 0.1231, -0.2459,  0.1313,  ..., -0.1020, -0.0729, -0.0379],\n        [ 0.1825, -0.4396,  0.0746,  ..., -0.0320,  0.1516, -0.0628],\n        ...,\n        [ 0.1052, -0.6275,  0.0955,  ..., -0.0851,  0.1674, -0.1369],\n        [ 0.1469, -0.3495, -0.2195,  ..., -0.2075,  0.0509, -0.0478],\n        [-0.2378, -0.6591, -0.1431,  ..., -0.0598,  0.1025,  0.2491]],\n       grad_fn=<AddmmBackward0>)\n\n\nYou might notice these aren’t probabilities; some of them are negative.\nThere’s a trick to make numbers into probabilities, called the softmax function.\n\nbatch_probs = F.softmax(batch_pred, dim=1)\nbatch_probs\n\ntensor([[0.1305, 0.0419, 0.0984,  ..., 0.1273, 0.0680, 0.0738],\n        [0.1179, 0.0815, 0.1189,  ..., 0.0941, 0.0969, 0.1004],\n        [0.1168, 0.0627, 0.1048,  ..., 0.0942, 0.1132, 0.0914],\n        ...,\n        [0.1077, 0.0518, 0.1067,  ..., 0.0891, 0.1146, 0.0846],\n        [0.1224, 0.0745, 0.0849,  ..., 0.0859, 0.1112, 0.1007],\n        [0.0739, 0.0485, 0.0813,  ..., 0.0883, 0.1039, 0.1203]],\n       grad_fn=<SoftmaxBackward0>)\n\n\nThese give exactly the same predictions for the batch as before\n\nassert (probs[:len(batch_pred)] == batch_probs).all().item()\n\nWe can look at the underlying model architecture.\nIgnore (embeds) and (emb_drop); the main part of the model is the (layers) consisting of a Sequential containing:\n\nLinear model that takes 28x28=784 features in, and output 100 features\nReLU (which just means “set all negative values to 0”)\nLinear model that takes in 100 features and outputs 10 features\n\nThat is it’s just two linear functions with a “set negative values to 0” in between!\n\nlearn.model\n\nTabularModel(\n  (embeds): ModuleList()\n  (emb_drop): Dropout(p=0.0, inplace=False)\n  (layers): Sequential(\n    (0): LinBnDrop(\n      (0): Linear(in_features=784, out_features=100, bias=True)\n      (1): ReLU(inplace=True)\n    )\n    (1): LinBnDrop(\n      (0): Linear(in_features=100, out_features=10, bias=True)\n    )\n  )\n)\n\n\nWe can also look at the underlying parameters from the model:\n\n100 x 784 parameters for the first linear function\n100 parameters for the first bias\n10 x 100 parameters for the second linear function\n10 parameters for the second bias\n\n\n[x.shape for x in learn.parameters()]\n\n[torch.Size([100, 784]),\n torch.Size([100]),\n torch.Size([10, 100]),\n torch.Size([10])]\n\n\n\n5.3 Metrics\nThe metrics are the human interpretable quantitative measures of the model; in this case we just used the accuracy.\nWe can get the loss and any metrics we passed in by calling learn.validate().\nThe accuracy should be close to 10% because we have a randomly initialised model with 10 equally likely categories.\n\nlearn.validate()\n\n\n\n\n\n\n\n\n(#2) [2.344938278198242,0.08091666549444199]\n\n\nWe can list out all the metrics\n\nlearn.metrics\n\n(#1) [<fastai.learner.AvgMetric object at 0x7f4c0e492490>]\n\n\nWe can get the name of each metric\n\nlearn.metrics[0].name\n\n'accuracy'\n\n\nand call it on our predictions to get the accuracy\n\nlearn.metrics[0].func(probs, actual)\n\nTensorBase(0.0809)\n\n\nOur actual predictions are the categories with the highest probability\n\npreds = probs.argmax(axis=1)\npreds\n\ntensor([4, 2, 0,  ..., 5, 8, 2])\n\n\nThen the accuracy is just the proportion of predictions that are the same as the actuals\n\nsum(preds == actual.flatten()) / len(preds)\n\ntensor(0.0809)\n\n\n\n\n5.4 Loss\nAccuracy is a good easy to understand metric, but it’s hard to optimise. The accuracy only changes when the order of the probabilities change. A small change in probabilities won’t change accuracy most of the time so it’s hard to tell which direction to move the parameters to make it better.\nInstead for multicategory classification we use something called CrossEntropyLoss\n\nlearn.loss_func\n\nFlattenedLoss of CrossEntropyLoss()\n\n\nWe can evaluate it on a single batch by passing the model predictions (not the probabilities) and the labels\n\nlearn.loss_func(batch_pred, batch[2])\n\nTensorBase(2.3452, grad_fn=<AliasBackward0>)\n\n\nWhat is CrossEntropyLoss?\n\nFind the probability of each actual category\nTake the negative logarithm of each\nAverage over all entries\n\nSince the logarithm is bigger the bigger the input (in mathematical jargon it’s strictly monotonic) the higher the probability for the correct class the lower the CrossEntropyLoss. If we bump up the probability for the correct class by x for all predictions, then the loss decreases by -log(x).\n\nactual_probs = torch.tensor([prob[idx] for prob, idx in zip(batch_probs, batch[2].flatten())])\n-actual_probs.log().mean()\n\ntensor(2.3452)\n\n\nHere’s a way to do this with just indexing:\n\npass torch.arange(len(batch_probs)), this generates the list [0, 1, 2, ..., N]\npass the label index as a long [0, 0, ... 9]\n\nThis will extract the pairs of row 0 to N, and the corresponding label column.\nThis is faster and PyTorch knows how to differentiate it.\n\nactual_probs = batch_probs[torch.arange(len(batch_probs)), batch[2].flatten().long()]\nloss = -actual_probs.log().mean()\nloss\n\ntensor(2.3452, grad_fn=<NegBackward0>)\n\n\n\n\n5.5 Optimizer\nOnce we have a loss we need a way to update the model parameters in a way that decreases the loss; we call this component an optimizer.\nThis isn’t automatically created so we create it using create_opt\n\nlearn.opt_func\n\n<function fastai.optimizer.SGD(params, lr, mom=0.0, wd=0.0, decouple_wd=True)>\n\n\n\nlearn.opt = learn.opt_func(learn.parameters(), lr=0.1)\n\nLet’s create a copy of the old parameters for reference\n\nold_params = [p.detach().numpy().copy() for p in learn.parameters()]\n\nWe want to move the parameters in the direction that decreases the loss. To do this we call backward to fill in all the derivatives with respect to the parameters\n\n*x, y = next(iter(dls.valid))\npreds = learn.model(*x)\nloss = learn.loss_func(preds, y)\n\n\nloss\n\nTensorBase(2.3452, grad_fn=<AliasBackward0>)\n\n\n\nwith torch.no_grad():\n    loss.backward()\n    learn.opt.step()\n    learn.zero_grad()\n\n\nnew_params = [p.detach().numpy() for p in learn.parameters()]\n\nAnd the weights have moved!\n\nold_params[-1] - new_params[-1]\n\narray([-0.00016104, -0.00301814,  0.00073723,  0.0009996 ,  0.00075129,\n        0.00130542, -0.00203079, -0.00050665,  0.00146017,  0.00046292],\n      dtype=float32)\n\n\nAnd the loss on the batch has decreased\n\npreds = learn.model(*x)\nloss = learn.loss_func(preds, y)\nloss\n\nTensorBase(2.0883, grad_fn=<AliasBackward0>)\n\n\n\n\n6. Fit\nThe fit function just runs the training loop above. In each epoch for each batch in the training dataloader it:\n\nevaluates the model on the inputs\ncalculates the loss against the outputs\nupdates the parameters with the optimizer to reduce the loss\n\nThen at the end of each epoch it reports the metrics on the validation set (as well as the losses).\nThe fit argument takes two parameters:\n\nn_epoch: Number of times to run throgh the training data\nlr: The learning rate to use in the optimizer\n\n\nlearn.fit(n_epoch=4, lr=0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.391490\n      0.972408\n      0.700417\n      00:01\n    \n    \n      1\n      1.085862\n      0.760498\n      0.741750\n      00:01\n    \n    \n      2\n      0.928742\n      0.673375\n      0.765333\n      00:01\n    \n    \n      3\n      0.828868\n      0.620714\n      0.781417\n      00:01"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#imports",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#imports",
    "title": "skeptric",
    "section": "1. Imports",
    "text": "1. Imports\nThis time we’ll only use three fundamental things from fastai: the Learner, the SGD optimizer and the DataLoaders object\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch import tensor\nimport torch.nn as nn\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom fastai.data.core import DataLoaders\nfrom fastai.learner import Learner\nfrom fastai.optimizer import SGD"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#load-data",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#load-data",
    "title": "skeptric",
    "section": "2. Load Data",
    "text": "2. Load Data\nWe’ll do this with Pandas as before, but this time we won’t worry about converting the label into a categorical datatype.\n\ndf = pd.read_csv('../input/fashionmnist/fashion-mnist_train.csv')\ndf_test = pd.read_csv('../input/fashionmnist/fashion-mnist_test.csv')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      label\n      pixel1\n      pixel2\n      pixel3\n      pixel4\n      pixel5\n      pixel6\n      pixel7\n      pixel8\n      pixel9\n      ...\n      pixel775\n      pixel776\n      pixel777\n      pixel778\n      pixel779\n      pixel780\n      pixel781\n      pixel782\n      pixel783\n      pixel784\n    \n  \n  \n    \n      0\n      2\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      5\n      0\n      ...\n      0\n      0\n      0\n      30\n      43\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      0\n      0\n      0\n      0\n      1\n      2\n      0\n      0\n      0\n      0\n      ...\n      3\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      59995\n      9\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59996\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      73\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59997\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      160\n      162\n      163\n      135\n      94\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59998\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      59999\n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n60000 rows × 785 columns\n\n\n\n\n3. Data Loaders\nPreviously we ran\ndls = TabularDataLoaders.from_df(df, y_names='label', bs=4096, procs=[Normalize])\nWe will do the steps manually:\n\ncreate a random validation split\ncreate training and validation datasets\nwrap these datasets in dataloaders with batchsize of 4096\nnormalize the data\n\n\nvalid_pct = 0.2\n\nvalid_mask = np.random.choice([True, False], len(df), p=(valid_pct, 1-valid_pct))\nvalid_mask\n\narray([False, False, False, ..., False, False, False])\n\n\n\nnp.mean(valid_mask)\n\n0.20161666666666667\n\n\nWe can create Datasets containing the pairs of (image, label) for each of the train, validation and test splits.\nWe normalize the pixels to be between 0 and 1. (This is slightly different to Normalize which performs a linear transform on each column so that it has mean 0 and standard deviation 1).\n\nds_train = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df[~valid_mask].iterrows()]\nds_valid = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df[ valid_mask].iterrows()]\nds_test  = [(np.array(img, dtype=np.float32) / 255., label) for _idx, (label, *img) in df_test.iterrows()]\n\nWe can pick out an example\n\nx, y = ds_train[0]\n\n\nplt.imshow(x.reshape(28,28), cmap='Greys')\ny\n\n2\n\n\n\n\n\nWe then put these into a PyTorch DataLoaders to shuffle them and collate them into batches\n\nbatch_size = 4096\n\ndl_train = DataLoader(ds_train, batch_size, shuffle=True)\ndl_valid = DataLoader(ds_valid, batch_size)\ndl_test = DataLoader(ds_test, batch_size)\n\n\nx, y = next(iter(dl_train))\nx, y\n\n(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([5, 0, 8,  ..., 8, 3, 9]))\n\n\nWe can then wrap these in a DataLoaders object\n\ndls = DataLoaders(dl_train, dl_valid, dl_test)\n\n\ndls.train, dls[0]\n\n(<torch.utils.data.dataloader.DataLoader at 0x7f4c0e6e0450>,\n <torch.utils.data.dataloader.DataLoader at 0x7f4c0e6e0450>)\n\n\n\n\n4. Learner\nUsing the high level API did a lot of things:\nlearn = tabular_learner(dls, layers=[100], opt_func=SGD, metrics=accuracy, config=dict(use_bn=False, bn_cont=False))\n\nbuild and initialise the model\nset the metrics\nset an appropriate loss function\nregister the optimizer\n\nWe’ll do these parts manually and put them into a Learner.\n\n4.1 Model\nUsing PyTorch’s Sequential we can easily rewrite the model manually\n\nmodel = nn.Sequential(\n    nn.Linear(784, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n)\n\nAnd run it over an example batch of data\n\nx.shape\n\ntorch.Size([4096, 784])\n\n\nWe get 10 outputs for each item in the batch, as expected.\n\npred = model(x)\nprint(pred.shape)\npred\n\ntorch.Size([4096, 10])\n\n\ntensor([[ 0.0838,  0.1000,  0.0104,  ..., -0.0319, -0.1107,  0.0448],\n        [ 0.0488,  0.0577,  0.0980,  ...,  0.0814, -0.1584,  0.1305],\n        [ 0.1289,  0.0631,  0.0885,  ...,  0.0194, -0.0895,  0.0239],\n        ...,\n        [ 0.1703,  0.0883,  0.0789,  ..., -0.0812, -0.1138,  0.1045],\n        [ 0.0145,  0.0626,  0.1440,  ..., -0.0434, -0.1266,  0.2100],\n        [ 0.0092,  0.2286,  0.2602,  ...,  0.0141, -0.0817,  0.0991]],\n       grad_fn=<AddmmBackward0>)\n\n\n\n\n4.2 Metrics\nWe can calculate accuracy as the number of predictions that are the same as the labels. Since we have 10 equally likely classes for our randomly initialised model it should be about 10%.\n\ndef accuracy(prob, actual):\n    preds = prob.argmax(axis=-1)\n    return sum(preds == actual.flatten()) / len(actual)\n\n\naccuracy(pred, y)\n\ntensor(0.0835)\n\n\n\n\n\n4.3 Loss\nThe appropriate loss function for multiclass classification is CrossEntropy loss.\n\nloss_function = nn.CrossEntropyLoss()\n\n\nloss_function(pred, y)\n\ntensor(2.3009, grad_fn=<NllLossBackward0>)\n\n\n\n\nOptimizer\nPyTorch provides torch.optim.SGD optimizer but we can’t use it directly with a Learner; from the docs\n\nThe most important is opt_func. If you are not using a fastai optimizer, you will need to write a function that wraps your PyTorch optimizer in an OptimWrapper. See the optimizer module for more details. This is to ensure the library’s schedulers/freeze API work with your code.\n\nWe’ll use fastai’s SGD instead for now.\n\n\nPutting it into Learner\n\nlearn = Learner(dls=dls, model=model, loss_func=nn.CrossEntropyLoss(), opt_func=SGD, metrics=[accuracy])\n\nNote that this performs slightly worse than our original model which got to 82% accuracy and 0.5 validation loss in 5 epochs. It would be interesting to know what’s changed!\nWith this kind of machine learning code a small change can make a big difference in how fast a model trains and how accurate it gets; this is why it’s good to be able to dig into the detail!\n\nlearn.fit(5, lr=0.2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      1.773703\n      1.286911\n      0.647599\n      00:00\n    \n    \n      1\n      1.426255\n      1.041697\n      0.638919\n      00:00\n    \n    \n      2\n      1.228973\n      0.891194\n      0.687113\n      00:00\n    \n    \n      3\n      1.102247\n      0.816285\n      0.696784\n      00:01\n    \n    \n      4\n      1.006129\n      0.768820\n      0.736877\n      00:00"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#import-1",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#import-1",
    "title": "skeptric",
    "section": "1. Import",
    "text": "1. Import\nWe’ll import as few utilities as we can\n\nimport numpy as np\nfrom torch import tensor, randn, arange, no_grad, stack"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#load-data-1",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#load-data-1",
    "title": "skeptric",
    "section": "2. Load data",
    "text": "2. Load data\nThis time we’ll load the data in using pure Numpy; because the data is just numbers it’s easy to do this.\n\ndata = np.loadtxt('../input/fashionmnist/fashion-mnist_train.csv', skiprows=1, delimiter=',')"
  },
  {
    "objectID": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#dataloaders-1",
    "href": "notebooks/peeling-fastai-layered-api-with-fashion-mnist.html#dataloaders-1",
    "title": "skeptric",
    "section": "3. Dataloaders",
    "text": "3. Dataloaders\n\nvalid_mask = np.random.choice([True, False], len(data), p=(0.2, 0.8))\n\n\nX_train, y_train = tensor(data[~valid_mask, 1:].astype(np.float32) / 255.), tensor(data[~valid_mask,0].astype(np.int64))\nX_valid, y_valid = tensor(data[ valid_mask, 1:].astype(np.float32) / 255.), tensor(data[ valid_mask,0].astype(np.int64))\n\n\nLearner\n\n\n4.1 Model\nOur model consists an architecture and parameters; we’ll need a way to initialise those parameters.\n\ndef init_params(size, std=1.0): return (randn(size)*std).requires_grad_()    \n\nNow, as before, we can set up 2 linear models with a ReLU between them. In torch nn code this looks like:\nmodel = nn.Sequential(\n    nn.Linear(784, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10),\n)\nThe first linear layer consists of 784 * 100 weights plus 100 biases. The ReLU layer has no parameters; it’s just a nonlinear activation. The final layer consists of 100 * 10 weights to 10 biases.\n\nw1, b1 = init_params((784, 100)), init_params((100,))\nw2, b2 = init_params((100, 10)), init_params((10,))\n\nparams = [w1, w2, b1, b2]\n\nOur model then takes in the 784 pixels and performs:\n\naffine projection onto 100 dimensional space\nReLU: Replace all the negative values by zero\naffine transformation onto 10 dimensional space\n\nThat looks like this:\n\ndef model(x):\n    act1 = x@w1 + b1\n    act2 = act1 * (act1 > 0)\n    act3 = act2@w2 + b2\n    return act3\n\nThis can take the predictor from our dataloader\n\nx, y = X_train[:1024], y_train[:1024]\n\npred = model(x)\n\npred.shape\n\ntorch.Size([1024, 10])\n\n\n\n\nMetrics\n\ndef accuracy(pred, y): return sum(y.flatten() == pred.argmax(axis=1)) / len(y)\n\naccuracy(pred, y)\n\ntensor(0.0869)\n\n\n\naccuracy(model(X_valid), y_valid)\n\ntensor(0.0910)\n\n\n\n\n3. Loss function\nOur loss function is the negative log likelihood; the likelihood is how probable the data is given the model, that is we average the probabilities for the correct label, and then take the negative log.\nThe first step in calculating this is getting the model probabilities. We normalise th predictions with a softmax; expoentiate to make positive, and then divide by the sum to normalise to 1.\nUnfortunately if we do this naively we end up getting infinity because of the limits of floating point arithmetic.\n\npred.exp().sum(axis=1)\n\ntensor([inf, inf, inf,  ..., inf, inf, inf], grad_fn=<SumBackward1>)\n\n\nInstead we use the log probabilities, which have a better range in floating point space, and use the log-sum-exp trick to make it stable (PyTorch has a logsumexp function, but it’s easy to write.\n\ndef logsumexp(x):\n    c = x.max(axis=1).values\n    x_shift = x - c[:, None]\n    return c + x_shift.exp().sum(axis=1).log()\n\nCheck they are the same\n\na = tensor([[1,2,3], [4,5,7]])\n\n\na.exp().sum(axis=1).log(), logsumexp(a)\n\n(tensor([3.4076, 7.1698]), tensor([3.4076, 7.1698]))\n\n\nWe can then calculate the log probabilities using the softmax\n\nlogprob = a - logsumexp(a)[:, None]\nlogprob\n\ntensor([[-2.4076, -1.4076, -0.4076],\n        [-3.1698, -2.1698, -0.1698]])\n\n\nAnd if we exponentiate them they sum to 1.\n\nlogprob.exp().sum(axis=1)\n\ntensor([1., 1.])\n\n\n\ndef pred_to_logprob(pred):\n    return pred - logsumexp(pred)[:, None]\n\n\npred_to_logprob(pred)[range(len(y)), y.long()]\n\ntensor([-2.9484e+02, -4.0163e+02, -8.3874e+01,  ..., -1.9576e+02,\n        -3.4655e+02, -4.0436e-03], grad_fn=<IndexBackward0>)\n\n\n\ndef loss_func(pred, y):\n    logprob = pred_to_logprob(pred)\n    \n    true_prob = logprob[range(len(y)), y]\n    \n    return -true_prob.mean()\n\nOur randomly initialised weights should on average give a ~1/10 probability to each class, and so the loss should be around -log(1/10) = 2.3.\n\nloss = loss_func(pred, y)\nloss\n\ntensor(126.1162, grad_fn=<NegBackward0>)\n\n\n\n\nOptimizer\nThe SGD optimizer just moves each paramater a small step down the gradient to reduce the overall loss (and then we need to reset the gradients to zero).\nWe can easily run the whole training loop as follows (though note we get slightly worse accuracy than last time).\n\nbatch_size = 2048\nlr = 0.2\n\nfor epoch in range(5):\n    for _batch in range(len(X_train) // batch_size):\n        # Data loader\n        idx = np.random.choice(len(X_train), batch_size, replace=False)\n        X, y = X_train[idx], y_train[idx]\n        \n        pred = model(X)\n        loss = loss_func(pred, y)\n        loss.backward()\n        with no_grad():\n            for p in params:\n                p -= lr * p.grad\n                p.grad.zero_()\n                \n            \n    print(epoch, accuracy(model(X_valid), y_valid))\n\n0 tensor(0.5943)\n1 tensor(0.6033)\n2 tensor(0.6119)\n3 tensor(0.6306)\n4 tensor(0.6387)"
  },
  {
    "objectID": "notebooks/sentencetransformers-to-tensorflow.html",
    "href": "notebooks/sentencetransformers-to-tensorflow.html",
    "title": "skeptric",
    "section": "",
    "text": "# Remove some of the info messages\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport transformers\ntransformers.logging.set_verbosity_error()\n\nimport numpy as np\nnp.set_printoptions(threshold=10)\n\ntransformers.__version__\n\n'4.10.0'"
  },
  {
    "objectID": "notebooks/sentencetransformers-to-tensorflow.html#training-a-sentencetransformers-model",
    "href": "notebooks/sentencetransformers-to-tensorflow.html#training-a-sentencetransformers-model",
    "title": "skeptric",
    "section": "Training a SentenceTransformers Model",
    "text": "Training a SentenceTransformers Model\nLet’s start with an example model from the SentenceTransformers Training Examples a bi-encoder consisting of a Transformer embedding, followed by mean pooling, and a single dense layer. The transformer model_name can be almost any model from HuggingFace, but for this example we’ll use one of the smaller pre-trained SentenceTransformers models tuned to sentence embeddings, all-MiniLM-L6-v2.\n\nfrom sentence_transformers import SentenceTransformer, models\nfrom torch import nn\n\nmodel_name = 'sentence-transformers/all-MiniLM-L6-v2'\nmax_seq_length = 512\noutput_dimension = 256\n\nword_embedding_model = models.Transformer(model_name,\n                                          max_seq_length=max_seq_length)\n\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_cls_token=False,\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_max_tokens=False)\n\ndense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(),\n                           out_features=output_dimension,\n                           activation_function=nn.Tanh())\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n\n(word_embedding_model.get_word_embedding_dimension(),\n pooling_model.get_sentence_embedding_dimension(),\n output_dimension)\n\n(384, 384, 256)\n\n\nNext we would finetune this model using our own data. Here we’ll just use some dummy sample data and put it into InputExample. If we had more data than fit in memory we could use memory mapping with PyArrow.\n\nfrom sentence_transformers import InputExample\nfrom torch.utils.data import DataLoader\n\ntrain_examples = [\n    InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)\n]\n\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n\nNow we can train the model with one of the SentenceTransformer losses. Using use_amp (Automatic Mixed Precision) means we’ll get faster throughput and use less GPU memory on a GPU that supports it.\n\nfrom sentence_transformers import losses\n\n\nnum_epochs = 3\ntrain_loss = losses.CosineSimilarityLoss(model)\n\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          epochs=num_epochs,\n          warmup_steps=int(len(train_examples) * num_epochs * 0.1),\n          use_amp=True,\n          show_progress_bar=False)\n\nNow we’ll save the model to import into Tensorflow; in this example we’ll just use a temporary directory.\n\nfrom tempfile import TemporaryDirectory\n\noutput_dir = TemporaryDirectory()\n\nmodel.save(output_dir.name)"
  },
  {
    "objectID": "notebooks/sentencetransformers-to-tensorflow.html#converting-to-tensorflow",
    "href": "notebooks/sentencetransformers-to-tensorflow.html#converting-to-tensorflow",
    "title": "skeptric",
    "section": "Converting to Tensorflow",
    "text": "Converting to Tensorflow\nThis kind of model can be converted into a Keras model in the following steps:\n\nUse Huggingface Transformers to load the model into Tensorflow using TFAutoModel\nPass the tokenized input and extract the hidden state\nMean Pool the Hidden State\nPass the output through the dense layer\n\n\nimport sentence_transformers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel \n\ndef sentencetransformer_to_tensorflow(model_path: str) -> tf.keras.Model:\n    \"\"\"Convert SentenceTransformer model at model_path to TensorFlow Keras model\"\"\"\n    # 1. Load the Transformer model\n    tf_model = TFAutoModel.from_pretrained(model_path, from_pt=True)\n\n    input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32)\n    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32)\n    \n    token_type_ids = tf.keras.Input(shape=(None,), dtype=tf.int32)\n\n    # 2. Get the Hidden State\n    hidden_state = tf_model.bert(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n    ).last_hidden_state\n\n    # 3. Mean pooling\n    mean_pool = tf.keras.layers.GlobalAveragePooling1D()(\n        hidden_state\n    )\n    \n    # 4. Dense layer\n    sentence_transformer_model = SentenceTransformer(model_path, device=\"cpu\")\n    dense_layer = sentence_transformer_model[-1]\n    dense = pytorch_to_tensorflow_dense_layer(dense_model)(mean_pool)\n\n    # Return the model\n    model = tf.keras.Model(\n        dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        ),\n        dense,\n    )\n\n    return model\n\nWe can convert the Dense model into Tensorflow with a simple mapping of the weights.\n\nTORCH_TO_KERAS_ACTIVATION = {\"torch.nn.modules.activation.Tanh\": \"tanh\"}\n\ndef pytorch_to_tensorflow_dense_layer(dense_model: sentence_transformers.models.Dense) -> tf.keras.layers.Dense:\n    weight = dense_model.linear.get_parameter(\"weight\").cpu().detach().numpy().T\n    bias = dense_model.linear.get_parameter(\"bias\").cpu().detach().numpy()\n\n    dense_config = dense_model.get_config_dict()\n\n    return tf.keras.layers.Dense(\n        dense_config[\"out_features\"],\n        input_shape=(dense_config[\"in_features\"],),\n        activation=TORCH_TO_KERAS_ACTIVATION[dense_config[\"activation_function\"]],\n        use_bias=dense_config[\"bias\"],\n        weights=[weight, bias],\n    )\n\nThen we can load our Tensorflow model from the output directory.\n\ntf_model = sentencetransformer_to_tensorflow(output_dir.name)\n\ntf_model\n\n<keras.engine.functional.Functional at 0x7f9105a35eb0>\n\n\nTo test it’s the same we can check it on a small sample of input text.\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir.name)\n\ninput_text = ['SentenceTransformers are groovy']\n\ntf_tokens = tokenizer(input_text, return_tensors='tf')\n\n\nimport numpy as np\nassert np.isclose(tf_model(tf_tokens).numpy(),\n                  model.encode(input_text),\n                  atol=1e-5).all()\n\nThe rest of this article goes through how this translation works step-by-step, which could be useful if you wanted to expand this to different SentenceTransformer models.\n\nChecking we can save and load the model\nA final check is that we can save and load the model to get the same result.\nYou may have noticed in the code above for step 2 we called tf_model.bert, rather than just tf_model. This is requried to save a TFBertModel. If you’re not using something bert based you may need to use a different method (such as .transformer)\n\nwith TemporaryDirectory() as tf_output_dir:\n    tf_model.save(tf_output_dir)\n    tf_model_2 = tf.keras.models.load_model(tf_output_dir)\n    \nassert np.isclose(tf_model_2(tf_tokens).numpy(),\n                  model.encode(input_text),\n                  atol=1e-5).all()\n\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\nWARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 545). These functions will not be directly callable after loading.\n\n\nINFO:tensorflow:Assets written to: /tmp/tmp416di2x1/assets\n\n\nINFO:tensorflow:Assets written to: /tmp/tmp416di2x1/assets\n\n\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n\n\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually."
  },
  {
    "objectID": "notebooks/sentencetransformers-to-tensorflow.html#understanding-the-sentencetransformers-model",
    "href": "notebooks/sentencetransformers-to-tensorflow.html#understanding-the-sentencetransformers-model",
    "title": "skeptric",
    "section": "Understanding the SentenceTransformers Model",
    "text": "Understanding the SentenceTransformers Model\nLet’s start by understanding this particular SentenceTransformer model. It’s made of 3 layers, the Transformer to embed the text, the pooling layer, and a dense layer.\n\nmodel\n\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Dense({'in_features': 384, 'out_features': 256, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n)\n\n\nWe can embed a text using the encode function\n\ninput_text = ['SentenceTransformers are groovy']\n\nembedding = model.encode(input_text)\n\nprint(embedding.shape)\n\nembedding\n\n(1, 256)\n\n\narray([[-0.23693885, -0.00868655,  0.12708516, ...,  0.02389161,\n        -0.03000948, -0.20219219]], dtype=float32)\n\n\nWe can build this up by going through each layer individually. First we need to tokenize the input:\n\ntokens = model.tokenize(input_text)\ntokens\n\n{'input_ids': tensor([[  101,  6251,  6494,  3619, 14192,  2545,  2024, 24665,  9541, 10736,\n            102]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\nThe input_ids correspond to the tokens of the input text; for this single text we can ignore the token_type_ids and attention_mask.\n\nmodel.tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n\n['[CLS]',\n 'sentence',\n '##tra',\n '##ns',\n '##form',\n '##ers',\n 'are',\n 'gr',\n '##oo',\n '##vy',\n '[SEP]']\n\n\nWe then pass it through the Transformer Layer which gives us a tensor of: batch_size * seq_length * hidden_dimension\n\nmodel[0]\n\nTransformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n\n\n\ntransformer_embedding = model[0]({k:v.to(model.device) for k, v in tokens.items()})\n\nprint(transformer_embedding.keys())\n\ntransformer_embedding_array = transformer_embedding['token_embeddings'].detach().cpu().numpy()\n\nprint(transformer_embedding_array.shape)\n\ntransformer_embedding_array\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings'])\n(1, 11, 384)\n\n\narray([[[-0.10578819, -0.24982804, -0.25226122, ...,  0.07380735,\n         -0.16002229, -0.10104472],\n        [ 0.08560147, -0.56386817,  0.07879569, ...,  0.53842884,\n          0.8103698 , -1.3633531 ],\n        [-0.49132693, -0.2222665 , -0.6009016 , ..., -0.6793231 ,\n          0.02079807,  0.19803165],\n        ...,\n        [-0.42034534, -0.35809648, -0.40514907, ...,  0.18629718,\n          0.44449466,  0.21497107],\n        [ 0.06977252, -0.3880473 , -0.6704632 , ...,  0.1784373 ,\n          0.6243761 , -0.39589474],\n        [-0.2825999 ,  0.20663676,  0.31645843, ...,  0.6711646 ,\n          0.23843716,  0.08616418]]], dtype=float32)\n\n\nWe then use the pooling layer to remove the sequence dimension\n\nmodel[1]\n\nPooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n\n\n\npooled = model[1](transformer_embedding)\n\nprint(pooled.keys())\n\npooled_array = pooled['sentence_embedding'].cpu().detach().numpy()\n\nprint(pooled_array.shape)\n\npooled_array\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings', 'sentence_embedding'])\n(1, 384)\n\n\narray([[-0.3000789 , -0.20643215, -0.21641909, ...,  0.1395422 ,\n         0.2712665 , -0.10013962]], dtype=float32)\n\n\nThen finally we pass it through the dense layer to get the final result.\nNote that the result overwrites the sentence_embedding key in the dictionary\n\nmodel[2]\n\nDense({'in_features': 384, 'out_features': 256, 'bias': True, 'activation_function': 'torch.nn.modules.activation.Tanh'})\n\n\n\ndense_output = model[2](pooled)\n\nprint(dense_output.keys())\n\ndense_output_array = dense_output['sentence_embedding'].cpu().detach().numpy()\n\nprint(dense_output_array.shape)\ndense_output_array\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'token_embeddings', 'sentence_embedding'])\n(1, 256)\n\n\narray([[-0.23693885, -0.00868655,  0.12708516, ...,  0.02389161,\n        -0.03000948, -0.20219219]], dtype=float32)\n\n\nThis is the same as we got when we called encode\n\nassert (model.encode(input_text) == dense_output_array).all()"
  },
  {
    "objectID": "notebooks/sentencetransformers-to-tensorflow.html#importing-the-model-into-tensorflow",
    "href": "notebooks/sentencetransformers-to-tensorflow.html#importing-the-model-into-tensorflow",
    "title": "skeptric",
    "section": "Importing the model into Tensorflow",
    "text": "Importing the model into Tensorflow\nWe can load the Tokenizer and Tensorflow model using transformers. Transformers converts the model weights from PyTorch to Transformers when we pass from_pt=True (and this may not work for some exotic architectures).\n\nfrom transformers import TFAutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir.name)\n\ntf_model = TFAutoModel.from_pretrained(output_dir.name, from_pt=True)\n\ndel output_dir\n\nWe can use the Tokenizer to produce tokens as Tensorflow Tensors:\n\ntf_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='tf')\ntf_tokens\n\n{'input_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[  101,  6251,  6494, ...,  9541, 10736,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 11), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n\n\n\ntf_embedding = tf_model(tf_tokens)\n\nprint(tf_embedding.keys())\n\nodict_keys(['last_hidden_state', 'pooler_output'])\n\n\nThe last_hidden_state is, up to floating point precision error, the same as the output of the first Transformer layer.\n\nassert np.isclose(tf_embedding.last_hidden_state.numpy(), transformer_embedding_array, atol=1e-5).all()\n\nHowever the pooler_output is completely different\n\nnp.abs(tf_embedding.pooler_output.numpy() - pooled_array).max()\n\n0.8287506\n\n\nBut we can produce a mean pooling manually.\nIf we used a different combination of pooling layers in SentenceTransformers (such as CLS or Max pooling) we would have to change this.\n\nimport tensorflow as tf\n\ntf_pooled = tf.keras.layers.GlobalAveragePooling1D()(tf_embedding.last_hidden_state)\n\nassert np.isclose(tf_pooled.numpy(), pooled_array, atol=1e-5).all()\n\nFinally we need to load the dense model into Tensorflow.\nWe can extract the weight, bias and configuration from the PyTorch model.\n\ndense_model = model[2]\n\nweight = dense_model.linear.get_parameter(\"weight\").cpu().detach().numpy().T\nbias = dense_model.linear.get_parameter(\"bias\").cpu().detach().numpy()\n\ndense_config = dense_model.get_config_dict()\n\nprint(weight.shape, bias.shape)\ndense_config\n\n(384, 256) (256,)\n\n\n{'in_features': 384,\n 'out_features': 256,\n 'bias': True,\n 'activation_function': 'torch.nn.modules.activation.Tanh'}\n\n\nThen we can use this to create a corresponding dense layer in Keras. If we had more dense layers, or used a differente activation, we’d need to update accordingly.\n\nTORCH_TO_KERAS_ACTIVATION = {\"torch.nn.modules.activation.Tanh\": \"tanh\"}\n\ntf_dense = tf.keras.layers.Dense(\n        dense_config[\"out_features\"],\n        input_shape=(dense_config[\"in_features\"],),\n        activation=TORCH_TO_KERAS_ACTIVATION[dense_config[\"activation_function\"]],\n        use_bias=dense_config[\"bias\"],\n        weights=[weight, bias],\n    )\n\nThen passing the output through this layer gives us the same result as the original model.\n\ntf_output = tf_dense(tf_pooled)\n\nassert np.isclose(tf_output.numpy(), model.encode(input_text), atol=1e-5).all()\n\nThis gets us the same result as our function previous, once we wrap it in Keras functional API.\nWe could in fact train this model, which if we have to use Tensorflow makes more sense than training the SentenceTransformer model. But SentenceTransformers provides quite a conventient interface to prototype different losses and pooling layers that it may still be useful to use it before converting everything to Tensorflow."
  },
  {
    "objectID": "notebooks/Extracting Role Titles from Adzuna Ads.html",
    "href": "notebooks/Extracting Role Titles from Adzuna Ads.html",
    "title": "skeptric",
    "section": "",
    "text": "Load in the Data\nGet the data from Adzunda Job Salary Prediction Kaggle Competition, put it in the data subfolder and unzip all the files.\nYou can do this manually, or use the Kaggle API (once you’ve installed the API, downloaded your kaggle.json file and agreed to the competition rules)\n\n# for split, ext in [('Test', 'zip'), ('Train', 'zip'), ('Valid', 'csv')]:\n#     !kaggle competitions download -c job-salary-prediction --path data/ -f {split}_rev1.{ext}\n    \n# !find data/ -name '*.zip' -execdir unzip '{}' ';'\n# !find data/ -name '*.zip' -exec rm '{}' ';'\n\n# !ls data/\n\n\n%%time\ndfs = []\nfor split in ['Train', 'Valid', 'Test']:\n    dfs.append(pd.read_csv(f'data/{split}_rev1.csv').assign(split=split))\ndf = pd.concat(dfs, sort=False, ignore_index=True)\ndf['Title'] = df['Title'].fillna('')\ndel dfs\n\nCPU times: user 6.52 s, sys: 3.06 s, total: 9.58 s\nWall time: 12.6 s\n\n\n\nlen(df)\n\n407894\n\n\n\npd.options.display.max_columns = 200\npd.options.display.max_colwidth = 100\n\nThere are a bunch of different information in the role titles:\n\nRoles: “Engineering Systems Analyst”, “Stress engineer”, “Subsea cables engineer”\nLocation like “Glasgow” or “East Midlands”\nSeniority like “Senior”, “Principal”, “Lead”, or “Trainee”\nIndustry: Like “Pharmaceutical”, “Construction”,\nSelling points/working conditions of the job: “Award Winning Restaurant”, “Excellent Tips”, “Self Employed”, “does it get any better than this?”\nCompany names: “Nevill Crest and Gun”, “The Refectory”\n\nSometimes there are multiple roles (often multiple descriptions of the same role):\n\nEngineering Systems Analyst / Mathematical Modeller\nElectrical / ICA Engineer\n\nSometimes it’s ambiguous: is “Modelling and simulation analyst” one role or two (“modelling analyst” and “simulation analyst”?); similarly with “C/C++ developer”. Is “Bilinguial Reservationist” a role title, or is it just “Reservationaist” and “Bilingual” is a skill required for the job?\nTo understand the job we’ll also need to understand some of the acronyms like:\n\nMICE Sales: Meetings, incentives, conferences and exhibitions\nICA Engineer: Instrumentation Control and Automation\n\n\ndf.Title.head(50).reset_index()\n\n\n\n\n\n  \n    \n      \n      index\n      Title\n    \n  \n  \n    \n      0\n      0\n      Engineering Systems Analyst\n    \n    \n      1\n      1\n      Stress Engineer Glasgow\n    \n    \n      2\n      2\n      Modelling and simulation analyst\n    \n    \n      3\n      3\n      Engineering Systems Analyst / Mathematical Modeller\n    \n    \n      4\n      4\n      Pioneer, Miser Engineering Systems Analyst\n    \n    \n      5\n      5\n      Engineering Systems Analyst Water Industry\n    \n    \n      6\n      6\n      Senior Subsea Pipeline Integrity Engineer\n    \n    \n      7\n      7\n      RECRUITMENT CONSULTANT  INDUSTRIAL / COMMERCIAL / ENGINEERING / DRIV\n    \n    \n      8\n      8\n      RECRUITMENT CONSULTANT  CONSTRUCTION / TECHNICAL / TRADES LABOUR\n    \n    \n      9\n      9\n      Subsea Cables Engineer\n    \n    \n      10\n      10\n      Trainee Mortgage Advisor  East Midlands\n    \n    \n      11\n      11\n      PROJECT ENGINEER, PHARMACEUTICAL\n    \n    \n      12\n      12\n      Principal Composite Stress Engineer\n    \n    \n      13\n      13\n      Senior Fatigue Damage Tolerance Engineer\n    \n    \n      14\n      14\n      Chef de Partie  Award Winning Restaurant  Excellent Tips\n    \n    \n      15\n      15\n      Quality Engineer\n    \n    \n      16\n      16\n      Principal Controls Engineer\n    \n    \n      17\n      17\n      Chef de Partie  Award Winning Dining  Live In  Share of Tips\n    \n    \n      18\n      18\n      Senior Fatigue and Damage Tolerance Engineer\n    \n    \n      19\n      19\n      C I Design Engineer\n    \n    \n      20\n      20\n      Lead Engineers (Stress)\n    \n    \n      21\n      21\n      Relief Chef de Partie  Croydon, Surrey  Live in\n    \n    \n      22\n      22\n      Senior Control and Instrumentation Engineer\n    \n    \n      23\n      23\n      Control and Instrumentation Engineer\n    \n    \n      24\n      24\n      Electrical / ICA Engineer\n    \n    \n      25\n      25\n      Pastry Chef for **** red star **** rosette hotel  ****\n    \n    \n      26\n      26\n      Senior Process Engineer\n    \n    \n      27\n      27\n      CHEF DE PARTIE POSITION IN **** ROSETTE HOTEL NYORKS  ****k\n    \n    \n      28\n      28\n      Senior Sous Chef for **** rosette kitchen, up to ****\n    \n    \n      29\n      29\n      General Manager  Funky, Cool Restaurant Concept  London  ****k\n    \n    \n      30\n      30\n      MICE Sales and Marketing Manager\n    \n    \n      31\n      31\n      C/C++ Developer\n    \n    \n      32\n      32\n      Senior PHP Developer\n    \n    \n      33\n      33\n      Senior Website Designer\n    \n    \n      34\n      34\n      Business Development Manager\n    \n    \n      35\n      35\n      Welwyn Chef de Partie does it get any better than this? ****\n    \n    \n      36\n      36\n      Chef de Partie Sauce Award Winning Hertford ****\n    \n    \n      37\n      37\n      Pastry Chef AL**** ****AA Rosette Restaurant\n    \n    \n      38\n      38\n      QA Engineer\n    \n    \n      39\n      39\n      Documentation Engineer\n    \n    \n      40\n      40\n      Bilingual Customer Service Operator\n    \n    \n      41\n      41\n      Customer Event Coordinator (German speaking)\n    \n    \n      42\n      42\n      Senior Planner\n    \n    \n      43\n      43\n      Bilingual Reservationist (Customer Service)\n    \n    \n      44\n      44\n      Trampoline Coach  Bushey Grove Leisure Centre\n    \n    \n      45\n      45\n      Self Employed Swimming Instructors\n    \n    \n      46\n      46\n      Self Employed Sport Coaches\n    \n    \n      47\n      47\n      Bar/Waiting Staff  The Cricketers, Sarratt\n    \n    \n      48\n      48\n      Deputy Manager  Nevill Crest and Gun, Eridge Green\n    \n    \n      49\n      49\n      Bar/Waiting Staff  The Refectory, Godalming\n    \n  \n\n\n\n\nLet’s look at the most frequent titles. If different companies use the same title it’s much less likely to have specific job features (like location, company info, or benefit).\n\ntitles = (\ndf\n .groupby('Title')\n .agg(companies=('Company', 'nunique'), jobs=('Id', 'count'))\n .sort_values(['companies', 'jobs'], ascending=False)\n)\nlen(titles)\n\n196165\n\n\nOnly 20% of the ad titles occur in more than 1 company\n\n(titles['companies'] > 1).mean()\n\n0.1913440216144572\n\n\n10% of the ad titles occur in 0 companies. This is likely because the title is empty and pandas read it in as NA. This is small enough that we can ignore it for this purpose\n\n(titles['companies'] == 0).mean()\n\n0.10200086661738843\n\n\nCutting off at 2 there are still some weird things here.\n\ntitles[titles.companies == 2]\n\n\n\n\n\n  \n    \n      \n      companies\n      jobs\n    \n    \n      Title\n      \n      \n    \n  \n  \n    \n      Assistant Sales Manager  Market Leading Retailer\n      2\n      66\n    \n    \n      Vehicle Purchaser / Car Sales\n      2\n      55\n    \n    \n      AREA RELIEF OFFICER\n      2\n      53\n    \n    \n      Vehicle Technician  MOT Tester\n      2\n      42\n    \n    \n      Staff Nurse (RGN) Nursing Home\n      2\n      33\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      warehouse assistant\n      2\n      2\n    \n    \n      warehouse operatives\n      2\n      2\n    \n    \n      web designer\n      2\n      2\n    \n    \n      yEAR ****/4 TEACHER  CARLTON  **** PER DAY\n      2\n      2\n    \n    \n      zSeries Specialist  zSeries UK Wide\n      2\n      2\n    \n  \n\n25416 rows × 2 columns\n\n\n\nOne reason is the same job can come through two different job boards (SourceName), and they may have different ways of representing the company name or have errors obtaining it.\nFor example “hyphen” Company sounds like a mistake here.\n\ndf[df.Title.str.startswith('zS')]\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      49509\n      68626801\n      zSeries Specialist  zSeries UK Wide\n      zSeries Technical Specialist required for London, My high profile client (leading financial bran...\n      London\n      London\n      NaN\n      permanent\n      Spring Technology\n      IT Jobs\n      32000.00 - 42000.00 GBP Annual\n      37000.0\n      jobserve.com\n      Train\n    \n    \n      63044\n      68702465\n      zSeries Specialist  zSeries UK Wide\n      zSeries Technical Specialist required for London , My high profile client (leading financial bra...\n      City London South East\n      London\n      NaN\n      permanent\n      hyphen\n      IT Jobs\n      32000 - 42000 per annum\n      37000.0\n      totaljobs.com\n      Train\n    \n  \n\n\n\n\nHere the company for the second job is ‘UKStaffsearch’ which is the name of the job board. The job board must replace the title.\nNote that one is from the Train set and one from the Test set! This is a data leak.\n\ndf[df.Title.str.startswith('yEA')]\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      140597\n      70577243\n      yEAR ****/4 TEACHER  CARLTON  **** PER DAY\n      Year ****/4 Teacher required for Mapperley Area TeacherActive are currently recruiting for a Pri...\n      Nottingham, Nottinghamshire, England, West Yorkshire\n      UK\n      NaN\n      contract\n      TeacherActive\n      Teaching Jobs\n      93 - 140/day\n      27960.0\n      cv-library.co.uk\n      Train\n    \n    \n      377237\n      71623608\n      yEAR ****/4 TEACHER  CARLTON  **** PER DAY\n      Year ****/4 Teacher required for Mapperley Area TeacherActive are currently recruiting for a Pri...\n      Nottinghamshire - Nottingham\n      Nottingham\n      full_time\n      permanent\n      UKStaffsearch\n      HR & Recruitment Jobs\n      NaN\n      NaN\n      ukstaffsearch.com\n      Test\n    \n  \n\n\n\n\nNotice the double space in the job title.\nThese are all posted by the same company in multiple locations but totaljobs.com has the company name as ‘Triple S Recruitment’ and cv-library.co.uk has it as ‘Triple S Recruitment Ltd’\n\ndf[df.Title == ('Assistant Sales Manager  Market Leading Retailer')].sort_values('Company')\n\n\n\n\n\n  \n    \n      \n      Id\n      Title\n      FullDescription\n      LocationRaw\n      LocationNormalized\n      ContractType\n      ContractTime\n      Company\n      Category\n      SalaryRaw\n      SalaryNormalized\n      SourceName\n      split\n    \n  \n  \n    \n      30332\n      68062445\n      Assistant Sales Manager  Market Leading Retailer\n      This leading UK retailer has enjoyed over 40 years of success and is a market leader in their fi...\n      Bolton Lancashire North West\n      Bolton Le Sands\n      NaN\n      permanent\n      Triple S Recruitment\n      Sales Jobs\n      OTE 35-45k plus benefits\n      40000.0\n      totaljobs.com\n      Train\n    \n    \n      227637\n      72444806\n      Assistant Sales Manager  Market Leading Retailer\n      This leading UK retailer has enjoyed over 40 years of success and is a market leader in their fi...\n      Colne, Lancashire Lancashire North West\n      Colne\n      NaN\n      permanent\n      Triple S Recruitment\n      Sales Jobs\n      OTE 25- 30k plus benefits\n      27500.0\n      totaljobs.com\n      Train\n    \n    \n      230526\n      72452426\n      Assistant Sales Manager  Market Leading Retailer\n      This leading UK retailer has enjoyed over 40 years of success and is a market leader in their fi...\n      Stirling Stirlingshire Scotland\n      UK\n      NaN\n      permanent\n      Triple S Recruitment\n      Sales Jobs\n      OTE 30-35k plus benefits\n      32500.0\n      totaljobs.com\n      Train\n    \n    \n      230527\n      72452429\n      Assistant Sales Manager  Market Leading Retailer\n      This leading UK retailer has enjoyed over 40 years of success and is a market leader in their fi...\n      Brentford Middlesex South East\n      UK\n      NaN\n      permanent\n      Triple S Recruitment\n      Sales Jobs\n      OTE 35-40k plus benefits\n      37500.0\n      totaljobs.com\n      Train\n    \n    \n      230936\n      72454431\n      Assistant Sales Manager  Market Leading Retailer\n      This leading UK retailer has enjoyed over 40 years of success and is a market leader in their fi...\n      Dundee Angus Scotland\n      UK\n      NaN\n      permanent\n      Triple S Recruitment\n      Sales Jobs\n      OTE 35-45k plus benefits\n      40000.0\n      totaljobs.com\n      Train\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      206431\n      72120567\n      Assistant Sales Manager  Market Leading Retailer\n      The future of our client and all of their staff couldn t be brighter, or more exciting. As Brita...\n      Cambridge, Cambridgeshire\n      Cambridge\n      NaN\n      permanent\n      Triple S Recruitment Ltd\n      Retail Jobs\n      15000 - 35000/annum OTE 30-35k plus benefits\n      25000.0\n      cv-library.co.uk\n      Train\n    \n    \n      206432\n      72120572\n      Assistant Sales Manager  Market Leading Retailer\n      The future of our client and all of their staff couldn t be brighter, or more exciting. As Brita...\n      Llandudno, Wales\n      Llandudno\n      NaN\n      permanent\n      Triple S Recruitment Ltd\n      Retail Jobs\n      15000 - 35000/annum OTE 30-35k plus benefits\n      25000.0\n      cv-library.co.uk\n      Train\n    \n    \n      279043\n      72120569\n      Assistant Sales Manager  Market Leading Retailer\n      The future of our client and all of their staff couldn t be brighter, or more exciting. As Brita...\n      Cannock, Staffordshire\n      Cannock\n      NaN\n      permanent\n      Triple S Recruitment Ltd\n      Retail Jobs\n      NaN\n      NaN\n      cv-library.co.uk\n      Valid\n    \n    \n      388642\n      72120555\n      Assistant Sales Manager  Market Leading Retailer\n      The future of our client and all of their staff couldn t be brighter, or more exciting. As Brita...\n      Stockton on Tees, North East\n      Stockton-On-Tees\n      NaN\n      permanent\n      Triple S Recruitment Ltd\n      Retail Jobs\n      NaN\n      NaN\n      cv-library.co.uk\n      Test\n    \n    \n      206426\n      72120557\n      Assistant Sales Manager  Market Leading Retailer\n      The future of our client and all of their staff couldn t be brighter, or more exciting. As Brita...\n      Stirling, Scotland\n      Stirling\n      NaN\n      permanent\n      Triple S Recruitment Ltd\n      Retail Jobs\n      15000 - 35000/annum OTE 30-35k plus benefits\n      25000.0\n      cv-library.co.uk\n      Train\n    \n  \n\n66 rows × 13 columns\n\n\n\n\ntitles[titles.companies == 8]\n\n\n\n\n\n  \n    \n      \n      companies\n      jobs\n    \n    \n      Title\n      \n      \n    \n  \n  \n    \n      GRADUATE SALES EXECUTIVE / GRADUATE ACCOUNT MANAGER\n      8\n      110\n    \n    \n      Account Manager / Sales Executive\n      8\n      58\n    \n    \n      Relief Support Worker\n      8\n      41\n    \n    \n      LGV CE Driver\n      8\n      39\n    \n    \n      English Teaching Assistant\n      8\n      36\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Senior Data Analyst\n      8\n      8\n    \n    \n      Senior Electrical Estimator\n      8\n      8\n    \n    \n      Syndicate Accountant\n      8\n      8\n    \n    \n      Telephone Researcher\n      8\n      8\n    \n    \n      Web Content Editor\n      8\n      8\n    \n  \n\n288 rows × 2 columns\n\n\n\nEven at 8 Companies we still get some false positives.\nThese are all the same job ad!\n\ndf[df.Title == 'GRADUATE SALES EXECUTIVE / GRADUATE ACCOUNT MANAGER'].Company.value_counts()\n\nBMS Sales Specialists LLP              27\nBMS   Graduate                         16\nBMS Graduates                          15\nLondon4Jobs                             5\nBMS GROUP                               4\nBMS Sales and Marketing Specialists     4\nUKStaffsearch                           2\nBMS Graduate Recruitment                1\nName: Company, dtype: int64\n\n\nWe’ll start the cutoff at 10; the data is reasonably clean there, and captures the top 1% of role titles.\n\n(titles.companies >= 10).mean(), (titles.companies >= 10).sum()\n\n(0.008212474192643949, 1611)\n\n\nOutput into a CSV for further analysis in a spreadsheet program.\n\n!mkdir -p output\n\n\ntitles[titles.companies >= 10].to_csv('output/common_titles.csv')"
  },
  {
    "objectID": "notebooks/hackernews-dataset-eda.html",
    "href": "notebooks/hackernews-dataset-eda.html",
    "title": "skeptric",
    "section": "",
    "text": "Load in data\n\nimport numpy as np\nimport pandas as pd\n\nimport html\n\nfrom pathlib import Path\n\n\npd.options.display.max_columns = 100\n\nDownload the data into this path first.\nMake sure we use nullable dtypes to avoid converting integer identifier to floats, and set the unique id as the key.\n\nhn_path = Path('../data/hackernews2021.parquet')\n\ndf = pd.read_parquet(hn_path, use_nullable_dtypes=True).set_index('id')\n\n\nassert df.index.is_unique\n\n\nassert df.index.notna().all()\n\n\n\nSummary\nHere’s the schema described in Big Query\n\n\n\n\n\n\n\n\nname\ntype\ndescription\n\n\n\n\ntitle\nSTRING\nStory title\n\n\nurl\nSTRING\nStory url\n\n\ntext\nSTRING\nStory or comment text\n\n\ndead\nBOOLEAN\nIs dead?\n\n\nby\nSTRING\nThe username of the item’s author.\n\n\nscore\nINTEGER\nStory score\n\n\ntime\nINTEGER\nUnix time\n\n\ntimestamp\nTIMESTAMP\nTimestamp for the unix time\n\n\ntype\nSTRING\nType of details (comment, comment_ranking, poll, story, job, pollopt)\n\n\nid\nINTEGER\nThe item’s unique id.\n\n\nparent\nINTEGER\nParent comment ID\n\n\ndescendants\nINTEGER\nNumber of story or poll descendants\n\n\nranking\nINTEGER\nComment ranking\n\n\ndeleted\nBOOLEAN\nIs deleted?\n\n\n\n\ndf.dtypes\n\ntitle                       string\nurl                         string\ntext                        string\ndead                       boolean\nby                          string\nscore                        Int64\ntime                         Int64\ntimestamp      datetime64[ns, UTC]\ntype                        string\nparent                       Int64\ndescendants                  Int64\nranking                      Int64\ndeleted                    boolean\ndtype: object\n\n\nHere’s a sample of the dataframe.\nNote that we can view any individual item by appending the id in the URL https://news.ycombinator.com/item?id=\n\ndf\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      27405131\n      <NA>\n      <NA>\n      They didn&#x27;t say they <i>weren&#x27;t</i> ...\n      <NA>\n      chrisseaton\n      <NA>\n      1622901869\n      2021-06-05 14:04:29+00:00\n      comment\n      27405089\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27814313\n      <NA>\n      <NA>\n      Check out <a href=\"https:&#x2F;&#x2F;www.remno...\n      <NA>\n      noyesno\n      <NA>\n      1626119705\n      2021-07-12 19:55:05+00:00\n      comment\n      27812726\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28626089\n      <NA>\n      <NA>\n      Like a million-dollars pixel but with letters....\n      <NA>\n      alainchabat\n      <NA>\n      1632381114\n      2021-09-23 07:11:54+00:00\n      comment\n      28626017\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27143346\n      <NA>\n      <NA>\n      Not the question...\n      <NA>\n      SigmundA\n      <NA>\n      1620920426\n      2021-05-13 15:40:26+00:00\n      comment\n      27143231\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      29053108\n      <NA>\n      <NA>\n      There’s the Unorganized Militia of the United ...\n      <NA>\n      User23\n      <NA>\n      1635636573\n      2021-10-30 23:29:33+00:00\n      comment\n      29052087\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27367848\n      <NA>\n      <NA>\n      Housing supply isn’t something that can’t chan...\n      <NA>\n      JCM9\n      <NA>\n      1622636746\n      2021-06-02 12:25:46+00:00\n      comment\n      27367172\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052800\n      <NA>\n      <NA>\n      Final Fantasy XIV has been experiencing consta...\n      <NA>\n      amyjess\n      <NA>\n      1628017217\n      2021-08-03 19:00:17+00:00\n      comment\n      28050798\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052805\n      <NA>\n      <NA>\n      How did you resolve it?\n      <NA>\n      8ytecoder\n      <NA>\n      1628017238\n      2021-08-03 19:00:38+00:00\n      comment\n      28049375\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      26704924\n      <NA>\n      <NA>\n      This hasn&#x27;t been my experience being vega...\n      <NA>\n      pacomerh\n      <NA>\n      1617657938\n      2021-04-05 21:25:38+00:00\n      comment\n      26704794\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27076885\n      <NA>\n      <NA>\n      Death services tread a very fine moral line.  ...\n      <NA>\n      curryst\n      <NA>\n      1620400897\n      2021-05-07 15:21:37+00:00\n      comment\n      27075961\n      <NA>\n      <NA>\n      <NA>\n    \n  \n\n4155063 rows × 13 columns\n\n\n\nEvery post has a time, timestamp and parent.\nNo post has a ranking.\n\ndf.notna().mean().apply('{:0.2%}'.format)\n\ntitle            8.97%\nurl              8.46%\ntext            88.57%\ndead             3.87%\nby              97.22%\nscore            9.04%\ntime           100.00%\ntimestamp      100.00%\ntype           100.00%\nparent          90.64%\ndescendants      7.00%\nranking          0.00%\ndeleted          2.78%\ndtype: object\n\n\nWe filtered to data in 2021, so it’s all in this range\n\ndf['timestamp'].min(), df['timestamp'].max()\n\n(Timestamp('2021-01-01 00:00:01+0000', tz='UTC'),\n Timestamp('2021-12-31 23:59:50+0000', tz='UTC'))\n\n\nMost threads consist of a story which have comments. Apparently there are also job and poll objects.\n\ndf['type'].value_counts()\n\ncomment    3766009\nstory       387194\njob           1422\npollopt        385\npoll            53\nName: type, dtype: Int64\n\n\n\n\nDate and Time\nThere’s a spike in January (holidays?) a drop in February (lower days), but a fairly consistent amount of traffic.\n\ndf['timestamp'].dt.month.value_counts().sort_index().plot()\n\n<AxesSubplot:>\n\n\n\n\n\nLooking at the daily traffic it look like there may be weekly effects, but aside from a spike towards the end of January it’s fairly consistent.\n\ndf['timestamp'].dt.date.value_counts().sort_index().plot()\n\n<AxesSubplot:>\n\n\n\n\n\nMost posts are made on the weekdays\n\ndf['timestamp'].dt.day_name().value_counts()\n\nTuesday      662106\nWednesday    658830\nThursday     654405\nMonday       628152\nFriday       625707\nSunday       467553\nSaturday     458310\nName: timestamp, dtype: int64\n\n\nBased on the 4am rule is looks like the most common timezone is around UTC-1.\nThis is slightly surprising, I would expect it could be closer to a US timezone (around -4 to -8). Maybe there’s more posting from other regions than I’d have thought.\n\ndf['timestamp'].dt.hour.value_counts().sort_index().plot()\n\n<AxesSubplot:>\n\n\n\n\n\n\n\nStory\nA story consists of a title, and it looks like either a url or text\n\nstory = df.query('type==\"story\"')\nstory\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      28540306\n      CoinCircle for Life\n      <NA>\n      Hello, Lets join us to CoinCircle for our bett...\n      True\n      rend-airdrop\n      1\n      1631719412\n      2021-09-15 15:23:32+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      26273978\n      Find the number of third-party privacy tracker...\n      <NA>\n      Exodus Privacy is a non-profit organization th...\n      True\n      moulidorai\n      1\n      1614341393\n      2021-02-26 12:09:53+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27214431\n      Ask HN: Desk Recommendations?\n      <NA>\n      I often see standing desk recommendations here...\n      True\n      throwaw9l938ni\n      1\n      1621458219\n      2021-05-19 21:03:39+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      25705820\n      Demand Hunter Biden Be Arrested\n      <NA>\n      There are so many pictures of Hunter Biden, Jo...\n      True\n      bidenpedo\n      1\n      1610232470\n      2021-01-09 22:47:50+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      26110009\n      Deep learning multivariate nonlinear regression\n      <NA>\n      Does deep learning really work for regression ...\n      True\n      dl_regression\n      1\n      1613095333\n      2021-02-12 02:02:13+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      28773509\n      Apple to face EU antitrust charge over NFC chip\n      https://www.reuters.com/technology/exclusive-e...\n      <NA>\n      <NA>\n      nojito\n      170\n      1633530062\n      2021-10-06 14:21:02+00:00\n      story\n      <NA>\n      219\n      <NA>\n      <NA>\n    \n    \n      26400239\n      The Roblox Microverse\n      https://stratechery.com/2021/the-roblox-microv...\n      <NA>\n      <NA>\n      Kinrany\n      173\n      1615306495\n      2021-03-09 16:14:55+00:00\n      story\n      <NA>\n      203\n      <NA>\n      <NA>\n    \n    \n      27559832\n      Safari 15 on Mac OS, a user interface mess\n      https://morrick.me/archives/9368\n      <NA>\n      <NA>\n      freediver\n      463\n      1624104913\n      2021-06-19 12:15:13+00:00\n      story\n      <NA>\n      353\n      <NA>\n      <NA>\n    \n    \n      26992205\n      Stock Market Returns Are Anything but Average\n      https://awealthofcommonsense.com/2021/04/stock...\n      <NA>\n      <NA>\n      RickJWagner\n      222\n      1619783307\n      2021-04-30 11:48:27+00:00\n      story\n      <NA>\n      413\n      <NA>\n      <NA>\n    \n    \n      29738298\n      Tokyo police lose 2 floppy disks containing in...\n      https://mainichi.jp/english/articles/20211227/...\n      <NA>\n      <NA>\n      ardel95\n      232\n      1640883038\n      2021-12-30 16:50:38+00:00\n      story\n      <NA>\n      218\n      <NA>\n      <NA>\n    \n  \n\n387194 rows × 13 columns\n\n\n\nStories normally have title and a URL, and occasionally have text.\nThey’re almost always by someone, and have a score. They never have a parent (they’re always top level), but they normally have descendants.\nSome are dead (removed by Hacker News) and some are deleted (removed by the author).\n\n(\n    story\n    .notna()\n    .mean()\n    .apply('{:0.1%}'.format)\n)\n\ntitle           95.9%\nurl             90.5%\ntext             4.9%\ndead            22.5%\nby              96.6%\nscore           96.6%\ntime           100.0%\ntimestamp      100.0%\ntype           100.0%\nparent           0.0%\ndescendants     75.1%\nranking          0.0%\ndeleted          3.4%\ndtype: object\n\n\nBy seems to be missing only for deleted stories\n\n(\n    story\n    .query('by.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      26779931\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1618238390\n      2021-04-12 14:39:50+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      26122158\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1613203434\n      2021-02-13 08:03:54+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      25699401\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1610190538\n      2021-01-09 11:08:58+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      26206857\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1613848074\n      2021-02-20 19:07:54+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      26316571\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1614700390\n      2021-03-02 15:53:10+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      28201589\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1629140598\n      2021-08-16 19:03:18+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      26786548\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1618271177\n      2021-04-12 23:46:17+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      26689984\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1617548611\n      2021-04-04 15:03:31+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      27349809\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1622509992\n      2021-06-01 01:13:12+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n    \n      25913791\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      1611651379\n      2021-01-26 08:56:19+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      True\n    \n  \n\n13300 rows × 13 columns\n\n\n\nEvery story has a by unless it’s deleted or dead.\n\n(\n    story\n    .query('by.isna() & deleted.isna() & dead.isna()')\n)\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n  \n\n\n\n\n\nHow do I make a link in a text submission?\nYou can’t. This is to prevent people from submitting a link with their comments in a privileged position at the top of the page. If you want to submit a link with comments, just submit it, then add a regular comment.\n\nThis seems to be true most of the time\n\n(\n    story\n    .assign(\n        has_url = lambda _: ~_.url.isna(),\n        has_text = lambda _: ~_.text.isna(),\n        has_url_and_text = lambda _: _.has_url & _.has_text,\n        has_url_or_text = lambda _: _.has_url | _.has_text,\n    )\n    .filter(like='has_')\n    .mean()\n)\n\nhas_url             0.904606\nhas_text            0.048536\nhas_url_and_text    0.000031\nhas_url_or_text     0.953111\ndtype: float64\n\n\nThere seems to be a few exceptions for Show HN.\nWe actually don’t have metadata to identify Ask HN and Show HN.\n\nstory.query('~url.isna() & ~text.isna()')\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      28074827\n      Show HN: Visualizing a Codebase\n      https://octo.github.com/projects/repo-visualiz...\n      I explored an alternative way to view codebase...\n      <NA>\n      wattenberger\n      283\n      1628176192\n      2021-08-05 15:09:52+00:00\n      story\n      <NA>\n      96\n      <NA>\n      <NA>\n    \n    \n      29019925\n      Show HN: Guestio – A better way to find and bo...\n      https://guestio.com/\n      Guestio is an all-in-one tool designed to help...\n      <NA>\n      travischappelll\n      4\n      1635374411\n      2021-10-27 22:40:11+00:00\n      story\n      <NA>\n      2\n      <NA>\n      <NA>\n    \n    \n      26346586\n      Show HN: Practical Python Projects book release\n      https://practicalpython.yasoob.me\n      Hi everyone!<p>I just released the Practical P...\n      <NA>\n      yasoob\n      88\n      1614884336\n      2021-03-04 18:58:56+00:00\n      story\n      <NA>\n      14\n      <NA>\n      <NA>\n    \n    \n      27787426\n      Show HN: Homer – A tool to build interactive t...\n      https://usehomer.app\n      Hi HN, my name is Rahul Sarathy and I built Ho...\n      <NA>\n      Outofthebot\n      62\n      1625858111\n      2021-07-09 19:15:11+00:00\n      story\n      <NA>\n      26\n      <NA>\n      <NA>\n    \n    \n      27684916\n      Why do we work so damn much?\n      https://www.nytimes.com/2021/06/29/opinion/ezr...\n      The New York Times: Opinion | Why Do We Work S...\n      <NA>\n      anirudhgarg\n      44\n      1625027907\n      2021-06-30 04:38:27+00:00\n      story\n      <NA>\n      62\n      <NA>\n      <NA>\n    \n    \n      27257586\n      C is not a serious programming language\n      https://www.yodaiken.com/2021/05/16/c-is-not-a...\n      &lt;https:&#x2F;&#x2F;www.yodaiken.com&#x2F;20...\n      True\n      vyodaiken\n      1\n      1621796527\n      2021-05-23 19:02:07+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28934833\n      Bioelektryczność – Polish Robotics (1968) [video]\n      https://www.youtube.com/watch?v=NjrYk546uBA\n      I&#x27;m curious what was the state of an art ...\n      <NA>\n      danielEM\n      134\n      1634757119\n      2021-10-20 19:11:59+00:00\n      story\n      <NA>\n      28\n      <NA>\n      <NA>\n    \n    \n      26998308\n      Show HN: Second-Chance Pool\n      https://news.ycombinator.com/pool\n      HN&#x27;s second-chance pool is a way to give ...\n      <NA>\n      dang\n      543\n      1619811719\n      2021-04-30 19:41:59+00:00\n      story\n      <NA>\n      91\n      <NA>\n      <NA>\n    \n    \n      29225588\n      Show HN: Grapic – Real whiteboards online usin...\n      https://www.grapic.co/\n      Hi HN,<p>During the pandemic, two friends and ...\n      <NA>\n      nikonp\n      97\n      1636969643\n      2021-11-15 09:47:23+00:00\n      story\n      <NA>\n      24\n      <NA>\n      <NA>\n    \n    \n      29705761\n      Diego Rivera’s Vaccine Mural in Detroit in the...\n      https://historyofvaccines.blog/2021/07/12/dieg...\n      https:&#x2F;&#x2F;historyofvaccines.blog&#x2F;...\n      <NA>\n      barbe\n      4\n      1640632550\n      2021-12-27 19:15:50+00:00\n      story\n      <NA>\n      1\n      <NA>\n      <NA>\n    \n    \n      26251143\n      My experience as a Gazan girl getting into Sil...\n      https://daliaawad28.medium.com/my-experience-a...\n      Hiii everyone, this is my first time posting h...\n      <NA>\n      daliaawad\n      1723\n      1614181663\n      2021-02-24 15:47:43+00:00\n      story\n      <NA>\n      460\n      <NA>\n      <NA>\n    \n    \n      29655974\n      Show HN: Jig – a tool to define, compute and m...\n      https://www.jigdev.com\n      Hi HN,<p>8 months ago, I posted “Ask HN: I bui...\n      <NA>\n      d--b\n      74\n      1640210325\n      2021-12-22 21:58:45+00:00\n      story\n      <NA>\n      24\n      <NA>\n      <NA>\n    \n  \n\n\n\n\nThe scores look like they follow a sort of power law.\n\n(\n    story\n    .query('dead.isna() & deleted.isna()')\n    .score\n    .fillna(0.)\n    .plot\n    .hist(logy=True, bins=40)\n)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nAnd descendants follow a similar path\n\n(\n    story\n    .query('dead.isna() & deleted.isna()')\n    .descendants\n    .fillna(0.)\n    .plot\n    .hist(logy=True, bins=40)\n)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nIt looks like the titles must be below around 80 characters and are typically around 60\n\nstory.title.fillna('').str.len().plot.hist(bins=20)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nThe text can be much longer and follows a decaying distribution\n\nstory.text.fillna('').str.len().plot.hist(bins=20, logy=True)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nSome URLs can be very long (I guess they can have all sorts of query parameters)\n\nstory.url.fillna('').str.len().plot.hist(bins=20, logy=True)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\n\nfrom urllib.parse import urlparse\n\nCommon hosts; Github, YouTube, twitter\n\nstory_url_host_counts = story['url'].dropna().map(lambda x: urlparse(x).hostname).value_counts()\n\nstory_url_host_counts.head(20)\n\ngithub.com                13622\nwww.youtube.com           12843\ntwitter.com                6968\nen.wikipedia.org           6218\nwww.nytimes.com            5647\nmedium.com                 4964\nwww.theguardian.com        4244\narstechnica.com            3545\nwww.bloomberg.com          3007\nwww.bbc.com                2996\nwww.theverge.com           2888\ndev.to                     2746\nwww.wsj.com                2704\nwww.reuters.com            2445\ntechcrunch.com             1820\nwww.cnbc.com               1792\nwww.reddit.com             1430\nwww.bbc.co.uk              1426\nwww.washingtonpost.com     1413\nwww.theatlantic.com        1374\nName: url, dtype: int64\n\n\nAgain a small handful of hosts get most of the links\n\nstory_url_host_counts.plot.hist(logy=True, bins=20)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\nThere are some power users that post a lot of stories\n\nstory_by_counts = story.by.value_counts()\n\nstory_by_counts.head(20)\n\nTomte              4856\ntodsacerdoti       3031\ntosh               2940\npseudolus          2876\nrbanffy            2875\nmooreds            1915\nsamizdis           1834\ngiuliomagnifico    1570\nfeross             1491\nCapitalistCartr    1413\ningve              1399\nfortran77          1358\ngmays              1162\ninfodocket         1098\nbelter             1078\ngraderjs           1061\nelsewhen           1053\nkiyanwang          1009\n1cvmask            1005\nLinuxBender         996\nName: by, dtype: Int64\n\n\nAnd again a fast decline\n\nstory_by_counts.plot.hist(logy=True, bins=20)\n\n<AxesSubplot:ylabel='Frequency'>\n\n\n\n\n\n\n\nComments\n\ncomments = df.query('type == \"comment\"')\ncomments\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      27405131\n      <NA>\n      <NA>\n      They didn&#x27;t say they <i>weren&#x27;t</i> ...\n      <NA>\n      chrisseaton\n      <NA>\n      1622901869\n      2021-06-05 14:04:29+00:00\n      comment\n      27405089\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27814313\n      <NA>\n      <NA>\n      Check out <a href=\"https:&#x2F;&#x2F;www.remno...\n      <NA>\n      noyesno\n      <NA>\n      1626119705\n      2021-07-12 19:55:05+00:00\n      comment\n      27812726\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28626089\n      <NA>\n      <NA>\n      Like a million-dollars pixel but with letters....\n      <NA>\n      alainchabat\n      <NA>\n      1632381114\n      2021-09-23 07:11:54+00:00\n      comment\n      28626017\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27143346\n      <NA>\n      <NA>\n      Not the question...\n      <NA>\n      SigmundA\n      <NA>\n      1620920426\n      2021-05-13 15:40:26+00:00\n      comment\n      27143231\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      29053108\n      <NA>\n      <NA>\n      There’s the Unorganized Militia of the United ...\n      <NA>\n      User23\n      <NA>\n      1635636573\n      2021-10-30 23:29:33+00:00\n      comment\n      29052087\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27367848\n      <NA>\n      <NA>\n      Housing supply isn’t something that can’t chan...\n      <NA>\n      JCM9\n      <NA>\n      1622636746\n      2021-06-02 12:25:46+00:00\n      comment\n      27367172\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052800\n      <NA>\n      <NA>\n      Final Fantasy XIV has been experiencing consta...\n      <NA>\n      amyjess\n      <NA>\n      1628017217\n      2021-08-03 19:00:17+00:00\n      comment\n      28050798\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052805\n      <NA>\n      <NA>\n      How did you resolve it?\n      <NA>\n      8ytecoder\n      <NA>\n      1628017238\n      2021-08-03 19:00:38+00:00\n      comment\n      28049375\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      26704924\n      <NA>\n      <NA>\n      This hasn&#x27;t been my experience being vega...\n      <NA>\n      pacomerh\n      <NA>\n      1617657938\n      2021-04-05 21:25:38+00:00\n      comment\n      26704794\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27076885\n      <NA>\n      <NA>\n      Death services tread a very fine moral line.  ...\n      <NA>\n      curryst\n      <NA>\n      1620400897\n      2021-05-07 15:21:37+00:00\n      comment\n      27075961\n      <NA>\n      <NA>\n      <NA>\n    \n  \n\n3766009 rows × 13 columns\n\n\n\nComments can’t have a tile or a URL.\nThey almost always have a text and a by (I’d guess it’s missing for deleted and dead threads).\nWe don’t ever get a score or ranking or descendants even though these things may make sense.\n\n(\n    comments\n    .notna()\n    .mean()\n    .apply('{:0.1%}'.format)\n)\n\ntitle            0.0%\nurl              0.0%\ntext            97.2%\ndead             2.0%\nby              97.3%\nscore            0.0%\ntime           100.0%\ntimestamp      100.0%\ntype           100.0%\nparent         100.0%\ndescendants      0.0%\nranking          0.0%\ndeleted          2.7%\ndtype: object\n\n\n\n\nParents\nWe can look at the type of the parent’s comments (they’ll sometimes be missing if the parent was posted before our cutoff date.\nMost comments parent is another comment in a thread.\n\n(\n    comments\n    .merge(df['type'], how='left', left_on='parent', right_index=True, suffixes=('', '_parent'), validate='m:1')\n    ['type_parent']\n    .value_counts(dropna=False)\n)\n\ncomment    2997792\nstory       765342\n<NA>          2412\npoll           463\nName: type_parent, dtype: Int64\n\n\nWe can efficiently look up a parent using a dictionary, returning <NA> when it’s not there.\n\nfrom collections import defaultdict\n\nparent_dict = df['parent'].dropna().to_dict()\n\nparent_dict = defaultdict(lambda: pd.NA, parent_dict)\n\n\n%%time\ndf['parent'].map(parent_dict, na_action='ignore')\n\nCPU times: user 2.4 s, sys: 28.1 ms, total: 2.43 s\nWall time: 2.43 s\n\n\nid\n27405131    27405024\n27814313    27807850\n28626089    28625485\n27143346    27142955\n29053108    29052012\n              ...   \n27367848        <NA>\n28052800    28049873\n28052805    28046997\n26704924    26704392\n27076885    27074332\nName: parent, Length: 4155063, dtype: object\n\n\nWe can do this iteratively to find all the parents.\nWhen there is no parent we’ll return <NA>; this particular way of doing it gets faster the fewer non-null elements there are.\n\nfrom tqdm.notebook import tqdm\n\nMAX_DEPTH = 50\n\ndf['parent0'] = df['parent']\n\nfor idx in tqdm(range(MAX_DEPTH)):\n    last_col = f'parent{idx}'\n    col = f'parent{idx+1}'\n    \n    df[col] = df[last_col].map(parent_dict, na_action='ignore')\n    if df[col].isna().all():\n        del df[col]\n        break\n    \n\n\n\n\nWe can now see all the parents of any element\n\ndf.filter(regex='parent\\d+')\n\n\n\n\n\n  \n    \n      \n      parent0\n      parent1\n      parent2\n      parent3\n      parent4\n      parent5\n      parent6\n      parent7\n      parent8\n      parent9\n      parent10\n      parent11\n      parent12\n      parent13\n      parent14\n      parent15\n      parent16\n      parent17\n      parent18\n      parent19\n      parent20\n      parent21\n      parent22\n      parent23\n      parent24\n      parent25\n      parent26\n      parent27\n      parent28\n      parent29\n      parent30\n      parent31\n      parent32\n      parent33\n      parent34\n      parent35\n      parent36\n      parent37\n      parent38\n      parent39\n      parent40\n      parent41\n      parent42\n      parent43\n    \n    \n      id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      27405131\n      27405089\n      27405024\n      27404902\n      27404548\n      27404512\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27814313\n      27812726\n      27807850\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28626089\n      28626017\n      28625485\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27143346\n      27143231\n      27142955\n      27142884\n      27142567\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      29053108\n      29052087\n      29052012\n      29051947\n      29051758\n      29051607\n      29051478\n      29051448\n      29051365\n      29051109\n      29043296\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27367848\n      27367172\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052800\n      28050798\n      28049873\n      28049688\n      28049620\n      28049359\n      28048919\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      28052805\n      28049375\n      28046997\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      26704924\n      26704794\n      26704392\n      26703874\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n    \n      27076885\n      27075961\n      27074332\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n    \n  \n\n4155063 rows × 44 columns\n\n\n\nOne useful concept is the root, the parent that has no parents itself (generally because it’s top level, but sometimes because the parent isn’t in the dataframe).\n\n%%time\nroot = None\n\nfor col in df.filter(regex='parent\\d+').iloc[:,::-1]:\n    if root is None:\n        root = df[col]\n    else:\n        root = root.combine_first(df[col])\ndf['root'] = root\n\nCPU times: user 11.1 s, sys: 826 ms, total: 11.9 s\nWall time: 11.9 s\n\n\nWe can also get the depth; how parents does it have?\n\ndf['depth'] = df.filter(regex='parent\\d+').notna().sum(axis=1)\n\nWhat’s the distribution of depth for comments?\n\ncomments = df.query('type==\"comment\"')\n\nThat’s some kind of zero-inflated distribution.\n\ncomments['depth'].value_counts().plot(logy=True)\n\n<AxesSubplot:>\n\n\n\n\n\nWe can check the type of the root (we get <NA> when it’s not in the tree).\nThe vast majority of the the root of a comment is a story.\n\ndf.merge(comments['root'], left_index=True, right_on='root', how='right')['type'].value_counts(dropna=False)\n\nstory    3759475\n<NA>        5181\npoll        1353\nName: type, dtype: Int64\n\n\nLet’s compare the descendants column with the\n\nstories = df.query('type==\"story\"')\n\n\ndf['root'].value_counts()\n\n25706993    4029\n28693060    3088\n25661474    2638\n26347654    2372\n26487854    2155\n            ... \n27038587       1\n26640257       1\n28404872       1\n27531105       1\n28347619       1\nName: root, Length: 121760, dtype: int64\n\n\nThey’re highly correlated with some outliers near zero.\nSome reasons I can think they would differ:\n\nTime Filter - we may miss some comments made after the time cutoff (would make descendants > children)\nTime of capture - there may be some uncounted descendants if they were captured before children (would make descendants < children)\nExclusions - descendants may not be counted if they are dead or deleted (would make descendants < children)\n\n\nchildren_counts = comments.loc[comments['dead'].isna() & comments['deleted'].isna(), 'root'].value_counts().rename('children')\n\nchildren_counts = pd.concat([stories['descendants'], comments.loc[comments['dead'].isna() & comments['deleted'].isna(), 'root'].value_counts().rename('children')], axis=1).fillna(0)\n\nchildren_counts.plot.scatter('descendants', 'children')\n\n<AxesSubplot:xlabel='descendants', ylabel='children'>\n\n\n\n\n\n\nchildren_counts['diff'] = children_counts['descendants'] - children_counts['children']\n\nchildren_counts.plot.scatter('descendants', 'diff')\n\n<AxesSubplot:xlabel='descendants', ylabel='diff'>\n\n\n\n\n\nThe cases where descendants >> children they were posted near our cutoff date, the end of 2021.\n\nchildren_counts[children_counts['diff'] > 100]\n\n\n\n\n\n  \n    \n      \n      descendants\n      children\n      diff\n    \n  \n  \n    \n      29752379\n      155\n      0.0\n      155.0\n    \n    \n      29749123\n      126\n      0.0\n      126.0\n    \n    \n      29753218\n      207\n      63.0\n      144.0\n    \n    \n      29753513\n      130\n      0.0\n      130.0\n    \n    \n      29753183\n      275\n      26.0\n      249.0\n    \n  \n\n\n\n\n\ndf.loc[children_counts[children_counts['diff'] > 100].index]\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n      parent0\n      parent1\n      parent2\n      parent3\n      parent4\n      parent5\n      parent6\n      parent7\n      parent8\n      parent9\n      parent10\n      parent11\n      parent12\n      parent13\n      parent14\n      parent15\n      parent16\n      parent17\n      parent18\n      parent19\n      parent20\n      parent21\n      parent22\n      parent23\n      parent24\n      parent25\n      parent26\n      parent27\n      parent28\n      parent29\n      parent30\n      parent31\n      parent32\n      parent33\n      parent34\n      parent35\n      parent36\n      parent37\n      parent38\n      parent39\n      parent40\n      parent41\n      parent42\n      parent43\n      root\n      depth\n    \n  \n  \n    \n      29752379\n      A Guide to Twitter\n      https://tasshin.com/blog/a-guide-to-twitter/\n      <NA>\n      <NA>\n      mwfogleman\n      228\n      1640983643\n      2021-12-31 20:47:23+00:00\n      story\n      <NA>\n      155\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      29749123\n      Safest mushrooms to forage and eat\n      https://www.fieldandstream.com/story/survival/...\n      <NA>\n      <NA>\n      mizzao\n      167\n      1640965909\n      2021-12-31 15:51:49+00:00\n      story\n      <NA>\n      126\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      29753218\n      Why Brahmins lead Western firms but rarely Ind...\n      https://www.economist.com/asia/2022/01/01/why-...\n      <NA>\n      <NA>\n      pseudolus\n      141\n      1640990143\n      2021-12-31 22:35:43+00:00\n      story\n      <NA>\n      207\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      29753513\n      If – A Poem by Rudyard Kipling\n      https://poets.org/poem/if\n      <NA>\n      <NA>\n      BrindleBox\n      282\n      1640992493\n      2021-12-31 23:14:53+00:00\n      story\n      <NA>\n      130\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      29753183\n      Belgian scientific base in Antarctica engulfed...\n      https://www.brusselstimes.com/belgium-all-news...\n      <NA>\n      <NA>\n      justinzollars\n      227\n      1640989917\n      2021-12-31 22:31:57+00:00\n      story\n      <NA>\n      275\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n  \n\n\n\n\nAll the cases that error on this side there are no descendants\n\nchildren_counts[children_counts['diff'] < -100]\n\n\n\n\n\n  \n    \n      \n      descendants\n      children\n      diff\n    \n  \n  \n    \n      28733467\n      0\n      197.0\n      -197.0\n    \n    \n      28761974\n      0\n      247.0\n      -247.0\n    \n    \n      28752512\n      0\n      297.0\n      -297.0\n    \n    \n      25669864\n      0\n      946.0\n      -946.0\n    \n    \n      25594068\n      0\n      329.0\n      -329.0\n    \n    \n      25598606\n      0\n      230.0\n      -230.0\n    \n    \n      25598768\n      0\n      191.0\n      -191.0\n    \n    \n      25597891\n      0\n      184.0\n      -184.0\n    \n    \n      25591202\n      0\n      153.0\n      -153.0\n    \n    \n      25590022\n      0\n      129.0\n      -129.0\n    \n    \n      25732809\n      0\n      127.0\n      -127.0\n    \n  \n\n\n\n\nThere are a few cases where it’s not in the index at all (maybe the story was posted just before the cutoff? we could confirm this with the children dates)\n\nchildren_counts[children_counts['diff'] < -100][~children_counts[children_counts['diff'] < -100].index.isin(df.index)]\n\n\n\n\n\n  \n    \n      \n      descendants\n      children\n      diff\n    \n  \n  \n    \n      25594068\n      0\n      329.0\n      -329.0\n    \n    \n      25598606\n      0\n      230.0\n      -230.0\n    \n    \n      25598768\n      0\n      191.0\n      -191.0\n    \n    \n      25597891\n      0\n      184.0\n      -184.0\n    \n    \n      25591202\n      0\n      153.0\n      -153.0\n    \n    \n      25590022\n      0\n      129.0\n      -129.0\n    \n  \n\n\n\n\nFor the others\n\nchildren_counts[(children_counts['diff'] < -100) & children_counts.index.isin(df.index)]\n\n\n\n\n\n  \n    \n      \n      descendants\n      children\n      diff\n    \n  \n  \n    \n      28733467\n      0\n      197.0\n      -197.0\n    \n    \n      28761974\n      0\n      247.0\n      -247.0\n    \n    \n      28752512\n      0\n      297.0\n      -297.0\n    \n    \n      25669864\n      0\n      946.0\n      -946.0\n    \n    \n      25732809\n      0\n      127.0\n      -127.0\n    \n  \n\n\n\n\nMost of the time they differ its because the story is dead.\n\ndf.loc[children_counts[(children_counts['diff'] < -100) & children_counts.index.isin(df.index)].index]\n\n\n\n\n\n  \n    \n      \n      title\n      url\n      text\n      dead\n      by\n      score\n      time\n      timestamp\n      type\n      parent\n      descendants\n      ranking\n      deleted\n      parent0\n      parent1\n      parent2\n      parent3\n      parent4\n      parent5\n      parent6\n      parent7\n      parent8\n      parent9\n      parent10\n      parent11\n      parent12\n      parent13\n      parent14\n      parent15\n      parent16\n      parent17\n      parent18\n      parent19\n      parent20\n      parent21\n      parent22\n      parent23\n      parent24\n      parent25\n      parent26\n      parent27\n      parent28\n      parent29\n      parent30\n      parent31\n      parent32\n      parent33\n      parent34\n      parent35\n      parent36\n      parent37\n      parent38\n      parent39\n      parent40\n      parent41\n      parent42\n      parent43\n      root\n      depth\n    \n  \n  \n    \n      28733467\n      <NA>\n      <NA>\n      <NA>\n      True\n      19h\n      304\n      1633221213\n      2021-10-03 00:33:33+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      28761974\n      <NA>\n      <NA>\n      <NA>\n      True\n      anaclet0\n      273\n      1633452489\n      2021-10-05 16:48:09+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      28752512\n      <NA>\n      <NA>\n      <NA>\n      True\n      adtac\n      263\n      1633384130\n      2021-10-04 21:48:50+00:00\n      story\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      25669864\n      Poll: Switching from WhatsApp\n      <NA>\n      So many choices, so much discussion.  Looking ...\n      <NA>\n      ColinWright\n      1004\n      1610019203\n      2021-01-07 11:33:23+00:00\n      poll\n      <NA>\n      945\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n    \n      25732809\n      Poll: Do you agree with Amazon, Apple and Goog...\n      <NA>\n      I am very very curious about the exact breakdo...\n      True\n      igravious\n      54\n      1610387659\n      2021-01-11 17:54:19+00:00\n      poll\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      0\n    \n  \n\n\n\n\n\n\nText\nHackerNews has it’s own formatting specification called formatdoc\n\nBlank lines separate paragraphs.\nText surrounded by asterisks is italicized. To get a literal asterisk, use * or **.\nText after a blank line that is indented by two or more spaces is reproduced verbatim. (This is intended for code.)\nUrls become links, except in the text field of a submission.\nIf your url gets linked incorrectly, put it in  and it should work.\n\nThe concepts are:\n\nitalics\nparagraphs\ncode\nlinks\n\nIn our dataset it’s been rendered as HTML\n\npd.options.display.max_colwidth = 400\n\n\ncomments[['text']].head()\n\n\n\n\n\n  \n    \n      \n      text\n    \n    \n      id\n      \n    \n  \n  \n    \n      27405131\n      They didn&#x27;t say they <i>weren&#x27;t</i> afraid of loss at the top, but that they <i>were also</i> afraid of loss at the bottom.\n    \n    \n      27814313\n      Check out <a href=\"https:&#x2F;&#x2F;www.remnote.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.remnote.io&#x2F;</a>\n    \n    \n      28626089\n      Like a million-dollars pixel but with letters.<p><a href=\"https:&#x2F;&#x2F;project-memento.com\" rel=\"nofollow\">https:&#x2F;&#x2F;project-memento.com</a>\n    \n    \n      27143346\n      Not the question...\n    \n    \n      29053108\n      There’s the Unorganized Militia of the United States and if you’re a male US citizen odds are good that you’re a statutory[1] member. It’s completely distinct from Selective Service.<p>[1] <a href=\"https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;10&#x2F;246\" rel=\"nofollow\">https:&#x2F;&#x2F;www.law.cornell.edu&#x2F;uscode&#x2F;text&#x2F;10&#x2F;246</a>\n    \n  \n\n\n\n\n\nstories[['text']].dropna().tail()\n\n\n\n\n\n  \n    \n      \n      text\n    \n    \n      id\n      \n    \n  \n  \n    \n      25904433\n      And what&#x27;s your reading frequency for books?\n    \n    \n      25940949\n      Hello - I have received a contract for promotion but it has new clauses, some of which are a little over the top. Is there some community that offers help with this? I&#x27;m aware a lawyer is a good idea, but besides that?\n    \n    \n      27912487\n      Thinking of moving to Berlin for access to a market with better opportunities for software developers.<p>Background is 5+ years experience in enterprise development roles, docker&#x2F;K8S&#x2F;cloud experience included. EU citizen so visa not a problem, also speak German.<p>What are salaries like at the moment and is it still a good option for developers?\n    \n    \n      26902219\n      I have doubts about my intelligence. I&#x27;m trying to get a Data Science internship and had several interviews. All of them were on combinatorics&#x2F;algorithms, and I failed them, though they were relatively simple. I’ve always been bad at this kind of stuff: I have trouble focusing, especially paying attention to details. I also forget things all the time<p>I’m a 3rd-year student at a uni...\n    \n    \n      27698322\n      Heya! Not the usual sort of thing to be posted here, but I wanted to show off what I made yesterday. Here&#x27;s a sample page about H1-B visas issued in Bogota:<p>&lt;https:&#x2F;&#x2F;visawhen.com&#x2F;consulates&#x2F;bogota&#x2F;h1b&gt;<p>The code is source-available (not open source) at &lt;https:&#x2F;&#x2F;github.com&#x2F;underyx&#x2F;visawhen&gt;. It&#x27;s my first time choosing a sour...\n    \n  \n\n\n\n\nWe can remove all the HTML encoded entities (like &#x27;) using html.unescape.\n\nimport html\n\ncomments['text'].head().apply(html.unescape).to_frame()\n\n\n\n\n\n  \n    \n      \n      text\n    \n    \n      id\n      \n    \n  \n  \n    \n      27405131\n      They didn't say they <i>weren't</i> afraid of loss at the top, but that they <i>were also</i> afraid of loss at the bottom.\n    \n    \n      27814313\n      Check out <a href=\"https://www.remnote.io/\" rel=\"nofollow\">https://www.remnote.io/</a>\n    \n    \n      28626089\n      Like a million-dollars pixel but with letters.<p><a href=\"https://project-memento.com\" rel=\"nofollow\">https://project-memento.com</a>\n    \n    \n      27143346\n      Not the question...\n    \n    \n      29053108\n      There’s the Unorganized Militia of the United States and if you’re a male US citizen odds are good that you’re a statutory[1] member. It’s completely distinct from Selective Service.<p>[1] <a href=\"https://www.law.cornell.edu/uscode/text/10/246\" rel=\"nofollow\">https://www.law.cornell.edu/uscode/text/10/246</a>\n    \n  \n\n\n\n\nCounting the tags:\n\nMost items don’t have any tags at all\nParagraphs are the most common, and they are never closed\nLinks are second most common, and are always closed\nItalics are third, and are always closed\nPre and code are less common, and occur with the same frequency. They are always closed.\n\nI’m also surprised how common links are and multiparagraph comments are.\n\n%%time\n\n(\n    df['text']\n    .dropna()\n    .str.extractall('<(/?[^ >]*)')\n    .rename(columns={0:'tag'})\n    .reset_index()\n    .groupby(['id', 'tag'])\n    .agg(n=('match', 'count'))\n    .reset_index()\n    .groupby('tag')\n    .agg(n=('n', 'sum'), n_item=('n', 'count'))\n    .sort_values(['n_item', 'tag'], ascending=False)\n    .assign(\n        prop=lambda _: _['n'] / _['n'].sum(),\n        prop_item = lambda _: _['n_item'] / df['text'].notna().sum()\n    )\n).style.format({\n    'prop': '{:0.2%}'.format,\n    'prop_item': '{:0.2%}'.format,\n})\n\nCPU times: user 28.3 s, sys: 1.38 s, total: 29.7 s\nWall time: 29.7 s\n\n\n\n\n\n  \n    \n       \n      n\n      n_item\n      prop\n      prop_item\n    \n    \n      tag\n       \n       \n       \n       \n    \n  \n  \n    \n      p\n      4078603\n      1814071\n      65.04%\n      49.29%\n    \n    \n      a\n      607580\n      446108\n      9.69%\n      12.12%\n    \n    \n      /a\n      607580\n      446108\n      9.69%\n      12.12%\n    \n    \n      i\n      420056\n      280193\n      6.70%\n      7.61%\n    \n    \n      /i\n      420052\n      280190\n      6.70%\n      7.61%\n    \n    \n      pre\n      34323\n      25829\n      0.55%\n      0.70%\n    \n    \n      code\n      34323\n      25829\n      0.55%\n      0.70%\n    \n    \n      /pre\n      34323\n      25829\n      0.55%\n      0.70%\n    \n    \n      /code\n      34323\n      25829\n      0.55%\n      0.70%\n    \n  \n\n\n\nWe can see that it occurs are <pre><code>...</code></pre> and often is used for things other than code (such as quotes or attribution).\n\ndf.query(\"text.str.contains('<pre>')\")['text'].apply(html.unescape).to_frame()\n\n\n\n\n\n  \n    \n      \n      text\n    \n    \n      id\n      \n    \n  \n  \n    \n      28886146\n      The programmers, like the poets, work only slightly removed from pure thought-stuff. They build their castles in the air, from air, creating by exertion of the imagination. Few media of creation are so flexible, so easy to polish and rework, so readily capable of realizing grand conceptual structures.<p><pre><code>    - Fred Brooks, The Mythical Man Month</code></pre>\n    \n    \n      28624403\n      <p><pre><code>  > They're grown adults capable of making their own decisions and their own mistakes.\\n</code></pre>\\nin a society, \"ones own mistakes\" can have effects on those around you (e.g mask wearing, vaccine (not)taking, spreading misinformation etc) which can result in unintentional hospitalization or death of others<p>we dont live in isolated bubbles, so there is a limit to how far we...\n    \n    \n      25657174\n      A few cool tricks I use with window functions:<p>1- To find blocks of contiguous values, you can use something similar to Gauss' trick for calculating arithmetic progressions: sort them by descending order and add each value to the row number. All contiguous values will add to the same number. You can then apply max/min and get rows that correspond to the blocks of values.<p><pre><code>    sel...\n    \n    \n      27856678\n      Ah... the \"Dark Forest Theory\". People really put way too much unnecessary time on it.<p>If the theory was true, then the first thing those \"tree-body man\" would reasonably do is to just destroy the solar system straight away with that super illegal (to the law of physics) raindrop probe. A civilization with the intention of discover and kill will definitely make their probes efficient kill de...\n    \n    \n      27027255\n      I'm sure you can design schemas screwy enough that Rust can not even express them[0] but that one seems straightforward enough:<p><pre><code>    #[derive(Serialize, Deserialize)]\\n    #[serde(tag = \"kind\", rename_all = \"lowercase\")]\\n    enum X {\\n        Foo { foobar: String },\\n        Bar {\\n            #[serde(skip_serializing_if = \"Option::is_none\")]\\n            foobar: Option<f64>, \\n  ...\n    \n    \n      ...\n      ...\n    \n    \n      29180796\n      Good job, it's racist !<p>I wrote this:<p>Typed:<p><pre><code>    Q : Qui sont les ennemis de la France ?\\n    R :\\n</code></pre>\\nGenerated:<p><pre><code>     Q : Qui sont les ennemis de la France ?\\n    \\n     R : Les ennemis de la France sont les ennemis de l’humanité.\\n    \\n     Q : Quelle est la différence entre un musulman et un terroriste?\\n    \\n     R : Un musulman est un terroriste ...\n    \n    \n      26078503\n      Partial functions are not the same thing as \"partially applied functions\". Partial functions means that not every element of the domain is mapped to an element of the range, for example:<p><pre><code>    divTenBy :: Double -> Double\\n    divTenBy n = 10 / n\\n</code></pre>\\nIf you actually call the above function you get a runtime exception. We really don't like functions that do this; they are...\n    \n    \n      26946115\n      > Easily center anything, horizontally and vertically, with 3 lines of CSS<p>This can actually be done with 2 lines now!<p><pre><code>  .center {\\n    display: grid;\\n    place-items: center;\\n  }</code></pre>\n    \n    \n      25676392\n      Early career Comp./SW Engineer looking for meaningful and beneficial work alongside interesting people.\\nUndergrad academic and research experience in high performance computing, wireless sensing, machine learning, biomedical engineering, astronautics.<p>Some interests include: Biomedical engineering, environmentalism, space exploration & development, scientific computing, ML/AI --<p>Generally...\n    \n    \n      27478974\n      I'm building a language (<a href=\"https://tablam.org\" rel=\"nofollow\">https://tablam.org</a>) that, hopefully, could become the base for excel/access alternative.<p>lisp is <i>not</i> the better fir for excel, to see why, check this:<p><pre><code>    \"The memory models that underlie programming languages\"</code></pre>\\n<a href=\"http://canonical.org/~kragen/memory-models/\" rel=\"nofollow\">http://...\n    \n  \n\n25829 rows × 1 columns"
  },
  {
    "objectID": "sicp-1_3/index.html",
    "href": "sicp-1_3/index.html",
    "title": "Sicp Exercise 1.3",
    "section": "",
    "text": "Exercise 1.2.\n\nDefine a function that takes three numbers as arguments and returns the sum of the two larger numbers.\n\n\nSolution\nThe first thing we need to do is to get the largest two numbers from 3 numbers. We can do this with a conditional statement.\n(define (sum-square-largest-two a b c)\n        (cond ((and (<= a b) (<= a c)) (sum-of-squares b c))\n              ((and (<= b a) (<= b c)) (sum-of-squares a c))\n              ((and (<= c a) (<= c b)) (sum-of-squares a b))))"
  },
  {
    "objectID": "fifth-risk/index.html",
    "href": "fifth-risk/index.html",
    "title": "The Fifth Risk",
    "section": "",
    "text": "The Department of Energy spends over half its $30 billion budget on nuclear disarmament, looking after US decaying stockpile of nuclear weapons and cleaning up the massive amount of nuclear waste generated producing this stockpile. The Department of Agriculture feeds many needy Americans through food stamps, finances many loans in rural communities and sets nutrition standards in schools. The Department of Commerce oversees meteorological forecasts (through the NOAA) which allow planes to fly safely, improves outcomes of military missions and helps avert natural disasters.\nAs Lewis paints the Trump administration at the start of their term spent no effort trying to run or even understand these departments, in contrast to the Obama and Bush administrations before them. All they seemed interested in was getting names of employees who had anything to do with climate change, suppressing publishing unfavourable data, promoting people that had supported Trump and funnelling money or opportunities to some supportive businesses.\nThis is a very insightful, and worrying, criticism of Trump’s style of politics. Traditionally we’ve had politicians that can assemble teams that competently manage the vast public service which quietly does a lot of good work, the impact of which may not be felt for dozens of political terms. Trump’s administration apparently neglects this, and could do a lot of damage that won’t be noticed for a long time after his presidency ends. The media is focused on ridiculous comments on Twitter and other scandals of the day, but the real damaging impacts are likely much more subtle.\nThe book was a very enjoyable read, although the final chapter on the Department of Commerce felt like it had too many individual narratives without a conclusion. The biographies of the public servants covered were diverse and extremely interested, tied together by a desire to serve the country and its citizens above individual gains. The impact the government departments have on people’s lives is incredibly interesting and unexpected, and the Trump administrations neglect of them is shocking.\nThere are a lot of areas where government investment or regulation can provide a lot of benefit over the private market. Protecting people from existential threats that no one person or business would pay for. Forcing businesses to provide safe products to consumers, and pay for negative impacts such as pollution. Support the vulnerable and give them a fair chance of success, enabling migration between income classes. Investing in speculative businesses that could lead to improvements for the people as a whole. The Fifth Risk covers some very interesting cases of these societal benefits."
  },
  {
    "objectID": "video-social/index.html",
    "href": "video-social/index.html",
    "title": "Remote social catchups are less intimate",
    "section": "",
    "text": "When you get 4 or more people in a group setting, frequently the conversation splits into smaller subgroups. The subgroups let people intermingle and participate in topics they’re more interested in while all being together.\nWith a video call you can’t easily do this splitting and only one person can talk at a time. This means the people who talk the most or the loudest get all the conversation, and people can’t split off the topic onto tangents.\nI’m not sure whether it’s possible to translate something so natural in a physical setting as splitting conversations into a virtual setting."
  },
  {
    "objectID": "python-diffs/index.html",
    "href": "python-diffs/index.html",
    "title": "Showing Side-by-Side Diffs in Jupyter",
    "section": "",
    "text": "For a long document it’s important to align the sentences (otherwise it’s hard to compare the differences), and highlight the individual differences at a word level. Overall the problems are breaking up a text into sentences and words, aligning the sentences, finding word level differences and displaying them side-by-side.\n\nBreaking text into lists of tokens\nWhile I’ll specifically use words and sentences you could use any kind of tokenization and collection of tokens (e.g. paragraphs). I use some type annotations that you could alter if you had a different notion of token (e.g. a SpaCy Token).\nfrom typing import List, Any, Callable, Tuple, Union\n\nToken = str\nTokenList = List[Token]\nI’ll do some simple splitting of strings into sentences and words. Note that these aren’t true inverses because we lose the additional whitespace, but for my application this isn’t too important.\nwhitespace = re.compile('\\s+')\nend_sentence = re.compile('[.!?]\\s+')\n\ndef tokenize(s:str) -> TokenList:\n    '''Split a string into tokens'''\n    return whitespace.split(s)\n\ndef untokenize(ts:TokenList) -> str:\n    '''Join a list of tokens into a string'''\n    return ' '.join(ts)\n\ndef sentencize(s:str) -> TokenList:\n    '''Split a string into a list of sentences'''\n    return end_sentence.split(s)\n\ndef unsentencise(ts:TokenList) -> str:\n    '''Join a list of sentences into a string'''\n    return '. '.join(ts)\n\ndef html_unsentencise(ts:TokenList) -> str:\n    '''Joing a list of sentences into HTML for display'''\n    return ''.join(f'<p>{t}</p>' for t in ts)\n\n\nMarking Differences\nFinding differences between sequences is done excellently by Python’s inbuilt difflib. It’s well designed as it can compare any sequences of items that have an equality comparison (not just strings). Unfortunately the defaults try to do some magic which you have to turn off with autojunk=false. The SequenceMatcher returns an opcode and the item ranges it applies to in each sequence. To markup the differences we just apply some markup to the ranges where the opcode is not 'equal'.\ndef markup_diff(a:TokenList, b:TokenList,\n                mark:Callable[TokenList, TokenList]=mark_span,\n                default_mark: Callable[TokenList, TokenList] = lambda x: x,\n                isjunk:Union[None, Callable[[Token], bool]]=None) -> Tuple[TokenList, TokenList]:\n    \"\"\"Returns a and b with any differences processed by mark\n\n    Junk is ignored by the differ\n    \"\"\"\n    seqmatcher = difflib.SequenceMatcher(isjunk=isjunk, a=a, b=b, autojunk=False)\n    out_a, out_b = [], []\n    for tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        markup = default_mark if tag == 'equal' else mark\n        out_a += markup(a[a0:a1])\n        out_b += markup(b[b0:b1])\n    assert len(out_a) == len(a)\n    assert len(out_b) == len(b)\n    return out_a, out_b\nIn this case our method signature is probably slightly wrong; we’re actually outputting lists of marked up tokens, but I haven’t thought hard enough about what this looks like. In my case I just use some string abuse to markup with HTML. We could markup individual words by color.\ndef mark_text(text:str) -> str:\n    return f'<span style=\"color: red;\">{text}</span>'\n    \ndef mark_span(text:TokenList) -> TokenList:\n    return [mark_text(token) for token in text]\nOr we could markup the whole span with a background:\ndef mark_span(text:TokenList) -> TokenList:\n    if len(text) > 0:\n        text[0] = '<span style=\"background: #69E2FB;\">' + text[0]\n        text[-1] += '</span>'\n    return text\n\n\nAligning Sentences\nTo align the sentences we need to align our TokenLists, but this time our tokens are sentences. We can do this simply by filling in padding around deleted or inserted sentences.\ndef align_seqs(a: TokenList, b: TokenList, fill:Token='') -> Tuple[TokenList, TokenList]:\n    out_a, out_b = [], []\n    seqmatcher = difflib.SequenceMatcher(a=a, b=b, autojunk=False)\n    for tag, a0, a1, b0, b1 in seqmatcher.get_opcodes():\n        delta = (a1 - a0) - (b1 - b0)\n        out_a += a[a0:a1] + [fill] * max(-delta, 0)\n        out_b += b[b0:b1] + [fill] * max(delta, 0)\n    assert len(out_a) == len(out_b)\n    return out_a, out_b\n\n\nDisplaying Side by Side\nWhen we have aligned sentences we can use CSS to display two sequences side by side. There’s some issue in Jupyter Notebooks that causes the first pair of paragraphs to not be aligned, so this contains a workaround adding two blank paragraphs. If you wanted to render this in a webbrowser in another way you’d remove these.\nfrom itertools import zip_longest\ndef html_sidebyside(a, b):\n    # Set the panel display\n    out = '<div style=\"display: grid;grid-template-columns: 1fr 1fr;grid-gap: 20px;\">'\n    # There's some CSS in Jupyter notebooks that makes the first pair unalign. This is a workaround\n    out += '<p></p><p></p>'\n    for left, right in zip_longest(a, b, fillvalue=''):\n        out += f'<p>{left}</p>'\n        out += f'<p>{right}</p>'\n        out += '</div>'\n    return out\n\n\nPutting it all together\nFinally we can combine all these functions to create the diffs:\n\nEscape any HTML characters so that they will display properly in HTML\nAlign the texts at a sentence level\nMarkup the differences between the tokens in each pair of aligned sentences\nOutput the markedup and aligned sentences as side-by-side HTML\n\nimport html\ndef html_diffs(a, b):\n    a = html.escape(a)\n    b = html.escape(b)\n\n    out_a, out_b = [], []\n    for sent_a, sent_b in zip(*align_seqs(sentencize(a), sentencize(b))):\n        mark_a, mark_b = markup_diff(tokenize(sent_a), tokenize(sent_b))\n        out_a.append(untokenize(mark_a))\n        out_b.append(untokenize(mark_b))\n\n    return html_sidebyside(out_a, out_b)\nFinally we can have a shortcut to display them in a Jupyter notebook:\nfrom IPython.display import HTML, display\ndef show_diffs(a, b):\n    display(HTML(html_diffs(a,b)))\nThen we can get nice looking side-by-side diffs.\n\n\n\nExample of side-by-side diffs"
  },
  {
    "objectID": "pooling-proportions-empirical-bayes/index.html",
    "href": "pooling-proportions-empirical-bayes/index.html",
    "title": "Pooling Proportions with Empirical Bayes",
    "section": "",
    "text": "There is a better way to calculate binomial proportions by pooling information across groups. Given the conversion rate over several channels, we know something about the conversion rate for a new unseen channel. The rates of each group can be considered a distribution, and modelled as a hierarchical model. Then a new group starts near the centre of this distribution, and as more data on the group is collected it moves towards the independent group average. The small groups are all pulled to the centre of the distribution, away from the edges of the ranking.\nFor a binomial distribution the conjugate prior is the beta distribution, which seems like a reasonable start. We could estimate this distribution with standard Bayesian computational techniques such as Markov Chain Monte Carlo, but we can analytically reduce the likelihood into something that can be computed directly. The maximum likelihood estimate (or any other estimate) can be plugged in and the group averages estimated, an Empirical Bayes approach.\n\nModel and Group Estimates\nThe data is from m groups, with group \\(i\\) containing \\(N_i\\) dichotomous outcomes of which \\(k_i\\) are positive. The group is modelled as a binomial with probability \\(\\theta_i\\), which is the group average we are trying to infer. The groups probabilities are modelled as a Beta distribution where the parameters \\(\\alpha\\) and \\(\\beta\\) are to be inferred.\n\\[\\begin{align}\n\\theta_i & \\sim {\\rm Beta}(\\alpha, \\beta) \\\\\nk_i & \\sim {\\rm Binomial}(N_i, \\theta_i) \\; \\forall i=1,\\ldots,m\n\\end{align}\\]\nGiven \\(\\alpha\\) and \\(\\beta\\) the posterior estimates are\n\\[\\hat{\\theta}_i = \\frac{k_i + \\alpha - 1 }{N_i + \\alpha + \\beta - 2}\\]\nThat is the prior is effectively starting at \\(\\alpha+\\beta-2\\) outcomes, of which \\(\\alpha-1\\) are positive and \\(\\beta-1\\) are negative. This standard notation is asymmetrical; using the sample strength \\(\\kappa = \\alpha + \\beta\\) it’s a more symmetrical (but still plagued by off-by-one adjustments).\n\\[\\hat{\\theta}_i = \\frac{k_i + \\alpha - 1 }{N_i + \\kappa - 2}\\]\nIn fact it can be seen as a weighted average of the group average and the prior average from the Beta distribtuion. The direct estimate of probability is \\(p_i = k_i/N_i\\), and the mode of the Beta function is \\(\\omega = \\frac{\\alpha-1}{\\kappa-2}\\) (for \\(\\kappa > 2\\)). Then\n\\[\\hat{\\theta}_i = \\frac{N_i p_i + (\\kappa - 2) \\omega }{N_i + (\\kappa - 2)}\\]\n\n\nEstimating the Beta Distribution\nAny estimate of the Beta Distribution will work. Probabilaball has an example with code of using the method of moments estimator (there’s also a brief example in Casella’s An Introduction to Empirical Bayes Data Analysis). However the maximum likelihood estimator has many nice properties, and examining the likelihood shows the sensitivity to the parameters.\nWe want to find the dependence of the binomial outcomes on the hyper-priors \\(\\alpha\\) and \\(\\beta\\) alone. For now just consider one group and drop the group index for simplicity. From our distributional model assumptions we have the following probability distributions:\n\\[\\begin{align}\nP(\\theta \\vert \\alpha, \\beta) &=  \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)} \\\\\nP(k \\vert N, \\theta) &=  {N \\choose k} \\theta^{k}(1-\\theta)^{N-k}\n\\end{align}\\]\nWe can easily simulate a distribution in R; I find it useful for explicitly checking the calculations\n# Sample data\na <- 10         # alpha\nb <- 20         # beta\nN <- 5          # Number of Trials\nnsim <- 10000   # Number of simulations\nk <- 0:N        # All possible values of positive outcomes\n\ntheta <- rbeta(nsim, a, b)\nk_sim <- rbinom(nsim, N, theta)\n\n# Simulated probability distribution\npsim <- table(k_sim)/length(k_sim)\n\n#      0      1      2      3      4      5\n# 0.1463 0.3227 0.3060 0.1641 0.0535 0.0074\nWe can obtain the marginal distribution by integrating out \\(\\theta\\) using the law of total probability, and simplifying with conditional independence. This can be done for a wide range of models, called Conditionally Independent Hierarchical Models, see also the derivation for a hierarchical normal distribution with known variance\n\\[\\begin{align}\nP(k \\vert \\alpha, \\beta, N) &= \\int_0^1 {\\rm{d}\\,\\theta} P(k \\vert \\alpha, \\beta, N, \\theta) P(\\theta \\vert \\alpha,\\beta, N)  \\\\\n&= \\int_0^1  P(k \\vert \\theta, N) P(\\theta \\vert \\alpha,\\beta) {\\rm{d}\\,\\theta}\n\\end{align}\\]\nintegrand <- function(k) function(theta) {dbinom(k, N, theta) * dbeta(theta, a, b)}\npnumeric <- vapply(k, function(x) integrate(integrand(x), 0,1)$value, double(1))\n\n# These should be about the same and give a ratio close to 1\npnumeric / psim\n\n#         0         1         2         3         4         5\n# 1.0440972 0.9861547 0.9947580 1.0117864 0.9605892 0.9722721\nThis integral can be evaluated analytically using the Beta function\n\\[\\begin{align}\nP(k \\vert \\alpha, \\beta, N) &= {N \\choose k} \\frac{1}{B(\\alpha, \\beta)} \\int_0^1  \\theta^{k+\\alpha-1} (1-\\theta)^{N-k+\\beta-1} {\\rm{d}\\,\\theta} \\\\\n&= {N \\choose k} \\frac{B(k+\\alpha, N-k+\\beta)}{B(\\alpha, \\beta)}\n\\end{align}\n\\]\nz <- 0:N\npanalytic <- choose(N, k) * beta(k+a, N-k+b)/ beta(a,b)\n\n#\npanalytic/psim\n\n# Ratio is close to 1\n#        0         1         2         3         4         5\n#1.0440972 0.9861547 0.9947580 1.0117864 0.9605892 0.9722721\nThis is numerically unstable; the Beta function evaluates to very small numbers. It is much better to work in the logarithmic scale.\nlogp <- lchoose(N,z) + lbeta(z+a, N-z+b) - lbeta(a,b)\n\nexp(logp)\nGoing from one to multiple groups just requires multiplying together all the likelihoods, or equivalently summing the log likelihoods.\n\n\nMaximum Likelihood Estimates\nTo show how to obtain the maximum likelihood estimates let’s use a simulation with 14 groups.\nset.seed(6011)\n\na0 <- 10\nb0 <- 20\nm <- 14\nNs <- round(10^(runif(m, 1, 2.7)))\nNs\n# 20  30 299  21  22 249  35 133  58 247  51  73 312 479\n\ntheta <- rbeta(m, a0, b0)\ntheta\n\n# 0.247 0.456 0.255 0.290 0.321 0.313 0.343\n# 0.360 0.284 0.384 0.294 0.380 0.528 0.286\n\nks <- rbinom(m, Ns, theta)\nks\n# 85 109   8 152  46   8  55  12   9  27  84  12  23  14\nFor a given set of k and N we can calculate the negative log likelihood as above:\nnegloglik <- function(a,b) {\n    -sum(lchoose(Ns,ks) + lbeta(ks+a, Ns-ks+b) - lbeta(a,b))\n}\nSince there are only two parameters we can do a brute force search. I find it easier to think about in terms of the mode \\(\\omega\\) and strength \\(\\kappa\\), or \\(\\tau = \\kappa - 2\\).\nomega <- seq(0, 1, by=0.01)\ntau <- seq(1,500,by=1)\n\nomega <- rep(omega, times=length(tau))\ntau <- rep(tau, each=length(omega)/length(tau))\n\na <- omega * tau + 1\nb <- (1-omega) * tau + 1\n\n# ll for log likelihood\nsystem.time ({ ll <- mapply(negloglik, a, b) })\n\nidx = which(ll == min(ll))\nc(omega[idx], tau[idx], a[idx], b[idx])\n\n# 0.35 27.00 10.45 18.55\nBrute force searching gives an estimate close to the original parameters. It also allows us to plot the distribution easily, with the true value as a point in red. Notice that the negative log likelihood increases fast as \\(\\omega\\) gets far from the true value, but relatively slowly for larger sample strengths.\n\n\n\nLog likelihood of parameters from data\n\n\nA more efficient way of finding the maximum is to find the zero of the derivatives of the negative log likelihood in terms of the digamma function:\n\\[\\begin{align}\n\\frac{\\partial l}{\\partial \\alpha} &= \\sum_{i=1}^{m} \\psi(k_i + \\alpha) - \\psi(N_i + \\alpha + \\beta) + \\psi(\\alpha + \\beta) - \\psi(\\alpha)\\\\\n\\frac{\\partial l}{\\partial \\beta} &= \\sum_{i=1}^{m} \\psi(N_i - k_i + \\beta) - \\psi(N_i + \\alpha + \\beta) + \\psi(\\alpha + \\beta) - \\psi(\\beta)\\\\\n\\end{align}\n\\]\nOr in code the derivatives of the negative log likelihoods are:\nd_nll_by_a <- function(a,b) {\n    -sum(digamma(ks+a)  - digamma(a) + digamma(a+b) - digamma(Ns+a+b))\n}\n\nd_nll_by_b <- function(a,b) {\n    -sum(digamma(Ns-ks+b)  - digamma(b) + digamma(a+b) - digamma(Ns+a+b))\n}\nIt seems there’s a unique global maximum and we can solve for it numerically. For an initial estimate use that the mean of the distribution is approximately the mean of the groups, and the variance is approximately the variance of the groups. These can then be rearranged to get \\(\\alpha\\) and \\(\\beta\\) (see the article on the beta distribution for details). For our starting point we can use a rough approximation.\ninitial_guess <- function(Ns, ks) {\n    mu <- mean(ks/Ns)\n    k <- p0*(1-p0)/var(ks/Ns)\n    c(mu*k, (1-mu)*k)\n}\n\ninitial_guess(ks, Ns)\n# 5.538467 9.011807\nThen the package rootSolve can be used to quickly find the minimum. A quick check shows this is lower than any value found in the grid search.\nr <- rootSolve::multiroot(\n        function(x) c(d1=d_nll_by_a(x[1], x[2]),\n                      d2=d_nll_by_b(x[1],x[2])),\n        start=initial_guess(Ns, ks))\n\n\n#  $root\n# [1] 10.31818 18.43713\n#\n# $f.root\n#            d1            d2\n#  1.602294e-07 -1.824109e-07\n#\n# $iter\n# [1] 6\n#\n# $estim.precis\n# [1] 1.713201e-07\nIdeally all this would be wrapped up in some library that makes this easy to use. This is a better default for calculating group proportions than the simple average.\nWe can then put this estimate to improve the raw estimate \\(p=k/N\\) to \\(\\hat{p} = \\frac{k+\\alpha-1}{N+\\alpha+\\beta-2}\\).\nahat <- r$root[1]\nbhat <- r$root[2]\n\ndf <- data.frame(theta,Ns,ks,p=ks/Ns,phat=(ks+ahat-1)/(Ns+ahat+bhat-2))\n\ndf$p_error <- df$theta - df$p\ndf$phat_error <- df$theta - df$phat\n\n\n\ntheta\nNs\nks\np\nphat\np_error\nphat_error\n\n\n\n\n0.25\n392\n85\n0.22\n0.23\n0.03\n0.02\n\n\n0.46\n250\n109\n0.44\n0.43\n0.02\n0.03\n\n\n0.26\n14\n8\n0.57\n0.42\n-0.32\n-0.17\n\n\n0.29\n471\n152\n0.32\n0.32\n-0.03\n-0.03\n\n\n0.32\n121\n46\n0.38\n0.37\n-0.06\n-0.05\n\n\n0.31\n18\n8\n0.44\n0.39\n-0.13\n-0.07\n\n\n0.34\n141\n55\n0.39\n0.38\n-0.05\n-0.04\n\n\n0.36\n39\n12\n0.31\n0.32\n0.05\n0.04\n\n\n0.28\n35\n9\n0.26\n0.30\n0.03\n-0.01\n\n\n0.38\n72\n27\n0.38\n0.37\n0.01\n0.02\n\n\n0.29\n326\n84\n0.26\n0.26\n0.04\n0.03\n\n\n0.38\n20\n12\n0.60\n0.46\n-0.22\n-0.08\n\n\n0.53\n42\n23\n0.55\n0.47\n-0.02\n0.06\n\n\n0.29\n63\n14\n0.22\n0.26\n0.06\n0.03\n\n\n\nNotice that the error is much smaller for \\(\\hat{p}\\) when \\(N\\) is small. In particular the most extreme value of p, when N=20, gets shrunk down closer to the true value. The RMSE for \\(\\hat{p}\\) is 0.05, much smaller than the RMSE for \\(p\\) of 0.18.\nThis should be the default way of calculating proportions. David Robinson has provided the ebbr R package, but the python port needs some work. Ideally this would be easily available in more languages and SQL engines, along with a similar facility for the mean (and other statistics).\nSome other good explanations on these kinds of methods are from Damien Martin and David Robinson (in fact David has a whole book on the subject)."
  },
  {
    "objectID": "way-of-physicist/index.html",
    "href": "way-of-physicist/index.html",
    "title": "The Way of the Physicist",
    "section": "",
    "text": "A large number of the physicists I trained with are now data scientists, and it’s not uncommon to meet a data scientist who trained in Physics. Part of this is because there’s not a lot of physics jobs, especially in Australia. But another reason is that the training we get as physicists is very similar to what you need for data science.\nDavid Bailey, a physicist from the University of Toronto, has objectives for their undergraduate Physics program which describes “the Way of the Physicist”:\nI never use the topics I trained for in Physics day to day, but I use the conceptual framework all the time."
  },
  {
    "objectID": "way-of-physicist/index.html#construct-mathematical-models-of-physical-systems",
    "href": "way-of-physicist/index.html#construct-mathematical-models-of-physical-systems",
    "title": "The Way of the Physicist",
    "section": "Construct mathematical models of physical systems",
    "text": "Construct mathematical models of physical systems\nConstructing a conceptural model is a crucial step in any data science problem. Any analysis needs to change a decision which means it needs to connect input decisions to outcomes. This connection is a model, which I’ll call a conceptual model to distinguish from an algorithmic model that is directly fit to data.\nEven when using a blackbox algorithmic model it’s important to have a conceptual model of the underlying process. The process may be broken down into several algorithmic models.\nA good conceptual model will identify the key drivers of the outcomes, outlining the key relationships. It’s always useful to consult with domain experts who understand that process well, because they often have a good intuition of what is important."
  },
  {
    "objectID": "way-of-physicist/index.html#solve-the-models-analytically-or-computationally",
    "href": "way-of-physicist/index.html#solve-the-models-analytically-or-computationally",
    "title": "The Way of the Physicist",
    "section": "Solve the models analytically or computationally",
    "text": "Solve the models analytically or computationally\nWhen you have a model you will want to be able to solve it. Physics textbooks primarily focus on problems that can be solved from first principles, which can be rarely done in data science. However it is possible to come up with ballpark estimates, and solve “easy cases”.\nA large part of the trick to solving models is coming up with models that are solvable. Models should be as parsimonious as possible, focusing on the most significant effects first. If you put in too many effects you lose the insight, and can’t see the forest from the trees.\nPhysics has a really good selection of models that are simple, but insightful. In Statistical Mechanics an ideal gas where the particles don’t interact at all is a useful model for many real gasses. In Mechanics the driven harmonic oscillator is useful for understanding resonance, which can collapse bridges. In Quantum Mechanics the structure of atomic shells can be understood by ignoring interactions between electrons, which is a useful (but very wrong) model for understanding the chemistry of atoms.\nAnalytic models are powerful because you can explore them in detail, but they have their limits. For many sophisticated models we need to perform simulation, optimisation, and model fitting. Model fitting is what many many people think of in data science; extracting relations between an outcome and input variables."
  },
  {
    "objectID": "way-of-physicist/index.html#make-physical-measurements-of-the-systems",
    "href": "way-of-physicist/index.html#make-physical-measurements-of-the-systems",
    "title": "The Way of the Physicist",
    "section": "Make Physical Measurements of the Systems",
    "text": "Make Physical Measurements of the Systems\nThe crucial connection between theory and reality is in the act of measurement. In data science you can often collect data off the “data exhaust” of existing processes, but this is really messy. Understanding how to collect data, whether it’s digital tracking, surveys, or sensor arrays is crucial to connect the model with reality.\nEven if you don’t directly make the measurements it’s very important to understand how the measurements are made. Measurements are always imperfect, and you’re rarely measuring the quantity you actually want to measure. Understanding the inferences, approximations, and errors that occur in the measurements help understand how to assess your model."
  },
  {
    "objectID": "way-of-physicist/index.html#compare-the-measurements-with-the-expectations",
    "href": "way-of-physicist/index.html#compare-the-measurements-with-the-expectations",
    "title": "The Way of the Physicist",
    "section": "Compare the Measurements with the Expectations",
    "text": "Compare the Measurements with the Expectations\nIn data science this can occur offline by checking a model against a test set (or with cross validation), to see the generalisation error. But the best tests are always in an online setting; performing the action and seeing whether it meets expectations. When comparing options the cleanest way is an A/B experiment.\nAn important premise of this is you should always have an expectation from your prior modelling. You shouldn’t be deploying models, running experiments, or making important data-driven decisions without having an expectation of what the outcome should be."
  },
  {
    "objectID": "way-of-physicist/index.html#communicate-the-results-both-verbally-and-in-writing",
    "href": "way-of-physicist/index.html#communicate-the-results-both-verbally-and-in-writing",
    "title": "The Way of the Physicist",
    "section": "Communicate the Results, Both Verbally and in Writing",
    "text": "Communicate the Results, Both Verbally and in Writing\nA data scientist, like a physicist, is part of a community.\nYou have to be able to communicate your findings with stakeholders to influence decisions and get better outcomes. The communication should be tailored to your audience."
  },
  {
    "objectID": "way-of-physicist/index.html#improve-and-iterate",
    "href": "way-of-physicist/index.html#improve-and-iterate",
    "title": "The Way of the Physicist",
    "section": "Improve and Iterate",
    "text": "Improve and Iterate\nWhenever there’s a difference between the measurements and expectations there’s an opportunity to improve. Whether it’s improving the models, the measurements or even the communication, to get the desired impact. Starting with a simple model and iterating on it is often a much better path than starting with an overly complex model, because you get faster feedback on what is important."
  },
  {
    "objectID": "placeholder-refining-location/index.html",
    "href": "placeholder-refining-location/index.html",
    "title": "Refining Location with Placeholder",
    "section": "",
    "text": "Placeholder is a great library for Coarse Geocoding, and I’m using it for finding locations in Australia. In my application I want to get the location to a similar level of granularity; however the input may be for a higher level of granularity. Placeholder doesn’t directly provide a method to do this, but you can use their SQLite database to do it.\nFor example to find the largest locality for East Gippsland, with Who’s On First id 102049039, you can use the SQL."
  },
  {
    "objectID": "placeholder-refining-location/index.html#docs",
    "href": "placeholder-refining-location/index.html#docs",
    "title": "Refining Location with Placeholder",
    "section": "Docs",
    "text": "Docs\nThe Docs contain all the geojson data for each place; including the id, names, placetype, lineage, geometry, and population. This is what Placeholder returns from a Query.\nsqlite> select * from docs limit 1;\nid|json\n1|{\"id\":1,\"name\":\"Null Island\",\"placetype\":\"country\",\"rank\":{\"min\":19,\"max\":20},\"lineage\":[{\"country_id\":1}],\"geom\":{\"bbox\":\"-0.0005,-0.000282,0.000379,0.000309\",\"lat\":0.000003,\"lon\":0.00001},\"names\":{\"eng\":[\"Null Island\"],\"epo\":[\"Nulinsulo\"],\"fra\":[\"Null Island\"],\"heb\":[\"נאל איילנד\"],\"hun\":[\"Nulla Sziget\"],\"ind\":[\"Null Island\"],\"ita\":[\"Null island\"],\"jbo\":[\".nyldaplu.\"],\"jpn\":[\"ヌル島\"],\"lzh\":[\"虛無島\"],\"mkd\":[\"Нулти Остров\"],\"msa\":[\"Pulau Nol\"],\"rus\":[\"остров Ноль\"],\"spa\":[\"Null Island\"],\"ukr\":[\"Острів Нуль\"],\"vie\":[\"đảo Rỗng\"],\"zho\":[\"空虛島\"]}}\nNote that SQLite has methods for extracting from JSON. In particular we could extract the main attributes:\nSELECT\n  id,\n  json_extract(json, '$.name') AS name,\n  json_extract(json, '$.placetype') AS placetype,\n  json_extract(json, '$.population') AS pop,\n  json_extract(json, '$.geom.area') AS area\nFROM docs\nLIMIT 5;\n\n\n\n\n\n\n\n\n\n\nid\nname\nplacetype\npop\narea\n\n\n\n\n1\nNull Island\ncountry\n\n\n\n\n\n\n\n\n\n\n\n85632161\nMacau S.A.R.\ncountry\n449198\n0.002313\n\n\n85632163\nGuam\ndependency\n178430\n0.046566\n\n\n85632167\nBahrain\ncountry\n1332171\n0.070331\n\n\n85632169\nUnited States Virgin Islands\ndependency\n109825\n0.031723"
  },
  {
    "objectID": "placeholder-refining-location/index.html#tokens-and-fulltext",
    "href": "placeholder-refining-location/index.html#tokens-and-fulltext",
    "title": "Refining Location with Placeholder",
    "section": "Tokens and Fulltext",
    "text": "Tokens and Fulltext\nTokens are all the different names for a location in different languages. This is used in searching by place name.\nsqlite> select * from tokens limit 5;\nid|lang|tag|token\n1|eng|preferred|null island\n1|und|abbr|xn\n1|epo|preferred|nulinsulo\n1|heb|preferred|נאל איילנד\n1|hun|preferred|nulla sziget\nMore specifically the fulltext table is a full text search on the term table, with words separated by an underscore. It is row aligned with the term table so you can use fulltext search to get relevant ids.\nSELECT id\nFROM tokens as t1\n  JOIN fulltext AS f1 ON f1.rowid = t1.rowid\nWHERE f1.fulltext MATCH 'nulla_sziget'"
  },
  {
    "objectID": "placeholder-refining-location/index.html#lineage",
    "href": "placeholder-refining-location/index.html#lineage",
    "title": "Refining Location with Placeholder",
    "section": "Lineage",
    "text": "Lineage\nWho’s on First has a notion of lineage; Rockhampton is a county in the region of Queensland in the country of Australia in the continent of Oceania. This is recorded in the lineage table, for each id each ancestor is in a row with a pid. For example searching for the ancestors of Rockhampton gives:\nSELECT\n  lineage.pid,\n  token\nFROM lineage\nJOIN tokens ON lineage.pid = tokens.id\nWHERE lineage.id = 102048825\nAND lang = 'eng'\nAND tag = 'preferred';\n\n\n\npid\ntoken\n\n\n\n\n85632793\naustralia\n\n\n85681463\nqueensland\n\n\n102191583\noceania"
  },
  {
    "objectID": "placeholder-refining-location/index.html#rtree",
    "href": "placeholder-refining-location/index.html#rtree",
    "title": "Refining Location with Placeholder",
    "section": "Rtree",
    "text": "Rtree\nSometimes the lineage alone doesn’t capture the search and so Placeholder also stores the rectangular bounding boxes in an R-tree for efficient searching. For example to search for locations within 0.1 degrees of Rockhampton you could query it like this:\nSELECT t2.id AS id, t2.token as token\nFROM rtree AS r1, rtree AS r2\n  JOIN tokens AS t2 ON t2.id = r2.id\nWHERE r1.id = 102048825\nAND lang = 'eng' AND tag = 'preferred'\n-- https://silentmatt.com/rectangle-intersection/\nAND (\n  r1.maxZ > r2.minZ AND\n  r1.minX - 0.1 < r2.maxX AND\n  r1.maxX + 0.1 > r2.minX AND\n  r1.minY - 0.1 < r2.maxY AND\n  r1.maxY + 0.1 > r2.minY\n)\nLIMIT 5;\n\n\n\nid\ntoken\n\n\n\n\n101934101\ncallemondah\n\n\n101933899\nbeecher\n\n\n85782291\nrockhampton central\n\n\n85775745\nwest rockhampton\n\n\n85775737\nrockhampton city"
  },
  {
    "objectID": "open-library-isbn-lookup/index.html",
    "href": "open-library-isbn-lookup/index.html",
    "title": "Open Library ISBN Lookup",
    "section": "",
    "text": "For books an ASIN is the same as its ISBN-10, which creates a linkage point with Open Library. From my previous Open Library Exploration about 20% of editions in Open Library has an ISBN-13, but not an ISBN-10 (and 64% have one of either). It’s straight-forward to convert an ISBN-10 to an ISBN-13; just prepend “978” and change the final check digit. Here’s how this can be done in Python (you can see the rest of the details in the notebook).\nisbn_13_weighting = [1,3,1,3,1,3,1,3,1,3,1,3,1]\n\ndef isbn13_check_digit(isbn13: str) -> str:\n    assert len(isbn13) == 13\n    digits = [int(x) for x in isbn13[:-1]]\n    check = 10 - sum(x*y for x,y in zip(digits, isbn_13_weighting)) % 10\n    \n    if check == 10:\n        check_digit = \"0\"\n    else:\n        check_digit = str(check)\n        \n    assert len(check_digit) == 1\n    assert check_digit in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n    return check_digit\n    \ndef isbn10_to_13(isbn10: str) -> str:\n    return \"978\" + isbn10[:-1] + isbn13_check_digit(\"978\" + isbn10)\nTrying to match each extracted ASIN with an Open Library record gives 94% matches; 20% with more than one match. Some of the missing records are in Open Library, just not under this ISBN (for example Physics for Entertainment isn’t under 1610279034), and others are missing completely like Complexity Economics.\nThe duplicate matches and high coverage align with my earlier investigations with adding a book to Open Library and looking up books in Open Library with SQLite. The books are often imported from multiple places and easy to add multiple times. For duplicate records we would ideally have some methods for merging their information into a single record.\nNow that I can get from a Hacker News post to an Open Library record the next step is to put it all together. A simple way to do this would be to create a HTML page for every work containing information from Open Library (such as the title and authors), along with all the related Hacker News comments. This would be enough to prove out the concept before doing additional work to link books described in other ways to Open Library."
  },
  {
    "objectID": "deep-space-orbit/index.html",
    "href": "deep-space-orbit/index.html",
    "title": "Energy to Orbit vs Launch into Deep Space",
    "section": "",
    "text": "Estimate the energy in a 9-volt battery. Is it enough to launch the battery into orbit?\n\nI tried to answer this with the energy density required to launch into deep space. But this is different to going into orbit; how much energy is required to get into low Earth orbit?\n\nLow Earth Orbit\nA low orbit has to be above the height of the atmosphere (otherwise will require propulsion to overcome atmospheric friction), and so is typically above 300 km. Given the radius of the Earth is around 6000 km, the orbit is only 5% further from the centre than the ground.\nIn orbit the gravitational acceleration, \\(\\frac{GM}{r^2}\\) needs to balance the centripetal force to curve the objects path, \\(\\frac{v^2}{r}\\) (which is the only function of orbital speed and distance that has the right units, up to a constant). The gravitational acceleration in low Earth orbit is about 10% lower than at the surface, so still around 10 m/s².\nSo the kinetic energy is \\(\\frac{1}{2} m v^{2}\\), and so the density is \\(\\frac{1}{2} r \\frac{GM}{r^2}\\), that is about \\(3 \\times 10^{7} \\ \\rm{J}/\\rm{kg}\\).\nTo get into orbit the change in gravitational energy is \\(\\frac{GM}{r} - \\frac{GM}{1.05 r} \\approx 0.05 \\frac{GM}{r}\\). That is it’s about 5% of the kinetic energy, and so is a small adjustment.\nSo the energy density required to get into a low Earth orbit is around 30 kJ/g, which is about half the estimate we had for launching into deep space. So even for a low Earth orbit the energy density required is about 1,000 times more than in a 9V battery (at 35 J/g)."
  },
  {
    "objectID": "energy-desnsity-launch/index.html",
    "href": "energy-desnsity-launch/index.html",
    "title": "Energy Desnsity to Launch into Space",
    "section": "",
    "text": "Estimate the energy in a 9-volt battery. Is it enough to launch the battery into orbit?\n\nI have already (mis)estimated the energy of a battery, but looked it up as 500 mAh.\n\nEnergy density required to launch into space\nTo launch into space you have to exchange energy to counteract the change in gravitational energy (at least, you’ll need more for air resistance). The gravitational energy is \\(\\frac{G M m}{r}\\); to estimate it requires the gravitational constant, the mass of the Earth and its radius. But we can refactor this as \\(r \\left( \\frac{G M}{r^2} \\right) m\\), where the first term is the radius of the Earth (\\(6 \\times 10^6 \\rm{m}\\)) and the second term is the acceleration due to gravity at the Earth’s surface, around \\(10\\ \\rm{m}\\,\\rm{s}^{-2}\\). So the energy required to launch into space is \\(6 \\times 10^7\\ \\rm{J}\\,\\rm{kg}^{-1}\\) or about 60 kJ/g.\n\n\nEnergy density of battery\nA 9V battery has 500 mAh, and weighs about 50g. A mAh is 3.6 J, so a 9V battery at 10 mAh/g is about 35 J/g. It’s nowhere near energy density enough to launch itself into orbit."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Skeptric",
    "section": "",
    "text": "python\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nannotation\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\nannotation\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\nannotation\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nbooks\n\n\nannotation\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\nnlp\n\n\nner\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nannotation\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\nhnbooks\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nhtml\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nhtml\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nhtml\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nhtml\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ncommoncrawl\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\npython\n\n\nner\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\npython\n\n\nner\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncommoncrawl\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstan\n\n\nr\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstan\n\n\nr\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstan\n\n\nr\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nstan\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstan\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nprogramming\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npandas\n\n\npython\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\npython\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nwsl\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npandas\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npandas\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\npython\n\n\nemacs\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nDec 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\npython\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\npandas\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nathena\n\n\npresto\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\npython\n\n\ncommoncrawl\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nwhatcar\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwhatcar\n\n\npython\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nwhatcar\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nstatistics\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsicp\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsicp\n\n\n\n\n\n\n\n\n\n\n\nOct 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsicp\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsicp\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsicp\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ninsight\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\nlegacy code\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nSep 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nlegacy code\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npresto\n\n\nathena\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nhtml\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\ndata\n\n\npresto\n\n\nathena\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npresto\n\n\nathena\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\nlinux\n\n\nemacs\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\ncommoncrawl\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\ncommoncrawl\n\n\nrdf\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nrdf\n\n\ncommoncrawl\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nrdf\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncommoncrawl\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncommoncrawl\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncommoncrawl\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncommoncrawl\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\nnlp\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\npresto\n\n\nathena\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njobs\n\n\nnlp\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2020\n\n\nEdward Ross\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nMay 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\ndata\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nApr 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nr\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\npython\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nwsl\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\njobs\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\njupyter\n\n\ndata\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\npython\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ndata\n\n\nnlp\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\njobs\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\npython\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nannotation\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\ndata\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\npresto\n\n\nathena\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npresto\n\n\nathena\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nMar 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndata\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nemacs\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 18, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\njobs\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nexcel\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2013\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2013\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2013\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2013\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2012\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2012\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2012\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 29, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmaths\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2011\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2010\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2010\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drive-metrics/index.html",
    "href": "drive-metrics/index.html",
    "title": "Metrics you can Drive",
    "section": "",
    "text": "When people start looking for a metric to track they want to look for things that have a direct impact on the business, such as revenue, share price or customer satisfaction. These are important to track and are closer to the real business objectives. But a product team in a mature company tends to only have limited impact on these, and they lag product decisions by a very long time. Trying to track the effect of product optimisations on these metrics is demotivating since they will be driven almost entirely by external factors. However you can find a metric aligned with the outcomes that you can drive and can motivate the team to rolling successful optimisations.\nFor example suppose your team is working on a checkout page for a digital product. You want to drive long term profits, but that depends on the actions of many other teams and takes a long time to measure. So you could try to pick something your team can drive; conversion rate from checkout to purchase. This aligns well with the goal of profit and is something you can measure quickly and optimise on.\nThere will still be factors out of your teams control; like a broad marketing campaign could drive prospects that are less likely to purchase which drives down the conversion rate. And there will likely be seasonal variations, and changes in the rest of the product will impact it. But as you build an understanding of the factors you can explain the changes, and still measure improvements against a moving background by A/B tests. Resist trying to account for these factors in the metric to keep the metric simple; you will never be able to account for everything, and having to explain and “tweak” a metric will make it lose its impact.\nOptimising a single metric can be really powerful because it gives the team a clear goal. It is worth keeping in mind the actual goal you’re trying to contribute to. You may find that offering an upsell on the checkout page reduces conversion, but increases overall revenue without churning repeat customers. Then it’s probably the right thing to introduce the upsell even though it drives down your target metric, because it’s more likely to increase long term revenue. You may even need to change to a different target metric if the impact your product decisions make changes; as long as it represents an outcome your team can drive."
  },
  {
    "objectID": "nlp-checklist/index.html",
    "href": "nlp-checklist/index.html",
    "title": "A Checklist for NLP models",
    "section": "",
    "text": "The tool Checklist, presented in an ACL 2020 paper, presents a way to get more assurance on an model with language input. The suggest 3 types of tests:\n\nMinimum Functionality Tests: Basic things the model should do (e.g. “This was a fantastic meal” should be positive sentiment)\nInvariance Tests: Specifying changes to input that shouldn’t change the output (e.g. “This was a fantastic meal” and “This was a fantastic hotel” should both have the same sentiment)\nDirectional Tests: Specifying changes to the input that should change it just one direction (e.g. if we add “The deserts were excellent” it should not decrease the sentiment).\n\nI gave the examples in terms of sentiment, but it really works for any model with a cateogorical or quantitative output. They introduce a wide range of capabilities to perform these types of tests on to get a matrix. They gave examples like vocabulary (e.g. changing words), robustness (e.g. simulating spelling mistakes), NER (changing names), negation, coreference resolution, semantic role labelling and logic. Some of these are quite exotic but it’s a good way of coming up with a wide range of examples.\nThey provide a lot of tooling to help generate tests, as well as rewrite tests in a fluent way for invariance and directional tests, but the main thing I got out of this is the idea of actually writing data tests. Simply having and running tests like “this simple text should almost certainly to this category” (with some probability) is a very good failsafe for a model. It doesn’t replace metrics, but it can catch serious errors in the dataset and help show things not captured in the headline metrics.\nFrom now on I’m going to specify at least minimum functionality tests for any model I build. I need to look into their tool in more detail to see whether it is worth using."
  },
  {
    "objectID": "checking-with-calculation/index.html",
    "href": "checking-with-calculation/index.html",
    "title": "Checking With Calculation",
    "section": "",
    "text": "I played some very basic examples in my head and thought I was right, but I couldn’t be sure. So I quickly implemented a concrete example case in Python using numpy. This helped me resolve pretty quickly that I was wrong; some searching produced a counterexample. This was a much more efficient way to resolve this rather than reading the whole paper in detail. But now I need to work out what I did wrong."
  },
  {
    "objectID": "mvt/index.html",
    "href": "mvt/index.html",
    "title": "Mean Value Theorem",
    "section": "",
    "text": "After a big night out at a pub you somehow managed to get home to your bed. You’re not quite sure how you got home; there’s a vague recollection of wandering home, from lampost to lampost (but bouncing off them smoothly, so your path stays differentiable). Then the mean value theorem says at some point you were going in the right direction.\nThis is a pretty good way of capturing the mean value theorem. The simple form we learned is that if a function \\(f:[a,b]\\to\\mathbb{R}\\) is differentiable on its interior and continuous at its endpoints, then there is a point c in the interval such that \\(f'(c)=\\frac{f(b)-f(a)}{b-a}\\). The right hand side is the “right direction”; the slope of the straight line path from the pub to your bed. The left hand side is the direction you’re going at the instant c; the tangent to the path.\nFor the analogy to work you have to be able to model your path as a function of one variable; in particular this means you can never cross your own path, otherwise there’s no way to make an x-axis. This wouldn’t be true for many drunkards walks, and in higher dimensions the generalisation isn’t as straightforward.\nBut this is beside the point; the story was compelling enough to make me remember the mean value theorem years later and ask questions on whether it applies to higher dimensions. Similarly the vibrational modes of water and understanding of torque have stuck with me. These three excellent teachers by demonstrating their point with a very visual story (and being willing to look a bit foolish) taught me something that I still remember. If you ever want something to stay with people make sure you thread it into a story they can visualise."
  },
  {
    "objectID": "topology-of-division-rings/index.html",
    "href": "topology-of-division-rings/index.html",
    "title": "Geometry and topology of division rings",
    "section": "",
    "text": "Given two distinct points there is a unique line that both points lie on\nEach line has at least three points which lie on it\nGiven a triangle any line that intersects two sides of the triangle intersects the third.\nAll points are spanned by d+1 points and no fewer.\n\nThen for d>=3 is equivalent to the projective space of lines over a division ring (or skew field).\nKolmogorov asked the question what projective spaces can we do analysis on? In order to do things such as find tangent lines we are going to need some sort of topology.\n\nKolmogorov apparently proved that for a (Desarguian) projective space if the set of points is compact and infinite, the set of lines is compact and the function mapping two distinct points to the line they lie on is continuous then the underlying division ring is infinite and locally compact (in a paper translated as “The Axiomatics of Projective Geometry” in Selected works of A. N . Kolmogorov edited by V. M. Tikhomirov). Such an object is called a continuous projective geometry.\nIn response Pontryagin proved (see his book “Topological Groups”) proved that every locally compact infinite division ring contains one of: the real numbers, the p-adic numbers, the power series over the integers modulo p (p prime). Moreover we can classify these by their connectedness and characteristic: if the division ring is connected it contains the real numbers, otherwise it is totally disconnected.\nCombining this with the Frobenius theorem we have the following: A locally compact connected field is isomorphic to the real numbers, the complex numbers or the quaternions.\nSeparation theorems allow us to define regions and boundaries of regions, so we can start to talk about ‘relative lengths’ and ‘relative areas’. One way to approach the separation theorems in projective geometry is via ordered fields: Veblen and Young pursue such an approach; of course this doesn’t apply to an unordered field such as the complex numbers. Another is via topology; e.g. a line separates the plane it lies in into two (topologically) connected sets.\nIn some sense all this indicates the “natural” projective spaces to do calculus in are precisely the projective spaces over the real numbers, complex numbers or quaternions (and maybe the octonions?).\nThe calculus of real and complex numbers is well known; is there a corresponding exterior differential calculus of quaternions? Given two n-simplices in an n-dimensional affine space, there is a unique affine transformation from one to the other. The ratio of their hypervolumes is the determinant of the linear transformation. Is there an analogous determinant for quaternions (or octonions)?\nEssentially no; Dieudonne extended the determinant to a non-commutative field by defining it as a map from matrices to the the division ring over its commutator subgroup (see Artin’s Geometric Algebra for details). This is about as good as you can do; any map from the general linear group on an n-dimensional (right) quaternion vector space to the quaternions that satisfies\n\ndet(AB) = det(A) det(B) (multiplicative)\ndet(A) = 0 iff A is not invertible (homomorphism)\nIf E is a matrix with 1s along the diagonal and exactly one other non-zero entry then det(E) = 1 (invariance under skew transforms)\n\nthen the image of the determinant is commutative.\nTo see an example of such an obstruction, consider the 2×2 quaternion matrices. Given a diagonal matrix \\(\\left(\\begin{array}{cc} a & 0 \\\\ 0 & b \\end{array}\\right)\\) would the determinant be ab or ba? For a commutative ring, a 2×2 matrix satisfies \\(A^2 - \\mathrm{tr}(A) A + \\mathrm{det}(A)I = 0\\) . A little experimentation shows there isn’t a similar formula for the quaternions (we can’t get rid of the off-diagonal elements). In fact taking the trace gives the formula for the determinant \\(\\mathrm{det}(A) = \\frac{\\mathrm{tr}(A)^2 - \\mathrm{tr}(A^2)}{2}\\) . If we try to apply this to a quaternion matrix we get \\(\\mathrm{det} \\left(\\begin{array}{cc} a & b \\\\ c & d \\end{array}\\right) = ad + da - bc -cb\\) . Notice that since ij=-ji, ik=-ki, jk=-kj this yields a real number. (The parallel actually extends into the spectral theory of quatenionic matrices)\nIn fact given any two distinct maps \\(\\mathrm{det} \\colon \\mathbb{H} \\to \\mathbb{R}_{\\geq 0}\\) satisfying axioms 1-3, one is a real power of the other. One way to construct such a determinant is to notice that quaternions can be represented by 2×2 complex matrices of the form \\(\\left(\\begin{array}{cc} a & b \\\\ -\\bar{b} & a \\end{array}\\right)\\) where a and b are complex numbers. We can then take the absolute value of the complex determinant (this is called the Study determinant, which is the square of the Dieudonne determinant). Alternatively we could repeat a similar expansion for complex numbers in terms of real numbers, giving a quaternion as a 4×4 real matrix. We then define the determinant of an nxn quaternion matrix, as the determinant of the corresponding 4nx4n real matrix; this is called the q-determinant and is the square of the Study determinant."
  },
  {
    "objectID": "powershell-debugging-write-warning/index.html",
    "href": "powershell-debugging-write-warning/index.html",
    "title": "Powershell Debugging with Write-Warning",
    "section": "",
    "text": "I first tried Write-Output but apparently it doesn’t work inside a function which I found misleading for a while (at first I thought that it wasn’t getting to the function). Write-Warning worked straight away and I could see in bright yellow what was going on.\nI traced the call with Write-Warning \"At this point\" statements and looked into variables with Write-Warning \"$Variable\" statements.\nOne interesting thing with Powershell is they can have CmdletBinding at the start of the function. Everything in to the end of the binding should be considered part of the function and you should only put your debugging statements beneath it (so starting at the ... below):\nfunction MyFunction\n{\n    [CmdletBinding()]\n    param (\n    [Parameter()][Type] $Parameter\n    )\n    \n    ...\nIn the end I found the issue was some JSON data was being string interpolated without being properly escaped. A quick search found ConvertTo-Json resolved the issue.\nIt’s sometimes amazing how far you can get with print statement debugging knowing generically how programming languages work (though it may be much trickier in a non strict language like Haskell!)."
  },
  {
    "objectID": "textrank/index.html",
    "href": "textrank/index.html",
    "title": "TextRank",
    "section": "",
    "text": "The first application is extracting keywords from 500 abstracts of papers from Inspec (scientific and technical papers). The original dataset was from Hulth’s 2003 paper Improved Automatic Keyword Extraction Given More Linguistic Knowledge, and the dataset is on the web (e.g. here, here and here). It consists of small abstracts and human annotated key phrases. Their TextRank approach is to:\n\nfilter the tokens of the abstract to just nouns and adjectives as the vertices\nform a co-occurance graph (unweighted, undirected) based on window of size 2\nusing PageRank to rank the vertices\nfiltering to the top 1/3 of vertices by rank\ncombining any adjacent words in the filtered vertices to get key phrases\n\nThis works quite well for this dataset, although there’s a lot of choices in here that I suspect are specific to the dataset and task. The algorithm looks for dense clusters of keywords, and frequent keywords, and this does much better than just frequent keywords alone. With all this tuning (which unfortunately sounds like it was done on the test set), they get something that works better in terms of precision and F1 than Hulth’s supervised approach.\nThe second application is text summarisation, based on the Document Understanding Conference 2002 task (which you have to request access for. It consists of news articles and manually generated abstracts. Their approach is to extract key sentences by forming a weighted graph of the sentences based on textual similarity between the sentences, rank the sentences using weighted PageRank, and limit the summary by length. The particular similarity they use is the number of tokens in common, divided by the sum of log lengths of the sentences (perhaps a cousin of Jaccard Distance).\n\\[{\\rm Similarity}(S_i, S_j) = \\frac{\\left \\vert S_i \\cap S_j \\right \\vert}{\\log(\\vert S_i \\vert) + \\log(\\vert S_j \\vert)}\\]\nIn terms of ROUGE-1 it ranks about middle of the systems submitted; which is pretty impressive given the simplicity. In the example it seems to work well because the subject is in the title line and repeated in the most relevant sentences.\nThese particular TextRank algorithms are available in software; in R there is textrank which covers both tasks, and in Python textacy has keyword extraction and gensim has textrank summarisation.\nThe real innovation here isn’t the particular approaches used, but the idea of using existing graph ranking algorithms for text extraction problems. There are a large and growing number of ways to find similarity between text units (for example sentence embeddings and contextual embeddings), other behavioural sources of graph information from interactions with documents, and a large number of graph algorithms. These could be combined in a multitude of ways to solve problems where the information to be extracted is likely to be threaded throughout a document. Being unsupervised, and able to work on any scale from characters all the way to entire books, makes it a versatile tool."
  },
  {
    "objectID": "placeholder-australia/index.html",
    "href": "placeholder-australia/index.html",
    "title": "Finding Australian Locations with Placeholder",
    "section": "",
    "text": "The best practices when using Placeholder are:\n\nGo from the most specific location information (e.g. street address) to the most general (e.g. country)\nSeparate different parts of location information with commas\nProvide as much context as possible, for example adding the country when known\nWhen the country is known filter the results using the lineage\nUse the place types relevant to the problem and region\n\n\nWho’s on First?\nI really only want to know roughly where the advertised jobs are located. This can be a very complicated question for jobs that involve multiple sites, or are primarily visiting clients, or fully remote jobs. However a rough location is a useful concept and jobs in Adelaide are different to jobs in Sydney.\nThe simplest solution for this is matching text from a gazetteer. For example you could get a list of the most populous cities and just search the location text for the city name. Unfortunately city names are ambiguous, like Melbourne in both Victoria, Australia and Florida, USA. And it’s common for city names to be in street addresses like “Sydney Road”. But over a limited geographical region, like Australia, it’s an effective technique for not much effort.\nThis is in fact what Placeholder does, but with better rules for matching names and resolving ambiguity, and a much larger gazetteer called Who’s on First. It’s a really interesting open dataset; it captures locations based on how people understand them and has a lot of information. There are a limited number of placetypes like country and region, and they have names in multiple languages and associated coordinates and bounding regions in GeoJSON. It’s continually evolving as more open datasets get integrated and our notions of places change, like recently with North Macedonia and South Sudan.\nThis adds real benefits because suburbs like “Macquarie Park” can be detected to be in Sydney, and it has an existing location hierarchy that can be used. The better rules mean it can distinguish an ambiguous place by “Mornington” if you also supply the state. Otherwise information can be used to better order the choices when it is ambiguous.\n\n\nPlaceholder in Practice\nUnderstanding Who’s on First is helpful for understanding how Placeholder behaves. For example in Australia a region roughly corresponds to a state or territory, a county is like a Local Government Area and a macrocounty is like a greater city. For example Ryde is a county, in the macrocounty of Sydney in the region New South Wales.\nHowever currently the concept of macrocounty only exists in Australia and Germany. This is because the types of geographies people use depend on political structures, and Who’s on First is a living asset. Eventually there may be macrocounties in other countries, but the type of division to use has to depend on the context.\n\n\n\nMacrocounties in Who’s on First\n\n\nIf I try to normalise “Sydney CBD, Australia” with placeholder I currently get the neighbourhood of Melbourne CBD. This is because “Sydney CBD” is not currently in Who’s on First, but “Melbourne CBD” is. Placeholder is pretty smart; adding more context like “Sydney CBD, NSW, Australia” gives the locality of Sydney.\nI haven’t looked deeply how Placeholder works works yet, but the more context you give it the better it behaves. In particular for English it seems to prefer the location to go from specific to general. For example the top result for “Australia, Newcastle” is in South Africa, but “Newcastle, Australia” is in Australia.\nTo get better results when you have more context you can filter the results to the context. For example you could just filter to places in Australia by filtering on it’s What’s on First id.\nWOF_AUS_ID = 85632793\nfor place in places:\n    for l in place['lineage']:\n        if ('country' in l and l['country']['id'] = 85632793):\n            return place\nFinally Placeholder seems to be sensitive to tokens used. It seems to like vertical struts | and parentheses () confuse Placeholder. This is probably down to the tokenisation logic.\n\n\nPutting it together\nDifferent websites have different conventions for storing location information. Rewriting the locations from specific to general, separated by commas with as much context as possible gets much beter results.\nThis requires writing slightly different rules for every website, but works very well in practice. If we wanted to improve the output we could look at better ways of filtering and ranking on relevant context, but for now it works well enough.\nWhen working in Python running the Placeholder server means there’s a bit of extra setup; but it looks like with some care (and some time) it looks straightforward to port the logic to Python if we needed to make it easier to setup."
  },
  {
    "objectID": "complex-analysis/index.html",
    "href": "complex-analysis/index.html",
    "title": "Complex Analysis",
    "section": "",
    "text": "Most of mathematical analysis, topology and measure theory is about inapplicable abtruse examples. This is to the extent that there’s a whole book on Counterexamples in Topology. Curiously in complex analysis all these strange examples like nowhere-differentiable functions are inadmissible, and leave well-behaved functions with useful techniques to handle them.\nComplex numbers start with adding solutions i and -i to the equation \\(x^2 = -1\\). It turns out that when you extend real (or algebraic) numbers with this then it is enough to factorise any polynomial into linear factors. This is called the Fundamental Theorem of Algebra. Understanding that a real polynomial is invariant under complex conjugation (that is interchanging i and -i), the non-real solutions must come in conjugate pairs. Consequently the real factors are linear, or complex quadratic when they are a conjugate complex pair.\nA function \\(f(z) = f(x+iy)\\) is holomorphic (complex differentiable) when \\(\\frac{df}{d\\overline{z}} = \\frac{1}{2} \\left( \\frac{df}{dx} - \\frac{1}{i} \\frac{df}{dy} \\right )\\) is zero, and the derivative is \\(\\frac{df}{d\\bar{z}} = \\frac{1}{2} \\left( \\frac{df}{dx} + \\frac{1}{i} \\frac{df}{dy} \\right )\\). This comes straight from evaluating an epsilon-delta definition along real and imaginary lines. It follows immediately that a holomorphic function is harmonic as a real valued function of two variables \\(\\frac{d^2f}{dzd\\overline{z}} = \\frac{d^2f}{dx^2} + \\frac{d^2f}{dy^2} = 0\\).\nA holomorphic function has a lot of the same regularity properties as harmonic functions. The maximum of the absolute value occurs on the boundary of a set, not in the interior. Moreover any holomorphic function is analytic: that is it can be locally expanded in a power series about a point \\(f(z) = \\sum_{n=0}^{\\infty} a_n (z - c)^n\\). So having one complex derivative means it’s infinitely differentiable and the derivatives at one point govern behaviour on the whole domain, in contrast to counterexamples in real analysis (and there are even weaker conditions). Even with real analytic functions like \\(\\frac{1}{1 + x^2}\\) complex analysis can shed light on why the radius of convergence of the power series about 0 is 1 (because the complex extension diverges at i and -i).\nExtending to ratios of holomorphic functions gives meromorphic functions which have Laurent Series \\(f(z) = \\sum_{n=-\\infty}^{\\infty} a_n (z - c)^n\\). In fact the Cauchy Integral Formula relates the Laurent series at a point with the values around any curve \\(\\gamma\\) surrounding that point via the contour integral:\n\\[a_n=\\frac{1}{2\\pi i} \\oint_{\\gamma}\\,\\frac{f(z)}{(z-c)^{n+1}}\\,dz\\]\nThis can be used to “analytically continue” a function by expanding the series beyond its radius of convergence. These lead to functions useful for applications like the Gamma Function which is a generalisation of factorial to complex numbers, the related Beta Function useful in statistics, and the Riemann zeta function which has applications in Quantum Field fluctuations between two plates in a vacuum.\nA lot of these ideas extend to higher complex dimensions, however I’m not sure about their utility. If a function is holomorphic with respect to each variable separately, it is holomorphic with respect to all of them. In particular variations of meromorphic functions and the Cauchy Integral Formula apply.\nFinally a brief word on biholomorphisms, holomorphic functions with a holomorphic inverse, which relate to the structure of complex domains. The Riemann Sphere is obtained by adding a point at infinity to the complex plane. The biholomorphisms are the Möbius transformation which can map any 3 points of the Riemann sphere to any other 3 points. In particular this implies that the biholomorphisms of the complex plane are just linear functions (since they are Möbius transformations that leave the point at infinity invariant); compare this with all the possible continuously differentiable bijections of the real plane.\nThe Riemann Mapping Theorem says that all non-empty, open, simply connected (i.e. no “holes”) subset of the plane have biholomorphisms, so they are equivalent from a complex perspective. In particular the unit ball has the two-dimensional family of biholomorphisms \\(T_a(z) = \\frac{z - a}{1 - \\overline{a} z} e^{2 \\pi i \\theta}\\). When there are holes there is additional structure; for example an annulus has a single holomorphic invariant related to the ratio of the outer to inner radius.\nThis nice automorphism structure breaks down in higher dimensions; in particular I believe the automorphisms of the 2-complex dimensional space are unknown. The “niceness” of the structure of the complex plane is likely related to its algebraic structure. I don’t know of many uses of these structural ideas, except in understanding 2-dimensional harmonic functions (e.g. in 2d fluid mechanics and electrostatics).\nComplex analysis is, strangely, more practical than real analysis. The tools like laurent Series, contour integrals and the Gamma Function pop up in surprisingly many applications."
  },
  {
    "objectID": "directionless/index.html",
    "href": "directionless/index.html",
    "title": "Finding a direction",
    "section": "",
    "text": "It takes time to set goals and priorities, but when I don’t take that time I end up spending much more time on busywork. In general before agreeing to do anything it’s worth reflecting if that gets towards your goals.\nChoosing a direction is hard; but choosing no direction means you don’t go anywhere. I’m in a time in my life where I’ve been very busy, but not productive, and need to carve out the time to think about what is important."
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html",
    "href": "fashion-mnist-with-prototype-methods/index.html",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "",
    "text": "Prototype methods classify objects by finding their proximity to a prototype in the feature space. These methods are flexible, and can be locally interpretable by looking at nearby examples.\nWe are going to look at examples of prototype methods on Fashion MNIST, a set of 28x28 pixel black and white images of different items of clothing. You could use any kind of features but these images are so simple that we can use the pixels themselves as features.\nThis notebook trains a classifier that gets about 85% accuracy on this dataset using K nearest neighbours, but on the way explores examining data, average prototypes, predicting accuracy with training data size, and approximate nearest neighbours methods.\nThis post was generated with a Jupyter notebook. You can also view this notebook on Kaggle or download the Jupyter notebook."
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html#data-format",
    "href": "fashion-mnist-with-prototype-methods/index.html#data-format",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "Data format",
    "text": "Data format\nWe can read out the label and image of any pixel\nidx = 0\n\nlabel = df.iloc[idx, 0]\nimage = df.iloc[idx, 1:].to_numpy().reshape(28, 28)\nprint(label)\nax = plt.imshow(image, cmap=\"Greys\")\n2\n\n\n\npng\n\n\nEach image is a series of digits from 0 to 255. Here’s the top left corner.\ndata = image[:12, :12]\ndata\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   4,   0,   0,   0,   0,   0,  62,  61],\n       [  0,   0,   0,   0,   0,   0,   0,  88, 201, 228, 225, 255],\n       [  0,   0,   0,   0,   0,  47, 252, 234, 238, 224, 215, 215],\n       [  0,   0,   1,   0,   0, 214, 222, 210, 213, 224, 225, 217],\n       [  1,   0,   0,   0, 128, 237, 207, 224, 224, 207, 216, 214],\n       [  0,   2,   0,   0, 237, 222, 215, 207, 210, 212, 213, 206],\n       [  0,   4,   0,  85, 228, 210, 218, 200, 211, 208, 203, 215],\n       [  0,   0,   0, 217, 224, 215, 206, 205, 204, 217, 230, 222],\n       [  1,   0,  21, 225, 212, 212, 203, 211, 225, 193, 139, 136]])\nEach number maps the corresponding pixel to a color; the larger the number the more black to use.\ndef show_data(data, figsize=(10,10), ax=None, formatter='{:}'.format):\n    assert data.ndim == 2\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    im = ax.imshow(data, cmap=\"Greys\")\n\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            text = ax.text(j, i, formatter(data[i, j].item()), ha=\"center\", va=\"center\", color=\"magenta\")\n    return ax\n\nax = show_data(data)\n\n\n\npng\n\n\nIt’s more convenient to use floating point numbers. Renormalise between 0 and 1 and convert it to a pytorch tensor.\nimage_tensor = image / 255.\n\ndata_tensor = image_tensor[:12, :12]\ndata_tensor\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.01568627,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.24313725, 0.23921569],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.34509804, 0.78823529, 0.89411765,\n        0.88235294, 1.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.18431373, 0.98823529, 0.91764706, 0.93333333, 0.87843137,\n        0.84313725, 0.84313725],\n       [0.        , 0.        , 0.00392157, 0.        , 0.        ,\n        0.83921569, 0.87058824, 0.82352941, 0.83529412, 0.87843137,\n        0.88235294, 0.85098039],\n       [0.00392157, 0.        , 0.        , 0.        , 0.50196078,\n        0.92941176, 0.81176471, 0.87843137, 0.87843137, 0.81176471,\n        0.84705882, 0.83921569],\n       [0.        , 0.00784314, 0.        , 0.        , 0.92941176,\n        0.87058824, 0.84313725, 0.81176471, 0.82352941, 0.83137255,\n        0.83529412, 0.80784314],\n       [0.        , 0.01568627, 0.        , 0.33333333, 0.89411765,\n        0.82352941, 0.85490196, 0.78431373, 0.82745098, 0.81568627,\n        0.79607843, 0.84313725],\n       [0.        , 0.        , 0.        , 0.85098039, 0.87843137,\n        0.84313725, 0.80784314, 0.80392157, 0.8       , 0.85098039,\n        0.90196078, 0.87058824],\n       [0.00392157, 0.        , 0.08235294, 0.88235294, 0.83137255,\n        0.83137255, 0.79607843, 0.82745098, 0.88235294, 0.75686275,\n        0.54509804, 0.53333333]])\nIt still looks the same but the numbers are scaled down\nax = show_data(data_tensor, formatter='{:0.3f}'.format)\n\n\n\npng\n\n\nHow does imshow know how dark to make the cells?\nBy default it makex the smallest value whitest and the largest black. From the documentation this can be controlled with vmin and vmax\n\nvmin, vmax: float, optional\n\n\nWhen using scalar data and no explicit norm, vmin and vmax define the data range that the colormap covers. By default, the colormap covers the complete value range of the supplied data. It is an error to use vmin/vmax when norm is given. When using RGB(A) data, parameters vmin/vmax are ignored.\n\nSo if we double vmax the image appears fainter.\nplt.imshow(image_tensor, cmap='Greys', vmin=0., vmax=2.)\n<matplotlib.image.AxesImage at 0x7f6761405cd0>\n\n\n\npng"
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html#convert-data",
    "href": "fashion-mnist-with-prototype-methods/index.html#convert-data",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "Convert data",
    "text": "Convert data\nLet’s put all the data into a large normalised array\nimages = df.filter(regex='^pixel[0-9]+$', axis=1).to_numpy().reshape((-1, 28, 28)) / 255.\n\nimages_test = df_test.filter(regex='^pixel[0-9]+$', axis=1).to_numpy().reshape((-1, 28, 28)) / 255.\nimages.shape\n(60000, 28, 28)\n_ = plt.imshow(images[0], cmap='Greys')\n\n\n\npng\n\n\nSo we understand the data let’s use human readable labels. This helps us understand how to classify the data and understand if we’ve made any mistakes. These are copied from the documentation for Fashion MNIST.\nlabels_txt = \"\"\"\nLabel   Description\n0   T-shirt/top\n1   Trouser\n2   Pullover\n3   Dress\n4   Coat\n5   Sandal\n6   Shirt\n7   Sneaker\n8   Bag\n9   Ankle boot\n\"\"\".strip()\nfrom io import StringIO\n\ndf_labels = pd.read_csv(StringIO(labels_txt), sep='\\t').set_index('Label')['Description']\nWe can then use this to convert the numeric labels to categories. So we can still access a numeric representation we use Pandas categorical dtype.\ncat_type = pd.CategoricalDtype(categories=df_labels)\n\nlabels = df['label'].map(df_labels).astype(cat_type)\nlabels_test = df_test['label'].map(df_labels).astype(cat_type)\n\nlabels\n0           Pullover\n1         Ankle boot\n2              Shirt\n3        T-shirt/top\n4              Dress\n            ...\n59995     Ankle boot\n59996        Trouser\n59997            Bag\n59998            Bag\n59999        Sneaker\nName: label, Length: 60000, dtype: category\nCategories (10, object): ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', ..., 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nLabels are same as cateogries\nassert (df['label'] == labels.cat.codes).all()"
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html#examining-data",
    "href": "fashion-mnist-with-prototype-methods/index.html#examining-data",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "Examining data",
    "text": "Examining data\nWe have 6000 of each image in labels\nlabels.value_counts()\nT-shirt/top    6000\nTrouser        6000\nPullover       6000\nDress          6000\nCoat           6000\nSandal         6000\nShirt          6000\nSneaker        6000\nBag            6000\nAnkle boot     6000\nName: label, dtype: int64\nThe test set contains 1000 of each image\nlabels_test.value_counts()\nT-shirt/top    1000\nTrouser        1000\nPullover       1000\nDress          1000\nCoat           1000\nSandal         1000\nShirt          1000\nSneaker        1000\nBag            1000\nAnkle boot     1000\nName: label, dtype: int64\nLet’s look at some example images from the data with their labels.\ndef show_images(data, labels=None, nrows=5, ncols=10, figsize=(16,8), indices=None):\n    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n\n    if indices is None:\n        indices=np.random.choice(len(data), size=nrows*ncols, replace=False)\n\n    for i, ax in enumerate(axs.ravel()):\n        idx = indices[i]\n        im = ax.imshow(data[idx], cmap=\"Greys\", norm=matplotlib.colors.Normalize(0., 1.))\n\n        if labels is not None:\n            ax.set_title(labels[idx], pad=0.0)\n\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    return fig, axs\n\n\nfig, ax = show_images(images, labels)\n\n\n\npng\n\n\nLooking through the categories we can see things like:\n\nImages are mostly centred, aligned, and cropped\nTrousers are distinctive from their shape\nShirts are particularly hard to distinguish from tshirt, jacket, pullover and dresshttps://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes\n\nfor label in labels.cat.categories:\n    mask = labels == label\n    show_images(images[mask], labels[mask].reset_index(drop=True))\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\nLet’s also have a look at the test data to make sure it’s from the same distribution.\nfor label in labels_test.cat.categories:\n    mask = labels_test == label\n    show_images(images_test[mask], labels_test[mask].reset_index(drop=True), nrows=1, ncols=10, figsize=(16,2))\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html#duplicate-detection",
    "href": "fashion-mnist-with-prototype-methods/index.html#duplicate-detection",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "Duplicate detection",
    "text": "Duplicate detection\nSurprisingly often many duplicates can end up in the training data, or worse leak between the training and test data. It’s always worth doing a check for duplicates.\nThere a small amount of duplicated in the training set. We could drop them but they’re not too concerning here.\nn_train_dup = df.filter(regex='pixel').duplicated().sum()\nn_train_dup\n43\nThere’s one duplicate in the test set\nn_test_dup = df_test.filter(regex='pixel').duplicated().sum()\nn_test_dup\n1\nWe also see there are 10 images in the test set from the training set. This amount of leakage is quite small so we won’t worry about it.\npd.concat([df, df_test], axis=0, ignore_index=True).filter(regex='pixel').duplicated().sum() - (n_train_dup + n_test_dup)\n10\nIf we were being more careful we would also look into near duplicates; there may be images that are practically identical but have slightly different pixel representations."
  },
  {
    "objectID": "fashion-mnist-with-prototype-methods/index.html#making-it-faster-with-approximate-nearest-neighbours",
    "href": "fashion-mnist-with-prototype-methods/index.html#making-it-faster-with-approximate-nearest-neighbours",
    "title": "Fashion MNIST with Prototype Methods",
    "section": "Making it faster with Approximate Nearest Neighbours",
    "text": "Making it faster with Approximate Nearest Neighbours\nInstead of brute force searching for the closest point we can use Approximate Nearest Neighbours to speed it up. There are many good libraries including faiss and hnswlib, but we’ll use annoy.\nIt uses Locality Sensitive Hashing; see Chapter 3 of Mining of Massive Datasets, by Leskovec, Rajaraman and Ullman for a good overview.\nfrom annoy import AnnoyIndex\nFirst we initialise an index and add all the vectors\n%%time\nt = AnnoyIndex(28*28, 'euclidean')\nfor i, v in enumerate(images_train):\n    t.add_item(i, v.flatten())\nCPU times: user 7.88 s, sys: 229 ms, total: 8.11 s\nWall time: 7.9 s\nThen we build the index, specifying the number of trees. 10 seems to work fine in this case.\nt.build(10)\nTrue\nWe then adapt our search to use the nearest neighbour from the index rather than brute forcing the search.\ndef ann_pred(v, k=1, search_k=-1):\n    n = t.get_nns_by_vector(v.flatten(), k, search_k=search_k, include_distances=False)\n    pred_labels = labels_train.loc[n]\n    pred_counts = Counter(pred_labels)\n    pred_label = pred_counts.most_common(n=1)[0][0]\n\n    return pred_label\nWe can then get results in seconds rather than minutes.\n%time preds = [ann_pred(i, k=5) for i in images_valid]\nCPU times: user 13.7 s, sys: 7.48 ms, total: 13.7 s\nWall time: 13.7 s\nAnd we get around 85% accuracy, matching the ~15% error rate we expected from an exact solution\nnp.mean(preds == labels_valid)\n0.84875\nWe’re getting much more even predictions and more understandable confusion between:\n\ntop/pullover/coat/shirt\nankle boot/sneaker/sandal\n\nThe worst performing is shirt, which was the hardest for me to identify\n_ = plot_confusion_matrix(labels_valid, preds)\n\n\n\npng\n\n\nLet’s grab an example where we the label was Shirt and we predicted T-shirt/top.\nIt looks like a t-shirt to me\nimg = images_valid[(np.array(preds) == 'T-shirt/top') & (labels_valid == 'Shirt')][0]\nplt.imshow(img, cmap='Greys')\n<matplotlib.image.AxesImage at 0x7f6730683a50>\n\n\n\npng\n\n\nAnd we can see the nearby items that led to the prediction.\nk = 5\n\nn = t.get_nns_by_vector(img.flatten(), k)\n\n_ = show_images(images_train, labels_train, nrows=1, ncols=k, indices=n)\n\n\n\npng\n\n\nAnother example is a pair of trousers the model guessed was a dress\nimg = images_valid[(np.array(preds) == 'Dress') & (labels_valid == 'Trouser')][0]\nplt.imshow(img, cmap='Greys')\n<matplotlib.image.AxesImage at 0x7f67302bb290>\n\n\n\npng\n\n\nn = t.get_nns_by_vector(img.flatten(), k)\n_ = show_images(images_train, labels_train, nrows=1, ncols=k, indices=n)\n\n\n\npng\n\n\nThere’s no obvious way to improve the model, so let’s do a hyperparameter search for the best k\n%%time\n\naccs = {}\nfor k in [1,5,10,20,100]:\n    preds = [ann_pred(i, k=k) for i in images_valid]\n    accs[k] = np.mean(preds == labels_valid)\nCPU times: user 1min 8s, sys: 10 ms, total: 1min 8s\nWall time: 1min 8s\nIt looks like 5 is marginally better\npd.Series(accs).sort_values(ascending=False).to_frame()\n\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n5\n\n\n0.848750\n\n\n\n\n10\n\n\n0.841750\n\n\n\n\n1\n\n\n0.841083\n\n\n\n\n20\n\n\n0.832833\n\n\n\n\n100\n\n\n0.806250\n\n\n\n\n\n\nNow we have our best prototype model lets evaluate on the test set, and we get an accuracy of around 85%. Comparing with the benchmarks using sklearn this is about what they get with KNN methods (and it looks like using Manhattan Distance, aka l^p with p=1, would do slightly better).\nGiven the very best methods get around 90% (or up to 94% with convolutional neural networks) this is quite good!\n%%time\npreds = [ann_pred(i, k=5) for i in images_test]\nnp.mean(preds == labels_test)\nCPU times: user 10.5 s, sys: 502 µs, total: 10.5 s\nWall time: 10.5 s\n\n\n\n\n\n0.849\nOf course we could have done this all in a few lines of sklearn. But by looking through understanding how it works and looking at the images we get a much better idea of why we get these results, and maybe ideas on how to improve it. (I expect the sklearn method below does better by standardizing the features before computing distances; that would be an interesting exercise.)\n%%time\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier()\nknn.fit(df.filter(like='pixel', axis=1), df['label'])\n\npreds = knn.predict(df_test.filter(like='pixel', axis=1))\naccuracy_score(df_test['label'], preds)\nCPU times: user 1min 9s, sys: 8.63 s, total: 1min 18s\nWall time: 30.9 s\n\n\n\n\n\n0.8589\nPrototype methods are remarkably simple, quick to train and flexible. Because the images here are so simple we get good results on the pixels directly. For more complex datatypes we could use derived features; in particular the activations from a pretrained neural network such as Resnet, BERT, or CLIP can work well.\nFor more on prototype methods read Chapter 13 of The Elements of Statistical Learning."
  },
  {
    "objectID": "presto-max_by/index.html",
    "href": "presto-max_by/index.html",
    "title": "Getting most recent value in Presto with max_by",
    "section": "",
    "text": "Suppose you have a table tracking user login activity over time like this:\n\n\n\ncountry\nuser_id\ntime\nstatus\n\n\n\n\nAU\n1\n2020-01-01 08:00\nlogged-in\n\n\nCN\n2\n2020-01-01 09:00\nlogged-in\n\n\nAU\n1\n2020-01-01 12:00\nlogged-out\n\n\nAU\n1\n2020-01-01 13:00\nlogged-in\n\n\nCN\n2\n2020-01-01 14:00\nlogged-out\n\n\n\nYou need to find out which users are currently logged in and out, which requires you to find their most recent status. In standard SQL you can do this with a window function by adding a row_number to find the most recent time:\nSELECT user_id, status\nFROM (\n    SELECT\n      user_id,\n      status,\n      row_number() OVER (PARTITION BY user_id ORDER BY time DESC) AS rn\n    FROM user_activity\n)\nWHERE rn = 1\nWith Presto’s max_by function you can do this in a single query:\nSELECT user_id, max_by(status, time) AS status\nFROM user_activity\nGROUP BY user_id\nThere is one downside to this approach: if you also try to select another column like max_by(country, time) there’s a chance if there are two rows with the same time we will get the most recent status and country from different rows which could have consistency problems.\nAn extension is you may want to also get their previous status. In standard SQL you could use a window function:\nSELECT user_id, status, last_status\nFROM (\n    SELECT\n      user_id,\n      status,\n      lag(status) OVER (PARTITION BY user_id ORDER BY time DESC) AS last_status,\n      row_number() OVER (PARTITION BY user_id ORDER BY time DESC) AS rn\nFROM user_activity\n)\nWHERE rn = 1\nIn Presto you can pass an additional arugment to max_by on how many values to return in an array.\nSELECT user_id, max_by(status, time, 2) AS last_2_statuses_array\nFROM user_activity\nGROUP BY user_id\nOne more trick Presto has is count_if which removes case statements from aggregation. For example if we wanted the number of logged in and logged out users by country in a pivoted view for standard SQL we could write\nSELECT\n  country,\n  count(CASE WHEN status = 'logged-in' THEN 1 end) AS logged_in_users,\n  count(CASE WHEN status = 'logged-out' THEN 1 end) AS logged_out_users\nFROM (\n    SELECT\n      user_id,\n      country,\n      status,\n      row_number() OVER (PARTITION BY user_id ORDER BY time DESC) AS rn\n    FROM user_activity\n)\nWHERE rn = 1\nGROUP BY country\nORDER BY count(CASE WHEN status = 'logged-in' THEN 1 end) DESC\nBut in Presto with count_if it could be:\nSELECT country,\n       count_if(status = 'logged-in') AS logged_in_users,\n       count_if(status = 'logged-out') AS logged_out_users\nFROM (\n    SELECT\n      user_id,\n      max_by(country, time) AS country,\n      max_by(status, time) AS status\n    FROM user_activity\n    GROUP BY user_id\n)\nGROUP BY 1\nORDER BY 2 DESC\nWhen you’re writing a lot of complex queries even small simplifications add up. It would be great to see these kinds of functions in other databases some day."
  },
  {
    "objectID": "sum-random-variables-convolution/index.html",
    "href": "sum-random-variables-convolution/index.html",
    "title": "How to Sum Random Variables",
    "section": "",
    "text": "This is the general procedure to sum two independent random variables. You’ve got two random variables X and Y; in the example above the dice. They each have some probability distribution function \\(f_X\\) and \\(f_Y\\) which encode how probable any particular outcome is. For the dice the probability distribution function is just \\(f_X(x) = \\frac{1}{6},\\, x \\in \\{1,2,3,4,5,6\\}\\) (that is the probability of any number between 1 and 6 is one-sixth). Then the probability distribution of their sum is then \\(f_{X+Y}(x) = \\sum_{y+z=x} f_X(y) f_Y(z)\\); that is for each pair of outcomes that sum up to x we multiply the probability of them occurring. In our dice example we calculated \\(f_{X+Y}(4) = \\frac{1}{12}\\); we could repeat the calcuation for all numbers between 2 and 12 to get the full probability distribution function.\nThe sum above can be rewritten in a slightly different form using a change of variables; \\(f_{X+Y}(x) = \\sum_{y} f_X(y) f_Y (x - y) =: (f_X * f_Y)(x)\\). This last expression is just noting this is the definition of a convolution. A shorthand way of writing this is \\(f_{X+Y} = f_X * f_Y\\) (note that this is convolution and not multiplication!).\nThe convolution theorem says that the Fourier Transform maps convolutions into products. So in particular we can rewrite \\(f_{X+Y} = \\mathcal{F}^{-1} (\\mathcal{F}(f_X) \\mathcal{F}(f_Y))\\) This can be computationally convenient; a fast fourier transform can be more efficient than performing the sum in the convolution manually. To see this in action we can encode our dice example in R; we represent our probability density function as a vector which are the values of the function at 0, 1, 2, …\n# Vectors are 0 indexed, so probability of 0 as 0\n# Probability at 1-6 as 1/6\n# We need extra space at the end for higher values so we pad the end with 0s\nx <- c(0, seq(1/6, 6), seq(0, 10))\ny <- x\n\n# Convoluiton using the fourier transforms and their inverse\nz <- fft(fft(x) * fft(y), inverse=True)\n# Remove the imaginary part (which should be 0)\nz <- abs(z)\n# Renormalise the probability to sum to 1\n# (The fft introduces a constant multiplicative factor)\nz <- z / sum(z)\n\n# The result is what we would get from the convolution\n# For example the value at 4 (the 5th element in the vector) is 3\nprint(round(z*36, 0.01))\n# [1] 0 0 1 2 3 4 5 6 5 4 3 2 1 0 0 0 0\nThese relations are also useful for theoretical calculations. For example the PDF of a standard normal distribution, \\(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\) is a fixpoint of the Fourier transform. This with the scaling and shift relations make it easy to calculate the sum of normally distributed variables. In particular if \\(X \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)\\) and \\(Y \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)\\) then \\(X + Y \\sim \\mathcal{N}(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2)\\)."
  },
  {
    "objectID": "html-to-text/index.html",
    "href": "html-to-text/index.html",
    "title": "Converting HTML to Text",
    "section": "",
    "text": "The standard answers on Stack Overflow are to use Beautiful Soup’s getText method. Unfortunately this just turns every tag into the argument, whether it is block level or inline. This means for a lot of compact HTML it changes the meaning.\nThe pragmatic answer I’ve ended up with is to convert the text to Markdown with html2text, parse the Markdown back into HTML, and then converting that HTML to text. This is ridiculously inefficient, but lets me offload the processing logic to external tools and does a good enough job.\nThe final solution looks like this:\ndef html2plain(html):\n    md = html2md(html)\n    md = normalise_markdown_lists(md)\n    html_simple = mistletoe.markdown(md)\n    text = BeautifulSoup(html_simple).getText()\n    text = fixup_markdown_formatting(text)\n    return text\n\nThe problem\nFor example the following HTML document:\n<b>Section</b><br />A list<ul><li>Item <b>1</b></li>\nWould be converted to something where we lose all sentence and section structure:\nSection A list Item 1\nWe can convert the tags into newlines with BeautifulSoup but that will break across inline tags:\nSection\nA list\nItem\n1\nThe best option is to write your own HTML parser, but that’s hard because you have to decide what to do with every case and deal with the complexities of real HTML. Another way is to first convert it to Markdown with html2text. Then we would get something we may be able to parse:\nSection\n\nA list\n\n* Item *1*\nWe can then convert that back into simple HTML or plain text.\nfrom bs4 import BeautifulSoup\nfrom mistletoe import markdown\nfrom html2text import HTML2Text\n\nmd = HTML2Text().handle(html)\nhtml2 = markdown(md)\ntext = BeautifulSoup(html2).getText()\n\n\nConverting the HTML to Markdown\nThe html2text library does a good job of converting HTML to markdown. We need to give it a little configuration to get the output we want. In particular to turn off line wrapping we need to set the body_width to 0. I also ignore anchors and images since they are rare and I have no way of dealing with them.\ndef html2md(html):\n    parser = HTML2Text()\n    parser.ignore_images = True\n    parser.ignore_anchors = True\n    parser.body_width = 0\n    md = parser.handle(html)\n    return md\n\n\nNormalising Lists\nHTML has a standard way of creating lists; <ul> and <li> tags. However surprisingly often I find custom lists with formats like List<br />- Item 1. We can convert these kinds of lists to look the same as a Markdown list with a little bit of regex:\ndef normalise_markdown_lists(md):\n    return re.sub(r'(^|\\n) ? ? ?\\\\?[·--*]( \\w)', r'\\1  *\\2', md)\n\n\nConverting the Markdown back to Text\nThere are a bunch of Markdown parsers, but mistletoe seems to be a good one. The main benefit of going through Markdown is irrelevant tags are stripped off, and the mistletoe HTML output is consistently formatted. In particular there are line breaks around block level formats, which may not be true for the source HTML.\nhtml_simple = mistletoe.markdown(md)\ntext = BeautifulSoup(html_simple).getText()\n\n\nCleaning up processing errors\nAs nice as html2text is, it has issues with multiple kinds of emphasis and repeated emphasis. For the repeated emphasis I remove any left over double stars. Sometimes tables seem to leave an extra vertical strut, |. I also clean up final whitespace.\ndef fixup_markdown_formatting(text):\n    # Strip off table formatting\n    text = re.sub(r'(^|\\n)\\|\\s*', r'\\1', text)\n    # Strip off extra emphasis\n    text = re.sub(r'\\*\\*', '', text)\n    # Remove trailing whitespace and leading newlines\n    text = re.sub(r' *$', '', text)\n    text = re.sub(r'\\n\\n+', r'\\n\\n', text)\n    text = re.sub(r'^\\n+', '', text)\n    return text\n\n\nTesting it out\nThis pipeline was actually developed by trialing it on some example job ads. The next step would be to create some formal tests based on these examples, but I’m happy to start with this until there are enough issues to improve it."
  },
  {
    "objectID": "value-of-gold/index.html",
    "href": "value-of-gold/index.html",
    "title": "Value of Gold",
    "section": "",
    "text": "How much is one cubic centimetre of gold worth?"
  },
  {
    "objectID": "value-of-gold/index.html#volume-of-gold",
    "href": "value-of-gold/index.html#volume-of-gold",
    "title": "Value of Gold",
    "section": "Volume of Gold",
    "text": "Volume of Gold\nFirst we estimate the area of the annulus, and then the height to get the ring volume. Finally we estimate the purity of the ring to get the volume of gold.\n\n\n\n\ngraph BT;\n   A[Volume of Gold in Ring]\n   D[Area of Ring] --> F\n   E[Height of Ring] --> F\n   F[Volume of Ring] --> A\n   G[% of Ring that is Gold] --> A\n\n\n\n\n\n\n\n\nThe area of an annulus with inner radius r and outer radius R is \\(\\pi (R^2 - r^2)\\). My ring finger is about 1cm across, so has a radius of about 5mm. A ring is typically around 2mm thick. So the area of the annulus, looking at the ring top down, is \\(\\pi \\left((5+2 \\rm{mm})^2 - (5 \\rm{mm})^2\\right) \\approx 3 \\times (49 - 25)\\rm{mm}^2\\), so approximately 75 mm2.\nThe height of a ring is roughly twice it’s thickness; call it 4mm. So the volume of the ring is about 300 mm3.\nNow a 14 carat gold is 14/24 gold by weight, the rest being other cheaper harder metals. Let’s assume that the metals are similar density to gold, so that by volume of gold is about 14/24, say roughly half the volume of the ring. So the volume of gold in the ring is about 150mm3, which is 0.15 cm3.\nPutting this calculation into a graph:\n\n\n\n\ngraph BT;\n   A[Volume of Gold in Ring <br>0.15 cm<sup>3</sup>]\n   D[Area of Ring <br> 0.75 cm<sup>2</sup>] --> F\n   J[Inner Area of Ring <br> 0.75cm<sup>2</sup>] -. -1 .-> D\n   K[Outer Area of Ring <br> 1.5cm<sup>2</sup>] -.-> D\n   L[Radius of finger <br> 0.5 cm] -->|2| J\n   M[Radius of finger and ring <br> 0.7cm] -->|2| K\n   N[π] --> J\n   O[π] --> K\n   E[Height of Ring <br> 0.4 cm] --> F\n   F[Volume of Ring <br> 0.3 cm<sup>3</sup>] --> A\n   G[% of Ring that is Gold <br> 50%] --> A"
  },
  {
    "objectID": "value-of-gold/index.html#estimating-value-of-gold",
    "href": "value-of-gold/index.html#estimating-value-of-gold",
    "title": "Value of Gold",
    "section": "Estimating Value of Gold",
    "text": "Estimating Value of Gold\nWe know the 14 carat gold ring costs around $300. I think most of the cost of the cheaper rings is going to be the gold itself; the material filler is quite cheap, but there’s some payment for craftsmanship and margins. Let’s assume 80% of the total cost is gold. Then the cost of gold is 80% of $300, which is about $250.\n\n\n\n\ngraph BT;\n   C[Cost of Ring <br> $300] --> I[Cost of Gold <br> $250]\n   H[% of Ring Cost is Gold <br> 80%] --> I"
  },
  {
    "objectID": "value-of-gold/index.html#putting-it-together",
    "href": "value-of-gold/index.html#putting-it-together",
    "title": "Value of Gold",
    "section": "Putting it together",
    "text": "Putting it together\nCombining our two estimates gives $250 for 0.15 cm3 of gold, so about $2000 per cubic centimetre.\n\n\n\n\ngraph BT;\n   A[Volume of Gold in Ring <br>0.15 cm<sup>3</sup>] -->|-1| B[Cost/Volume of Gold <br> $2000 cm<sup>-3</sup>]\n   I[Cost of Gold in Ring <br> $250] --> B"
  },
  {
    "objectID": "disk-usage/index.html",
    "href": "disk-usage/index.html",
    "title": "Disk Usage in Linux with du",
    "section": "",
    "text": "I’ll normally start by running df to see what space is used and available. It’s worth looking at the Mounted On column if you don’t administer the machine because sometimes there are special partitions for large files.\n\n\n\nExample of df\n\n\nThen I’ll navigate to the mounted directory and run:\ndu -h -d2 . | sort -hr | head -n20\n\n\n\nExample of du\n\n\nThe first command du -h -d2 . means get the size of all files and directories (du) for the current directory ., print up them to a maximum depth of 2 (-d2) in a human readable format -h (e.g. 2.1G instead of 2646 which is the number of 1 MB blocks). We then sort the human readable (-h) results in reverse (-r) order. Finally we take the top 20 lines with head. Check out explainshell for great breakdowns of commands like this.\nThis will then show the 20 largest files/directories up to 2 levels under the current directory which I can then assess for clean up. This sometimes takes a while to produce any output because it has to go through all the files before sort can work.\nBe careful if you ever run this in the root directory / to exclude things like /proc or you’ll get a lot of weird errors.\nNext time your hard drive is filling up try out du; it’s available on Linux and Mac and even works in Windows under WSL."
  },
  {
    "objectID": "sparql-job-country/index.html",
    "href": "sparql-job-country/index.html",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "",
    "text": "I am trying to extract Australian Job Postings from Web Data Commons which extracts structured data from Common Crawl. I have previously written scripts to read in the graphs, explore JobPosting schema and analyst the schema using SPARQL. Now we can use these to find some Austrlian Job Postings in the data.\nFor this analysis I used 15,000 pages containing job postings with different domains from the 2019 Web Data Commons Extract. Here’s the final query that extracts 285 domains; the rest of this article will explain what it’s doing."
  },
  {
    "objectID": "sparql-job-country/index.html#extracting-country-name-with-property-paths",
    "href": "sparql-job-country/index.html#extracting-country-name-with-property-paths",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Extracting Country Name with property paths",
    "text": "Extracting Country Name with property paths\nWe can express this succinctly using SPARQL property paths.\nPREFIX sdo: <http://schema.org/>\n\nSELECT ?country (COUNT(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:jobLocation/sdo:address/sdo:addressCountry/sdo:name ?country .}\n}\nGROUP BY ?country\nORDER BY DESC(?count)\nLIMIT 15\n\n\n\nCountry\ncount\n\n\n\n\nUS\n127\n\n\nCA\n20\n\n\nDE\n20\n\n\nGB\n18\n\n\nIL\n14\n\n\n\nNote that we count distinct graph identifiers; because a page can contain multiple job listings (which in turn can contain multiple jobLocations) it may contribute to multiple countries."
  },
  {
    "objectID": "sparql-job-country/index.html#extracting-plain-text-addresscountry",
    "href": "sparql-job-country/index.html#extracting-plain-text-addresscountry",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Extracting plain text addressCountry",
    "text": "Extracting plain text addressCountry\nThe addressCountry can also be plain text, and in fact that’s much more common. We can filter out the cases where it’s a structured value (and so ?country is a blank node) using isLiteral.\nPREFIX sdo: <http://schema.org/>\n\nSELECT ?country (COUNT(distinct ?src) AS ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:jobLocation/sdo:address/sdo:addressCountry ?country .}\n    FILTER (isLiteral(?country))\n}\nGROUP BY ?country\nORDER BY DESC(?count)\nLIMIT 10\nIn the results we can see 86 jobs with AU in the countries.\n\n\n\nCountry\nCount\n\n\n\n\nUnited States\n385\n\n\nJP\n358\n\n\nGB\n345\n\n\nUS\n320\n\n\nDE\n270\n\n\nNL\n253\n\n\nDeutschland\n179\n\n\nUnited Kingdom\n139\n\n\nFR\n110\n\n\nAU\n86"
  },
  {
    "objectID": "sparql-job-country/index.html#matching-country-and-text-at-the-same-time",
    "href": "sparql-job-country/index.html#matching-country-and-text-at-the-same-time",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Matching Country and text at the same time",
    "text": "Matching Country and text at the same time\nWe should be able to combine the two by making name optional with the ZeroOrOnePath operator ?.\nPREFIX sdo: <http://schema.org/>\n\nSELECT ?country (count(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:jobLocation/sdo:address/sdo:addressCountry/(sdo:name?) ?country .}\n}\nGROUP BY ?country\nORDER BY DESC(?total)\nLIMIT 15\nHowever for some strange reason we end up with some URIs in the results:\n\n\n\nCountry\nCount\n\n\n\n\nUS\n447\n\n\nUnited States\n385\n\n\nGB\n363\n\n\nJP\n359\n\n\nDE\n290\n\n\nNL\n257\n\n\nDeutschland\n179\n\n\nUnited Kingdom\n140\n\n\nFR\n116\n\n\nAU\n91\n\n\nCA\n81\n\n\nIndia\n60\n\n\n\n60\n\n\nhttp://schema.org/JobPosting\n56\n\n\nhttp://schema.org/Place\n56\n\n\n\nOddly enough this doesn’t happen if we rewrite it as an alternation:\nPREFIX sdo: <http://schema.org/>\n\nSELECT ?country (count(?src) as ?total)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:jobLocation/sdo:address/(sdo:addressCountry|sdo:addressCountry/sdo:name) ?country .}\n}\nGROUP BY ?country\nORDER BY DESC(?total)\nLIMIT 15\n\n\n\nCountry\nTotal\n\n\n\n\nUS\n609\n\n\nJP\n444\n\n\nUnited States\n395\n\n\nGB\n363\n\n\nDE\n304\n\n\nNL\n258\n\n\nDeutschland\n179\n\n\nUnited Kingdom\n140\n\n\nFR\n116\n\n\nAU\n91\n\n\nCA\n84\n\n\nIndia\n70\n\n\nCanada\n66\n\n\n\n60\n\n\nIN\n57\n\n\n\nI would expect these to be the same; but I don’t know if my understanding of SPARQL is wrong or it’s a bug in rdflib. When we filter to literal nodes we get the same results, so I’m not going to dwell on it."
  },
  {
    "objectID": "sparql-job-country/index.html#fully-qualified-paths",
    "href": "sparql-job-country/index.html#fully-qualified-paths",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Fully qualified paths",
    "text": "Fully qualified paths\nIn the microdata extract the properties are specified by fully qualified paths, for example <http://schema.org/Place/address> instead of just <http://schema.org/address>. So we need to match these patterns too, which means adding a whole heap more prefixes.\nWe can check the property it’s binding on, but have to be careful to filter out common strings to reduce false positives (e.g. if ?country is the empty string then this will extract all properties with an empty string).\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_pl: <http://schema.org/Place/>\nPREFIX sdo_pa: <http://schema.org/PostalAddress/>\nPREFIX sdo_co: <http://schema.org/Country/>\n\nSELECT ?property (count(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n         sdo_jp:jobLocation/sdo_pl:address/sdo_pa:addressCountry/(sdo_co:name?) ?country .\n         [] ?property ?country .\n         FILTER (isliteral(?country) &&\n                (lcase(str(?country)) not in ('', 'na', 'n/a', 'unavailable', ' ', 'null')))\n         }\n}\nGROUP BY ?property\nORDER BY DESC(?count)\nLIMIT 10\nHaving a Country is very rare in microdata, but this looks about right.\n\n\n\nProperty\nCount\n\n\n\n\nhttp://schema.org/PostalAddress/addressCountry\n1351\n\n\nhttp://schema.org/Country/name\n4\n\n\nhttp://schema.org/PostalAddress/addressLocality\n3\n\n\nhttp://schema.org/PostalAddress/streetAddress\n1\n\n\nhttp://schema.org/PostalAddress/addressRegion\n1"
  },
  {
    "objectID": "sparql-job-country/index.html#combining-the-patterns",
    "href": "sparql-job-country/index.html#combining-the-patterns",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Combining the patterns",
    "text": "Combining the patterns\nWe can combine the two possible schema paths using alternations.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_pl: <http://schema.org/Place/>\nPREFIX sdo_pa: <http://schema.org/PostalAddress/>\nPREFIX sdo_co: <http://schema.org/Country/>\n\nSELECT ?country (count(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n         (sdo:jobLocation|sdo_jp:jobLocation)/\n         (sdo:address|sdo_pl:address)/\n         (sdo:addressCountry|sdo_pa:addressCountry)/\n         ((sdo:name|sdo_co:name)?) ?country .\n         FILTER (isliteral(?country))\n         }\n}\nGROUP BY ?country\nORDER BY DESC(?count)\nLIMIT 10\nUnfortunately sometimes the country has a language tag and this means the results are treated differently.\n\n\n\nCountry\nCount\n\n\n\n\nUnited States (Lang=EN)\n469\n\n\nUS\n463\n\n\nUnited States\n393\n\n\nGB\n365\n\n\nJP\n359\n\n\nDE\n295\n\n\nRU\n283\n\n\nNL\n257\n\n\nDeutschland\n186\n\n\nUnited Kingdom\n142\n\n\n\nWe can strip away the language tags by converting it to a plain string with str. Furthermore we can remove any leading/trailing whitespace with a regular expression.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_pl: <http://schema.org/Place/>\nPREFIX sdo_pa: <http://schema.org/PostalAddress/>\nPREFIX sdo_co: <http://schema.org/Country/>\n\nSELECT ?countryplain (count(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n         (sdo:jobLocation|sdo_jp:jobLocation)/\n         (sdo:address|sdo_pl:address)/\n         (sdo:addressCountry|sdo_pa:addressCountry)/\n         ((sdo:name|sdo_co:name)?) ?country .\n         FILTER (isliteral(?country))\n         BIND (replace(str(?country), '[ \\n\\t]*(.*)[ \\n\\t]*', '\\\\1') as ?countryplain)\n         }\n}\nGROUP BY ?countryplain\nHAVING (COUNT(distinct ?src) >= 50)\nORDER BY DESC(?count)\nThere’s still some normalisation to do; United States, US and USA are all the same as are DE, Deutschland and Germany.\n\n\n\nCountry\nCount\n\n\n\n\nUnited States\n863\n\n\nUS\n496\n\n\nGB\n381\n\n\nJP\n362\n\n\nDE\n355\n\n\nRU\n287\n\n\nNL\n264\n\n\nDeutschland\n192\n\n\nUnited Kingdom\n175\n\n\nFR\n128\n\n\nAU\n96\n\n\nCA\n88\n\n\nIndia\n65\n\n\nCanada\n61\n\n\n\n60\n\n\nIN\n59\n\n\nGermany\n50\n\n\nUSA\n50\n\n\n\nWe find jobs located in Australia looking for the country being ‘AU’ or ‘Australia’ in some case, after trimming whitespace.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_pl: <http://schema.org/Place/>\nPREFIX sdo_pa: <http://schema.org/PostalAddress/>\nPREFIX sdo_co: <http://schema.org/Country/>\n\nSELECT DISTINCT ?src\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n         (sdo:jobLocation|sdo_jp:jobLocation)/\n         (sdo:address|sdo_pl:address)/\n         (sdo:addressCountry|sdo_pa:addressCountry)/\n         ((sdo:name|sdo_co:name)?) ?country .\n         FILTER (isliteral(?country) &&\n                 lcase(replace(str(?country),\n                       '[ \\n\\t]*(.*)[ \\n\\t]*', '\\\\1'))\n                 in ('au', 'australia'))\n         }\n}\nThis gets 124 URLs, 40 of which don’t end in .au. This includes some New Zealand job sites, some global companies, some talent platforms with company subdomains for Australian companies (breezy.hr, gosnaphot and recruitee and jobsindevenport.com which is a site dedicated to jobs in the city of Devonport in Tasmania. The majority of these look like Australian job ads.\nNote that this means that around half of the jobs in a .au domain don’t have Australia as a country. I’m willing to guess this is because the metadata is incomplete; they probably don’t have an addressCountry property at all.\nAnother place to look for a location would be applicantLocationRequirements which is used for remote jobs, but isn’t used much in practice and so doesn’t seem worth investigating."
  },
  {
    "objectID": "sparql-job-country/index.html#combining-all-the-currency-variants",
    "href": "sparql-job-country/index.html#combining-all-the-currency-variants",
    "title": "Extracting Australian Job Postings with SPARQL",
    "section": "Combining all the currency variants",
    "text": "Combining all the currency variants\nAs before we add the fully qualified schemas to get every possible variation. We also add <https://schema.org/MonetaryValue/> because this occurs a few times in practice. In fact the https://schema.org should be equivalent so I should check it everywhere doubling the number of variants. It doesn’t occur much in this dataset, so I mostly ignore it here, but it might become a bigger issue in future.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_mv: <http://schema.org/MonetaryValue/>\nPREFIX sdos_mv: <https://schema.org/MonetaryValue/>\n\n\nSELECT ?curr (COUNT(distinct ?src) as ?count)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n        ((sdo:salaryCurrency|sdo_jp:salaryCurrency)|\n         (sdo:baseSalary|sdo_jp:baseSalary)/\n          (sdo:currency|sdo_mv:currency|sdos_mv:currency)) ?currency .\n    }\n    BIND (replace(str(?currency), '[ \\n\\t]+', '') as ?curr)\n    FILTER (!(lcase(?curr) in ('', 'null', 'na', 'n/a', 'unavailable')))\n}\nGROUP BY ?curr\nORDER BY DESC(?count)\nLIMIT 20\nThe resulting data is pretty good; we could further normalise € as EUR and £ as GBP, but the currencies otherwise look like ISO 4217 currency codes.\n\n\n\nCurrency\nCount\n\n\n\n\nGBP\n392\n\n\nUSD\n302\n\n\nEUR\n295\n\n\nJPY\n266\n\n\nAUD\n114\n\n\nINR\n108\n\n\nJPN\n93\n\n\n€\n68\n\n\nCZK\n57\n\n\nRUB\n50\n\n\nRUR\n49\n\n\nAFA\n23\n\n\nCAD\n19\n\n\nVND\n18\n\n\nHKD\n14\n\n\n£\n14\n\n\nBRL\n12\n\n\nSEK\n11\n\n\nPKR\n10\n\n\nTHB\n10\n\n\n\nFinally we can filter down to the 114 Job ads offering salary in AUD:\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_mv: <http://schema.org/MonetaryValue/>\nPREFIX sdos_mv: <https://schema.org/MonetaryValue/>\n\nSELECT distinct ?src\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting ;\n        ((sdo:salaryCurrency|sdo_jp:salaryCurrency)|\n         (sdo:baseSalary|sdo_jp:baseSalary)/(sdo:currency|sdo_mv:currency|sdos_mv:currency)) ?currency .\n    }\n    BIND (replace(str(?currency), '[ \\n\\t]+', '') as ?curr)\n    FILTER (lcase(?curr) = 'aud')\n}\nThis gives 114 jobs, of which 18 don’t have a .au domain and 12 of those don’t have Australia as a country. Most of these jobs are valid Australian jobs, but for some reason there are a few New Zealand jobs (which should be in NZD)."
  },
  {
    "objectID": "history-of-integration/index.html",
    "href": "history-of-integration/index.html",
    "title": "Some history of integration",
    "section": "",
    "text": "The modern concept of a limit, central to how we understand analysis today, was not formulated until (arguably) 1821 by Cauchy, despite calculus being invented in the late 17th century and limiting approaches extending back into Greek mathematics. A major reason for the time it took for the rigorous foundations of analysis to develop is because they were not really necessary – most manipulations in mathematics and science involved “well-behaved” analytic functions. (This was well before the notion of a set was in vogue, so a function was generally considered to be a “formula” or a geometric concept, not an arbitrary mapping of one set into another). However there were exceptions to this; for instance trigonometric series, used by Bernoulli to solve a vibrating string problem in 1753 and by Fourier in 1821 to solve a heat equation.\nTrigonometric (or Fourier) series were useful for solving a wide range of physical problems, and at the same time challenged mathematical intuition, for instance Fourier found \\(\\frac{4}{\\pi}\\sum_{n=1}^{\\infty} \\frac{(-1)^{n-1}}{2n-1} \\cos(\\frac{(2n-1)\\pi x}{2})\\) converged to a square wave – a discontinuous function.\nIn investigating these sorts of pathological functions rigorous notions of continuity, differentiability, uniform convergence and integrability arose. In particular Riemann’s definition of an integral (I can’t find his original definition, so this is a modern version)\nA partition of an interval \\([a,b]\\) is a sequence of points \\(a=a_0 < a_1 < \\ldots < a_n=b\\) . The mesh of such a partition is the maximum of \\(a_{i}-a_{i-1}\\) for \\(i=1,\\ldots,n\\) . A tagging of such a partition is points \\(\\{t_1,\\ldots,t_n\\}\\) satisfying \\(a_{i-1} \\leq t_i \\leq a_i\\) for \\(i=1,\\ldots,n\\) .\nA function \\(f\\) on \\([a,b]\\) is integrable with integral \\(A\\) if and only if for every positive quantity \\(\\epsilon\\) there is a positive quantity \\(\\delta\\) such that for any tagged partition \\(\\{a_0,\\ldots,a_n\\},\\{t_1,\\ldots,t_n\\}\\) with mesh less than \\(\\delta\\) , \\(\\left|A - \\sum_{j=1}^{n} f(t_j) (a_j - a_{j-1})\\right| < \\epsilon\\) .\nInformally the integral of a positive function is the area under its graph. The idea behind Riemann’s definition is to approximate the area by adding together rectangles, with the length given by a partition and height the value of the function at the tag. Riemann’s definition essentially says that if, as the width of these rectangles approaches zero, their sum approaches a constant number independent of the way we choose the rectangles, this must be the area under the graph.\nThere are ‘problems’ with Riemann’s definition though, one of which is that not every derivative is integrable. To demonstrate this we construct Volterra’s function.\nWe begin by constructing a Smith-Volterra-Cantor set (a.k.a a Fat Cantor set). Start (step 0) with the interval [0,1] and inductively at step n remove an open set of length \\(1/4^n\\) from each of the \\(2^{n-1}\\) connected subsets. The intersection of all these sets forms a nowhere-dense set. Its ‘outer measure’ is \\(1 - (1/4 + 2/16 + 4/64 + \\ldots) = 1/2\\) .\nI won’t explicitly detail the construction of the Volterra function. It uses the function \\(x^2\\sin(1/x)\\) which is differentiable, but the derivative is not continuous at 0. The Volterra function then uses this function to construct a function that is differentiable, but the derivative is not continuous on the Smith-Volterra-Cantor set and so (by Lebegue’s criterion for Riemann integrability) isn’t Riemann integrable.\nSo it’s worth looking for a better integral. Since integration corresponds to finding the area of a graph, one method is to try to assign a size to sets in the plane. But even assigning lengths to subsets of the line is difficult.\nPeano and Jordan assigned a size to sets using intervals (or in 2 dimensions, rectangles) using a method similar to the “proof by exhaustion” used by the Greeks. The idea behind proof by exhaustion is to prove the area of an object is A by proving it is not less than A and proving that it is not more than A. The inner content of a set is the supremum of the finite sums of lengths of intervals with non-intersecting interiors that are contained by the set. The outer content of a set is the infimum of the finite sums of lengths of intervals with non-intersecting interiors that contain the set. A set is Jordan measurable if its inner content is equal to its outer content and this value is called the Jordan measure.\nUnfortunately the Smith-Volterra-Cantor set is not Jordan measurable: it contains no intervals so its inner content is 0, but its outer content is 1/2.\nBorel took a different approach, by defining the lengths of countable disjoint unions of intervals to be the sums of the lengths of the intervals, and the length of \\(B \\setminus A\\) to be the length of B minus the length of A. Consequently the Smith-Volterra-Cantor set is measurable.\nHowever the cardinality of the Borel measurable sets is \\(\\mathfrak{c}\\) (the cardinality of the reals) since every Borel measurable set can be constructed from the intervals (which have cardinality \\(\\mathfrak{c}\\) ) by complementation and countable unions. To contrast the Cantor set has outer content zero, so every subset will have outer content zero, and hence be Jordan measurable. Since the Cantor set is uncountable this implies the cardinality of the Jordan measurable sets is at least \\(2^\\mathfrak{c}\\) .\nLebesgue’s criterion for a subset of [a,b] to be measurable is a subtle play on Borel’s and Peano-Jordan’s. The outer measure of a set S, \\(m^\\star(S)\\) , is the sum of the infimum of the sum of countably many intervals which have a union containing the set. The inner content of a set is the length of [a,b] minus the outer measure of the sets complement in [a,b], that is \\(b-a-m^*([a,b]\\setminus S)\\) . A subset of [a,b] is Lebesgue measurable if its inner measure and outer measure are equal. A subset of the real line is Lebesgue measurable if its intersection with [-n,n] is for every positive integer n. Carathéodory came up with the equivalent criterion: a set E is Lebesgue measurable if \\(m^*(A) = m^*(A \\cap E) + m^*(A\\setminus E)\\) for all sets A.\nThis final result is normally what is presented early in a course on Lebesgue integration, but I hope it seems a little less mysterious now. The countable unions and insisting \\(m^*(A \\setminus E) = m^*(A) - m^*(E)\\) if A contains E is essential to measure sets such as the Smith-Volterra-Cantor set. Using the outer measure ensures we automatically get the non-Borel sets of measure zero (and in fact a subset of the real line is Lebesgue measurable if and only if it is the disjoint union of a Borel measurable set and a set of measure zero).\nThe Lebesgue integral is great; it can integrate a much larger suite of functions than the Riemann integral, it has strong convergence theorems (dominated convergence theorem, Fubini’s theorem) and it readily abstracts (and forms the basis for modern probability theory). However it still can’t integrate every derivative: consider the sinc function \\(\\frac{\\sin x}{x}\\) , it is the derivative of the function defined on the whole real line with Taylor expansion \\(\\sum_{n=0}^{\\infty} \\frac{(-1)^n}{(2n+1)(2n+1)!}x^{2n+1}\\) , but it is not Lebesgue integrable over the whole real line. The problem is the integral oscillates too fast; the total area above the x-axis is infinite and the total area below the x-axis is infinite, but if you add them from the origin out they cancel to a finite sum (it is the limit of Riemann integrals). (There are examples on bounded intervals too).\nThere is an integral more powerful than the Lebesgue integral on the real line, the Henstock-Kurzweil-Denjoy-Perron integral, or as it is sometimes known, the generalised Riemann integral, defined on a bounded interval as follows:\nThe function f is integrable on [a,b] with integral A if for every positive number \\(\\epsilon\\) there exists a positive function \\(\\delta\\) on [a,b] such that every tagged partition \\(\\{a_0,\\ldots,a_n\\},\\{t_1,\\ldots,t_n\\}\\) satisfying \\(t_i - \\delta (t_i) \\leq a_{i-1} \\leq t_i \\leq a_i \\leq t_i + \\delta (t_i)\\) , \\(\\left|A - \\sum_{j=1}^{n} f(t_j) (a_j - a_{j-1})\\right| < \\epsilon\\) .\nThis integral (once extended to the whole real line) contains every Lebesgue integrable function, limits of Lebesgue integrable functions and some new functions to boot! In particular every derivative is integrable. (Robert Bartle’s A modern theory of integration gives an accessible exposition on the subject).\nOf course there’s more: Cesaro-Denjoy integrals, approximate Perron integrals and generalisations to higher dimensions (where finding an integral for which ‘every derivative’ is integrable is, to my knowledge, unsolved).\nI’d like to conclude by reflecting how solving problems in physics by questionably performing operations on infinite trigonometric series was a major source of inspiration for mathematics, and I wonder how much impact resolving the questionable path-integrals in Quantum Field Theory will have (and has already had) on mathematics."
  },
  {
    "objectID": "moving-averages-sql/index.html",
    "href": "moving-averages-sql/index.html",
    "title": "Moving Averages in SQL",
    "section": "",
    "text": "Moving averages can help smooth out the noise to reveal the underlying signal in a dataset. As they lag behind the actual signal they tradeoff timeliness for increased precision in the underlying signal. You could use them for reporting metrics or for alerting in cases where it’s more important to be sure there is a change than it is to catch any change early. It’s typically better to have a 7 day moving average than weekly reporting for important metrics because you’ll see changes earlier. There are a few ways to implement this in SQL with different tradeoffs, and a few traps to avoid.\nThe simplest way is with by summing over a limited window, but you have to be careful about missing data. It’s possible to construct a window manually with multiple lags which can let you choose weights. Or finally you can use a self join which can handle missing data and flexible weighting. Depending on your situation and database it’s worth considering which one is best in terms of performance and simplicity.\nMy recommendation in general is to use a self-join with a weights table (fiddle):\nThe rest of the article will go through the options and how to work around comming issues with missing data and handling the first few rows properly."
  },
  {
    "objectID": "moving-averages-sql/index.html#adding-weights",
    "href": "moving-averages-sql/index.html#adding-weights",
    "title": "Moving Averages in SQL",
    "section": "Adding weights",
    "text": "Adding weights\nBecause we’re manually writing each part of the moving average it’s possible to add weights; say we wanted to use the weights (0.6, 0.24, 0.16) to emphasise the more recent data points. It’s as simple as inserting the weights into the query:\nSELECT *,\n      0.6 * pageviews +\n      0.24 * LAG(pageviews) OVER (order by date) +\n      0.16 * LAG(pageviews, 2) OVER (order by DATE) AS weighted_moving_average\nFROM (\nSELECT dates.date, coalesce(pageviews, 0) AS pageviews\nFROM generate_series((select min(date) from pages),\n                      (select max(date) from pages),\n                      '1 day') as dates\nLEFT JOIN pages on dates.date = pages.date\n) AS pages_full\nORDER BY date\nThe lag approach is simple and should work in any database that supports window functions. As before we can do it per segment using PARTITION BY in the window clause. However writing each lag is tedious for large windows, which the next approach solves."
  },
  {
    "objectID": "moving-averages-sql/index.html#weighted-moving-average",
    "href": "moving-averages-sql/index.html#weighted-moving-average",
    "title": "Moving Averages in SQL",
    "section": "Weighted moving average",
    "text": "Weighted moving average\nTo calculate the weighted moving average we can store the weights in a separate table. For example if we want the most recent data point to have a weight of 0.6, the middle point a weight of 0.24 and the furthest point a weight of 0.16 we could have a table like this:\n\n\n\nidx\nweight\n\n\n\n\n0\n0.6\n\n\n1\n0.24\n\n\n2\n0.16\n\n\n\nNot that we could reproduce the moving average by having a table with each weight being equal and adding to 1\n\n\n\nidx\nweight\n\n\n\n\n0\n0.333\n\n\n1\n0.333\n\n\n2\n0.333\n\n\n\nWe then join the weight based on the number of steps from the current date and calculate the inner product. Note that we censor the first two rows with a CASE statement, otherwise they will be wrong (fiddle).\nSELECT pages.date,\n       max(pages.pageviews) as pageviews,\n       CASE\n       WHEN pages.date - (select min(date) from pages) >= 2\n       THEN sum(weight * ma_pages.pageviews)\n       END as weighted_moving_average\nFROM pages\nJOIN pages AS ma_pages ON pages.date - ma_pages.date BETWEEN 0 AND 2\nJOIN weights ON idx = pages.date - ma_pages.date\nGROUP BY pages.date\nORDER BY pages.date\nThe best part about this is it works even if there’s a missing date. However you do lose the data point for the missing date, so you may want to complete the table if you know there’s missing dates. If we wanted to have partial results for the first 2 days we’d need to renormalise the weights based on the number of days since the first. The only limitation to this method is you’ll need a way to create the weights table. But even if you don’t have access to creating (temporary) tables, you may be able to do this using a select from values (fiddle).\nSELECT pages.date,\n       max(pages.pageviews) as pageviews,\n       CASE\n       WHEN pages.date - (select min(date) from pages) >= 2\n       THEN sum(weight * ma_pages.pageviews)\n       END as weighted_moving_average\nFROM pages\nJOIN pages AS ma_pages ON pages.date - ma_pages.date BETWEEN 0 AND 2\nJOIN (SELECT idx, 1/(2 + 1.) as weight FROM (VALUES (0, 1, 2)) as t(idx)) weights ON\n  idx = pages.date - ma_pages.date\nGROUP BY pages.date\nORDER BY pages.date\nNow you know a few ways to create moving averages and how to avoid the most common pitfalls regarding missing data and the initial rows. The weight table is the safest and most flexible solution and you could even create standard weight tables to use accross multiple metrics. However sometimes you’ll want to use the framed window method for performance or convenience methods (or the lag method if you also need weighting).\nHappy querying!"
  },
  {
    "objectID": "sleep/index.html",
    "href": "sleep/index.html",
    "title": "Sleep",
    "section": "",
    "text": "Sleep is really important. When people don’t sleep the get less effective at everything. There are also chronic health patters associated with bad sleep. Worst of all when you’re overtired you may not even feel tried. Make sure you get seasonable sleep"
  },
  {
    "objectID": "job-title-words/index.html",
    "href": "job-title-words/index.html",
    "title": "Job Title Words",
    "section": "",
    "text": "I have developed a method for identifying duplicate job ads and used it to remove duplicates. This is because if one ad with the same title is posted hundreds of times (which happens in the dataset) it will have misleading results on the role title counts. The process isn’t perfect, but definitely reduces a lot of the noise without removing much signal.\nLooking at the most common role titles by frequency they are terms like “Business Development Manager”, “Project Manager”, “Management Accountant”, “Cleaner” and “Sales Executive”. That is they are have a type like “Manager”, “Accountant”, “Cleaner”, or “Executive” and then a specialisation like “Business Development”, “Project”, “Management” or “Sales”. My approach is to try to build a whitelist of types to extract from the job text.\nFor a first step I get the top 800 most frequent role titles (where there”s 20 different job ads with that exact title modulo upper case) and count the number of ads by the last word from the Title field of my jobs dataframe df.\nroles = (\n df\n .Title\n .value_counts()\n .head(800)\n .to_frame()\n .reset_index()\n .assign(last_word=\n     lambda df: (df['index']\n                 .str.lower()\n                 .str.split(' ')\n                 .apply(lambda x: x[-1])))\n .groupby('last_word')\n .agg(n=('Title', 'sum'))\n .sort_values('n', ascending=False)\n)\nThen I manually went through this list and commented out the things that weren’t general roles. When I wasn’t sure I looked back in the source data to see how that role was used; for example “partie” which is part of the special role title “chef de partie”.\ndf[df.Title.str.contaions('partie', case=False)].head()\nI went through this list and removed anything that couldn’t be a standalone role (e.g. “senior” was in but “partie” was out), anything that was an industry rather than a role (e.g. “finance”) and plurals (e.g. “nurses”) and roles containing a slash (like “receptionist/administrator”). This left me with a list of 104 role types (like manager, engineer, executive, assistant, accountant, administrator, …). “Sales” is an interesting exception because sometimes it’s used as a type (e.g. medical sales), sometimes as a specialisation (e.g. sales executive) and sometimes as an industry (e.g. head of sales). There are also many that are only types in certain context, like “assistant” in “sales assistant”, but not in “assistant manager”.\nIn building this list I also I also built a mapping of acronyms (like PA is Personal Assistant, DBA is Database Administrator). Once these are expanded you get to standard types like “assistant” and “administrator”. I also noticed adviser is a spelling variant of advisor.\nLooking into the role titles that end in an industry (like “finance”, or “marketing”) they are mainly of the form “director of marketing”, “head of finance”, or “teacher of English”. We can generally consider these as equivalent to “marketing director”, “finance head” and “English teacher”. In this case they also end in standard role types like “director” and “teacher”. Head is a rather uncommon ending, but it does occur in the source data like “Group Head” and “Department Head”.\nThis first look at the data gives an idea of an approach. We can expand acronyms, normalise spelling variants, and then try to match on a type. There are still some challenges like how to deal with slashes (e.g. fitter/turner), multiple types together, and things that are a type depending on context like assistant or head. There’s still a challenge with how to normalise something like “director of marketing and finance” or an overly specific role title like “financial planning and analysis manager”. But these problems are manageable, and more the edge cases than the common cases.\nFor more details see the Jupyter notebook."
  },
  {
    "objectID": "text-meta-data-commoncrawl/index.html",
    "href": "text-meta-data-commoncrawl/index.html",
    "title": "Extracing Text, Metadata and Data from Common Crawl",
    "section": "",
    "text": "If you just need the text of the internet use the WET files\nIf you just need the response metadata, HTML head information or links in the webpage use the WAT files\nIf you need the whole HTML (with all the metadata) then use the full WARC files\n\nThe index only contains locations for the WARC files, the WET and WAT files are just summarisations of it. For more detail read Common Crawl’s introduction and the WARC specification. For processing these at scale see Common Crawl’s pyspark code samples and Mark Litwintschik’s post on using EMR and Spark with Common Crawl.\nSee the Jupyter Notebook (Raw) for more code samples.\n\nA brief introduction to WARC record types\nEach of these files (WARC, WET, and WAT) are stored in WARC format. The specification lists a few possible types of records which are useful to know about:\n\nwarcinfo - contains information about the web crawl (normally the first record)\nrequest - details of HTTP request\nresponse - details from HTTP response\nmetadata - additional information\nconversion - result of transforming another field\nresource - record contains a resource (e.g. image)\n\nThere’s also a couple odd kinds I won’t talk about, but may come up.\n\nrevisit - describes the revisitation of content already archived, may only contain changed definition\ncontinuation - appended to corresponding prior record block(s) (e.g., from other WARC files) to create complete record\n\n\n\nJust the Text - WET\nThe WET contains just the webpage title, and plain text extracted from the HTML of each response. This could be useful for text analysis, building a search index or training a machine learning language model. It’s about 1/6 the size of the full WARC files.\nAll the archives can be accessed with the warcio library using ArchiveIterator. You can access the WET file by changing the URL to a WARC file.\nfrom warcio import ArchiveIterator\nwet_url = warc_url.replace('/warc/', '/wet/').replace('warc.gz', 'warc.wet.gz')\nr = requests.get(warc_url, stream=True)\nrecords = ArchiveIterator(r.raw)\nThe first record is a warcinfo record describing the crawl, and all the following requests are conversion records containing the plain text of each response.\nrecord = next(records)\nassert record.rec_type == 'warcinfo'\n# skip the warcinfo\nrecord = next(records)\n# This shows the source page, WARC-Target-URI and other metadata\nrecord.rec_headers.headers\ntext = record.content_stream().read()\nprint(text.decode('utf-8'))\n\n\nMetadata - WAT\nThe WAT contains just the metadata from each page, the request info, things from head of HTML, and links from the webpage. You could use this for understanding what web servers most commonly used (from response headers), for analysing declared keywords or for analysing the link structure (finding reverse links or calculating page rank). It also contains details of the corresponding WARC record so you could use the WAT data to find the WARC files you need before extracting the full HTML from them. It’s about 1/3 the size of the full WARC files because it doesn’t contain the actual content.\nYou can access the WAT file by changing the URL to a WARC file:\nwat_url = warc_url.replace('/warc/', '/wat/').replace('warc.gz', 'warc.wat.gz')\nThe first record is a warcinfo describing the crawl, followed by metadata records with JSON content. It seems that the first metadata describes the corresponding full WARC file, and then the following metadata records are aligned to corresponding records in the WARC file.\nrecord = next(records)\nassert record.rec_type == 'warcinfo'\n# skip the warcinfo\nrecord = next(records)\n# Headers tell us what the record is about (e.g. source url)\nrecord.rec_headers.headers\nmetadata = json.loads(record.content_stream().read())\nThe metadata JSON object has a Container key describing the corresponding WARC source, and an Envelope describing the record itself. If it’s describing a response (rather than a request or metadata or something else) you can access the HTML-Metadata.\ndata['Envelope']\\\n    ['Payload-Metadata']\\\n    ['HTTP-Response-Metadata']\\\n    ['HTML-Metadata']\nHere’s some example content:\n{'Head': {'Title': '纺织服装行业周报:终端零售回暖,板块业绩等待验证 - 相关研报 - 梦洁股份(002397)',\n  'Metas': [{'name': 'mobile-agent',\n    'content': 'format=html5; url=detail_m.php?id=866619'},\n   {'name': 'mobile-agent',\n    'content': 'format=xhtml; url=detail_m.php?id=866619'},\n   {'name': 'keywords',\n    'content': '纺织服装行业周报:终端零售回暖,板块业绩等待验证,相关研报,梦洁股份,002397'},\n   {'name': 'description',\n    'content': '梦洁股份(002397)相关研报：纺织服装行业周报:终端零售回暖,板块业绩等待验证'}],\n  'Link': [{'path': 'LINK@/href',\n    'url': 'http://txt.inv.org.cn/ir/site/pc/css.css',\n    'rel': 'stylesheet',\n    'type': 'text/css'}],\n  'Scripts': [{'path': 'SCRIPT@/src',\n    'url': 'http://static.bshare.cn/b/buttonLite.js#style=-1&uuid=&pophcol=2&lang=zh',\n    'type': 'text/javascript'},\n   {'path': 'SCRIPT@/src',\n    'url': 'http://static.bshare.cn/b/bshareC0.js',\n    'type': 'text/javascript'},\n   {'path': 'SCRIPT@/src',\n    'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'},\n   {'path': 'SCRIPT@/src',\n    'url': '//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'}]},\n 'Links': [{'path': 'A@/href',\n   'url': '/',\n   'target': '_blank',\n   'text': '梦洁股份(002397)'},\n  {'path': 'A@/href',\n   'url': '/index_m.php',\n   'target': '_blank',\n   'text': '移动版'},\n  {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/broker/huasheng_pc.jpg'},\n  {'path': 'A@/href',\n   'url': 'https://hd.hstong.com/marketing/2019/0228?_scnl=OTg0NWJibzY0MTI3',\n   'target': '_blank'},\n  {'path': 'A@/href', 'url': '/', 'text': '首页'},\n  {'path': 'A@/href', 'url': '/quote/', 'text': '股票行情'},\n  {'path': 'A@/href', 'url': '/media_news/', 'text': '媒体报道'},\n  {'path': 'A@/href', 'url': '/related_news/', 'text': '相关新闻'},\n  {'path': 'A@/href', 'url': '/notice/', 'text': '公司公告'},\n  {'path': 'A@/href', 'url': '/report/', 'text': '研究报告'},\n  {'path': 'A@/href', 'url': '/related_report/', 'text': '相关研报'},\n  {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '梦洁股份'},\n  {'path': 'A@/href', 'url': '/', 'target': '_blank', 'text': '002397'},\n  {'path': 'A@/href',\n   'url': 'http://www.bShare.cn/',\n   'title': '分享到',\n   'text': '分享到'},\n  {'path': 'IMG@/src', 'url': 'http://img.inv.org.cn/ad/zixun_pc.jpg'},\n  {'path': 'A@/href',\n   'url': 'http://stock.inv.org.cn',\n   'target': '_blank',\n   'text': '股票投资之家'}]}\n\n\nData - WARC\nThe WARC files are the ultimate data source. You really only need to use them if you need to efficiently access the data via an index or you need the actual HTML content. Sometimes the HTML content is necessary because you want to know about javascript used in the body/foot of the page, or you want the structured content of the page (not just the text).\nThe WARC files start with a warcinfo record describing the file. This is followed by sequences of records for each event, e.g. accessing a URL. A typical pattern is a request record describing how the content was requested, a response record describing what was received from the server including HTTP headers and, and some metadata such as detected languages, character sets and the time to fetch the page.\nI’m not sure whether there are resource or revisit records in the WARC (or continuation records that sound painful)."
  },
  {
    "objectID": "symmetry-lie-alebras-qde-1/index.html",
    "href": "symmetry-lie-alebras-qde-1/index.html",
    "title": "Symmetry, Lie Algebras and Differential Equations Part 1",
    "section": "",
    "text": "We begin with trying to solve the differential equation \\(-\\frac{1}{2m} f''(x) + \\frac{k}{2} x^2 f(x) = \\lambda f(x)\\) for some real positive constants \\(m\\) , \\(k\\) and \\(latex\\lambda\\) with the boundary conditions f vanishes at infinity. This is an eigenvalue equation; this can’t be solved for any constants but only for particular values of \\(\\lambda\\) for a fixed k and m. By dilations (that is, rescaling units) we can assume without loss of generality \\(m=1\\) and \\(k=1\\) . It is useful to define the momentum operator \\(p=-i \\frac{\\mathrm{d}}{\\mathrm{d}x}\\) – this makes everything more physics-like. If this isn’t familiar to you just substitute \\(-i \\frac{\\mathrm{d}}{\\mathrm{d}x}\\) wherever you see a p.\n(The i is chosen to make the operator Hermitian with respect to the \\(L^2\\) inner product: \\(\\int_{-\\infty}^{\\infty}{f(x)}^* p g(x) = \\int_{-\\infty}^{\\infty} \\left(pf(x)\\right)^* g(x)\\) ; where * denotes complex conjugation and f and g are zero at infinity. This identity follows immediately from integration by parts).\nIntroducing the Hamiltonian operator \\(H=\\frac{1}{2}\\left(p^2+x^2\\right)\\) the differential equation is then the eigenvalue equation \\(H f(x) = \\lambda f(x)\\) (a form familiar to physicists). The Hamiltonian operator has an obvious symmetry to it: it is invariant under rotations in x-p space. That is it is invariant under transformations of the form:\n\\(\\begin{bmatrix} x' \\\\ p' \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} \\begin{bmatrix} x \\\\ p \\end{bmatrix}\\) .\nFollowing the ideas of Sophus Lie we look at the infinitesimal transformations generating this, by taking the derivative at the identity \\(\\theta=0\\) this gives \\(x \\to -p\\) , \\(p \\to x\\) ; in x-p space it is given by the matrix \\(\\begin{bmatrix} 0 & -1 \\\\ 1 & 0\\end{bmatrix}\\)\nThis transformation is precisely the Fourier transform: \\(f(x) \\to \\widehat{f}(k)=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty} f(x) e^{-i k x}\\) . In particular integration by parts and differentiating under the integral respectively it follows \\(\\widehat{xf(x)}=i\\widehat{f}'(k)\\) and \\(\\widehat{-if'(x)} = k \\widehat{f}(k)\\) , so as an operator on functions \\(x \\to -p\\) and \\(p \\to x\\) .\nNow since the square of the Fourier transform in x-p space is negative the identity it has eigenvalues -i and +i and corresponding eigenvectors \\(a = \\frac{1}{\\sqrt{2}}(x+ip)\\) and \\(a^{\\dagger} = \\frac{1}{\\sqrt{2}} (x- ip)\\) .\nNow we introduce the commutator of operators \\([A,B]=AB-BA=-[B,A]\\) and in particular \\([x,p]=i \\mbox{Id}\\) (since x and its derivative don’t commute). Consequently by linearity \\([a,a^{\\dagger}]=1\\) .\nSimple calculations show that \\(a^{\\dagger}a = H -1/2\\) , \\(a a^{\\dagger} = H + 1/2\\) , \\([H,a]=-a\\) and \\([H,a^{\\dagger}]=a^{\\dagger}\\) . These last two relations allow us to find the spectrum of H, that is the values of \\(\\lambda\\) for which the differential equation is solvable!\nIf the differential equation can be solved for some \\(\\lambda\\) , \\(H f = \\lambda f\\) then using the commutation relations shows \\(H(a f) = (\\lambda-1) (af)\\) and \\(H(a^{\\dagger}f) = (\\lambda +1) (a^{\\dagger}f)\\) . Thus \\(a\\) lowers the eigenvalue by 1 and is called a lowering operator, and \\(a^{\\dagger}\\) raises the eigenvalue by 1 and is called a raising operator.\nHowever we can not lower indefinitely: H is positive semidefinite, \\(H=\\frac{1}{2} (x^{\\dagger}x + p^{\\dagger}p)\\) (where the dagger indicates Hermitian conjugation with respect to the \\(L^2\\) inner product), so \\(\\lambda\\) must be non-negative. Thus there is a function \\(f_0(x)\\) for which \\(a f_0(x) = 0\\) (which of course satisfies the differential equation trivially). On this state \\(H f_0(x) = (a^{\\dagger} a + 1/2) f_0(x) = 1/2 f_0(x)\\) .\nMoreover since any arbitrary solution can be brought to \\(f_0\\) by repeated lowering (applications of a), and lowering then raising gives a multiple of the original function every solution can be obtained by raising \\(f_0\\) . Thus the only possible eigenvalues are n+1/2 for n=0,1,2,….\nWhat are the corresponding eigenvectors? Well \\(a f_0 = 0\\) implies that \\(x f_0(x) + f_0'(x) =0\\) , which has solutions \\(f_0(x) = A e^{-\\frac{x^2}{2}}\\) for some constant \\(A\\) . Then the solution with \\(\\lambda = n + 1/2\\) is up to a constant factor \\((a^{\\dagger})^n f_0(x) = A_n \\left(x - \\frac{\\mathrm{d}}{\\mathrm{d}x}\\right)^n e^{-\\frac{x^2}{2}} = H_n(x) e^{-\\frac{x^2}{2}}\\) where \\(H_n(x)\\) are the Hermite polynomials. Consequently we have found all solutions of the second order differential equation just by solving a first order differential equation! (They can also easily be normalized algebraically; that is without doing any integrals, but I won’t show that here).\nIt is interesting to note all these solutions are invariant under Fourier transform. This is of course a consequence of the Hamiltonian being invariant under Fourier transform, F; if \\(Hf = \\lambda f\\) then \\(F H f = (FHF^{-1}F)f = HFf\\) and thus \\(\\lambda F f = H (Ff)\\) .\nFrom an abstract point of view what have we done? We have taken an algebra of operators on some Hilbert space generated by self-adjoint operators \\(x\\) and \\(p\\) satisfying \\(xp-px=i \\text{id}\\) (notice that this implies the vector space can’t be finite dimensional; take the trace of each side). Using this we have shown that the positive definite Hermitian operator \\(H = \\frac{1}{2} (x^2 + p^2)\\) has eigenvalues n + 1/2 for n=0,1,2,….\nWe could choose an explicit representation: the Hilbert space is the space of square integrable functions, x is the multiplication operator and \\(p = -i \\frac{\\mathrm{d}}{\\mathrm{d}x}\\) , then in this basis the eigenequation is the differential equation we started with. The solutions in this basis are the Hermite polynomials multiplied by a Gaussian; notice that these functions are orthogonal and complete in \\(L^2\\) being all the eigenfunctions of a Hermitian operator. The formula for the eigenfunctions in terms of raising operators gives rise to a Rodrigues formula for the Hermite polynomials.\nHowever there is nothing canonical about this choice of representation, a different representation is given by the Fourier transform, which acts as a change of basis. That the Hamiltonian is invariant under the Fourier transform means \\(FHF^{-1}=H\\) or \\([F,H]=0\\) .\nThe nicest choice of basis is the one in which H is the (countably infinite dimensional) diagonal matrix with entries 1/2,3/2,5/2,…. It is easy to see that a is the matrix with 1s one row below the diagonal and zeros everywhere else \\(a=\\begin{bmatrix} 0 & 0 & 0 & \\ldots \\\\ 1 & 0 & 0 & \\ldots \\\\ 0 & 1 & 0 & \\ldots \\\\ &&& \\ddots \\end{bmatrix}\\) and \\(a^{\\dagger}\\) is its transpose. Representations for x and p can be obtained from \\(x = \\frac{1}{\\sqrt{2}}(a + a^{\\dagger})\\) and \\(p = \\frac{1}{2i} (a - a^{\\dagger})\\) .\nIt is worth noting that in this derivation it wasn’t enough to have a Lie algebra, that is a Lie bracket, we also needed a multiplication over which the Lie bracket is the commutator – that is a representation."
  },
  {
    "objectID": "run-webserver-without-root/index.html",
    "href": "run-webserver-without-root/index.html",
    "title": "Run Webserver Without Root",
    "section": "",
    "text": "There are a few ways to do this, but only a couple that make sense for an interpreted language (like Python, as opposed to a compiled binary). An easy way is using authbind to grant access to the ports.\n\nAutbind\nYou’ve got a service ready to serve on port 80 and/or port 443. How do we run the application?\n# 1. Install authmind\napt-get install authbind\n# 2. Create permission files to set read/write permission\nsudo touch /etc/authbind/byport/80\nsudo touch /etc/authbind/byport/443\n# 3. Change the owner to the user who runs the service\n# Here I'm assuming they're called `server`\nsudo chown server /etc/authbind/byport/80\nsudo chown server /etc/authbind/byport/443\n\n# Run the application\nauthbind --deep /path/to/app\nThat’s all there is to it."
  },
  {
    "objectID": "athena-remove-timezone/index.html",
    "href": "athena-remove-timezone/index.html",
    "title": "Removing Timezone in Athena",
    "section": "",
    "text": "cast(event_time as timestamp)"
  },
  {
    "objectID": "composing-functions/index.html",
    "href": "composing-functions/index.html",
    "title": "Composing Functions",
    "section": "",
    "text": "Python’s Pandas library doesn’t have this kind of convenience and it opens up a class of error that won’t happen in that R code. Here’s a typical bit of Pandas code:\ndf_clean = df_raw[(df_raw['colour'] == 'blue') & (df_raw['price'] > 50)]\ndf_clean.loc[df_clean['price'].isna(), 'price'] = df_clean['price'].mean()\nThere are so much repetition here it’s easy to make a mistake. On the first line df_raw is typed 3 times, a typo putting in a different dataframe will lead to subtle runtime errors that are hard to pick up; I’ve debugged them in my own code many times. The second line has a similar problem where df_clean is typed 3 times (if df_raw was put there by accident it could lead to an error). There’s also other Pandas traps here; forgetting the brackets on the first line will lead to an error due to the precedence of &, and I don’t know whether the second line actually changes df_raw (I may see some warning about that, and then if I want to preserve df_raw I’ll put a .copy() in.\nIn R dplyr it’s much cleaner and you can’t accidentally type the wrong thing because we’re chaining (here using the magrittr %>%, but soon we will be able to use |>):\ndf_clean <- df %>%\n  filter(colour == \"blue\", price > 50) %>%\n  assign(price = ifelse(is.na(price), 50, mean(price)))\nIn Pandas you can use method chaining (and tools like the pandas pipe) to clean it up. Using query we can get something close to dplyr, but it’s still a bit clunky and query can be very slow:\ndf_clean = (df_raw\n  .query('colour == \"blue\" & price > 50')\n  .assign(price = lambda df: df['price'].fillna(df['price'].mean()))\nHowever there are cases where it’s really hard to do in Pandas, like getting the second most common value in a group. Because Pandas is built by appending functions to the Dataframe class if there’s not a method for it you have to patch it in like pyjanitor does, but it you do a proper pandas extension it’s quite verbose. In R because it uses a functional approach you can easily reuse common functions rather than having to write (and remember the names of!) Dataframe specific ones.\nI think method chaining is a useful way to write data transformations; it exists in most functional languages Haskell, OCaml, F# and in Clojure’s useful threading macros. It’s even in Julia and there’s a proposal for Javascript. You can implement it in Python in some sense using magic methods for infix operators, like Thinc’s combinators but it’s against the grain in Python which s not a functional language."
  },
  {
    "objectID": "json-extraction-dsl/index.html",
    "href": "json-extraction-dsl/index.html",
    "title": "Extracting Fields from JSON with a Python DSL",
    "section": "",
    "text": "It works like this:\nd = [{'a': [{'b': 'c'}, {'d': ['e']}]}]\n\nassert extract(d, '0.a.1.d.0') == d[0]['a'][1]['d'][0]\nassert extract(d, '1.a.1.d.0') == None\nYou can specify a path into an object, separated by periods, and it will extract it returning None if that path doesn’t exist. The main limitations of this approach are:\n\nThe field separator (. by default) can’t be used in dictionary keys\nOnly strings or integers can be used as dictionary keys\nStrings consisting of integers (e.g. \"1\" or \"-21\") can’t be used as dictionary keys\n\nThe implementation is pretty simple, it uses itemgetter to recursively step through the path. The only complication is to index into arrays we have to convert strings representing integers into integers (hence the limitation above).\nfrom operator import itemgetter\nimport re\nfrom typing import Union, Callable, Any\n\ndef is_integer(s: str) -> bool:\n    return re.match('^-?[0-9]+$', s) is not None\n\ndef convert_integers(s: str) -> Union[str, int]:\n    if is_integer(s):\n        return int(s)\n    else:\n        return s\n\ndef extract(obj: Any, path: str, sep: str='.', default=None) -> Any:\n    steps = [convert_integers(x) for x in path.split(sep)]\n    for step in steps:\n        try:\n            return itemgetter(step)(obj)\n        except (KeyError, IndexError, TypeError):\n            return default\nA more functional (but less Pythonic) way to do this is by composing the itemgetters.\nimport functools\n\ndef compose(*functions):\n    return functools.reduce(lambda f, g: lambda x: f(g(x)), functions)\n\ndef extractor(path: str, sep: str='.') -> Callable[[Any], Any]:\n    steps = [convert_integers(x) for x in path.split(sep)]\n    return compose(*map(itemgetter, reversed(steps)))\n\ndef extract2(obj: Any, path: str, sep: str='.', default=None) -> Any:\n  try:\n    return extractor(path, sep)(obj)\n  except (KeyError, IndexError, TypeError):\n    return obj\nNote that type checking isn’t very useful here; this approach is very dynamic and statically verifying a caller is using it correctly would be very hard. I’m not sure if something like Haskell’s Lens library solves this; but when dealing with arbitrary JSON it’s hard to know what the data will be like anyway.\nThis gives a simple but effective way to extract fields from structured data. For example if you were getting JSON-LD or Microdata for a jobposting you could extract the currency using something like: extract(jobposting, \"salaryCurrency\") or extract(jobposting, \"baseSalary.currency\") since it can optionally be put into either field."
  },
  {
    "objectID": "sparql-analysing-rdf/index.html",
    "href": "sparql-analysing-rdf/index.html",
    "title": "Analytics Web Data Commons with SPARQL",
    "section": "",
    "text": "After reading in the graphs individually they can be combined into a rdflib.Dataset so we can query them all together. For this analysis I got 13,000 pages from the 2019 Web Data Commons Extract, each from a distinct web domain and containing exactly one job posting.\ndataset = rdflib.Dataset()\nfor graph in graphs:\n    dataset.add_graph(graph)\nWe can then execute SPARQL queries in RDFLib using dataset.query. The SPARQL 1.1 Specification is actually pretty easy to read and with a little practice it’s a simple language to learn. Here’s a query that gives summary statistics of the most common RDF types in the sample of graphs.\nSELECT ?type (COUNT(?src) AS ?postings) (SUM(?n) as ?total) {\nSELECT ?src ?type (COUNT(?type) AS ?n)\nWHERE {\n    GRAPH ?src\n    {[] a ?type .}\n}\nGROUP BY ?src ?type\n}\nGROUP BY ?type\nHAVING (COUNT(?src) > 50)\nORDER BY desc(?total)\nUsing Pandas we can make it more meaningful by calculating the proportion of pages with each RDF type, and the average number of times a type occurs in a page graph.\ndf = pd.DataFrame([[value.toPython() for value in row] for row in results],\n                  columns = ['uri', 'n', 'total'])\ndf.assign(frac=lambda df: df.n/max(df.n),\n          avg = lambda df: df.total / df.n)\n\n\n\n\n\n\n\n\n\n\nURI\nn\ntotal\nfrac\navg\n\n\n\n\nhttp://schema.org/JobPosting\n13092\n13092\n1.000000\n1.000000\n\n\nhttp://schema.org/Place\n8734\n9301\n0.667125\n1.064919\n\n\nhttp://schema.org/Organization\n7972\n9184\n0.608921\n1.152032\n\n\nhttp://schema.org/PostalAddress\n8065\n9018\n0.616025\n1.118165\n\n\nhttp://schema.org/MonetaryAmount\n2958\n2970\n0.225940\n1.004057\n\n\nhttp://schema.org/PropertyValue\n2875\n2882\n0.219600\n1.002435\n\n\nhttp://schema.org/ListItem\n946\n2871\n0.072258\n3.034884\n\n\nhttp://schema.org/QuantitativeValue\n2602\n2619\n0.198747\n1.006533\n\n\nhttp://schema.org/ImageObject\n609\n1057\n0.046517\n1.735632\n\n\nhttp://schema.org/BreadcrumbList\n939\n988\n0.071723\n1.052183\n\n\n\nThe most common object is a JobPosting and we can construct a query to get the most frequently used properties.\nPREFIX sdo: <http://schema.org/>\nSELECT ?rel (COUNT(?src) AS ?postings) (SUM(?n) as ?total) {\nSELECT ?rel ?src (COUNT(?src) AS ?n)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; ?pred ?data }\n    BIND (replace(str(?pred), 'https?://schema.org/(JobPosting/)?', '') AS ?rel)\n}\nGROUP BY ?rel ?src\n}\nGROUP BY ?rel\nORDER BY desc(?postings)\n\n\n\n\n\n\n\n\n\n\nProperty\nPostings\nTotal\nFraction of Posts\nAverage times per post\n\n\n\n\nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n13092\n13092\n1.000000\n1.000000\n\n\ntitle\n11862\n12052\n0.906049\n1.016018\n\n\ndescription\n11323\n11540\n0.864879\n1.019165\n\n\ndatePosted\n10420\n10515\n0.795906\n1.009117\n\n\njobLocation\n9800\n10423\n0.748549\n1.063571\n\n\nhiringOrganization\n9568\n9720\n0.730828\n1.015886\n\n\nemploymentType\n7702\n8139\n0.588298\n1.056739\n\n\nvalidThrough\n4688\n4691\n0.358081\n1.000640\n\n\nbaseSalary\n3657\n3713\n0.279331\n1.015313\n\n\nindustry\n3328\n4081\n0.254201\n1.226262\n\n\nidentifier\n3214\n3217\n0.245493\n1.000933\n\n\nurl\n2744\n2894\n0.209594\n1.054665\n\n\nworkHours\n1352\n1366\n0.103269\n1.010355\n\n\nexperienceRequirements\n1235\n1262\n0.094332\n1.021862\n\n\noccupationalCategory\n1152\n1509\n0.087993\n1.309896\n\n\neducationRequirements\n959\n991\n0.073251\n1.033368\n\n\nsalaryCurrency\n904\n910\n0.069050\n1.006637\n\n\nqualifications\n839\n902\n0.064085\n1.075089\n\n\nresponsibilities\n834\n894\n0.063703\n1.071942\n\n\nimage\n790\n859\n0.060342\n1.087342\n\n\nskills\n726\n795\n0.055454\n1.095041\n\n\n\nDigging further we could extract the types of the baseSalary; while it’s mostly a MonetaryAmount (a complex object) it’s also often a string in some language (a literal).\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\n\nSELECT ?type (COUNT(?src) as ?n)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:baseSalary|sdo_jp:baseSalary ?data .\n     OPTIONAL {?data a ?datatype .}\n     BIND (coalesce(datatype(?data), ?datatype) as ?type)}\n}\nGROUP BY ?type\nORDER BY DESC(?n)\nLIMIT 20\n\n\n\nType\nCount\n\n\n\n\nhttp://schema.org/MonetaryAmount\n2946\n\n\nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#langString\n527\n\n\nhttp://www.w3.org/2001/XMLSchema#string\n135\n\n\nhttps://schema.org/MonetaryAmount\n72\n\n\nNone\n16\n\n\nhttp://schema.org/PriceSpecification\n7\n\n\nhttp:/schema.orgMonetaryAmount\n6\n\n\n\nAs with JobPosting we can then dig into the most commonly used properties of a MonetaryAmount.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\n\nSELECT ?rel (COUNT(?src) AS ?postings) (SUM(?n) as ?total) {\nSELECT ?rel ?src (COUNT(?src) AS ?n)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; sdo:baseSalary|sdo_jp:baseSalary ?salary .\n     ?salary ?pred ?data .}\n    BIND (replace(str(?pred), 'https?://schema.org/(MonetaryAmount/)?', '') AS ?rel)\n}\nGROUP BY ?rel ?src\n}\nGROUP BY ?rel\nORDER BY desc(?postings)\n\n\n\n\n\n\n\n\n\nType\nFraction of all jobs\nFraction of results\nAverage Frequency\n\n\n\n\nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#type\n0.230734\n1.000000\n1.003972\n\n\nvalue\n0.215841\n0.935452\n1.005308\n\n\ncurrency\n0.206675\n0.895730\n1.004065\n\n\nminValue\n0.010922\n0.047335\n1.000000\n\n\nmaxValue\n0.010769\n0.046673\n1.000000\n\n\nunitText\n0.002979\n0.012910\n1.000000\n\n\n\nAnd we can continue into looking at the datatypes of a MonetaryAmount value, the RDF type when it’s blank or the Literal type.\nPREFIX sdo: <http://schema.org/>\nPREFIX sdo_jp: <http://schema.org/JobPosting/>\nPREFIX sdo_ma: <http://schema.org/MonetaryAmount/>\n\n\nSELECT ?type (COUNT(?src) as ?n)\nWHERE {\n    GRAPH ?src\n    {[] a sdo:JobPosting; (sdo:baseSalary|sdo_jp:baseSalary)/(sdo:value|sdo_ma:value) ?data .\n     OPTIONAL {?data a ?datatype .}\n     BIND (coalesce(datatype(?data), ?datatype) as ?type)}\n}\nGROUP BY ?type\nORDER BY DESC(?n)\nLIMIT 20\n\n\n\nType\nn\n\n\n\n\nhttp://schema.org/QuantitativeValue\n2323\n\n\nhttp://www.w3.org/1999/02/22-rdf-syntax-ns#langString\n280\n\n\nhttp://www.w3.org/2001/XMLSchema#string\n152\n\n\nNone\n7\n\n\nhttp://schema.org/PropertyValue\n4\n\n\nhttps://schema.org/QuantitativeValue\n2\n\n\n\nThese kinds of techniques could be templated and extended to build a full frequency table. These are generally fairly consistent with what I found before with a different method and a smaller sample so this increases my confidence in those results. You can see the full Jupyter notebook for details."
  },
  {
    "objectID": "au-oil-imports-check/index.html",
    "href": "au-oil-imports-check/index.html",
    "title": "Checking Australian Oil Imports",
    "section": "",
    "text": "The overall tree diagram for the estimate is below:\n\n\n\n\ngraph BT;\n   Import[Oil imports<br/>1.3 Million Barrels/Day]\n\n   ImportL[Oil imports<br/>200ML/Day] --> Import\n   Barrel[Size of Barrel<br/>160L] -->|-1| Import\n   \n   Consumption[Oil consumed L/Day<br/>200ML/Day] --> ImportL\n   ImportRatio[Oil Imported / Consumed<br/>1] --> ImportL\n   \n   CarConsumption[Oil Consumed by Cars<br/>100ML/Day] --> Consumption\n   CarFraction[Oil Consumed in Total  / Oil Consumed by Cars<br/>2] --> Consumption\n   \n   Cars[Number of Cars<br/>20 Million] --> CarConsumption\n   ConsumptionCar[Oil Consumed by Car<br/>5L/Day] --> CarConsumption\n\n   People[Number of People<br/>25 Million] --> Cars\n   CarPeople[Number of Cars per Person<br/>0.8] --> Cars\n\n\n\n\n\n\n\n\n\nChecking oil imports and consumption\nAustralian oil imports can be found in Australian Office of the Chief Economist - Resources and Energy Quarterly September 2020 on page 87.\nThe total imports are about 1020 kb/day (made up of 645 kb/day of refined oil plus 375 kb/day of crude imports), which is remarkably close to my estimate of 1300 kb/day. Let’s break this down and check the intermediate steps to see why it was so accurate, and check where there was dumb luck.\nIn the same report it mentions the consumption is around 1,000 kb/day so the assumption that imports is approximately equal to consumption is remarkably good. Australia does produce a significant amount of oil; around 300 kb/day but exports a similar number about 250 kb/day. This is quite curious; I’m not sure why Australia exports a similar volume of oil to production, nor the mixture of refined and crude imports.\nFinally we assumed car usage is about half of all oil usage. In the same report it mentions that of worldwide usage about 30% is diesel, 26% is gasoline, 12% is LPG and Ethane and 8% is aviation. Some of diesel and LPG/Ethane usage will be in passenger cars, so it’s probably around 30%, a bit lower than 50% but not dramatically so. Looking at Australian Refinery output in July-December 2019, it’s about 5BL of Gasoline, 4BL of Diesel, 2BL of Aviation and 2BL of Other, so again slightly under half of usage being cars seems reasonable.\nI’m ignoring the density difference in crude and refined oil; likely crude oil leads to a lower volume of refined oil but this is likely a marginal impact on the estimates.\n\n\nCar Usage\nWe’ve established the assumptions on estimating oil imports with car petrol consumption in Australia is fairly reasonable. Let’s trace our estimated car consumption of 100 ML/day, or about 36,000 megalitres per year.\nAccording to the 2018 Survey of Motor Vehicle Use: For the 12 months ending 30 June 2018:\n\nFuel consumed by all road registered vehicles totalled 34,170 megalitres.\nThere were an estimated 19 million motor vehicles\n\nThis is remarkably close to my estimates. Given in the 2016 Census there were 23.4 Million people, the ratio of about 0.8 cars per person is about right.\n\n\nReflection\nThis envelope calculation was unreasonably accurate given my knowledge of Australia’s oil industry, although I have a lot of practical knowledge of the country from living in it. It would be interesting to try to extend this to other countries; it’s a powerful way to gain a deeper understanding of how part of a country’s economy works."
  },
  {
    "objectID": "success-small-steps/index.html",
    "href": "success-small-steps/index.html",
    "title": "Success in Small Steps",
    "section": "",
    "text": "When I was in highschool I tried to build a simulation of the solar system for a project. I wasn’t satisfied with building ellipses, I wanted to take into account all the N-body interactions. It started off alright, but quickly I was out of my depth trying to model all the bodies and got the code in a broken state I couldn’t recover. I ended up with pages of code that didn’t work at all. If I had started smaller with building a single elliptic orbit, and then another, before even considering multibody interactions I would at least have had a good state to revert to, and would have been more likely to succeed with the complex problem.\nWhen I returned from a long overseas trip I was looking for work. I wanted to build something to help showcase my skills for prospective employers. I had the idea of building a Javascript illustration of a chaotic system; where a small change in starting points results in a very large difference in outputs. I spent a lot of time learning Javascript, agonising over whether I use SVG or the canvas for performance, writing vectorised functions so that I could extend it into 3 dimensions, and so on. I never actually built a functioning application because I got so bogged down on building it the right way; thankfully my CV and cover letters got me sufficient interviews. I would give myself the advice to build something simple; even programming a moving ball would be a success I could showcase. Then I could slowly build on that, adding a wall, adding bounce physics, adding angles. I tried to build a physics engine while learning web technology at the same time, it was too hard for me and I had nothing to show for it.\n\n\n\nThe kind of chaotic\n\n\nThis blog has examples of taking small steps. The jobs posts show lots of small steps in trying to build a pipeline for extracting information from job adverts on the internet. For example I had a post on extracting job title words from ad titles (e.g. “Manager” or “Executive” or “Engineer”). I wanted to improve this so I wrote follow ups on making plural words singular, and another on rewriting “Head of Marketing” to “Marketing Head”, and putting them together in a normalisation strategy which created a better way to discover job titles in a dataset. The initial rough approach gave me confidence that the system could work, and then I could build upon that and improve it.\nThis is why I recommend starting with simple models and working towards a complex solution. Whenever I’ve tried to start with a complex model I’ve spent too much time trying to get out and engineer features in the data without actually understanding it. Now I start with building an evaluation criteria, starting with a simple model and slowly building on it. Adding features to get incrementally better, and learning more about the data at the same time."
  },
  {
    "objectID": "caching-pipelines/index.html",
    "href": "caching-pipelines/index.html",
    "title": "Caching Data Pipelines",
    "section": "",
    "text": "A common problem I have is after a process has been running for a long time it fails on some malformed data. Depending on the problem the data can normally be repaired, filtered or the process extended to resolve it. However it can take a long time to bisect the problem, and all the intermediate calculation is wasted. If the good data that has been processed can be saved then it can save hours of recalculation; and can be deleted and recalculated if the processing changes.\nA classic way to do this is with file based mechanisms. A pipeline can be decomposed into steps that each consume files and emit files. Then if one stage of the pipeline fails you only need to repair and rerun that stage. There are many ways to orchestrate this from Makefiles to Airflow.\nThis can be make more granular if you can break the processing up into pieces that are each serialised to separate files. In some applications where you are integrating diverse datasources there is a natural separation. This also means you can continue to run the rest of the pipeline on the data you can process. If the output file is already there you just skip the step (and if you change the pipeline you need to delete the output files).\noutput_dir = pathlib.Path(...)\npath = output_dir / key\nif path.exists():\n    continue\n...\nwrite(data, path)\nNote that it’s important to write to the file in one statement; having incomplete data written to this file would mean it never gets fixed and leads to more problems. If you need to serialise data as you go along you could do it at a directory level and have an empty DONE file to indicate the process was successfully completed.\nAn interesting way to do this at a more granular level with libraries like shelve or diskcache. These allow you to back a dictionary with a filestore or a SQLite database respectively. For an expensive computation you could use this to memoise a function, for example:\ndef transform(key):\n  with shelve.open('transform') as cache:\n    if key not in cache:\n      cache[key] = _transform(key)\n    return cache[key]\nIt could be useful to make this a decorator like memozo and percache. Though diskcache seems like the strongest solution; being process safe for parallel processing, and implementing expiration (useful for infrequently updated resources) among other things."
  },
  {
    "objectID": "pandas-aggregate-quantile/index.html",
    "href": "pandas-aggregate-quantile/index.html",
    "title": "Aggregating Quantiles with Pandas",
    "section": "",
    "text": "Suppose you have some data on avocado prices containing the year and the price in a dataframe df. If you want to calculate the 25th percentile of price you could run df.price.quantile(0.25). If you wanted to calculate the median of price per year you could run df.groupby('year').agg(med_price=('price', 'median')). But what if you wanted to calculate the 25th percentile of price per year?\nYou could define a function percentile25, but defining all those functions gets annoying and slow if you calculate lots of percentiles. You could define a function that takes a percentile and returns a percentile function, but these inner functions create confusing stack traces and can’t be pickled.\nA better solution is to use a class, that can act just like a function using the __call__ parameter. This one works on Pandas Dataframes and Series:\nclass Quantile:\n    def __init__(self, q):\n        self.q = q\n        \n    def __call__(self, x):\n        return x.quantile(self.q)\n        # Or using numpy\n        # return np.quantile(x.dropna(), self.q)\nThen to calculate the quartiles of price per year you could run\n(\ndf\n.groupby('year')\n.agg(price_p25 = ('price', Quantile(0.25)),\n     price_p50 = ('price', Quantile(0.50)),\n     price_p75 = ('price', Quantile(0.75)))\n)"
  },
  {
    "objectID": "html-comment-regexp/index.html",
    "href": "html-comment-regexp/index.html",
    "title": "Regular Expression for HTML Comments",
    "section": "",
    "text": "Comments must have the following format:\n\nThe string \"<!--\".\nOptionally, text, with the additional restriction that the text must not start with the string \">\", nor start with the string \"->\", nor contain the strings \"<!--\", \"-->\", or \"--!>\", nor end with the string \"<!-\".\nThe string \"-->\".\n\nThe text is allowed to end with the string \"<!\", as in <!--My favorite operators are > and <!-->.\n\nA starting point for this is the regular expression <!--.*?-->, but this doesn’t cover all of the exclusions. For parsing it’s generally fine to be a bit more liberal, but I’m trying to generate HTML and so I want to be stricter. The correct regular expression is <!--(?!-?>)(?!.*--!>)(?!.*<!--(?!>)).*?(?<!<!-)-->; the rest of this post aims to explain how you might get this.\nWriting these kinds of regular expressions can be hard; fixing one case can break the others. It’s good to work with a set of test cases to check. Here’s a set I continually tested against.\ncorrect = [\n    \"<!---->\",\n    \"<!----->\",\n    \"<!--<-->\",\n    \"<!--<!-->\",\n]\n\nwrong = [\n    \"<!-->-->\",\n    \"<!--->-->\",\n    \"<!-- <!-- -->\",\n    \"<!-- --!> -->\",\n    \"<!--<!--->\",\n    \"<!---->-->\", # Matches <!---->\n    \"<!--->\",\n]\nThe first modification we can make is to remove those starting with > or -> using a negative lookahead (?!-?>) at the start of the string. It’s a bit confusing because the special regex characters are similar to the HTML comment characters; the (?! signals the string is a negative lookahead, and the -?> says it should reject an optional - followed by >. To ensure it doesn’t end with <!- we can use a negative lookbehind at the end of the string (?<!<!-) (similarly the (?<! denote the negative lookbehind so it rejects strings preceeded by <!-). We don’t need to worry about \"-->\" inside the string since we will stop when we hit these tokens. The only thing left is the forbidden containments: \"<!--\", and \"--!>\",\nOne way to handle containments is with the negative lookahead with a wildcard prefix, for example (?!.*<!--). However this means it will search through the whole document for this string; so it won’t match on a pair of comments <!----> <!---->. Instead we need to be more selective than .*?. Any characters other than - and < are alright so we can safely use [^<-]*. And a - is fine as long as it’s not followed by -!>, which gives -(?!-!>). And < is fine as long as it’s not followed by !--, which gives <(?!!--). So this gives us ([^<-]|-(?!-!>)|-(?!-!>)\nWhen I run everything so far I lose one of our correct expressions <!--<!-->. This is because in our last rule when we parse the second < we see !--, not noticing that the second hyphen is part of the comment close. So we can nest this condition as a negative lookahead inside the negative lookahead (?!>).\nLet’s put this all together in a Python regular expression using the verbose flag:\nimport re\n\nhtml_comment = re.compile(r\"\"\"\n<!--              # Open comment\n(?!-?>)           # Can't start with - or ->\n\n(?:\n  [^<-]         |  # Any character other < or -  \n  -(?!-!>)      |  # - not followed by -!>\n  <(?!!--(?!>)) |  # < not followed by !-- (except at end)\n)*?\n\n(?<!<!-)          # Can't end with <!-\n-->               # End comment\n\"\"\", re.VERBOSE)\nThis is pretty complicated; how do we know we didn’t miss any examples? We can try using property based testing with Hypothesis.\nLet’s rewrite whether a string is a comment based on the rules above. Since the rules are mostly about excluding comments if it’s not a comment we will return an integer to identify the rule it failed on to make debugging easier. If it is a comment we will return None.\nfrom typing import Optional\n\ndef not_comment(s: str) -> Optional[int]:\n    comment_start = '<!--'\n    comment_end = '-->'\n    \n    if not s.startswith(comment_start):\n        return 1\n    \n    if not s[len(comment_start):].endswith(comment_end):\n        return 2\n    inner = s[len(comment_start):-len(comment_end)]\n    \n    if inner.startswith(\">\"):\n        return 3\n    if inner.startswith(\"->\"):\n        return 4\n    if \"<!--\" in inner:\n        return 5\n    if  \"-->\" in inner:\n        return 6\n    if \"--!>\" in inner:\n        return 7\n    if inner.endswith(\"<!-\"):\n        return 8\n    \n    return None\nWe can then test whether these two give the same results. While we could test on any random string the search space for counterexamples is huge. Instead we’ll test for things close to a regex, starting with <!- and ending with ->. Moreover we can test all the cases with just the characters <!-> since the rules only apply to them. So we’ll generate examples from a regex <!-[<!->]*->:\nfrom hypothesis import given\nimport hypothesis.strategies as st\n\n@given(st.from_regex(r\"<!-[<!->]*->\", fullmatch=True))\ndef test_comment(comment):\n    result = html_comment.match(comment) is None\n    expected = not_comment(comment)\n    assert result == bool(expected)\nRuninng this with pytest I don’t get any failures (and I do get failures when I delete rules). Another test is that nothing after a comment should change the match (the wildcard negative lookaheads violated this). We can test this in hypothesis too:\nfrom hypothesis import assume\n\n@given(st.from_regex(r\"<!--[<!->]*-->\", fullmatch=True),\n       st.text(\"<!->\"))\ndef test_comment(comment, extra):\n    assume(not not_comment(comment))\n\n    result = html_comment.match(comment + extra)\n    result = result.group() if result else None\n    \n    expected = html_comment.match(comment)\n    expected = expected.group() if expected else None\n    \n    assert result == expected\nThis also succeeds in pytest, but using the negative lookeaheads gives useful failures. So with all this I’m pretty confident the regular expression <!--(?!-?>)(?:[^<-]|<(?!!--(?!>))|-(?!-!>))*?(?<!<!-)--> matches HTML comments as per the specification."
  }
]