<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-08-07">

<title>skeptric - Priors as Regularisation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">skeptric</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/EdwardJRoss"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Priors as Regularisation</h1>
  <div class="quarto-categories">
    <div class="quarto-category">data</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 7, 2021</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>In Bayesian statistics you have to choose a prior distribution for the parameters to combine with the data to get a posterior distribution. Choosing a tight prior, assuming that the parameters should live in a particular space, reduces the impact of the data on the posterior estimates. This is just like <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularisation</a> in machine learning where adding a penalty to the loss function prevents over-fitting. This is more than just an analogy, and this article will explore a couple of cases with constant regression and classification.</p>
<p>A typical machine learning approach to regression is to minimise the root mean squared error. A probabilistic perspective for this is to consider the regression <span class="math inline">\(y = f_\theta(X) + \epsilon\)</span>, where y is the outcome, X are the predictors, <span class="math inline">\(f_\theta\)</span> is a function parameterised by <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\epsilon\)</span> is the error term. If we assume that <span class="math inline">\(\epsilon \in N(0, \sigma^2)\)</span> is normally distributed, this is equivalent to saying that <span class="math inline">\(y \in N(f_\theta^2(X), \sigma^2)\)</span>. We then need to pick the most likely parameters <span class="math inline">\(\theta\)</span> given the data.</p>
<p>The Bayesian perspective on this is if we have a prior on the parameters <span class="math inline">\(p(\theta)\)</span>, and data <span class="math inline">\(X_i, y_i\)</span> then the posterior estimate is <span class="math inline">\(p(\theta \vert X_i, y_i) = \frac{p(X_i, y_i \vert \theta) p(\theta)}{p(\{X_i,y_i\}_i)}\)</span>. In Bayesian statistics we estimate the whole distribution, but we can focus on the maximum likelihood estimator, the value of <span class="math inline">\(\theta\)</span> that maximises the posterior probability. Since the logarithm is a monotonic function, the maximum likelihood occurs as the same point as the maximum log likelihood. Taking the logarithm and plugging in the normal distribution for <span class="math inline">\(p(X,y \vert \theta)\)</span> gives <span class="math inline">\(l(\theta, \sigma) = -\frac{1}{2\sigma^2} \sum_{i=1}^{N} (f_{\theta}(X_i) - y_i)^2 + \log(p(\theta)) - N \log(\sigma) + c\)</span> for some constant c. In the case of a flat prior, <span class="math inline">\(p(\theta) = 1\)</span> then the maximum likelihood estimator is equivalent to minimising the (root) mean squared error. However in general the prior acts as a regularisation; for example if we take a prior that the parameters are normally distributed it reduces to <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov Regularisation</a>. However we could pick other prior <a href="../distribution-between-mean-median">distributions</a> to recover an Lᵖ regularisation, and in particular a <a href="https://en.wikipedia.org/wiki/Laplace_distribution">Laplace distribution</a> recovers the <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a>.</p>
<p>There’s more here, in Bayesian statistics people tend to use a Horseshoe Prior instead of a Laplace Distribution, and Michael Betancourt has an article on my reading list on <a href="https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html">Bayes Sparse Regression</a> that goes through the trade-offs with different regularising priors.</p>
<section id="binary-classification" class="level1">
<h1>Binary classification</h1>
<p>Similar ideas can be applied in Binary Classification, here the metric is typically <a href="https://en.wikipedia.org/wiki/Cross_entropy">Binary Cross Entropy</a>. From a probabilistic perspective we can assume the data comes from a Binomial distribution <span class="math inline">\(y \in B(f_\theta(X))\)</span>. Here <span class="math inline">\(p(X_i, y_i \vert \theta) = f_\theta(X_i)^{y_i} (1 - f_\theta(X_i))^{1-y_i}\)</span> (keeping in mind that <span class="math inline">\(y_i\)</span> can only take the values 0 or 1). Then, as in the normal regression case, we can find the maximum likelihood estimator by minimising the log likelihood <span class="math inline">\(l(\theta) = \sum_{y_i = 1} \log(f_\theta (X_i)) + \sum_{y_i=0} \log(1 - f_\theta(X_i) + \log(p(\theta)) + c\)</span>. With a flat prior this maximising the log likelihood is equivalent to minimising the Binary Cross Entropy.</p>
<p>Consider in particular the <a href="../constant-model">constant model</a> <span class="math inline">\(f_\theta(X_i) = \theta\)</span>, where this reduces to <span class="math inline">\(l(\theta) = s \log(\theta) + (N-s) \log(1-\theta) + \log(p(\theta))\)</span>, where s is the number of successes and N is the total number of trials. A bit of calculus and algebra shows that with a flat prior this is maximised when <span class="math inline">\(\hat{\theta} = \frac{s}{N}\)</span>.</p>
<p>One problem with this is the <a href="../bernoulli-binomial">variance of the binomial</a> is <span class="math inline">\(\sqrt{\frac{\theta(1-\theta)}{N}}\)</span>, and so if we have 0 or N successes the maximum likelihood estimate for the variance is 0, which in most cases isn’t right - we’re not going to be exactly zero. A method for handling this, which I learned in the book Regression and Other Stories, is to set a prior of <span class="math inline">\({\rm Beta}(3,3)\)</span> which is equivalent to adding 4 extra trials with 2 successes. Then the maximum likely estimate for the parameter is <span class="math inline">\(\hat{\theta} = \frac{s+2}{N+4}\)</span> and the variance will always be non-zero.</p>
<p>In the log likelihood this adds a penalty of <span class="math inline">\(\log(\theta^2 (1-\theta)^2) + c'\)</span>, for some constant <span class="math inline">\(c'\)</span>. Rewriting <span class="math inline">\(\psi = \theta - \frac{1}{2}\)</span> and rearranging gives the penalty, up to a constant, as <span class="math inline">\(2 \log(\frac{1}{4} - \psi^2)\)</span>. For small <span class="math inline">\(\psi\)</span> we can do a Taylor expansion to get <span class="math inline">\(-8 \psi^2 = -8 (\theta - \frac{1}{2})^2\)</span>. So this transformation is similar to a <span class="math inline">\(l^2\)</span> penalty (I suspect this is for the same reason a Binomial converges to a Gaussian for large samples and moderate probabilities).</p>
<p>What’s interesting here is the Beta prior gives a more reasonable and understandable regularisation than <span class="math inline">\(l^2\)</span> regularisation, especially for probabilities close to 0 or 1. I would never have thought of a log Beta penalty, but thinking of it as a prior it makes really good sense. On the other hand being able to switch to a maximum likelihood, and thinking of the prior as a penalty, makes things much quicker to calculate than trying to estimate the whole posterior. There’s a Wikipedia article on <a href="https://en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization">Bayesian interpretation of Kernel Regularisation</a> It’s useful being able to switch between the two viewpoints.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>