<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2019-09-29">

<title>skeptric - Constant Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">skeptric</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/EdwardJRoss"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Constant Models</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 29, 2019</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>When predicting outcomes using machine learning it’s always useful to have a baseline to compare results against. A simple baseline is the best <em>constant model</em>; that is a model that gives the same prediction for any input. This is a really simple check to perform against any dataset, and can be informative to check across validation splits.</p>
<p>There are simple algorithms for finding the best constant model. For categorical predictions just evaluate every possible category to choose as the constant prediction. For continuous predictions with a <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> loss function you can use bisection, starting with the smallest and largest values of the predictor. For common loss functions the best constant model is a familiar <a href="https://en.wikipedia.org/wiki/Central_tendency#Solutions_to_variational_problems">measure of central tendency</a>.</p>
<table class="table">
<thead>
<tr class="header">
<th>Loss Function</th>
<th>Best constant</th>
<th>Minimum Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>Mode</td>
<td>Maximum proportion</td>
</tr>
<tr class="even">
<td>Cross Entropy</td>
<td>Proportion</td>
<td></td>
</tr>
<tr class="odd">
<td>Root Mean Squared Error</td>
<td>Mean</td>
<td>Standard Deviation</td>
</tr>
<tr class="even">
<td>Mean Absolute Error</td>
<td>Median</td>
<td><a href="https://en.wikipedia.org/wiki/Average_absolute_deviation#Mean_absolute_deviation_around_the_median">MAD median</a></td>
</tr>
<tr class="odd">
<td>Mean Weighted Absolute Error</td>
<td>Quantile</td>
<td></td>
</tr>
<tr class="even">
<td>Maximum Absolute Error</td>
<td>Midrange</td>
<td>Half the range</td>
</tr>
</tbody>
</table>
<p>This is useful to know when using piecewise constant models (like decision trees) because on each piece they will use these best constants. The rest of this article will explain these examples in detail and end with a general family of loss functions that covers many use cases.</p>
<section id="accuracy" class="level1">
<h1>Accuracy</h1>
<p>For predicting which category an object is in the most common metric is accuracy. The accuracy of constantly predicting a category is the number of items which have that category divided by the total number of items. The best constant model predicts the most common category (the mode), and the accuracy is the proportion of items in the most common category.</p>
<p>A claim like a model is 85% accurate may sound impressive, but if the most common category covers 83% of cases it’s probably not a great achievement. There are different metrics (like <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">ROC AUC</a>) and sampling methods (like balanced resampling) that can remove this advantage to predicting common cases, but just being aware of the best constant accuracy is helpful in understanding how much better a model is performing.</p>
</section>
<section id="cross-entropy" class="level1">
<h1>Cross Entropy</h1>
<p>A more discerning metric for categorical data is the cross-entropy. If <code>P</code> is a <code>N x C</code> matrix of the probabilities of each of the <code>C</code> categories and <code>y</code> are the actual categories, as a one-hot encoded <code>C x N</code> matrix, then the probability for the actual category is <code>pred_prob = apply(P * y, 1, sum)</code>. The cross-entropy is just the sum of the logs of the predicted probabilities for the actual categories, <code>sum(log(pred_prob))</code>.</p>
<p>The best constant <code>1 x C</code> vector here is where each category is just the number of times it occurred divided by the total number of occurrences. That is <code>apply(y, 2, sum) / sum(y)</code>; and the value is then just the log of the probability of the outcome.</p>
<p>In the special case of binary classification, when <code>C</code> is 2, we can just focus on the case of positive outcomes. Then given a 0-1 encoded <code>y</code>, the best constant is <code>p = sum(y)/length(y)</code>, and the cross-entropy is <code>length(y) * (p * log(p) + (1-p) * log(1-p))</code>.</p>
</section>
<section id="root-mean-squared-error" class="level1">
<h1>Root Mean Squared Error</h1>
<p>Root Mean Squared Error (RMSE) is the most common measure for regression problems. The best constant model is the mean of the values, and its RMSE is the standard deviation.</p>
<p>This is because the mean is the unique point whose differences from each data point sum to zero. Let’s denote our vector of data points as <code>x</code>, the mean as <code>xbar</code>, the standard deviation as <code>sd</code> and our constant prediction as <code>a</code>. Then the mean satisfies <code>sum(x - xbar) == 0</code> (which is just a rearrangement of the standard definition <code>xbar = sum(x) / length(x)</code>). The RMSE is given by <code>rmse(a) = sqrt(mean((x - a)^2))</code>. Expanding the quadratic around the mean gives:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rmse</span>(a) <span class="sc">==</span> <span class="fu">rmse</span>((a <span class="sc">-</span> xbar) <span class="sc">+</span> xbar)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>        <span class="sc">==</span> <span class="fu">sqrt</span>((a <span class="sc">-</span> xbar)<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>(a <span class="sc">-</span> xbar)<span class="sc">*</span><span class="fu">mean</span>(x <span class="sc">-</span> xbar) <span class="sc">+</span> <span class="fu">mean</span>((x <span class="sc">-</span> xbar)<span class="sc">^</span><span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since the middle of the sum term vanishes by definition of the mean, this leaves</p>
<p><code>rmse(a) = sqrt((a - xbar)^2 + mean((x - xbar)^2))</code></p>
<p>In mathematical notation</p>
<p><span class="math display">\[\sqrt{\frac{1}{N}\sum\limits_{i=1}^{N} (x_i - a)^2} = \sqrt{(a - \bar{x}^2) + \frac{1}{N} \sum\limits_{i=1}^{N}(x_i - a)^2}.\]</span></p>
<p>Looking at this formula it’s clear that the root mean square error is minimum at the mean, and at the mean it’s value is <code>sqrt(mean(x-xbar)^2)</code>, which is the definition of the standard deviation.</p>
<p>It’s actually common to use the constant model as a benchmark in this context; in terms of the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><em>coefficient of determination</em></a>, or more commonly <em>R²</em>. It is defined as <code>R2 = 1 - (RMSE / sd)^2</code>, where <code>sd</code> is the standard deviation. So the best constant model has an <em>R²</em> value of 0, any better model has a positive value and a perfect model (with 0 RMSE) has a value of 1. Note that it’s possible to have a negative <em>R²</em> value if the prediction is worse than the mean.</p>
</section>
<section id="mean-absolute-error" class="level1">
<h1>Mean Absolute Error</h1>
<p>The Mean Absolute Error (MAE) is a robust measure for regression; it’s not sensitive to outliers. The best constant prediction is the median of the values.</p>
<p>To see why this is consider how the MAE, which is the average of the absolute distances, changes as we move the constant prediction point. As in the diagram below we have m data points to the left of our “test point” and n data points to the right.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/absolute_deviation_example.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Mean Absolute Error Diagram</figcaption><p></p>
</figure>
</div>
<p>As we move a distance ε to the left (without crossing any data points) the constant prediction is ε closer to each of the m points on the left, and ε further from each of the n points on the right. So the mean average error changes by <code>ε * (n - m) / (n+m)</code>. This is decreasing as long as we’re moving the prediction towards more points (m &gt; n), and will be minimal when m = n. So the minimum occurs when there are an equal number of data points to the left or the right, which is the median.</p>
<p>Note that there is some ambiguity here when the number of points is even. For example in the dataset 1, 2, 3, 4 any number between 2 and 3 will minimise the MAE; typically we take the midpoint 2.5.</p>
<p>The minimum value of the MAE isn’t so familiar, it’s the <a href="https://en.wikipedia.org/wiki/Average_absolute_deviation#Mean_absolute_deviation_around_the_median">Mean Average Deviation of the median</a> (or MAD Median for short). However it’s a useful benchmark; analogous to the coefficient of determination we could consider a measure like <code>1 - MAE / mad_median</code> to assess improvement over the constant model.</p>
<section id="mean-weighted-absolute-error" class="level2">
<h2 class="anchored" data-anchor-id="mean-weighted-absolute-error">Mean Weighted Absolute Error</h2>
<p>Sometimes in prediction tasks it’s better to conservatively over- or under-predict to temper expectations. One way to do this is to penalise errors in one direction greater than errors in the other. The Mean Weighted Average Error applies this to the MAE:</p>
<p><code>mwae(w, a) = w * (x &gt;= a) * (x - a) - (1 - w) * (x &lt; a) * (x - a)</code></p>
<p><span class="math display">\[\rm{MWAE(a; w)} = \frac{1}{N} \sum\limits_{i=1}^{N} \begin{cases}  w (x_i - a) &amp; x_i \geq a  \\ (1-w) (a - x_i) &amp; x_i \lt a  \end{cases}\]</span></p>
<p>The constant model that minimises MWAE is the wth quantile. To understand why, as with MAE, consider how the MWAE changes as the prediction moves a distance ε towards m points away from n points.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/absolute_deviation_example.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Mean Absolute Error Diagram</figcaption><p></p>
</figure>
</div>
<p>Then the prediction is ε further from n points which increases MWAE by <code>w * n * ε / (m + n)</code> and ε further from m points which decreases MWAE by <code>(1 - w) * m * ε / (m + n)</code>. This will be minimised when the two changes balance; when we can’t decrease it by moving in one particular direction (because MWAE is convex). This happens when <code>w * n == (1 - w) * m</code>, that is when <code>w == m / (m + n)</code>. So it’s minimised when the fraction of data to the left of the test point is <code>w</code>. In particular when <code>w = 0.5</code> then <code>MWAE == MAE / 2</code> and the best constant model is the median.</p>
<p>Note that again this is ambiguous; we can treat it the same way as the median by averaging the minimiser of quantiles taken from limits above and below.</p>
</section>
</section>
<section id="generalisation-lᵖ-error" class="level1">
<h1>Generalisation: Lᵖ Error</h1>
<p>Almost all of these example metrics are a special case of a weighted <a href="https://en.wikipedia.org/wiki/Lp_space#The_p-norm_in_finite_dimensions">Lᵖ error</a>. The Lᵖ error is given by</p>
<p><code>lp(a) = mean((x - a)^p) ^ (1/p)</code></p>
<p><span class="math display">\[L^p(a) = \sqrt[p]{\frac{1}{N}\sum\limits_{i=1}^{N} {\left\Vert{x_i - a}\right\Vert ^p}}\]</span></p>
<p>In particular when p is 1 we get the Mean Average Error and when p is 2 we get the Root Mean Square Error.</p>
<p>As <span class="math inline">\(p \rightarrow \infty\)</span> this converges to the maximum norm: <code>l_infinity(a) = max(x - a)</code>. This depends only on the furthest points, and is minimised at the midrange, halfway between the maximum and minimum point, and the minimum value is half the range. There are a whole range of metrics between the MAE at <span class="math inline">\(p=1\)</span>, which is completely insensitive to outliers, and the Maximum Norm at <span class="math inline">\(p = \infty\)</span> which depends only on the most extreme outliers. As p increases the best constant moves continuously towards the outliers.</p>
<p>We can further generalise this by weighting the norm as we did for MWAE:</p>
<p><code>weighted_norm(w, x) = sum(w * (x &gt;= 0) * x - (1 - w) * (x &lt; 0) * x)</code></p>
<p><span class="math display">\[\left\Vert x \right\Vert_w = \sum\limits_{i=1}^{N} \begin{cases}  w x_i &amp; x_i \geq 0  \\ -(1-w) x_i &amp; x_i \lt 0  \end{cases}\]</span></p>
<p>It’s also possible to generate the <a href="https://en.wikipedia.org/wiki/Truncated_mean">trimmed mean</a> or <a href="https://en.wikipedia.org/wiki/Winsorized_mean">Winsorized mean</a> by modifying how the metric treats the most extreme points; removing or capping the most extreme values. There are certainly metrics that don’t fit in this framework, but this connection between a metric and best constant statistic gives simple benchmarks to common models and suggests a family metrics for different regression problems.</p>
<p>Next time you look at any predictive model first ask: “How much better is this than the best constant model?”</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>