<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-10-12">

<title>skeptric - Estimating Group Means with Empirical Bayes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">skeptric</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/EdwardJRoss"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Estimating Group Means with Empirical Bayes</h1>
  <div class="quarto-categories">
    <div class="quarto-category">maths</div>
    <div class="quarto-category">data</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 12, 2021</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>When calculating the averages of lots of different groups it doesn’t make sense to treat the groups as independent, but to <em>pool information</em> across groups, especially on groups with little data. One way to do this is to build a model on covariates of the groups or on <a href="../categorical-embeddings">categorical embeddings</a> to use information from other observations to inform that observation. Surprisingly <a href="../stein-paradox">Stein’s Paradox</a> says if we’re trying to minimise the root mean square error, have only one observation per group, and they’re all normally distributed with the same standard deviation we’re always better off shrinking the means towards a common point than treating them separately (even if they’re completely unrelated quantities!). Many of these conditions can be weakened, and in practice when estimating averages of related quantities the gain from shrinking (even when we don’t have covariates) could be substantial.</p>
<p>Calculating averages (or proportions) in groups is a very common procedure for analysts, which is why <code>GROUP BY</code> is an essential feature of SQL. Some examples are calculating the average sales by salesperson, the conversion by referrer, <a href="../job-ad-title-salary">salary by job ad title</a>, the success rate of a surgery by surgeon, <a href="http://doingbayesiandataanalysis.blogspot.com/2012/11/shrinkage-in-multi-level-hierarchical.html">hit-rates of baseball players</a>, average runs of cricket players, or student test scores by classroom. The ordinary procedure is to treat each group independently, and sum and divide, but this can lead to very unstable estimates for small groups. An extreme example is calculating a proportion with only 1 observation which is necessarily 0% or 100% and so always will be at the top or the bottom. A common approach to these is to filter out groups with too few observations, and a less common approach is to use <a href="https://www.evanmiller.org/how-not-to-sort-by-average-rating.html">confidence intervals</a> to represent the uncertainty. A less common approach is to use information from other groups to inform the estimate for that group; but is used and known by names like random effects models (or more generally mixed-effects models), multilevel models or hierarchical models (among others).</p>
<section id="empirical-bayesian-model" class="level1">
<h1>Empirical Bayesian Model</h1>
<p>This can be easily motivated by a Bayesian approach, following chapters 6 and 7 of <a href="https://web.stanford.edu/~hastie/CASI/">Computer Age of Statistical Inference</a>. Assume that the observations in each group are normally distributed with an unknown mean, <span class="math inline">\(\theta_i\)</span> for the ith group, and known standard deviation <span class="math inline">\(\sigma_i\)</span> (more on how we could know this later), so a random draw X from group i satisfies <span class="math inline">\(X \sim {\mathcal N}(\theta_i, \sigma_i^2)\)</span>. For the sake of argument assume we’ve got one observation from each group; in general if we’ve got several observations <span class="math inline">\(X_{i;1}, \ldots, X_{i;N_i} \sim {\mathcal N}(\theta_i, \sigma_i^2)\)</span> we can replace it with their average <span class="math inline">\(X_i := \frac{1}{N_i} \sum_{j=1}^{N_i} X_{i;j} \sim {\mathcal N}(\theta_i, \sigma_i^2/N_i)\)</span>. If we’ve got a binomial proportion we can use the <a href="https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation">normal approximation</a> or the <a href="https://en.wikipedia.org/wiki/Binomial_distribution#Arcsine_method">arcsine transformation</a> to treat it this way.</p>
<p>So after some rewriting we have one observation in each group <span class="math inline">\(X_i \sim {\mathcal N}(\theta_i, \sigma_i^2)\)</span> and are trying to estimate the <span class="math inline">\(\theta_i\)</span>. The maximum likelihood estimate (and the best unbiased estimate) is simply <span class="math inline">\(\hat\theta_i = X_i\)</span>, but that assumes we know nothing about the means. Suppose we had some prior distribution for the <span class="math inline">\(\theta_i \sim g(\cdot)\)</span>; then we could use Bayes rule to get a posterior estimate given the data:</p>
<p><span class="math display">\[\begin{align}
{\mathbb P}(\theta_i \vert X_i) &amp;= \frac{{\mathbb P}(X_i \vert \theta_i) {\mathbb P}(\theta_i)}{\int {\mathbb P}(X_i \vert \theta_i){\mathbb P}(\theta_i)\,{\rm d}\theta_i}  \\
&amp;= c e^{-(X_i - \theta_i)^2/(2\sigma_i^2)} g(\theta_i)
\end{align}\]</span></p>
<p>where c is a normalising constant independent of theta. We can then summarise our estimate by a measure of centrality such as the mean, median or mode (maximum likelihood estimate). The default method of count and divide is simply choosing an (improper) flat prior separately for each of K groups and using the maximum likelihood. Could we do better by choosing a prior and pooling the information?</p>
<p>If we used our knowledge of the problem we could at least form a weak prior; the batting average of a cricket player is unlikely to be 0 or over 100 (although <a href="https://en.wikipedia.org/wiki/Don_Bradman">Bradman</a> got close), a salary is unlikely to be below minimum wage (although it happens) or above a million dollars (with a few exceptions). We could use this information to form a very weak prior that wouldn’t change the estimates much, but if we have enough groups it could make a substantial difference to the estimates (this is the realm of multilevel models). Another approach is to directly <em>estimate the prior from the data</em>, an Empirical Bayes approach; we wouldn’t want to overfit to our dataset but with lots of groups and a low dimensional model it can work strikingly well. We could look at a histogram of our maximum likelihood estimates of <span class="math inline">\(\theta_i\)</span> and guess or model the prior function g.</p>
<p>In particular let us assume that our means are themselves normally distributed with some unknown mean and standard deviation, <span class="math inline">\(\theta_i \sim {\mathbb N}(M, A)\)</span>. This assumption may or may not be reasonable for a given dataset; a rough check would be to plot the group means and see what the distribution looks like and choose a parametric model for it. How much does the prior change our estimates and how to we estimate the parameters from the data?</p>
</section>
<section id="posterior-estimates" class="level1">
<h1>Posterior Estimates</h1>
<p>To get a posterior estimate for the group mean <span class="math inline">\(\theta_i\)</span> we apply Bayes rule</p>
<p><span class="math display">\[\begin{align}
{\mathbb P}(\theta_i \vert X_i) &amp;= \frac{{\mathbb P} (X_i \vert \theta_i) {\mathbb P} (\theta_i)}{\int {{\mathbb P} (X_i \vert \theta_i) {\mathbb P} (\theta_i)} \,d\theta_i} \\
&amp;= c_1 e^{-(X_i - \theta_i)^2/2 \sigma_i^2} e^{-(\theta_i - M)^2/2A} \\
&amp;= c_2 e^{-(\theta_i - B_i (X/\sigma_i^2 + M/A))^2/2B_i}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are (different) normalising constants, and B is half the harmonic mean of the variances, that is <span class="math inline">\(B_i = \frac{1}{1/\sigma_i^2 + 1/A}\)</span> (the last line comes from expanding the exponent and completing the square for <span class="math inline">\(\theta_i\)</span>). So we get <span class="math inline">\(\theta_i \vert X_i \sim {\mathcal N}\left(\frac{X_i/\sigma_i^2 + M/A}{1/\sigma_i^2 + 1/A}, \frac{1}{1/\sigma_i^2 + 1/A}\right)\)</span>. Note the most likely value of <span class="math inline">\(\theta_i\)</span> is a weighted average between <span class="math inline">\(X_i\)</span> and M, weighted by the inverse variances (sometimes called precision). So in particular for <span class="math inline">\(\sigma_i^2 \ll A\)</span> then the estimate of <span class="math inline">\(\theta_i\)</span> is very close to <span class="math inline">\(X_i\)</span>, and for <span class="math inline">\(A \ll \sigma_i^2\)</span> it’s very close to M. Also as either A or <span class="math inline">\(\sigma_i^2\)</span> gets small the uncertainty in <span class="math inline">\(\theta_i\)</span> gets small.</p>
<p>Another convenient way to write the most likely estimate is <span class="math inline">\(\frac{X_i/\sigma_i^2 + M/A}{1/\sigma_i^2 + 1/A} = X_i + (M-X_i)\frac{\sigma_i^2}{A + \sigma_i^2}\)</span>, leaving us to estimate M and <span class="math inline">\(\frac{1}{A + \sigma_i^2}\)</span>.</p>
</section>
<section id="estimating-the-parameters" class="level1">
<h1>Estimating the parameters</h1>
<p>We don’t observe the <span class="math inline">\(\theta_i\)</span> directly, but through only their influence on the <span class="math inline">\(X_i\)</span>. So to estimate M and A it makes sense to look at the marginal distribution f of the <span class="math inline">\(X_i\)</span> over <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[\begin{align}
{\mathbb P}(X_i \vert M, A) &amp;= \int {\mathbb P} (X_i, \theta_i \vert M, A) \, {\rm d}\theta_i \\
&amp;= \int {\mathbb P}(X_i \vert \theta_i) {\mathbb P}(\theta_i \vert M, A) \, {\rm d}\theta_i\\
&amp;= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma_i^2}}e^{-(X_i-\theta_i)^2/2\sigma_i^2}\frac{1}{\sqrt {2 \pi A}}e^{-(\theta_i - M)^2/2A} \\
&amp;= \frac{1}{\sqrt{2 \pi \sigma_i^2} \sqrt{2 \pi A}} e^{-(X-M)^2/(2(A+\sigma_i^2))} \int_{-\infty}^{\infty} e^{-(\theta_i - B_i (X/S + M/A))^2/2B_i} \\
&amp;= \frac{1}{\sqrt{2 \pi (A + \sigma_i^2)}} e^{-(X-M)^2/(2(A+\sigma_i^2))}
\end{align}\]</span></p>
<p>where the fourth line comes from completing the squares of the exponents, and So we have marginally <span class="math inline">\(X_i \sim {\mathcal N}(M, A + \sigma_i^2)\)</span>. This is a good way to check the assumptions; the points <span class="math inline">\(X_i\)</span> (in practice the group means) should be approximately normally distributed.</p>
<p>In a Bayesian approach the next step would be to set a prior on M and A and then come up with posterior estimates based on the data (for example with Markov Chain Monte Carlo methods). The difference with an Empirical Bayes approach is we estimate M and A directly with maximum likelihood; when there’s a lot of signal in the data so that the resulting posterior is sharply peaked they will give approximately the same result as a fully Bayesian approach with weak priors, but the Empirical Bayes approach is a lot more computationally efficient.</p>
<p>The most likely values <span class="math inline">\(\hat A\)</span> and <span class="math inline">\(\hat M\)</span>, are where the probability is maximised, or equivalently where the negative log likelihood is minimised. The negative log likelihood is:</p>
<p><span class="math display">\[-l(M, A) = \frac{1}{2}\sum_{i=1}^{K} \ln(2 \pi) + \ln(A + \sigma_i^2) + \frac{(X_i - M)^2}{A + \sigma_i^2}\]</span></p>
<p>The optima occur where the partial derivatives are zero; the derivatives are:</p>
<p><span class="math display">\[\begin{align}
-\frac{\partial l}{\partial M}(M,A) &amp;= \sum_{i=1}^{K} \frac{(X_i - M)}{A + \sigma_i^2} \\
-\frac{\partial l}{\partial A}(M,A) &amp;= \frac{1}{2}\sum_{i=1}^{K} \frac{1}{A + \sigma_i^2} - \frac{(X_i - M)^2}{(A + \sigma_i^2)^2}
\end{align}\]</span></p>
<p>This yields coupled equations for <span class="math inline">\(\hat M\)</span> and <span class="math inline">\(\hat A\)</span></p>
<p><span class="math display">\[\begin{align}
\hat M &amp;= \left.\sum_{i=1}^{K} \frac{X_i}{\hat A + \sigma_i^2} \middle/ \sum_{i=1}^{K} \frac{1}{\hat A + \sigma_i^2} \right. \\
0 &amp;= \sum_{i=1}^{K} \frac{1}{\hat A + \sigma_i^2} -  \frac{(X_i - \hat M)^2}{(\hat A + \sigma_i^2)^2}
\end{align}\]</span></p>
<p>The midpoint is the weighted average of the data, weighted by the inverse <em>total</em> variance. Substituting this into the second equation we get an equation in terms of <span class="math inline">\(\hat A\)</span> alone, but not one that I know how to solve analytically.</p>
</section>
<section id="putting-it-all-together" class="level1">
<h1>Putting it all together</h1>
<p>To recap, we started assuming that we’ve got K points <span class="math inline">\(X_i \sim {\mathcal N}(\theta_i, \sigma_i^2)\)</span>, and we know the standard deviations <span class="math inline">\(\sigma_i\)</span> and want to estimate the means <span class="math inline">\(\theta_i\)</span>. This could arise in calculating the means of groups; when we’ve got enough points by the Central Limit Theorem the group sample means will be approximately normally distributed and their variances inversely proportional to the number of points (and for smaller groups for normal variables and binomial proportions). If we further assume that the mean parameters are normally distributed <span class="math inline">\(\theta_i \sim {\mathcal N}(M, A)\)</span> then the best estimate for them is</p>
<p><span class="math display">\[\begin{align}
\hat\theta_i &amp;= \frac{X_i/\sigma_i^2 + M/A}{1/\sigma_i^2 + 1/A} \\
&amp; = \omega_i M + (1-\omega_i) X_i
\end{align}\]</span></p>
<p>where <span class="math inline">\(\omega_i = \frac{1/A}{1/\sigma_i^2 + 1/A} = \frac{\sigma_i^2}{\sigma_i^2 + A}\)</span> and the best estimates for M and A are given by the equations</p>
<p><span class="math display">\[\begin{align}
\hat M &amp;= \left.\sum_{i=1}^{K} \frac{X_i}{\hat A + \sigma_i^2} \middle/ \sum_{i=1}^{K} \frac{1}{\hat A + \sigma_i^2} \right. \\
0 &amp;= \sum_{i=1}^{K} \frac{1}{\hat A + \sigma_i^2} -  \frac{(X_i - \hat M)^2}{(\hat A + \sigma_i^2)^2}
\end{align}\]</span></p>
<p>So the <span class="math inline">\(\theta_i\)</span> lie somewhere between the data point for the group and M, the (weighted) midpoint for all the data. Let’s analyse these in some special cases.</p>
<section id="equal-within-group-variance" class="level2">
<h2 class="anchored" data-anchor-id="equal-within-group-variance">Equal within-group variance</h2>
<p>Suppose all the points <span class="math inline">\(X_i\)</span> have the same standard deviation, <span class="math inline">\(\sigma_i = \sigma\)</span>, and so we have <span class="math inline">\(X_i \sim {\mathcal N}(\theta_i, \sigma)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\hat M &amp;= \frac{1}{K} \sum_{i=1}^{K} X_i =: \bar X \\
\hat A + \sigma^2 &amp;= \frac{1}{K} \sum_{i=1}^{K} (X_i - \bar X)^2 =: \frac{S}{K} \\
\hat \omega &amp;= \frac{K\sigma^2}{S} \\
\hat \theta_i &amp;= \omega \bar X + (1 - \omega) X_i
\end{align}\]</span></p>
<p>These estimators are biased; since <span class="math inline">\(X_i \sim {\mathcal N}(M, A + \sigma^2)\)</span> then <span class="math inline">\(S := \sum_{i=1}^{K} (X_i - \bar X)^2 \sim (A + \sigma^2) \chi^2_{K - 1}\)</span>. Given <span class="math inline">\(Z \sim \chi^2_\nu\)</span> then</p>
<p><span class="math display">\[\begin{align}
{\mathbb E}(1/Z) &amp;= \int_0^{\infty} \frac{1}{x} \frac{ x^{\nu/2-1} e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)} \,{\rm d}x\\
&amp;= \frac{2^{\nu/2-1}\Gamma(\nu/2-1)}{2^{\nu/2}\Gamma(\nu/2)} \int_0^{\infty} \frac{ x^{(\nu-2)/2-1} e^{-x/2}}{2^{(\nu-2)/2}\Gamma((\nu-2)/2)}  \,{\rm d}x\\
&amp;= \frac{1}{2 (\nu/2 - 1)} \cdot 1 \\
&amp;= \frac{1}{\nu - 2}
\end{align}\]</span></p>
<p>so an <em>unbiased estimator</em> of the <span class="math inline">\(\theta_i\)</span> is</p>
<p><span class="math display">\[\hat \theta_i = \frac{(K - 3)\sigma^2}{S} \bar X + \left(1 - \frac{(K - 3)\sigma^2}{S} \right) X_i\]</span></p>
<p>this is precisely a James-Stein estimator (when M is known it replaces <span class="math inline">\(\bar X\)</span> and removes one degree of freedom from S, meaning we get for <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(\theta_i = \frac{K-2}{S} M + \left(1 - \frac{K-2}{S}\right) X_i\)</span>, which is the original estimator proposed by James and Stein which has lower risk than <span class="math inline">\(\hat\theta_i^{\rm MLE} = X_i\)</span> under the sole assumption <span class="math inline">\(X_i \sim {\mathcal N}(\theta_i, 1)\)</span>).</p>
<p>The crucial factor here is the ratio between the variance within groups to that between groups <span class="math inline">\(\frac{\sigma^2}{S/(K-3)}\)</span>. When this is small the estimate is close to the actual points, and when this is large the estimate is close to the overall average <span class="math inline">\(\bar X\)</span>. As we more precisely estimate the group mean the more we gain from biasing towards it, and the lower the within-group variation the less there is to benefist from biasing the estimate by shrinking.</p>
</section>
<section id="lower-within-group-variance-than-between-group" class="level2">
<h2 class="anchored" data-anchor-id="lower-within-group-variance-than-between-group">Lower within-group variance than between-group</h2>
<p>Suppose <span class="math inline">\(\sigma_i^2 \ll A\)</span> for all i. Then <span class="math inline">\(\hat \theta_i \approx X_i (1 - \sigma_i^2/A) + M \sigma_i^2 / A\)</span>, so the estimates are closer to the data points and the Maximum Likelihood Estimate. In this case the benefit of this approach will be marginal. Also <span class="math inline">\(\hat M \approx \frac{1}{K} \sum_{i=1}^{K} X_i\)</span> and <span class="math inline">\(\hat A \approx \frac{1}{K}\sum_{i=1}^{K} (X_i - \hat M)^2\)</span> as in the equal variance case.</p>
</section>
<section id="lower-between-group-variance-than-within-group" class="level2">
<h2 class="anchored" data-anchor-id="lower-between-group-variance-than-within-group">Lower between-group variance than within-group</h2>
<p>Suppose <span class="math inline">\(A \ll \sigma_i^2\)</span> for all <span class="math inline">\(\sigma_i\)</span>. Then <span class="math inline">\(\hat M \approx \left.\sum_{i=1}^{K} \frac{X_i}{\sigma_i^2} \middle/ \sum_{i=1}^{K} \frac{1}{\sigma_i^2} \right.\)</span>. In particular if the <span class="math inline">\(X_i\)</span> are formed as group averages with equal variance, so <span class="math inline">\(\sigma_i^2 = \sigma_0^2/N_i\)</span>, then <span class="math inline">\(\hat M \approx \frac{\sum_{i=1}^{K} N_i X_i}{\sum_{i=1}^{K} N_i}\)</span>, that is it’s the average of all the points themselves (instead of averages of the group centres). The means are approximately <span class="math inline">\(\hat \theta_i \approx M (1 - A/\sigma_i^2) + X_i \sigma_i^2/A\)</span> is much closer to the mean of the data than the points themselves.</p>
</section>
<section id="estimating-the-variances" class="level2">
<h2 class="anchored" data-anchor-id="estimating-the-variances">Estimating the variances</h2>
<p>We have assumed the <span class="math inline">\(\sigma_i\)</span> are known; in some cases like a Binomial distribution they are part of the model, but in practice they may need to be estimated. When the <span class="math inline">\(X_i\)</span> are formed from groups with more than one data point they can be estimated over a prior distribution in a similar way to the mean. More simply we can assume they are the same and estimate it from the mean of the sample standard deviations (correcting for sample size).</p>
<p>To solve the equations for <span class="math inline">\(\hat A\)</span> an extremely low starting value would be 0. An extremely high value would be when <span class="math inline">\(\hat A \gg \sigma_i^2\)</span>, in which case we get estimates of <span class="math inline">\(\hat A\)</span> identical to the equal variance case. In general then I’d expect the solution to be between these and could be solved by bisection (though I’m not completely sure of the upper bound, or that it has a unique solution).</p>
</section>
</section>
<section id="whats-next" class="level1">
<h1>What’s next</h1>
<p>This gives some of the theory of estimating the means of groups, but it would be good to exercise it against some practical examples. The benefits are largest when the between-group variance is lower than within-group variance, so in particular when there are lots of groups and a few noisy estimates in each group. The estimators we derived are biased, but it’s not a big issue when there are a large number of groups (say when <span class="math inline">\(K \gg 3\)</span>).</p>
<p>The amount of shrinking changes with the standard deviation which can change the ranking of the results. This provides a methodological tool that covers a substantial problem in analysis; the most extreme data points tend to come from the data with most variation. This could potentially be used to address these problems (and even construct probabilistic orderings) in a principled way.</p>
<p>It could also be interesting whether we could construct Empirical Bayes models for other distributions, in particular binomial distributions could be interesting (when some observations only have a few data points and the normal approximation breaks down).</p>
<p>Finally we could take this further and build, and estimate, multilevel models directly. However these are harder to analyse in general and a lot more computationally intensive.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>