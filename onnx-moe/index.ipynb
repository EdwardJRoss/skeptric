{
 "cells": [
  {
   "cell_type": "raw",
   "id": "822dc742-d00b-4b28-8441-b7e8389f8c01",
   "metadata": {},
   "source": [
    "---\n",
    "categories:\n",
    "  - onnx\n",
    "date: '2025-06-13T22:10:00+10:00'\n",
    "image: ./moe_onnx.jpg\n",
    "title: Exporting Nomic's Mixture of Experts model to ONNX\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e5594-5699-4545-8700-60ea2910b59f",
   "metadata": {},
   "source": [
    "Sparse [mixture of experts](https://huggingface.co/blog/moe) models can contain the parameters and information of a large model but run as fast as a much smaller model by only multiplying some of the parameters, depending on the input, on every prediciton. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) is a way to represent models independent of the framework it was trained in, making it easy to serve the model in a wide variety of languages using a run-time like [onnxruntime](https://onnxruntime.ai/).\n",
    "Unfortunately onnx doesn't have many tools for running conditional computations and so exporting a Mixture of Experts model to ONNX model isn't always easy.\n",
    "\n",
    "Nomic AI recently released a very good multilingual open source mixture-of-experts embedding model, [nomic-embed-text-v2-moe](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe). This article will go through one way to exporting this model to ONNX, working through the problems that arise on the way.\n",
    "\n",
    "::: {.callout-note}\n",
    "This document is an executable Jupyter notebook and the [soure code](https://github.com/EdwardJRoss/skeptric/tree/master/onnx-moe) is available\n",
    ":::\n",
    "\n",
    "Before we get into the details I want to give some motivation of why you might want to do this; if you're already motivated then skip ahead to the first ONNX export section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c431836-662a-4b15-a448-bed5b493317b",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Embedding models are really useful for \"semantic search\", that matches the meaning but not the exact wording of a query, making them a good complement to text matching algorithms like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25).\n",
    "\n",
    "To get things set up import some libraries including [Sentence Transformers](https://www.sbert.net/) which makes it easy to use the embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d82270f-8459-45d5-b133-f55cfc4bf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "def showwarning(message, category, filename, lineno, file=None, line=None):\n",
    "    filename = filename.replace(os.path.expanduser(\"~\"), \"\")\n",
    "    msg = warnings.WarningMessage(message, category, filename, lineno, file, line)\n",
    "    warnings._showwarnmsg_impl(msg)\n",
    "\n",
    "warnings.showwarning = showwarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29f822f-5956-4003-9104-d894194b2fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/micromamba/envs/onnx-moe/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "#| warning: false\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import re\n",
    "import traceback\n",
    "from urllib.request import urlretrieve\n",
    "import time\n",
    "\n",
    "# Display\n",
    "from IPython.display import HTML\n",
    "from html import escape\n",
    "\n",
    "# Computation Libraries\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_dir = Path('models')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a2d99-8bdd-4676-a0b6-6e659143f5f8",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e319a-64e1-4e1b-8987-79b0c7497d92",
   "metadata": {},
   "source": [
    "Any human readable text data is a good candidate for embedding; as an example we will use [Captain Cook's Journal During His First Voyage Round the World from Project Gutenberg](https://www.gutenberg.org/ebooks/8106)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4651ae97-d4e1-4dec-ad29-a972d457d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = 'https://www.gutenberg.org/cache/epub/8106/pg8106.txt'\n",
    "\n",
    "text_path = data_dir / text_url.split('/')[-1]\n",
    "\n",
    "if not text_path.exists():\n",
    "    urlretrieve(text_url, text_path)\n",
    "\n",
    "with open(text_path, 'rt') as f:\n",
    "    full_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ada0b-1bf1-4f87-9ab5-17d3b721e1c5",
   "metadata": {},
   "source": [
    "The full text is split into individual passages that can be searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "390b19ac-34bf-4ec4-90c1-140a862a38c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages = [p for p in \n",
    "            full_text.split('\\n\\n')   # All paragraphs\n",
    "                [472:1989]            # in main body text\n",
    "            if not p.startswith('[')  # except editor comments\n",
    "            and not (p == p.upper())  # and chapter headings\n",
    "            and len(p) > 35           # and very short paragraphs\n",
    "           ]\n",
    "\n",
    "len(passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ae82b-c44c-4b22-abc7-52a74bf1729b",
   "metadata": {},
   "source": [
    "The first few passages show the style of the text is succinct updates of what happened each day. There's a lot of details about the weather, which I suppose is very important on a sea voyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7703713-8f45-470b-8ade-94544b3bcce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER THAMES, Friday, May 27th, to Friday, July 29th. Moderate and fair\n",
      "weather; at 11 a.m. hoisted the Pendant, and took charge of the Ship,\n",
      "agreeable to my Commission of the 25th instant, she lying in the Bason in\n",
      "Deptford Yard. From this day to the 21st of July we were constantly\n",
      "employed in fitting the Ship, taking on board Stores and Provisions, etc.\n",
      "The same day we sailed from Deptford and anchored in Gallions reach, were\n",
      "we remained until the 30th. The transactions of Each Day, both while we\n",
      "lay here and at Deptford, are inserted in the Log Book, and as they\n",
      "contain nothing but common Occurrences, it was thought not necessary to\n",
      "insert them here.\n",
      "\n",
      "July 30th to August 7th. Saturday, July 30th, Weighed from Gallions, and\n",
      "made sail down the River, the same day Anchored at Gravesend, and the\n",
      "next Morning weighed from thence, and at\n",
      "Noon Anchored at the Buoy of the Fairway. On Wednesday, 3rd of August,\n",
      "Anchored in the Downs in 9 fathoms of water, Deal Castle North-West by\n",
      "West. On Sunday, 7th, I joined the Ship, discharged the Pilot, and the\n",
      "next day saild for Plymouth.\n",
      "\n",
      "Monday, 8th. Fresh Breezes and Cloudy weather the most part of these 24\n",
      "hours. At 10 a.m. weighed and came to sail; at Noon the South Foreland\n",
      "bore North-East 1/2 North, distant 6 or 7 Miles. Wind West by North,\n",
      "North-West.\n",
      "\n",
      "Tuesday, 9th. Gentle breezes and Cloudy weather. At 7 p.m. the Tide being\n",
      "against us, Anchored in 13 fathoms of Water; Dungeness South-West by\n",
      "West. At 11 a.m. Weighed and made Sail down Channel; at Noon, Beachy\n",
      "Head, North by East 1/2 East, distant 6 Leagues, Latitude observed 50\n",
      "degrees 30 minutes North. Wind North-West to North.\n",
      "\n",
      "Wednesday, 10th. Variable: light Airs and Clear weather. At 8 p.m. Beachy\n",
      "Head North-East by East, distant 4 Leagues, and at 8 a.m. it bore\n",
      "North-East by North, 9 Leagues. Found the Variation of the Compass to be\n",
      "23 degrees West; at Noon the Isle of Wight North-West by North. Wind West\n",
      "by North, North-East by East.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for passage in passages[:5]:\n",
    "    print(passage)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01440de0-4b48-4632-922a-1e3f330f304e",
   "metadata": {},
   "source": [
    "### Embedding the texts\n",
    "\n",
    "Text embedding models work by mapping each text into a vector in a high dimensional vector space, in such a way that queries end up close to relevant documents.\n",
    "The model nomic-embed-v2-moe is a model that has been explicitly trained to do this and, [according to its report](https://arxiv.org/abs/2502.07972), gets good scores on semantic search benchmarks like [BEIR](https://openreview.net/forum?id=wCu6T5xFjeJ) (which covers English retrieval across multiple domains) and [MIRACL](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering) (which covers search over Wikipedia in multiple languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84ab0c3-95a1-47fd-a2a1-d3476ab50d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1634: UserWarning: Install Nomic's megablocks fork for better speed: `pip install git+https://github.com/nomic-ai/megablocks.git`\n"
     ]
    }
   ],
   "source": [
    "model_name = 'nomic-ai/nomic-embed-text-v2-moe'\n",
    "revision = '1066b6599d099fbb93dfcb64f9c37a7c9e503e85'\n",
    "\n",
    "st_model = SentenceTransformer(model_name,\n",
    "                               trust_remote_code=True,\n",
    "                               revision=revision,\n",
    "                              device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc73fb-2da0-4efe-9d3e-f8e62a413d91",
   "metadata": {},
   "source": [
    "The model has a pre-determined maximum length in tokens, which are the smallest pieces of text the model recognises.\n",
    "Actually this model can handle sequences as long as you can fit into memory, but the embeddin training was only done to 512 tokens.\n",
    "Any text longer than this will be truncated from the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01547083-fce4-462c-9b6e-8a4a7926ac2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model.max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153ff92-18ad-4162-9251-b6c9a6f68777",
   "metadata": {},
   "source": [
    "Sentence Transformers can then convert all the passages to embeddings, in this case of dimension 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d13cc4a-46b8-4601-85e1-b5bfb070286f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = st_model.encode(\n",
    "    passages,\n",
    "    prompt_name=\"passage\",     # Embed these as passages to search\n",
    "    normalize_embeddings=True, # Normalise the vectors to unit length\n",
    ")\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d01de-8ce0-4e6e-8839-6b64196b80ac",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "To run a search the query needs to be embedded with the same model, and then we need to find the closest passages in the embedding space.\n",
    "This model works with cosine similarity, and since we've normalised the embeddings, is just the dot product.\n",
    "If the index contained millions of passages it would make sense to use an [approximate nearest neighbours method](https://ann-benchmarks.com/index.html) to search quickly, but with a thousand passages it's quick enough to calculate the distance to every passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3171bce3-b20c-4245-be2e-bf706fbb54b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k, embeddings=embeddings, st_model=st_model):\n",
    "    query_embedding = st_model.encode(\n",
    "        query, \n",
    "        prompt_name=\"query\",       # Encode as a query\n",
    "        normalize_embeddings=True, # Unit normalize\n",
    "    )\n",
    "\n",
    "    scores = embeddings @ query_embedding   # Calculate the index\n",
    "    idxs = np.argsort(-scores)    # Indices in descending distance order\n",
    "\n",
    "    return [passages[idx] for idx in idxs[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b57f8-cf44-4389-be9c-eeda4ec0b949",
   "metadata": {},
   "source": [
    "As an example the passages closest to storm; all but the second last are related to stormy weather, but only the first and the last contain the string \"Storm\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc21b345-6e14-40a9-bb73-5a5e1d6c9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thursday, 2nd. Winds and weather as yesterday, or rather more Stormy; we\n",
      "have now no Success in the Sein fishing, hardly getting above 20 or 30\n",
      "pounds a day.\n",
      "\n",
      "Sunday, 9th. First and latter parts ditto weather, middle squally with\n",
      "rain. In the P.M. sent on shore a Boat load of empty casks, and at the\n",
      "same time went myself in order to forward the things we wanted, and in\n",
      "the evening sent on board the new Pump, with some other stores that were\n",
      "immediately wanting.\n",
      "\n",
      "Monday, 22nd, which was usher'd in with thick Cloudy weather, and\n",
      "Excessive hard Showers of rain and very much Thunder and Lightning, which\n",
      "Continued the Greater part of the day.\n",
      "\n",
      "Saturday, 4th. Little wind and pleasant weather. At 6 A.M. the Portland\n",
      "made the Signal to unmoor, and at Noon to Weigh, at which time the Ships\n",
      "began to get under Sail. Wind Ditto. At noon at Anchor in St. Helena\n",
      "Road.\n",
      "\n",
      "Wednesday, 18th. All the Middle and Latter parts of this day it blow'd\n",
      "very strong from the South-South-West and South-West, attended with Snow,\n",
      "Hail and Rain, and brought such a Sea into the Bay, which rose the Surf\n",
      "to such a Height that no Boat could land. The same Stormy weather and\n",
      "Surf continued all\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for passage in search(\"storm\", k=5):\n",
    "    print(passage)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d5a7b-08f8-4002-b112-992644ecc6af",
   "metadata": {},
   "source": [
    "As another example we could ask a natural language question like \"What food did they eat?\" and get passages talking about food without containing the word \"eat\" or \"food\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa44e425-ee6a-4362-a097-5e329e69ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday, 7th. From this day till Monday 14th we were employ'd wooding and\n",
      "watering, being frequently interrupted by heavy rains. Having now\n",
      "compleated both we hoisted in the Long boat, and made ready to put to\n",
      "Sea, having on board a pretty good stock of refreshments, which we\n",
      "purchased of the natives, such as Turtle, Fowls, Fish, two species of\n",
      "Deer, one about as big as a small sheep, the other no bigger than a\n",
      "Rabbit; both sorts eat very well, but are only for present use, as they\n",
      "seldom lived above 24 hours in our possession. We likewise got fruit of\n",
      "several sorts, such as Cocoa Nutts, plantains, Limes, etc. The Trade on\n",
      "our part was carried on chiefly with money (Spanish Dollars); the natives\n",
      "set but little value upon any thing else. Such of our people as had not\n",
      "this Article traded with Old Shirts, etc., at a great disadvantage.\n",
      "\n",
      "In the Article of Food these People have no great Variety; Fern roots,\n",
      "Dogs, Fish, and wild fowl is their Chief diet, for Cocos, Yams, and Sweet\n",
      "Potatoes is not Cultivated every where. They dress their Victuals in the\n",
      "same Manner as the people in the South Sea Islands; that is, dogs and\n",
      "Large fish they bake in a hole in the ground, and small fish, birds, and\n",
      "Shell fish, etc., they broil on the fire. Fern roots they likewise heat\n",
      "over the fire, then beat them out flat upon a stone with a wooden Mallet;\n",
      "after this they are fit for Eating, in the doing of which they suck out\n",
      "the Moist and Glutinous part, and Spit out the Fibrous parts. These ferns\n",
      "are much like, if not the same as, the mountain ferns in England.\n",
      "\n",
      "The produce of this Island is Bread Fruit, Cocoa Nuts, Bonanoes,\n",
      "Plantains, a fruit like an Apple, sweet Potatoes, Yams, a Fruit known by\n",
      "the name of Eag Melloa, and reck'ned most delicious; Sugar Cane which the\n",
      "inhabitants eat raw; a root of the Salop kind, called by the inhabitants\n",
      "Pea; the root also of a plant called Ether; and a fruit in a pod like a\n",
      "Kidney bean, which when roasted eats like a Chestnut, and is called Ahee;\n",
      "the fruit of a Tree which they call Wharra, something like a Pine Apple;\n",
      "the fruit of a Tree called by them Nano; the roots of a Fern and the\n",
      "roots of a plant called Thive. All these Articles the Earth almost\n",
      "Spontaniously produces, or, at least, they are raised with very little\n",
      "Labour. In the Article of food these people may almost be said to be\n",
      "exempt from the Curse of our Forefathers, scarcely can it be said that\n",
      "they Earn their bread with the sweat of their brow; benevolent Nature\n",
      "hath not only Supply'd them with necessarys, but with abundance of\n",
      "Superfluities. The Sea coast supplies them with vast Variety of most\n",
      "Excellent fish, but these they get not without some Trouble and\n",
      "Perseverance. Fish seems to be one of their greatest Luxuries, and they\n",
      "Eat it either raw or Dressed and seem to relish it one way as well as the\n",
      "other. Not only fish but almost everything that comes out of the Sea is\n",
      "Eat and Esteem'd by these People; Shell Fish, Lobsters, Crabs, and even\n",
      "sea insects, and what is commonly called blubbers of many kinds, conduce\n",
      "to their support.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for passage in search(\"What food did they eat?\", k=3):\n",
    "    print(passage)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0df91e-7407-4895-b110-4f2d0a6d838b",
   "metadata": {},
   "source": [
    "As a final example the passages can be searched to find the \"worst conditions\" of the trip, and the resulting passages sound pretty bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0eae23-4e3e-4eb2-8de4-960d0871a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friday, 26th. Set up the Ship's Tent for the reception of the Ship's\n",
      "Company, several of them begin to be taken ill, owing, as I suppose, to\n",
      "the extream hot weather.\n",
      "\n",
      "Sunday, 14th. Wind Westerly, gentle breezes. In the P.M. got all the Sick\n",
      "on board, many of whom are yet in a very bad state of health; 3 died\n",
      "here, but this loss was made up by the opportunity we had of compleating\n",
      "our full complement. In the morning unmoor'd and got ready for Sailing.\n",
      "\n",
      "Be this as it will, Batavia is certainly a place that Europeans need not\n",
      "covet to go to; but if necessity obliges them, they will do well to make\n",
      "their stay as short as possible, otherwise they will soon feel the\n",
      "effects of the unwholesome air of Batavia, which, I firmly believe, is\n",
      "the Death of more Europeans than any other place upon the Globe of the\n",
      "same extent. Such, at least, is my opinion of it, which is founded on\n",
      "facts. We came in here with as healthy a Ship's Company as need go to\n",
      "Sea, and after a stay of not quite 3 months left it in the condition of\n",
      "an Hospital Ship, besides the loss of 7 men; and yet all the Dutch\n",
      "Captains I had an opportunity to converse with said that we had been very\n",
      "lucky, and wondered that we had not lost half our people in that time.*\n",
      "(* Batavia bears an evil reputation for health to this day; but it must\n",
      "be remembered that the Endeavour lay there during the rainy or most\n",
      "unhealthy season.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for passage in search(\"worst conditions\", k=3):\n",
    "    print(passage)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a34758-8612-4062-a7ae-65768e8004b3",
   "metadata": {},
   "source": [
    "Now suppose we wanted to put this into production in a language other than Python; we could use a binding to Torch's C++ API which brings in some complexity in porting the model, or we could export it to ONNX and use something like onnxruntime to serve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6c663-909f-48a2-8f02-0a64e2410d0d",
   "metadata": {},
   "source": [
    "# Exporting to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564e9b5-b80f-47b2-b1e7-ab7b30a38159",
   "metadata": {},
   "source": [
    "## Attempt 1: Sentence Transformers (Optimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e88cfc-a5ec-4a8d-a462-f5183951251c",
   "metadata": {},
   "source": [
    "The first obvious thing to try is to use [SentenceTransformer's ONNX export](https://www.sbert.net/docs/sentence_transformer/usage/efficiency.html#onnx), which uses [HuggingFace Optimum](https://huggingface.co/docs/optimum/index) under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea2e9ba-ef4e-4eb8-ac0e-a86103109cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No 'model.onnx' found in 'nomic-ai/nomic-embed-text-v2-moe'. Exporting the model to ONNX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Trying to export a nomic-bert model, that is a custom or unsupported architecture, but no custom onnx configuration was passed as `custom_onnx_configs`. Please refer to https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#custom-export-of-transformers-models for an example on how to export custom models. Please open an issue at https://github.com/huggingface/optimum/issues if you would like the model type nomic-bert to be supported natively in the ONNX export.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    onnx_model = SentenceTransformer(model_name,\n",
    "                               trust_remote_code=True,\n",
    "                               revision=revision,\n",
    "                               backend='onnx')\n",
    "except ValueError as e:\n",
    "    print('Error: %s' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49aca86-9912-4a19-b2d6-f97c737bf47b",
   "metadata": {},
   "source": [
    "Unfortunately it only works with a short list of specifically supported architectures, and Nomic's models are not on that list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe122c-4854-44b7-bbb5-912ed3b0e750",
   "metadata": {},
   "source": [
    "## Embedding with Transformers\n",
    "\n",
    "PyTorch can export models to ONNX directly so we can try to use that insead, but first we need to unwrap the SentenceTransformer abstraction and look at what's going on under the hood.\n",
    "In particular we'll directly use [huggingface transformers](https://huggingface.co/docs/transformers/) library to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b37d82-9618-41a4-828d-890267aa9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80939444-25ed-4b1f-ba72-613140c13b64",
   "metadata": {},
   "source": [
    "We can load the model weights and tokenizer (SentenceTransformers wraps both of these); as before we have a maximum length of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5777072-5292-4422-bc03-5c4ecb9b8a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_name,\n",
    "                                  trust_remote_code=True,\n",
    "                                  revision=revision)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          trust_remote_code=True,\n",
    "                                          revision=revision)\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2c5ee-482b-465d-b40f-34c55253600e",
   "metadata": {},
   "source": [
    "Then to embed the following queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "989c49bb-4a94-4e0f-ad12-5c038595979f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What food did they eat?', 'thunderstorms']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"What food did they eat?\", \"thunderstorms\"]\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a7627-3140-4da6-b1ef-73c99232f453",
   "metadata": {},
   "source": [
    "In SentenceTransformers we need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3e06256-523d-4d16-8bab-30ed320a607f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_embeddings = st_model.encode(queries,\n",
    "                                prompt_name=\"query\",\n",
    "                                normalize_embeddings=True,\n",
    "                                convert_to_tensor=True)\n",
    "\n",
    "st_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ed24b-381a-44cb-8678-29b7fb845507",
   "metadata": {},
   "source": [
    "The `prompt_name` corresponds to a task-specific prefix that the model was trained on so that it knows whether the input is a document or a query.\n",
    "We can find out what this is by [reading the technical report](https://arxiv.org/abs/2502.07972) checking the [`config_sentence_transformers.json`](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe/blob/main/config_sentence_transformers.json) or looking in the `.prompts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba2485e-12e1-448c-9951-fafe9c6bf2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'search_query: ',\n",
       " 'passage': 'search_document: ',\n",
       " 'Classification': 'classification: ',\n",
       " 'MultilabelClassification': 'classification: ',\n",
       " 'Clustering': 'clustering: ',\n",
       " 'PairClassification': 'classification: ',\n",
       " 'STS': 'classification: ',\n",
       " 'Summarization': 'classification: ',\n",
       " 'Speed': 'search_document: '}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model.prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb87201-5e6d-4d5b-a5b1-f033eb125983",
   "metadata": {},
   "source": [
    "To get the correct embeddings `search_query: ` needs to be prefixed to each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0670e6a2-0a36-47e6-84e1-444acddf5bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['search_query: What food did they eat?', 'search_query: thunderstorms']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"search_query: \" + query for query in queries]\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98889a3e-6d34-4d50-b6c3-8af2a24e9631",
   "metadata": {},
   "source": [
    "Then the tokenizer is used to convert the text into a series of numerical ids that the model knows how to handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80c3542b-c8bd-4d73-b9f9-c1c31a5a94f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,  33938,    454,    944,   1294,     12,   4865,  15381,   6777,\n",
       "           1836,  73203,     32,      2],\n",
       "        [     0,  33938,    454,    944,   1294,     12,   4911,   7944, 129857,\n",
       "              7,      2,      1,      1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(texts,\n",
    "                   padding=True,\n",
    "                   return_tensors='pt')\n",
    "\n",
    "input_ids = tokens['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde060ad-a492-4fb5-bfe5-b9839eeb637b",
   "metadata": {},
   "source": [
    "The `padding=True` was necessary to get a rectangular array (which Torch requires) instead of a jagged array by inserting `pad` tokens.\n",
    "To make sure the padding doesn't change the answer there is an `attention_mask` that is 0 on all the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b729f4ae-8cb5-45fc-974c-54bc092357a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556a387-ee1b-41d9-b063-8b65ac9d3e04",
   "metadata": {},
   "source": [
    "To see this explicitly let's write a little function to show the individual tokens, their ids and the attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d87dd9-40f2-4f20-a2ec-65ca60e2f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lookup = {v:k for k,v in tokenizer.vocab.items()}\n",
    "\n",
    "def html_row(items: list[str], header=None) -> str:\n",
    "    \"\"\"Return html to show a row of items\n",
    "\n",
    "    If items is empty return empty string\"\"\"\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    row = ''.join([\"<td>\" + escape(item) + \"</td>\" for item in items])\n",
    "    header = \"<th>\" + escape(header) + \"</th>\" if header is not None else \"\"\n",
    "    return \"<tr>\" + header + row + \"</tr>\"\n",
    "\n",
    "def html_tokens(token_ids, formatters=None, id_lookup=id_lookup, **kwargs):\n",
    "    if formatters is None:\n",
    "        formatters = {}\n",
    "    \n",
    "    token_id_str = [str(t.item()) for t in token_ids]\n",
    "    tokens = [id_lookup[t.item()] for t in token_ids]\n",
    "\n",
    "    extra_rows = []\n",
    "    for kw, tensor in kwargs.items():\n",
    "        formatter = formatters.get(kw, lambda x: str(x.item()))\n",
    "        extra_rows.append(html_row([formatter(t) for t in tensor], header=kw))\n",
    "\n",
    "    html_extra_rows = '\\n'.join(extra_rows)\n",
    "\n",
    "\n",
    "    return f\"\"\"<table>\n",
    "    {html_row(tokens, \"Token\")}\n",
    "    {html_row(token_id_str, \"ID\")}\n",
    "    {html_extra_rows}\n",
    "    </table>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02800847-9186-4833-9a5c-8d71a58c075c",
   "metadata": {},
   "source": [
    "Here's the first query; notice that the first six tokens just encode the start token `<s>` and the query prompt.\n",
    "The attention mask is all 1 since they are all contentful tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9d29b4d-5e5b-46b0-b8fb-14158b95becc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>Token</th><td>&lt;s&gt;</td><td>▁search</td><td>_</td><td>que</td><td>ry</td><td>:</td><td>▁What</td><td>▁food</td><td>▁did</td><td>▁they</td><td>▁eat</td><td>?</td><td>&lt;/s&gt;</td></tr>\n",
       "    <tr><th>ID</th><td>0</td><td>33938</td><td>454</td><td>944</td><td>1294</td><td>12</td><td>4865</td><td>15381</td><td>6777</td><td>1836</td><td>73203</td><td>32</td><td>2</td></tr>\n",
       "    <tr><th>AttentionMask</th><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html_tokens(input_ids[0], AttentionMask=attention_mask[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366acb3-b035-4209-b07a-d425360abe6f",
   "metadata": {},
   "source": [
    "For the second query the last two tokens are special `<pad>` tokens that indicate it is past the end of the string, and the `attention_mask` is zero on these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805ced27-7814-4819-8de8-763628582907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>Token</th><td>&lt;s&gt;</td><td>▁search</td><td>_</td><td>que</td><td>ry</td><td>:</td><td>▁thu</td><td>nder</td><td>storm</td><td>s</td><td>&lt;/s&gt;</td><td>&lt;pad&gt;</td><td>&lt;pad&gt;</td></tr>\n",
       "    <tr><th>ID</th><td>0</td><td>33938</td><td>454</td><td>944</td><td>1294</td><td>12</td><td>4911</td><td>7944</td><td>129857</td><td>7</td><td>2</td><td>1</td><td>1</td></tr>\n",
       "    <tr><th>AttentionMask</th><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td></tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html_tokens(input_ids[1], AttentionMask=attention_mask[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24132702-3e04-4542-8bc7-320e1315bce3",
   "metadata": {},
   "source": [
    "When the `input_ids` are put through the model we get an output 768-dimensional embedding for each token of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1cf2a14-3ada-41e1-93f0-db7e59ca9201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6144,  0.5655,  0.0732,  ..., -0.1502, -0.3853,  0.5019],\n",
       "         [ 0.8870,  0.3930, -0.5521,  ...,  0.2549, -0.7876,  1.1182],\n",
       "         [ 0.5724,  1.2011, -0.0462,  ..., -0.1216,  0.3720,  1.4457],\n",
       "         ...,\n",
       "         [ 0.4333,  0.1363, -0.1364,  ...,  0.1581, -0.2119, -0.2602],\n",
       "         [ 0.7541,  0.9300, -0.1350,  ..., -0.2900, -0.2110,  1.2073],\n",
       "         [ 0.6275,  0.6497, -0.0373,  ..., -0.3004, -0.5346,  0.6286]],\n",
       "\n",
       "        [[ 0.4435,  0.0964,  0.3501,  ..., -0.0903, -0.3210,  0.1285],\n",
       "         [ 1.0036,  0.2632,  0.2588,  ..., -0.8356, -0.4994,  1.2230],\n",
       "         [ 0.8230,  0.5872,  0.2623,  ..., -0.9592,  0.1922,  1.6073],\n",
       "         ...,\n",
       "         [ 0.4239, -0.0415, -0.0790,  ..., -0.7102, -0.3594,  0.3975],\n",
       "         [ 0.5177,  0.0198, -0.2920,  ...,  0.3866,  0.0566,  0.3639],\n",
       "         [ 0.4159,  0.0270, -0.2100,  ...,  0.2291,  0.1682,  0.2517]]]), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model_output = model(input_ids, attention_mask)\n",
    "\n",
    "model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d912bf-200d-406a-88f4-40261bfe8433",
   "metadata": {},
   "source": [
    "We had 2 examples of 13 tokens, ans so the output is 2 x 13 x 768."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01682c3c-11ba-47c1-b39b-4b7c023b9c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = model_output[0]\n",
    "\n",
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a153aac7-168c-4548-b37e-0ce686ea5c37",
   "metadata": {},
   "source": [
    "To get a single embedding we need some way of \"pooling\" the embeddings from the different tokens.\n",
    "A common method, used by this model, is \"mean pooling\" which averages the scores over all the embeddings.\n",
    "Then when we unit normalise it we get the same result as we did with Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ed56347-8a30-4a4a-ba21-cec2c41a85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = last_hidden_state[0].mean(dim=0)\n",
    "embedding = mean_pooled / ((mean_pooled ** 2).sum() ** 0.5)\n",
    "\n",
    "assert torch.allclose(st_embeddings[0], embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11365be5-f9a4-40e2-ad2e-fc5989876aa0",
   "metadata": {},
   "source": [
    "We need to be a bit more careful with mean pooling the second example; any amount of padding should not change the final answer.\n",
    "This can be done by only calculating the average over the tokens where `attention_mask` is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4571b71f-7994-48b4-91c1-feaa80e7482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to zero all hidden states where attention_mask is 0\n",
    "masked_last_hidden_state = last_hidden_state[1] * attention_mask[1].unsqueeze(-1)\n",
    "# Calculate the total numerator for the average\n",
    "numerator = masked_last_hidden_state.sum(dim=0)\n",
    "# Calculate the denominator, the number of tokens where attention_mask is 1\n",
    "denominator =  attention_mask[1].sum()\n",
    "# Mean pool\n",
    "mean_pooled = numerator / denominator\n",
    "# Unit normalise\n",
    "embedding =  mean_pooled / ((mean_pooled ** 2).sum() ** 0.5)\n",
    "\n",
    "assert torch.allclose(st_embeddings[1], embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847dd3f-1923-44e3-a5d2-d24ff23bc78a",
   "metadata": {},
   "source": [
    "Let's wrap this up in a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c76671f-e622-4111-a9ed-47eef933e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def mean_pooling(token_embeddings, # [B,L,D]\n",
    "                 attention_mask,   # [B,L]\n",
    "                 normalize: bool = True,\n",
    "                ): # -> [B,D]\n",
    "    # [B, L, 1]\n",
    "    attention_mask_expanded = attention_mask.unsqueeze(-1)\n",
    "    # [B, D]\n",
    "    numerator = (token_embeddings * attention_mask_expanded).sum(dim=1)\n",
    "    # [B, 1]\n",
    "    denominator = attention_mask_expanded.sum(axis=1)\n",
    "\n",
    "    embeddings = numerator / denominator\n",
    "\n",
    "    if normalize:\n",
    "        embeddings = F.normalize(embeddings)\n",
    "    return embeddings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad475941-8573-4572-89f4-d07d31d89e9a",
   "metadata": {},
   "source": [
    "Which gives the same result as SentenceTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2915c68-6bbf-444d-8145-6d7b4c1d7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = mean_pooling(last_hidden_state, attention_mask)\n",
    "\n",
    "assert torch.allclose(embeddings, st_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87feea8d-2267-4d60-b008-e753abda9193",
   "metadata": {},
   "source": [
    "## Attempt 2: Torch (JIT Trace)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab75651-10ff-49c7-9f61-837a7eae020a",
   "metadata": {},
   "source": [
    "Following the [PyTorch ONNX export tutorial](https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html) we can export the model which works, but with a bunch of warnings, which we'll blithely ignore for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e07438e0-995a-4e15-b3a8-b95a03ce612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1386: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1339: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1272: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1212: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1215: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py:1216: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/micromamba/envs/onnx-moe/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "/micromamba/envs/onnx-moe/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:6040: UserWarning: Warning: ONNX export does not support duplicated values in 'index' field, this will cause the ONNX model to be incorrect.\n"
     ]
    }
   ],
   "source": [
    "onnx_path = model_dir / 'nomic-v2-moe.onnx'\n",
    "\n",
    "torch.onnx.export(model.eval(),\n",
    "                  (input_ids, attention_mask),\n",
    "                  onnx_path,\n",
    "                  input_names = ['input_ids', 'attention_mask'],\n",
    "                  dynamic_axes = {'input_ids': {0: 'batch', 1: 'sequence_length'},\n",
    "                                  'attention_mask': {0: 'batch', 1: 'sequence_length'},\n",
    "                                 },\n",
    "                  dynamo=False,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1f186-f832-4b7a-aa44-f2bfa7cf72dc",
   "metadata": {},
   "source": [
    "## ONNX Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bc87b-2179-4047-94de-8c57df52a090",
   "metadata": {},
   "source": [
    "Now we've exported the ONNX model we cal load it in with ONNX runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f60e9323-b2e2-40d8-808e-44494a0ac8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    onnx_path, providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d1deae-0a68-40ce-a715-8369c4682ccf",
   "metadata": {},
   "source": [
    "And when we run inference we get a very close result to running the model with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c6916da-b033-4df4-b4da-b89aa5e054d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ort_session.run(None, {\"input_ids\": input_ids.numpy(), \"attention_mask\": attention_mask.numpy()})[0]\n",
    "\n",
    "assert np.allclose(output, last_hidden_state, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7192571-212c-4d5a-b5d1-c0b665c51130",
   "metadata": {},
   "source": [
    "But if we try to run any other query something bad happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6d54f9f-46ab-4de1-b0d8-7e355007b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Non-zero status code returned while running Expand node. Name:'/encoder/layers.1/mlp/experts/Expand_1' Status Message: invalid expand shape\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-06-13 22:37:41.194455682 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Expand node. Name:'/encoder/layers.1/mlp/experts/Expand_1' Status Message: invalid expand shape\u001b[m\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(['search_query: weather'])\n",
    "\n",
    "try:\n",
    "    output = ort_session.run(None, {\"input_ids\": tokens['input_ids'], \"attention_mask\": tokens['attention_mask']})[0]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112fa04-b669-42bd-a932-115ca32c5695",
   "metadata": {},
   "source": [
    "What happened is that when Torch exported the model with JIT Trace it captured the control flow with how the model executed *on that example*.\n",
    "If there's conditional execution (which the warnings were telling us about) then a different output could raise an error, or succesfully return the wrong answer.\n",
    "\n",
    "The newer Torch Dynamo (setting `dynamo=True`) can capture conditional execution but it also fails on this code with an error:\n",
    "\n",
    "> Could not guard on data-dependent expression Eq(u0, 1) (unhinted: Eq(u0, 1)).\n",
    "\n",
    "To find out what's going wrong requires digging deeper into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82374a55-ed02-443e-b0ea-f7715545ccdd",
   "metadata": {},
   "source": [
    "## Isolating the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112c6e3-8b39-4100-a135-2c96ec22f2d8",
   "metadata": {},
   "source": [
    "To understand what's going wrong we need to look into the model code.\n",
    "The first place to look is in the [`config.json`](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe/blob/1066b6599d099fbb93dfcb64f9c37a7c9e503e85/config.json#L11) in the nomic-embed-text-v2-moe repository which tells what class is used for `AutoModel`:\n",
    "\n",
    "```\n",
    "\"AutoModel\": \"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertModel\",\n",
    "```\n",
    "\n",
    "This comes from the [nomic-bert-2048](https://huggingface.co/nomic-ai/nomic-bert-2048) repository.\n",
    "As an aside note that this doesn't pin a revision, so pinning the reivision of `nomic-embed-text-v2-moe` doesn't actually protect from malicious code changes because it always pulls in the latest `nomic-bert-2048`.\n",
    "In any case we can read the modelling code in this repository under [`modeling_hf_nomic_bert.py`](https://huggingface.co/nomic-ai/nomic-bert-2048/blob/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py#L1868).\n",
    "\n",
    "The onnxruntime error message gives a good hint for where the issue is: `/encoder/layers.1/mlp/experts/Expand_1`.\n",
    "We can pull this layer out of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a688712e-548b-4b1a-9d23-84ed008a4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = model.encoder.layers[1].mlp.experts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a2ab2-f379-45bd-bc25-e90c0c6f2a5c",
   "metadata": {},
   "source": [
    "And look at the `forward` method that's being executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3430d739-0373-4866-833b-bdb86ea1cead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m\n",
       "experts.forward(\n",
       "    x: torch.Tensor,\n",
       "    weights: torch.Tensor,\n",
       "    top_weights: torch.Tensor,\n",
       "    top_experts: torch.LongTensor,\n",
       ") -> torch.Tensor\n",
       "\u001b[31mDocstring:\u001b[39m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m forward(self, x: torch.Tensor, weights: torch.Tensor,\n",
       "                top_weights: torch.Tensor,\n",
       "                top_experts: torch.LongTensor) -> torch.Tensor:\n",
       "        bsz, q_len, hidden_size = x.shape\n",
       "        x = x.view(-\u001b[32m1\u001b[39m, hidden_size)\n",
       "        out = torch.zeros_like(x)\n",
       "\n",
       "        expert_mask = nn.functional.one_hot(\n",
       "            top_experts, num_classes=self.moe_num_experts).permute(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n",
       "        \u001b[38;5;28;01mfor\u001b[39;00m expert_idx \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m0\u001b[39m, self.moe_num_experts):\n",
       "            topk_idx, token_idx = torch.where(expert_mask[expert_idx])\n",
       "            \u001b[38;5;28;01mif\u001b[39;00m token_idx.shape[\u001b[32m0\u001b[39m] == \u001b[32m0\u001b[39m:\n",
       "                \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
       "\n",
       "            token_list = token_idx.tolist()\n",
       "            topk_list = topk_idx.tolist()\n",
       "\n",
       "            expert_tokens = x[\u001b[38;5;28;01mNone\u001b[39;00m, token_list].reshape(-\u001b[32m1\u001b[39m, hidden_size)\n",
       "            expert_out = self.mlp(\n",
       "                expert_tokens, expert_idx) * top_weights[token_list, topk_list,\n",
       "                                                         \u001b[38;5;28;01mNone\u001b[39;00m]\n",
       "\n",
       "            out.index_add_(\u001b[32m0\u001b[39m, token_idx, expert_out)\n",
       "\n",
       "        out = out.reshape(bsz, q_len, hidden_size)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m out + self.bias\n",
       "\u001b[31mFile:\u001b[39m      ~/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??experts.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853f7df-f609-429f-ada1-4334e301736e",
   "metadata": {},
   "source": [
    "This might look a bit complicated at first but what's going on in this Mixture of Experts layer is actually quite straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "070493e0-d490-4537-882c-3ee392866bef",
   "metadata": {},
   "source": [
    "## Sparse Mixture of Experts\n",
    "\n",
    "The underlying idea of a Mixture of Experts model is there are multiple subnetworks called \"experts\" (in this case multilayer perceptrons), and a router network chooses how to weight the outputs from them. In a sparse mixture of experts most of the weights are zero, so we can skip the computation from the non-zero ones. Huggingface have a good [blog post on Mixture of Experts](https://huggingface.co/blog/moe) that goes into much more detail.\n",
    " \n",
    "The inputs to this model are the hidden state `x`, the `weights` from the router and the `top_weights` and `top_experts` selected from the router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5639cce0-69fc-4997-8579-d81bf942b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'weights', 'top_weights', 'top_experts']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captured_variables = {}\n",
    "names = [name for name in experts.forward.__annotations__ if name != 'return']\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1adfbc5-a38e-42ed-b95d-c3dd11f8955c",
   "metadata": {},
   "source": [
    "Their values can be captured in the model using a [Pytorch Hook](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b75bd6a7-53fe-4749-ad17-d2593ea805f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = None\n",
    "\n",
    "def capture_input_hook(module, args, output):\n",
    "    for name, value in zip(names, args, strict=True):\n",
    "        captured_variables[name] = value.detach().clone()\n",
    "    captured_variables[\"return\"] = output\n",
    "\n",
    "# If this cell gets run twice only register the hook once\n",
    "if hook:\n",
    "    hook.remove()\n",
    "\n",
    "hook = experts.register_forward_hook(capture_input_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1934c4-676b-4bf6-bcef-af7e59285747",
   "metadata": {},
   "source": [
    "Then we can run the model and capture their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "49e730bc-a5b2-426c-b1a0-6271663ecfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x           : [2, 13, 768] (torch.float32)\n",
      "weights     : [26, 8]      (torch.float32)\n",
      "top_weights : [26, 2]      (torch.float32)\n",
      "top_experts : [26, 2]      (torch.int64)\n",
      "return      : [2, 13, 768] (torch.float32)\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model_output = model(input_ids, attention_mask)\n",
    "\n",
    "for k,v in captured_variables.items():\n",
    "    print(f\"{k:12s}: {str(list(v.shape)):12s} ({v.dtype})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff563ab-7830-414f-95f4-bc3284c2d5cf",
   "metadata": {},
   "source": [
    "With:\n",
    "\n",
    "* Batch Size: `B`\n",
    "* Sequence Length: `L`\n",
    "* Embedding Dimension: `D`\n",
    "* Number of non-zero experts: `K`\n",
    "* Number of Experts: `M`\n",
    "\n",
    "Then the dimensions of the tensors are\n",
    "\n",
    "* x (hidden state): `B, L, D`\n",
    "* weights: `B * L, E`\n",
    "* top_weights: `B * L, K`\n",
    "* top_experts: `B * L, K`\n",
    "* output: `B, L, D`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a3799fd-cd0f-4d3b-a49c-1224b8251ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = captured_variables['x']\n",
    "weights = captured_variables['weights']\n",
    "top_weights = captured_variables['top_weights']\n",
    "top_experts = captured_variables['top_experts']\n",
    "\n",
    "experts_output = captured_variables['return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb68d7-3a9d-4015-9634-1f012a3fbd2f",
   "metadata": {},
   "source": [
    "Let's look a bit more closely in at the weights. First capture all the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "333c4486-8301-4682-bf38-f5296bd9cb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, L, D = x.shape\n",
    "\n",
    "K = model.encoder.layers[1].mlp.router.moe_top_k\n",
    "K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed23d58a-4e04-4461-99fe-f3f68050615d",
   "metadata": {},
   "source": [
    "Note that the weights come out of a softmax and sum to one over the experts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dadac5ad-b4b0-4fde-86ac-454f464673d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(weights.sum(axis=1), torch.ones(B*L))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b63a29-f0ab-4d57-9c9f-41915edd0519",
   "metadata": {},
   "source": [
    "The top weights and top experts [are just](https://huggingface.co/nomic-ai/nomic-bert-2048/blob/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py#L1133) the values and indices of the largest weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ffaa1a5-30d1-4b03-86dd-d371258ba107",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = torch.topk(weights, k=K)\n",
    "\n",
    "assert torch.equal(topk.indices, top_experts)\n",
    "assert torch.equal(topk.values, top_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217cb82-c1d1-4e14-bce6-4609c8355a00",
   "metadata": {},
   "source": [
    "We can realign the top experts and weights to see what they are for each input token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8268e900-f3ab-4daa-8d41-9e06dd76a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_experts_aligned = top_experts.view(B, L, K)\n",
    "top_weights_aligned = top_weights.view(B, L, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c9851-08fb-4b8b-a527-34e420758125",
   "metadata": {},
   "source": [
    "In this case there are 2 experts, each token has different experts with different weights.\n",
    "\n",
    "For example the token `<s>` has experts 0 and 2 with weights 0.80 and 0.14 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb10fade-9308-4f8d-85de-c279f3b95e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>Token</th><td>&lt;s&gt;</td><td>▁search</td><td>_</td><td>que</td><td>ry</td><td>:</td><td>▁What</td><td>▁food</td><td>▁did</td><td>▁they</td><td>▁eat</td><td>?</td><td>&lt;/s&gt;</td></tr>\n",
       "    <tr><th>ID</th><td>0</td><td>33938</td><td>454</td><td>944</td><td>1294</td><td>12</td><td>4865</td><td>15381</td><td>6777</td><td>1836</td><td>73203</td><td>32</td><td>2</td></tr>\n",
       "    <tr><th>firstExpert</th><td>0</td><td>1</td><td>7</td><td>5</td><td>6</td><td>2</td><td>4</td><td>1</td><td>4</td><td>6</td><td>1</td><td>2</td><td>0</td></tr>\n",
       "<tr><th>firstWeight</th><td>0.80</td><td>0.39</td><td>0.17</td><td>0.74</td><td>0.31</td><td>0.42</td><td>0.59</td><td>0.50</td><td>0.19</td><td>0.36</td><td>0.29</td><td>0.37</td><td>0.31</td></tr>\n",
       "<tr><th>secondExpert</th><td>2</td><td>4</td><td>3</td><td>3</td><td>7</td><td>6</td><td>7</td><td>5</td><td>1</td><td>4</td><td>4</td><td>7</td><td>3</td></tr>\n",
       "<tr><th>secondWeight</th><td>0.14</td><td>0.27</td><td>0.16</td><td>0.07</td><td>0.24</td><td>0.19</td><td>0.10</td><td>0.20</td><td>0.16</td><td>0.17</td><td>0.26</td><td>0.18</td><td>0.15</td></tr>\n",
       "<tr><th>position</th><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td></tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_weight(t):\n",
    "    return f'{t.item():0.2f}'\n",
    "\n",
    "HTML(html_tokens(input_ids[0],\n",
    "                 firstExpert = top_experts_aligned[0,:,0],\n",
    "                 firstWeight = top_weights_aligned[0,:,0],\n",
    "                 secondExpert = top_experts_aligned[0,:,1],\n",
    "                 secondWeight = top_weights_aligned[0,:,1],\n",
    "                 position=torch.arange(len(input_ids[0])),\n",
    "                 formatters=dict(firstWeight=format_weight,\n",
    "                                 secondWeight=format_weight),\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75654de2-a08d-4589-b4b2-0a1a458d73d5",
   "metadata": {},
   "source": [
    "Similarly the second element of the batch also has different experts for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a817dbf-28a8-47b1-bf0e-c7ae078f9e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>Token</th><td>&lt;s&gt;</td><td>▁search</td><td>_</td><td>que</td><td>ry</td><td>:</td><td>▁thu</td><td>nder</td><td>storm</td><td>s</td><td>&lt;/s&gt;</td><td>&lt;pad&gt;</td><td>&lt;pad&gt;</td></tr>\n",
       "    <tr><th>ID</th><td>0</td><td>33938</td><td>454</td><td>944</td><td>1294</td><td>12</td><td>4911</td><td>7944</td><td>129857</td><td>7</td><td>2</td><td>1</td><td>1</td></tr>\n",
       "    <tr><th>firstExpert</th><td>0</td><td>1</td><td>7</td><td>5</td><td>6</td><td>2</td><td>3</td><td>6</td><td>1</td><td>7</td><td>0</td><td>4</td><td>4</td></tr>\n",
       "<tr><th>firstWeight</th><td>0.82</td><td>0.40</td><td>0.19</td><td>0.73</td><td>0.29</td><td>0.36</td><td>0.70</td><td>0.48</td><td>0.59</td><td>0.40</td><td>0.38</td><td>0.34</td><td>0.34</td></tr>\n",
       "<tr><th>secondExpert</th><td>2</td><td>4</td><td>0</td><td>3</td><td>7</td><td>6</td><td>5</td><td>1</td><td>5</td><td>6</td><td>4</td><td>0</td><td>0</td></tr>\n",
       "<tr><th>secondWeight</th><td>0.13</td><td>0.29</td><td>0.15</td><td>0.08</td><td>0.25</td><td>0.19</td><td>0.25</td><td>0.42</td><td>0.28</td><td>0.29</td><td>0.15</td><td>0.16</td><td>0.17</td></tr>\n",
       "<tr><th>position</th><td>13</td><td>14</td><td>15</td><td>16</td><td>17</td><td>18</td><td>19</td><td>20</td><td>21</td><td>22</td><td>23</td><td>24</td><td>25</td></tr>\n",
       "    </table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html_tokens(input_ids[1],\n",
    "                 firstExpert = top_experts_aligned[1,:,0],\n",
    "                 firstWeight = top_weights_aligned[1,:,0],\n",
    "                 secondExpert = top_experts_aligned[1,:,1],\n",
    "                 secondWeight = top_weights_aligned[1,:,1],\n",
    "                 position=torch.arange(len(input_ids[0]), len(input_ids.flatten())),\n",
    "                 formatters=dict(firstWeight=format_weight,\n",
    "                                 secondWeight=format_weight),\n",
    "                ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eab596f-8550-4742-8f20-17c548fb8c2e",
   "metadata": {},
   "source": [
    "The experts layer has an `mlp` that takes the input and an expert index and outputs the value of that expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84526aa4-3997-45a2-a3a9-fa7092685793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m experts.mlp.forward(x: torch.Tensor, expert_idx: int) -> torch.Tensor\n",
       "\u001b[31mDocstring:\u001b[39m\n",
       "Define the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m forward(self, x: torch.Tensor, expert_idx: int) -> torch.Tensor:\n",
       "        expert_w1 = self.w1.view(self.moe_num_experts, self.ffn_hidden_size,\n",
       "                                 self.hidden_size)[expert_idx]\n",
       "        expert_w2 = self.w2.view(self.moe_num_experts, self.ffn_hidden_size,\n",
       "                                 self.hidden_size)[expert_idx]\n",
       "\n",
       "        x1 = x.matmul(expert_w1.t())\n",
       "        act_out = self.activation_fn(x1)\n",
       "        x2 = act_out.matmul(expert_w2)\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m x2\n",
       "\u001b[31mFile:\u001b[39m      ~/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py\n",
       "\u001b[31mType:\u001b[39m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??experts.mlp.forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0de550-2520-4423-96a1-bf9f0ce5b98e",
   "metadata": {},
   "source": [
    "The straight-forward way to calculate the output of the mixture of experts is to loop over every element of the batch, sequence length, and top-k tokens and pass the corresponding hidden state through the appropripriate expert index, and multiply by the weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee5c41f2-f62d-40ab-b3d9-c12940a6a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    result = torch.zeros_like(x)\n",
    "    for b in range(B):\n",
    "        for l in range(L):\n",
    "            for k in range(K):\n",
    "                result[b,l] += (\n",
    "                    experts.mlp(x[b,l],\n",
    "                                top_experts_aligned[b, l, k])\n",
    "                    * top_weights_aligned[b, l, k]\n",
    "                )\n",
    "\n",
    "assert torch.allclose(result + experts.bias, experts_output, atol=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ec1123b-f3fd-4ecf-ac87-d0ee5d261e26",
   "metadata": {},
   "source": [
    "## How Nomic MoE calculates\n",
    "\n",
    "In the Nomic Mixture of Experts layer there are some tricks to make it run faster than this naive loop.\n",
    "There's a specialised implementation called [Megablocks](https://arxiv.org/abs/2211.15841) that run much faster on a GPU, and Nomic have [a version](https://github.com/nomic-ai/megablocks) forked from the [Databricks implementation](https://github.com/databricks/megablocks); however this is Triton code that doesn't export easily and is harder to debug.\n",
    "\n",
    "They've got a fallback implementation that we will look at that tries to do all the calculation for a single expert at once, to reduce the amount of memory movement loading the expert weights.\n",
    "\n",
    "To start make a mask that has 1 in `[i, j, k]` if `i = top_experts[k,j]` and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3a13c54-3c64-49cd-9c16-d81712462273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 26])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "expert_mask = (\n",
    "    nn.functional.one_hot(\n",
    "        top_experts,\n",
    "        num_classes=experts.moe_num_experts)\n",
    "        .permute(2, 1, 0)\n",
    ")\n",
    "expert_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2529b-438d-44ac-8d58-2f14cadeaa70",
   "metadata": {},
   "source": [
    "This can be used to find all tokens across the batch and sequence length that use a given expert.\n",
    "\n",
    "For example for `expert_idx = 0` it's the first expert for tokens 0, 12, 13, and 23 (wrapping around; so token 13 is actually token 0 for batch item 1) and the second expert for tokens 15, 24, and 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e50976b-21e2-455a-a0b9-41bd41c4ead0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 1, 1, 1], [0, 12, 13, 23, 15, 24, 25])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_idx = 0\n",
    "\n",
    "topk_idx, token_idx = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "token_list = token_idx.tolist()\n",
    "topk_list = topk_idx.tolist()\n",
    "\n",
    "topk_list, token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc782e3-aa7e-44d3-b010-81362b80800f",
   "metadata": {},
   "source": [
    "We then pluck out the corresponding components of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f46c93d-472a-4c0d-9d77-72d6f584068c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_tokens = x.view(-1, D)[None, token_list].reshape(-1, D)\n",
    "\n",
    "expert_tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9628579-e123-41ab-a369-107dd037b53b",
   "metadata": {},
   "source": [
    "Then run all of these through the expert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecce4758-313b-490b-be08-083750372f89",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (793253010.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m* top_weights[token_list, topk_list, None]\u001b[39m\n                                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "expert_out = (\n",
    "    experts.mlp(expert_tokens, expert_idx)\n",
    "    * top_weights[token_list, topk_list, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae2728-4468-4dc6-a922-7e2c4e52e5b0",
   "metadata": {},
   "source": [
    "The nomic implementation does this for each expert, with some special logic to skip the computation if the expert isn't used for any token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9960cfc-00d9-45df-a4d4-abcf5f9f20cd",
   "metadata": {},
   "source": [
    "## Exporting Nomic MoE to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b67ef-d8de-4872-8a8e-00c70b339fa6",
   "metadata": {},
   "source": [
    "So now it's a bit easier to see what's going wrong with the ONNX export: the shape of `topk_list` and `token_list` will change each iteration, but PyTorch's JIT Tracing can't capture that dynamic execution, and in particular when we run `tolist()` it just captures the values from the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e28387-ef06-447b-a606-c1a1448b6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(experts,\n",
    "          (x, weights, top_weights, top_experts),\n",
    "          model_dir / 'experts.onnx',\n",
    "          input_names = ['x', 'weights', 'top_weights', 'top_experts'],\n",
    "          dynamic_axes = {'x': {0: 'batch', 1: 'sequence_length', 2: 'hidden'},\n",
    "                          'weights': {0: 'batch_sequence_lengh', 1: 'num_experts'},\n",
    "                          'top_weights': {0: 'batch_sequence_lengh', 1: 'top_k'},\n",
    "                          'top_experts': {0: 'batch_sequence_lengh', 1: 'top_k'},\n",
    "                         },\n",
    "          dynamo=False,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea0142-2a96-40a0-b5c2-250b1cd04146",
   "metadata": {},
   "source": [
    "This can be seen directly looking into the model here's a screenshot from [netron](https://github.com/lutzroeder/netron) where we can see the token list `[0, 12, 13, 23, 15, 24, 25]` from this particular input is hard-coded into the model. This means it won't work correctly on other inputs!\n",
    "\n",
    "![Netron view of the network showing token list values](moe_onnx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a630531-b141-443c-ba54-6f45c5422a07",
   "metadata": {},
   "source": [
    "## Unconditional Mixture of Experts\n",
    "\n",
    "The Mixture of Experts doesn't need any conditional control flow if we do some extra redundant computation.\n",
    "If we just run *all* the tokens through all the experts and multiply all but the top-k experts by zero we get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655aea8-8c37-47be-9781-fb112c056654",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 8    # Number of experts\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # Get the output for every expert\n",
    "    # Shape: B, L, E, D\n",
    "    mlp_output = torch.stack([experts.mlp(x, expert_idx)\n",
    "                              for expert_idx \n",
    "                              in range(experts.moe_num_experts)],\n",
    "                             dim=-2)\n",
    "\n",
    "\n",
    "    # Get the weight matrix with only the values of the top_weights\n",
    "    # at the indices of top_experts, and zeros elsewhere\n",
    "    # (This is the same as masking all non-top weights with 0)\n",
    "    # B, L, E, 1\n",
    "    weight_mask = (\n",
    "        torch.scatter(torch.zeros_like(weights),\n",
    "                      1, top_experts, top_weights)\n",
    "            .view(B, L, E)\n",
    "            .unsqueeze(-1)\n",
    "    )\n",
    "\n",
    "    # Calculate the weighted sum; a lot of these multiplcations are zero\n",
    "    result = (mlp_output * weight_mask).sum(dim=-2) + experts.bias\n",
    "    \n",
    "\n",
    "assert torch.allclose(result, experts_output, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd598a70-304c-418e-840d-657532072f4d",
   "metadata": {},
   "source": [
    "At first this seems really bad from a performance perspective; we're doing a bunch of extra computation that's going to waste.\n",
    "But it depends whether the computation is compute bound or memory bound; since we're running the experts serially it has reasonable data locality.\n",
    "There are other ways to do this - but let's first see whether this approach even works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade734d1-f68f-4659-9b96-c4cf632f406a",
   "metadata": {},
   "source": [
    "### Putting the computation in a Module\n",
    "\n",
    "Now let's create an `ExportableNomicExpert` module that takes an existing `NomicExpert` layer and replaces the `forward` method with the computation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5580a9-2f09-4205-bca4-6fb50ce2ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportableNomicExpert(nn.Module):\n",
    "    def __init__(self, other):\n",
    "        super().__init__()\n",
    "        self.moe_num_experts = other.moe_num_experts\n",
    "        self.mlp = other.mlp\n",
    "        self.bias = other.bias\n",
    "\n",
    "    def forward(self, x, weights, top_weights, top_experts):\n",
    "        bsz, q_len, hidden_size = x.shape\n",
    "    \n",
    "        weight_mask = (\n",
    "            torch.scatter(torch.zeros_like(weights),\n",
    "                          1, top_experts, top_weights)\n",
    "                .view(bsz, q_len, self.moe_num_experts)\n",
    "                .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        mlp_output = torch.stack(\n",
    "            [self.mlp(x, expert_idx)\n",
    "             for expert_idx\n",
    "             in range(self.moe_num_experts)],\n",
    "            dim=-2)\n",
    "    \n",
    "        out = (mlp_output * weight_mask).sum(dim=-2)\n",
    "        \n",
    "        out = out.reshape(bsz, q_len, hidden_size)\n",
    "        return out + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b7c2f-887f-4677-a483-f027b608f13e",
   "metadata": {},
   "source": [
    "Checking this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d54541-81ac-43f9-85e2-c29fb7946179",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    layer_output = experts(x, weights, top_weights, top_experts)\n",
    "\n",
    "exportable_experts = ExportableNomicExpert(experts)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    exportable_layer_output = exportable_experts(x, weights, top_weights, top_experts)\n",
    "\n",
    "assert torch.allclose(layer_output, exportable_layer_output, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314be5ea-9ad3-4c4c-b3ee-6a76baa48107",
   "metadata": {},
   "source": [
    "### Monkey patching the model\n",
    "\n",
    "Now there's a solution for making the experts layer exportable we should replace every NomicExperts layer with a ExportableNomicExperts layer.\n",
    "\n",
    "We will use a fresh copy of the model to modify for exporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7380bb-5c66-4253-b160-498361cc558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = AutoModel.from_pretrained(model_name,\n",
    "                                         trust_remote_code=True,\n",
    "                                         revision=revision)\n",
    "\n",
    "for layer in export_model.encoder.layers:\n",
    "    if type(layer.mlp).__name__ == 'NomicMoELayer':\n",
    "        layer.mlp.experts = ExportableNomicExpert(layer.mlp.experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae10d0-863c-4995-97e3-37a5e01f8187",
   "metadata": {},
   "source": [
    "## ONNX Export\n",
    "\n",
    "Now we'll try to export the model once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385b8f9-6870-47f8-b1d6-7c377c71ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.unlink(onnx_path)\n",
    "\n",
    "torch.onnx.export(export_model.eval(),\n",
    "          (input_ids, attention_mask),\n",
    "          onnx_path,\n",
    "          input_names = ['input_ids', 'attention_mask'],\n",
    "          dynamic_axes = {'input_ids': {0: 'batch', 1: 'sequence_length'},\n",
    "                          'attention_mask': {0: 'batch', 1: 'sequence_length'},\n",
    "                         },\n",
    "          dynamo=False,\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75d94f-c10a-4304-bddd-5aa475334d2f",
   "metadata": {},
   "source": [
    "When the model is loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9940100-c0d0-4482-a466-d62f23581514",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\n",
    "    onnx_path, providers=[\"CPUExecutionProvider\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051fd4ac-9689-47f5-bf20-396abdd11b30",
   "metadata": {},
   "source": [
    "It gets a similar result to the original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad2a92-98be-465d-9f1c-96b576450a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['search_query: weather'])\n",
    "\n",
    "output = ort_session.run(None, {\n",
    "    \"input_ids\": tokens['input_ids'],\n",
    "    \"attention_mask\": tokens['attention_mask']}\n",
    "                        )[0]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    expected = model(**{k:torch.tensor(v)\n",
    "                        for k,v in tokens.items()}).last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf97b3e-8f22-4291-8645-014c4a6f72fa",
   "metadata": {},
   "source": [
    "The model has been successfully exported!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535132a4-0495-4b51-81d6-ad6afc19cfc8",
   "metadata": {},
   "source": [
    "# What we didn't look at\n",
    "\n",
    "There are a few more things I haven't done here that would be part of a more production complete solution.\n",
    "\n",
    "## Putting the pooling layer into ONNX\n",
    "\n",
    "For most applications we don't want the individual token embeddings, but just the mean pooled and normalised embeddings.\n",
    "Ideally this would be in the ONNX model so that logic doesn't need to be implemented in the calling code.\n",
    "This can be done by wrapping the module in a similar way to how SentenceTransformers does before exporting.\n",
    "\n",
    "## Conditional computation in RoPE\n",
    "\n",
    "When the model is exported there are warnings even after patching the Mixture of Experts model (which are not visible above due to how Python suppresses multiple occurrences of the same warning).\n",
    "This is because there is [caching of the computation](https://huggingface.co/nomic-ai/nomic-bert-2048/blob/7710840340a098cfb869c4f65e87cf2b1b70caca/modeling_hf_nomic_bert.py#L1339) of the RoPE matrices to the maximum length seen.\n",
    "If the model is exported before running any inference (as we did above) then PyTorch JIT will capture the cache needs to be recalculated each time and you will always get the correct result.\n",
    "However if the model is called before export, and when is exported uses that cache, then it will always use that cache and there will be errors in ONNX inference if it's called on an input larger than the cached length.\n",
    "\n",
    "This is easily monkey patched in a similar way to the NomicExpert models by removing all caching (or alternatively building a fixed maximum size cache).\n",
    "\n",
    "## Torch Dynamo Export\n",
    "\n",
    "The TorchScript JIT export we used is going to eventually deprecated and replaced with [Torch Dynamo](https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html).\n",
    "Torch Dynamo is much more flexible and safer - it refuses to export the original malfunctioning model we had (with a very opaque error message).\n",
    "After replacing the Experts and RoPE layers the model can be exported with Torch Dynamo.\n",
    "\n",
    "## Conditional Computation in ONNX\n",
    "\n",
    "Torch Dynamo also let's us use some conditional computation in ONNX.\n",
    "ONNX has an [If operator](https://onnx.ai/onnx/operators/onnx__If.html) that can take a boolean condition and then execute a different branch depending on that condition.\n",
    "PyTorch allows using this, in a restricted setting using [torch.cond](https://docs.pytorch.org/docs/stable/generated/torch.cond.html) and has a [good tutorial on this](https://docs.pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html).\n",
    "This could potentially be used to only execute some of the experts; for example if the expert is selected we calculate the MLP otherwise we return a tensor of zeros.\n",
    "\n",
    "What impact this, or any other changes to how the mixture of experts calculation, have on the speed of inference needs to be benchmarked in ONNX.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "PyTorch makes exporting many models to ONNX very simple, but when they use a lot of Python logic it can be quite difficult.\n",
    "The best approach is to disect the model and find the layers that are preventing the export and then replace them with layers that can be exported.\n",
    "If you're exporting with the TorchScript JIT (`dynamo=false`) then read the warnings carefully and test the ONNX model gives the correct result against a wide range of inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
