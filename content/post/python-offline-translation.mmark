+++
tags = ["nlp", "python"]
title = "Offline Translation in Python"
date = "2021-09-03T20:35:21+10:00"
feature_image = "/images/translate_samples.png"
+++

Suppose you want to translate text from one language to another.
Most people's first point of call is an online translation service from one of the big cloud providers, and most translation libraries in Python wrap Google translate.
However the free services have rate limits, the paid services can quickly get expensive, and sometimes you have private data you don't want to upload online.
An alternative is to run a machine translation model locally, and thanks to Hugging Face it's pretty simple to do.

There are some downsides to running these models locally.
The quality of the translations will be lower than the cloud providers, and even they produce very strange results [like translating gibberish into religious prophecies](https://www.vice.com/en/article/j5npeg/why-is-google-translate-spitting-out-sinister-religious-prophecies) and sometimes amusing, like in a [Google translation of Final Fantasy IV](https://legendsoflocalization.com/funky-fantasy-iv/).
The quality is often quite good, but sometimes the output is bizarre or wrong.
It can also be quite slow to run, especially if you don't run it on a GPU.

We can run a minimal example using the example from [Hugging Face's wrapper](https://huggingface.co/transformers/model_doc/marian.html) of [Marian Machine Translation](https://marian-nmt.github.io/), the engine behind Microsoft Translator.
First you need to [install PyTorch](https://pytorch.org/), then pip install `sentencepiece` and `huggingface`, the you can run:

```python
from transformers import MarianMTModel, MarianTokenizer
from typing import Sequence

class Translator:
    def __init__(self, source_lang: str, dest_lang: str) -> None:
        self.model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{dest_lang}'
        self.model = MarianMTModel.from_pretrained(self.model_name)
        self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)
        
    def translate(self, texts: Sequence[str]) -> Sequence[str]:
        tokens = self.tokenizer(list(texts), return_tensors="pt", padding=True)
        translate_tokens = self.model.generate(**tokens)
        return [self.tokenizer.decode(t, skip_special_tokens=True) for t in translate_tokens]
        
        
ru_en = Translator('ru', 'en')
ru_en.translate(['что слишком сознавать — это болезнь, настоящая, полная болезнь.'])
# Returns: ['That being too conscious is a disease, a real, complete disease.']
```

The `Translator` object takes a list of sentences from a source language to a destination language.
The languages are two letter [ISO 639 codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), above we create a translator from Russian (`ru`) to English (`en`).
This will then get a model and a tokenizer, which will be downloaded if necessary (and weighs around 300MB).
Marian supports a very large list of languages, you can see all the models on [Hugging Face's model hub](https://huggingface.co/Helsinki-NLP).

It is incredible to me that this is so easy (thanks to Hugging Face), that there are so many languages (thanks to Neural Machine Translation), and that the models are freely available (thanks to Helsinki NLP).
I looked into this about 5 years ago and the best open solution was [Moses Machine Translation](http://www.statmt.org/moses/), which I couldn't get set up in a few hours, has worse translations (statistical methods can get quite good, but require a lot of work and tend to work poorly for informal text, such as web based data) and many fewer languages.
The space of language technology has come a remarkably long way in a few years.
However there are some limitations of these models, in terms of the length of text it can translate, how fast it runs and how it responds to unusual punctuation (relative to the data it was trained on).

# Length of text

If you run it on a long text you will get `IndexError: index out of range in self`; this is a limitation of Transformer models where they have a maximum size input.
The model only supports up to 512 tokens (where a token is dependent on the [SentencePiece encoding](https://github.com/google/sentencepiece), most words are made up of at most a few tokens), and if you pass any more it fails.
The `tokens` returned by the tokenizer is a dictionary containing two tensors, the `input_ids` (which are the actual sentencepiece token ids) and `attention_mask` (which seems to be all 1s).
The tensors are rank 2 with shape the number of texts by the *maximum* number of tokens in any of the texts.
For long texts the best thing to do is to break the text apart at paragraph (ideally, for maximum context), or sentence boundaries and then paste it back together again.

Because the all the texts are put into a single tensor you need to make sure you have the memory (CPU or GPU) available to store it all.
So you'll want to [minibatch](/python-minibatching) a large number of texts into small groups.
In fact since it needs to be processed in a rectangular block you should try to process all the shortest texts together and all the longest texts together for best efficiency.
Moreover if you've got a lot of repeated texts you should cache the results since it's relatively expensive to re-translate it.

# Translation Quality

Finally sometimes punctuation causes some strange hallucinations from the model.
First consider some pure punctuation:

```python
for translation in ru_en.translate(['', '.', '!', '-', '&']):
    print(translation)
```

The model gives some rather creative translations:

```
It's okay. It's okay, it's okay, it's okay.
I don't know.
Hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey.
- Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah.
♪ I don't know ♪
```

For some examples of how it performs I tested it on translations from the [Avito Demand Prediction competition](https://www.kaggle.com/c/avito-demand-prediction), which is in Russian (not a language I know well).
Here's some examples from titles (which are difficult because they're fracments).
Generally the MarianMT translation is close but imprecise; spot checking these against the images, the descriptions from Google Translate are much closer to the meaning.

| Text                                          | MarianMT Translation                             | Google Translate                             |
|-----------------------------------------------|--------------------------------------------------|----------------------------------------------|
| Кокоби(кокон для сна)                         | Cocoby (sleep cocoon)                            | Cocobi (sleeping cocoon)                     |
| Стойка для Одежды                             | Clothes stand                                    | Clothes Rack                                 |
| Philips bluray                                | Philips bluray                                   | Philips bluray                               |
| Автокресло                                    | Truck, car seat                                  | Car seat                                     |
| ВАЗ 2110, 2003                                | VAZ 2110, 2003                                   | VAZ 2110, 2003                               |
| Авто люлька                                   | Car bulwark                                      | Car carrycot                                 |
| Водонагреватель 100 литров нержавейка плоский | The water absorber is 100 litres stainless flat. | Water heater 100 liters stainless steel flat |
| Бойфренды colins                              | Collins Boyfriends                               | boyfriends colins                            |
| Платье                                        | Clothes                                          | Dress                                        |
| Полу ботиночки замш натур.Бамбини             | Half the boots are for nature. Bambini.          | Semi boots suede nature Bambini              |
| 1-к квартира, 25 м², 2/2 эт.                  | One-to-one apartment, 25 m2, 2/2 ot.             | 1-room apartment, 25 m², 2/2 fl.             |
| Джинсы                                        | Jeans.                                           | Jeans                                        |
| Атласы и Контурныя карты за 8 класс           | Atlass and end-of-grade maps                     | Atlases and Contour maps for grade 8         |
| Монитор acer 18.5                             | Monitor acer 18.5                                | acer 18.5 monitor                            |
| Продаются щенки немецкой овчарки              | German shepherd's puppies are for sale.          | German Shepherd puppies for sale             |
| Платье женское новое                          | A woman's new dress.                             | Women's dress new                            |
| Chevrolet Lanos, 2008                         | Chevrolet Lanos, 2008                            | Chevrolet Lanos, 2008                        |
| Объемная цифра 2                              | Volume 2                                         | 3D figure 2                                  |
| Куртка весенняя(осенняя)                      | Spring jacket (Spring)                           | Jacket spring (autumn)                       |
| Сниму коттедж                                 | I'll take the cottage off.                       | Cottage for rent                             |

## Effect of punctuation

The MarianMT model is very sensitive to punctuation and capitalisation.
This is likely because the underlying sentencepiece tokenizer doesn't treat these specially; this makes it incredibly flexible for languages with non-European punctuation and tokenisation (for example Thai, Arabic and Mandarin Chinese).
However it means it performs less well with strange punctuation and capitalisation.
Consider the following description fragment from the Avito competition:

> Чтобы посмотреть весь ассортимент нашего магазина перейдите по ссылке в блоке справа ⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒/
>
> НАЛИЧИЕ ТОВАРА УТОЧНЯЙТЕ ПО КОТАКТНОМУ ТЕЛЕФОНУ./
>
> Продам Кулер для компьютера COOLER MASTER/
>
> /
>
> В НАШЕМ МАГАЗИНЕ НА ТЕХНИКУ ДАЕТСЯ ГАРАНТИЯ!!!!!!/
> ========================================/

Google Translate gives a plausible translation:

> To view the entire range of our store, follow the link in the block on the right ⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒⇒ /
>
> CHECK OUT THE AVAILABILITY OF THE GOODS BY CONTACT PHONE. /
>
> Selling Cooler for computer COOLER MASTER /
>
> /
>
> IN OUR STORE IT IS GIVEN A WARRANTY !!!!!! /
>
> ======================================== / 

MarianMT loses a lot of the punctuation information, it drops the middle sentence about contacting by telephone, and misses some translations (МАГАЗИНЕ should be shop or store like in the Google translate, but it's been just transliterated to magasine)

> To look at the entire range of our store, you can cross-reference in the right-hand-hand box to sell the Cooler Master/// in our magasine to technic to produce garantium.

It's very sensitive to the punctuation, if we just remove the last forward slash we get a bunch of hallucinated punctuation:

> To look at the entire range of our store, I want you to cross-reference to the right-hand box in our computer, COLER MASTER// IN TECHNOLOGY, GARANTIA!!!!!!!!!!!!/=======================================================================================)============)========================================)=========================================== )))))))))))))))))) )))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))

This could be due to sentencepiece's encoding being impacted by the punctuation (you can see Google translated broke apart the forward slashes from the spaces). What if we remove the extra punctuation?

> Чтобы посмотреть весь ассортимент нашего магазина перейдите по ссылке в блоке справа. 
>
> НАЛИЧИЕ ТОВАРА УТОЧНЯЙТЕ ПО КОТАКТНОМУ ТЕЛЕФОНУ.
>
> Продам Кулер для компьютера COOLER MASTER.
>
> В НАШЕМ МАГАЗИНЕ НА ТЕХНИКУ ДАЕТСЯ ГАРАНТИЯ!!!!!!

Strangely MarianMT drops all but the first line (and it's not because of the newlines):

> To look at the entire range of our store, you can cross the link in the block on the right.

Translating it line by line gives a better result, but it's more literal:

> To see the full range of our store, cross the link in the block on the right.
>
> Cash the product, get it off the phone.
>
> I'll sell Cooler for COOLER MASTER.
>
> THE TECHNOLOGY GUARANTIES IN OUR MAGAZINE!!!!!!!

If we first lowercase it all (using `.lower()` in Python) the line-by-line translation gets a little better:

> To see the entire range of our store, cross the link in the block on the right.
>
> Check whether the product is available on the kitty phone.
>
> I'm gonna sell a cooler master cooler.
>
> in our hardware store there's a guarantee!!!!!!!!!!!


# Speed

The speed of translation can be slow for a large amount of text, but it is trivially paralellisable.
Running on my CPU for titles (typically a few words/tokens) I could get through about 1000 per 15 minutes of CPU time (on my laptop).
Descriptions, which average around 27 words, would take about 25 CPU minutes per 100.
So to process on a single CPU the whole 1.5 million Avito titles would take about 2 weeks, and the descriptions almost a year.

It automatically runs on multiple cores, and you could distribute it between multiple machines.
Running on a GPU is around 50 times faster; on a Kaggle GPU I could get through 1000 titles in under 30 seconds, and 100 descriptions in 40 seconds (tuning the mini-batch size appropriately), so you could translate all 1.5 million descriptions in about a week.

Here's the code I used to benchmark these including caching and mini-batching.
Note that for descriptions I split the text first on newlines before passing it.
As noted above for a more robust solution you would also need to preprocess punctuation; but I'm not sure if there are general heuristics here that work well across many datasets.

```
class Translator:
    def __init__(self, source_lang, dest_lang, cuda=False) -> None:
        self.cuda = cuda
        self.model_name = f'Helsinki-NLP/opus-mt-{source_lang}-{dest_lang}'
        self.model = MarianMTModel.from_pretrained(self.model_name)
        if cuda:
            self.model = self.model.cuda()
        self.tokenizer = MarianTokenizer.from_pretrained(self.model_name)
        
    def translate(self, texts: Sequence[str], batch_size:int=10) -> Sequence[str]:
        if isinstance(texts, str):
            raise ValueError('Expected a sequence of texts')
        texts = list(texts)
        translations = {t: None for t in texts}
    
        for text_batch in minibatch(sorted(translations, key=len, reversed=True), batch_size):
            tokens = self.tokenizer(text_batch, return_tensors="pt", padding=True)
            if self.cuda:
                tokens = {k:v.cuda() for k, v in tokens.items()}
            translate_tokens = self.model.generate(**tokens)
            translate_batch = [self.tokenizer.decode(t, skip_special_tokens=True) for t in translate_tokens]
            for (text, translated) in zip(text_batch, translate_batch):
                translations[text] = translated
            
        return [translations[t] for t in texts]
```

# Conclusion

I think it's amazing that in a relatively short amount of time you can get reasonable machine translation on a commodity PC.
There's some hoops you have to jump through to get it to work, and it's a little bit slow on CPU, but it's great that it's even possible and that these models are in the open.
You couldn't use it to replace a professional machine translation service (without at least some fine tuning on a more specific dataset), but it's definitely good enough to be useful.
It would be interesting to test it out on some low resource languages to see how well the model performs.
