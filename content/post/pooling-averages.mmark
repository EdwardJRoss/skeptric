+++
tags = ["statistics"]
title = "Better than Average Statistics"
date = "2022-12-20T09:05:05+11:00"
feature_image = "/images/partial_pooling.png"
draft = true
+++

# Better than Average Statistics

Averages by group are the standard tool for summarising groups, comparing groups, and showing changes over time.
At SEEK averages help us facilitate an efficient employment marketplace by informing our customers about the market, and to deliver fair value to our customers.
We show the typical salary for a role to both hirers and candidates to inform them of the market rates and set their expectations.
We help candidates find potential career paths in [SEEK Career Advice](https://www.seek.com.au/career-advice) using average rates of career transitions from SEEK profiles.
We help hirers with their talent and sourcing strategies by providing [SEEK Employment Reports](https://www.seek.com.au/about/news/seek-employment-reports/) which shows how the average supply of jobs and candidate applications is changing over time.

There are more accurate ways to calculate group averages than adding up the values and dividing by the count in each group separately.
The groups are usually related in some way and calculating the averages separately discards relevant information.
Using information about how the groups are related can almost always lead to better estimates, which in turn leads to better decisions.
Even when the groups are independent, shrinking the averages closer together leads to a more accurate overall estimate by reducing the overall variance.

Averages can be improved by shrinking the estimates on small groups.
In a group with few observations it's much more likely that the group estimate will be far from the true average.
The sample variance halves as the group size doubles, and so the estimates on the small groups are unstable.
Adjusting these estimates to be closer to the averages from other groups reduces the variance substantially, at the cost of a small increase in bias.
The magnitude of the adjustment depends on the variation within the group relative to the variation between groups; the smaller the group the more it should be shrunk.
In many applications there is a long tail of small groups and the overall accuracy can be improved substantially.

A simple way to adjust for small groups is to estimate them all with the overall average across all data points.
Groups below some threshold size, the small groups, are all estimated with the overall average.
Groups above the threshold size are estimated by their independent group average.
This is very simple to implement and can produce better estimates, but is a rough heuristic.

A more principled and effective way to adjust for group size is to represent them as a hierarchical model.
Instead of treating each group average as independent, they are modelled as being drawn from some common distribution.
Each data point from one group informs the common distribution, which adjusts the estimates for data from other groups.
The estimates for smaller groups, where there is less information, end up closer to the centre of this distribution.
This leads to substantially better estimates, and is generally a better default way to calculate averages.

Even better estimates can potentially be obtained with more complex models, at the cost of developing these models and less transparency.
Reframing the problem of calculating averages on groups as a machine learning problem enables more sophisticated approaches.
When there are multiple groupings these can be treated as separate predictors in a fallback or heirarchical model.
Related data sources can inform relationships between the groups which can be incorporated into the model.
These methods can tease out subtle relationships that enable more fine grained comparisons between groups.

The best method to calculate averages on groups depends on the use case.
When there is low variance in the groups the simplest model of calculating individual group averages works well.
When there is high variance and small groups using more complex approaches can lead to better estimates.
The complexity worth having to achieve better accuracy depends on how much that accuracy will inform better outcomes.
This can be demonstrated with an example from SEEK's marketplace.


## Show me the money

When an employer posts a job ad on SEEK they can choose whether to show the salary range for the role.
For some kinds of roles it is very common to show the salary, and for other kinds it's not.
We're going to calculate how likely it is for a hirer to show the salary range for each role.

Below is a sample of 12 roles with the percentage of advertisers who show the salary on SEEK.
For example choosing a random advertiser who hires dentists, then choosing a random dentist ad from that hirer, there is a 13% chance the salary will be displayed.
This is just an estimate of the true probability, but it's within a few percentage points because of the large number of advertisers on SEEK.
We'll then try to estimate the this probability from a small sample of 450 job ads.


| Role Title             | Population Salary Shown | Sample Ads | Sample Ads with Salary |
|------------------------|-------------------------|------------|------------------------|
| Dentist                | 13%                     | 30         | 4                      |
| Enrolled Nurse         | 21%                     | 7          | 1                      |
| Dental Hygienist       | 22%                     | 2          | 1                      |
| Registered Nurse       | 22%                     | 47         | 9                      |
| General Practitioner   | 23%                     | 20         | 7                      |
| Medical Receptionist   | 27%                     | 130        | 39                     |
| Dental Assistant       | 28%                     | 105        | 41                     |
| Dental Receptionist    | 29%                     | 27         | 6                      |
| Chiropractor           | 31%                     | 0          | 0                      |
| Physiotherapist        | 38%                     | 50         | 23                     |
| Occupational Therapist | 40%                     | 28         | 16                     |
| Chiropractic Assistant | 43%                     | 4          | 2                      |

The sample has 30 Dentist job ads, from different advertisers, 4 of which have salary.
The simple average 4/30 is around 13% which happens to be very close to the whole population probability.
For an enrolled nurse the simple average of 1/7 is 14% which is substantially lower than the probability of 21%.
Because the sample is so small the estimates have high variation.

Comparing the accuracy of different estimates in representing the population requires a metric.
For this example we will use the Root Mean Squared Average (RMSE) for each group because it is easy to interpret as the typical percentage point error. 
The logistic loss (or equivalently binary cross entropy) is more appropriate for a binary outcome, but in this specific case it doesn't change the outcomes.
For some applications it makes sense to weight the error, for example by volume, but we will treat each group as equally important.

In this example there are only 12 groups, but in many applications real at SEEK we have tens of thousands or more.
The techniques here work much better the more groups there are, but 12 is enough to show the impact and make it easy to follow.
These group estimates have very high variance which makes these techniques particularly effective.
In contrast when estimating average salary, which may vary by 20% within a role, but between roles can vary from $20 an hour to over $100 an hour, there is little benefit in shrinking the estimates.

# Baseline Averages

In any machine learning problem it is always good to start with a simple baseline.
The best constant model is the proportion of ads with salary shown.
That is the 149 ads with salary shown, divided by 450 ads in the sample, which is 33%.

| Role Title             | Overall Sample Average | Population Salary Shown | Error |
|------------------------|------------------------|-------------------------|-------|
| Dentist                | 33%                    | 13%                     | 20%   |
| Enrolled Nurse         | 33%                    | 21%                     | 12%   |
| Dental Hygienist       | 33%                    | 22%                     | 11%   |
| Registered Nurse       | 33%                    | 22%                     | 11%   |
| General Practitioner   | 33%                    | 23%                     | 10%   |
| Medical Receptionist   | 33%                    | 27%                     | 6%    |
| Dental Assistant       | 33%                    | 28%                     | 5%    |
| Dental Receptionist    | 33%                    | 29%                     | 4%    |
| Chiropractor           | 33%                    | 31%                     | 2%    |
| Physiotherapist        | 33%                    | 38%                     | -5%   |
| Occupational Therapist | 33%                    | 40%                     | -7%   |
| Chiropractic Assistant | 33%                    | 43%                     | -10%  |

Squaring the errors and calculating the mean gives an RMSE of 9.8%.
This estimate is about 10 percentage points from the population values, which matches the table above.

The other obvious baseline is calculating the average for each group individually.
This is difficult for chiropractors where there are no ads in the sample; there is no average to calculate.
A reasonable value to impute is the overall sample average of 33%; it's a better guess than 0%, 50% or 100%.

| Role Title             | Group Average | Population Salary Shown | Error |
|------------------------|---------------|-------------------------|-------|
| Dentist                | 13%           | 13%                     | 0%    |
| Enrolled Nurse         | 14%           | 21%                     | -7%   |
| Dental Hygienist       | 50%           | 22%                     | 28%   |
| Registered Nurse       | 19%           | 22%                     | -3%   |
| General Practitioner   | 35%           | 23%                     | 12%   |
| Medical Receptionist   | 30%           | 27%                     | 3%    |
| Dental Assistant       | 39%           | 28%                     | 11%   |
| Dental Receptionist    | 22%           | 29%                     | -7%   |
| Chiropractor           | *33%*         | 31%                     | *2%*  |
| Physiotherapist        | 46%           | 38%                     | 8%    |
| Occupational Therapist | 57%           | 40%                     | 17%   |
| Chiropractic Assistant | 50%           | 43%                     | 7%    |


Squaring the errors and calculating the mean gives an RMSE of 11.4%.
Because many of the samples are small the variance in the estimates is very high, giving a worse estimate than the overall average.
In particular for a Dental Hygienist there are only 2 ads, 1 with salary and so the sample estimate of 50% is much too high.

The two baselines are different extremes of the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
The overall average is the lowest variance estimate, all predictions are the same, but the estimates are biased away from their true values to the overall average.
The group average is the lowest bias estimate, but the variance is high, especially in small groups, and it gives a worse estimate than the overall average.
The group average makes its largest errors on smaller groups, wheras the overall average makes errors independent of the size of the group.
A better estimate would be somewhere between the two extremes.

![Squared error of group average vs overall average](/images/group_vs_overall_average_squared_error.png)


# Fallback Method

The fallback method uses the overall average for small groups, and the group average for large groups.
When calculating the group average we had to impute the average for a Chiropractor, with no ads, as the overall average.
But for a small group like a Dental Hygienist with only 2 ads the overall average is a better estimate as well.
Assigning any group below some threshold size to the overall average reduces the largest part of the variance.

The threshold could be chosen through statistical heuristics.
Ideally the variance within groups would be balanced against the variance between groups.
The variance between the groups can be estimated as the standard deviation of the group averages, which is 15%.
Including the small groups, where the variance is unstable, inflates the between group standard deviation so this will be an overestimate.
To estimate the binomial sample with standard deviation $$\sigma$$ requires a sample size of $$n=\frac{p(1-p)}{\sigma^2}$$.
Using the overall average $$p=0.33$$ and $$\sigma=0.15$$ which gives a sample size of 10, or a bit larger.
Alternatively cross-validation could be used to estimate the threshold; in this case it's not too sensitive.

![Comparing squared error of Fallback with Group Average and Overall Average](/images/fallback_average_squared_error.png)

The RMSE of the fallback average is 8.1%, substantially lower than the overall average RMSE of 9.8% and the group average of 11.4%.
The error in Dental Hygienist is significantly reduced using the fallback, and the errors for Enrolled Nurse and Chiropractic Assistant are slightly increased.
Together this leads to a net benefit and a better estimate.

The change in the estimate across the threshold is very sharp.
For groups below the threshold size all the specific data from that group is ignored.
For groups above the threshold size all the data from other groups is ignored.
A better approach would be to interpolate between the two extremes.

# Partial Pooling Model

A simple way to balance the information from each group and the information between groups is a weighted average.
In each group we have $$n$$ ads, $$k$$ of which have salary shown, given a group proportion of $$p=k/n$$.
For a group with no data the proportion should be $$P$$ (close to the overall average), and there's some effective number of ads $$N$$.
Then the estimate for the group is the weighted average:

$$ \frac{N P + np}{N+n} $$

So when there are no ads the estimated average is $$P$$, and as the number of ads increases it gets closer to $$p$$.

The values for $$N$$ and $$P$$ can be estimated by a statistical model.
The probability of an ad having salary from a group are assumed to be drawn from a common distribution, rather than independently.
In particular it's assumed to be a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) peaked at $$P$$, with strength $$N+2$$.
The likelihood of the data under any given $$P$$ and $$N$$ can be calculated.
The best choice is the values for which the actual data is most likely.


![Likelihood at different values of N and P](/images/likelihood_heirarchical_pooling.png)

This gives most likely values of $$P=30\%$$ and $$N=100$$.
Notice that the most likely probablity is a little different to the overall average of 33%.
The overall average finds the centre of the groups weighted by size, whereas $$P$$ is the centre of the groups weighted equally.
Partial pooling then shrinks the estimates towards $$P=30\%$$, weighting this prior estimate by $$N=100$$.
A Dental Assistant, with 105 ads, shrinks around halfway between the group average and 30%.
A Dental Hygenist with 2 ads shrinks almost all the way to 30%.

![Partial Pooling shrinks group averages towards the centre](/images/partial_pooling.png)

The RMSE of Partial Pooling is 7.3%, a full 2.5 percentage points lower than the overall average, and 0.8 percentage points lower than the fallback method.
It does particularly well for large numbers of ads where it always beats the worst of the group average and the overall average.

![Partial Pooling has lower mean error than Group or Overall Average](/images/partial_pooling_squared_error.png)

# Averages as Machine Learning

Calculating a group average is a regression problem where all the predictors are categorical.
Treating group averages as a statistical inference problem rather than a descriptive procedure of adding values and dividing counts can lead to significantly more accurate results.
Typical measures of central tendancy such as the mean, median and proportion can be reframed as the problem of estimating a value measured with independent random errors from a particular statistical error distribution.
This is equivalent to finding the estimate that minimises the related loss function, the negative log likelihood of the error distribution.


| Measure of Centrality | Statistical Error Distribution                                | Loss Function                                                | Loss at Average Value                                                                                            |
|-----------------------|---------------------------------------------------------------|--------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Mean                  | Normal                                                        | Mean Squared Error                                           | Standard Deviation                                                                                               |
| Median                | [Laplace](https://en.wikipedia.org/wiki/Laplace_distribution) | Mean Absolute Error                                          | [MAD median](https://en.wikipedia.org/wiki/Average_absolute_deviation#Mean_absolute_deviation_around_the_median) |
| Proportion            | Binomial                                                      | [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) |                                                                                                                  |

Treating all of the categorical predictors as independent will overfit the small groups when their variance is high.
Calculating the proportion of jobs that showed salary for each group led to very poor estimates on small groups.
Falling back to the overall average on small groups reduced the error by decreasing variance significantly for a small increase in bias; this is similar to the common practice of putting small groups together into an "Other" category.
Modelling the proportions as random variables drawn from a common distribution regularised the estimates and resulted in the best estimators on this data.

These ideas can be generalised to more complex settings than a single categorical predictor.
At SEEK we're interested in understanding the employment market across roles, locations, customer segments, and time.
The intersection of all of these factors is very sparse and so modelling to reduce the uncertainty is even more valuable.

Fallback methods can be generalised to a heirarchy of categories.
For example whether a hirer will show salary depends on the role and location, but location may be less important.
Then a fallback model could first try to estimate based on both role and location, and when there's not enough data fallback to role, and for uncommon roles fallback to the overall average.
These are straightforward to interpret, and can significantly reduce the variance.
A more data driven, but less interpretable, approach would be to use decision trees to form groups with similar outcomes.
This can be combined with time series methods at each level to handle seasonality and trends.

Partial pooling methods can be generalised to more general mixed effects models, also known as multilevel models.
Whether a type of job will display salary can be modelled as a mixed effects logistic regression.
This can be fit using, among other methods, Restricted Maximum Likelihood methods (such as in [lme4](https://cran.r-project.org/web/packages/lme4/index.html)) which are fast, or Markov Chain Monte Carlo methods (such as in [Stan](https://mc-stan.org/) or [PyMC](https://docs.pymc.io/)) which are flexible but can be slow on large datasets.
These can be extended to handle dependent data, such as multiple jobs posted by the same hirer, changes over time and adjusting for a non-representative sample.

Incorporating additional data sources can further improve the models.
The models discussed so far treat all role titles the same, and only distinguish them by the number of samples in the group.
However some roles are more similar than others, and this information can be used to create better models.
For example whether a role has a salary shown depends partly on the policies of the employer.
So two roles that are commonly posted by the same kinds of employer may have a similar likelihood to have the salary shown.
This similarity can be represented as a matrix of how likely an employer is to have advertised for one kind of role, given they have advertised for another.
These can then be used to form clusters of similar roles that can be used as a feature in the models.

![Heatmap showing likelihood of a hirer advertising a job for a role given they have advertised a job for another role](/images/role_cross_advertise.png)

Additional data sources can also be used as embeddings.
The external data can be used to form categorical embeddings; representing each category as a dense vector.
These can then be used as continuous predictors in a large number of models; from linear and logistic regression to boosted trees and neural networks.
This is a form of transfer learning.
The similarity matrix above is a form of embedding, and can be visualised by projecting into a two dimensional space.

![Two dimensional embedding of roles based on cross-advertisement](/images/embedding_cross_advertising)

# Calculating better averages

The usual way of calculating group averages, summing the values and dividing by the count, can give very bad estimates on small groups.
The best generic way of handling this is with an empirical Bayes mdoel.
To tease out more information we can use machine learning.

# Further reading


The original paper showing inadmissibility is [Inadmissibility of the usual estimator for the mean of a multivariate normal distribution](https://projecteuclid.org/download/pdf_1/euclid.bsmsp/1200501656) by Stein (1956).
This was followed with the explicit estimator in [Estimation with Quadratic Loss](https://projecteuclid.org/ebook/Download?urlId=bsmsp%2F1200512173&isFullBook=False) by James and Stein (1961).


https://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10481215
Limiting the Risk of Bayes and Empirical Bayes Estimatorsâ€”Part II: The Empirical Bayes Case
Efron and Morris (1970)

Efron, B., & Morris, C. (1975). Data analysis using Stein's estimator and its generalizations. Journal of the American Statistical Association, 70(350), 311-319 (link to pdf)
http://www.medicine.mcgill.ca/epidemiology/hanley/bios602/MultilevelData/EfronMorrisJASA1975.pdf



Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models)
https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478825
RE Kass, D Steffey - Journal of the American Statistical Association, 1989 - Taylor & Francis
