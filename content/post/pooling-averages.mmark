+++
tags = [""]
title = "Better than Average Statistics"
date = "2021-12-20T09:05:05+11:00"
feature_image = "/images/"
draft = true
+++

# Better than Average Statistics

Small groups are a big pain when calculating averages.
Some will float to the top and others sink to the bottom of most rankings; not because they're actually more extreme, but because they have more random variation.
Often there are many small groups (as in [Zip's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)), and the sampling error in estimating the mean roughly halves when the sample size quadruples.
When the variation within the small groups is larger than the true variation between groups, then by chance some of the small groups will have the largest and smallest averages.
Generally when the variation within small groups is too big the average isn't really representative of the group.

There are better ways to calculate averages when the variation within groups is too large.
Re-framing the descriptive act of calculating averages as a predictive machine learning problem enables comparing a wide variety of approaches.
The groups themselves can be changed, and by merging together small groups the estimate's variance can be reduced, at the cost of increasing its bias.
Information can be pooled between groups; instead of treating each group as independent, the estimates on larger groups can inform the estimates on smaller groups.
Taking this further, if the groups can be meaningfully embedded in some vector space and the average can be calculated as a regression on the embeddings, which allows larger groups to further inform smaller groups.

At SEEK we've used these techniques to improve our understanding of the employment marketplace and better inform decisions.

# Averages as Machine Learning

An average is a machine learning model.
It doesn't seem like a machine learning model; you just sum the values and divide by the count.
But averages are related to *constant models*, the model that returns the same prediction for all inputs.
In particular assuming that each data point is a constant plus a random error from a normal distribution, then the best estimate for the constant minimised the Mean Squared Error loss.
And the constant that minimises mean squared error is the mean; that is the mean is the parameter of the constant model under a normal error distribution.
This can be generalised to other kinds of averages such as the median, and the proportion of positive results in a binary outcome as in the table below (and can be further extended through transformations such as taking logarithms).

| Average    | Loss Function Minimised                                      | Error Distribution                                            | Minimum Loss                                                                                                     |
|------------|--------------------------------------------------------------|---------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| Mean       | Mean Squared Error                                           | Normal                                                        | Standard Deviation                                                                                               |
| Median     | Mean Absolute Error                                          | [Laplace](https://en.wikipedia.org/wiki/Laplace_distribution) | [MAD median](https://en.wikipedia.org/wiki/Average_absolute_deviation#Mean_absolute_deviation_around_the_median) |
| Proportion | [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) | Binomial                                                      |                                                                                                                  |


Averages on groups can also be viewed as a machine learning problem.
In fact calculating the mean by group is equivalent to linear regression on the groups as categorical variables, and the proportion is equivalent to logistic regression.
As in the single average case we can pick the related loss function and calculate it elementwise (which could be called the micro-loss).
This allows comparing statistics between different groupings, since the loss function only depends on the points.
However in some applications the groups really matter and should have an equal weight, in which case the loss should be calculated on each group separately and averaged (which could be called the macro-loss).

# Making Larger Groups

When someone is placing a job ad on SEEK it's useful to know how many people typically apply for that sort of job.
It helps the hirer with their recruitment strategy (should they place a premium ad or consider proactive sourcing), and it helps SEEK to make sure each ad gets a fair share of attention from candidates who may apply.
Ideally we could calculate the average number of applicants for each possible role title and location, but there are thousands of potential role titles and many combinations are uncommon.
There are very few Data Scientist roles in Bundaberg or Metallurgist roles in Canberra (if any), and so the averages will be unstable (or unavailable).

One solution is falling back to larger groups when there's not enough data for a stable estimate.
There is a trade-off between the error due to variance (which is related to the variation within groups) and bias (which is related to the variation between groups).
One extreme is to use the whole market average for every group; every role is the same and would get the same amount of applications.
The other is to calculate each group average individually; even if there's only one job ad in that group.
We can balance the two by using the group average for groups above a certain threshold size, and the market average for groups below that threshold.
Then the threshold itself is a hyperparameter we can tune to minimise loss in a hold-out set.

There are lots of different ways of making groups and fallback strategies.
When the data has an underlying hierarchical structure this makes a natural fallback sequence for groups; cities occur within states, and states occur within countries.
Groups can also be formed from the underlying data for example by clustering on related attributes; for example if candidates who apply for one kind of role tend to apply for another then those roles could be grouped together.
A decision tree can be used to automatically construct relevant groups from the data.
The more static a model is the easier it is to explain and debug underlying data issues.
The data driven models can create groupings with a smaller loss, but need to be updated as the market changes (which may completely change the group structure); while a role may have been getting a similar number of applicants in both Melbourne and Perth in 2019, the effects of COVID restrictions would have significantly changed that in 2020 and 2021.

There are not many off the shelf libraries for fallback models.
There are lots of great libraries for decision trees, and in particular [ctree](https://cran.r-project.org/web/packages/partykit/vignettes/ctree.pdf) in R implements [Conditional Inference Trees](https://www.tandfonline.com/doi/abs/10.1198/106186006X133933) which overcome the selection bias and overfitting issues in decision trees.
A different approach for handling small groups is to use a [Rare Label Encoder](https://feature-engine.readthedocs.io/en/1.2.x/api_doc/encoding/RareLabelEncoder.html) to combine the small groups into an "Other" group and then perform a linear regression with [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) or [statsmodels](https://www.statsmodels.org/stable/regression.html).
However we've found empirically that for small groups falling back to a larger group gave better estimates than creating an "other" group, and have built a SEEK internal scikit-learn model for handling custom fallbacks.

While changing the groupings creates interpretable models, it cleaves a sharp division between groups.
Once the groups are formed they are treated completely independently.


# Pooling data

Suppose there's a role we've never seen before (for example COVID testers), what's our best guess for the number of applications it will receive.
Without knowing how it relates to other roles a reasonable guess is the overall market average.
As we start getting some observations, seeing how many applications these jobs receive, the market average becomes less relevant.
In the fallback model we cut over to the group average once we reached some threshold; but instead of a sharp transition would it be possible to smoothly interpolate between the two?

Instead of treating each groups mean as independent, we could model them as random variables themselves drawn from a normal distribution, with its own mean and standard deviation (related to the variation between groups).
Then before having any data, our best estimate of the mean number of people applying for the new role would be the mean of this distribution.
But as we get some observations the best estimate starts pulling towards the observations, until we have enough observations.
This neatly handles the small group problem by shrinking the unstable estimates of small groups towards the overall mean; and determines the amount to shrink from the data itself.

This approach is called a multilevel model or a mixed effects model, and is very flexible.
The model proposed above is very simple; each observation comes from a normally distributed group with a mean drawn from a larger group.


lme4, https://www.statsmodels.org/stable/mixed_linear.html
brms, Stan, PyMC
Empirical Bayes


# Embeddngs

Behavioural embeddings: cross-apply.
Linguistic embeddings: title similarity.
Co-occurance embeddings: skills that co-occur across roles (TF-IDF)

For running these evaluations I've found Logistic Regression with a L2 regularisation (i.e. logistic ridge regression) works quite well.
In scikit learn this is the default for [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and you just need to tune the regularisation parameter C (you can do this automatically with cross-validation using [`LogisticRegressionCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)), and it can handle large sparse term-frequency matrices using [`scipy.sparse.csr_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html).
Regularisation allows use of large dimensional embedding matrices without over-fitting.

# Further reading


The original paper showing inadmissibility is [Inadmissibility of the usual estimator for the mean of a multivariate normal distribution](https://projecteuclid.org/download/pdf_1/euclid.bsmsp/1200501656) by Stein (1956).
This was followed with the explicit estimator in [Estimation with Quadratic Loss](https://projecteuclid.org/ebook/Download?urlId=bsmsp%2F1200512173&isFullBook=False) by James and Stein (1961).
