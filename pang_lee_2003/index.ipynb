{
 "cells": [
  {
   "cell_type": "raw",
   "id": "622f0834",
   "metadata": {},
   "source": [
    "---\n",
    "categories:\n",
    "  - nlp\n",
    "  - sentiment\n",
    "date: '2023-05-25T21:18:00+10:00'\n",
    "image: thumbs_up_pang_lee_2002.png\n",
    "title: Thumbs Up? Sentiment Classification Like it's 2002\n",
    "toc: true\n",
    "toc-depth: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7832b7",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In July 2002 Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan published [*Thumbs up? Sentiment Classification using Machine Learning Techniques.*](https://aclanthology.org/W02-1011/) at EMNLP, one of the earliest works of using machine learning for Sentiment Classification.\n",
    "It was an influential paper, [winning a test of time award](https://vimeo.com/280330773) at NAACL 2018, and at the time of writing has over 11,000 citations.\n",
    "This work led to their follow up [Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales](https://arxiv.org/abs/cs/0506075) and this dataset was the basis for the Stanford Sentiment Treebank dataset released in [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://aclanthology.org/D13-1170.pdf) by Socher et al., which is widely used partly because of it's inclusion in GLUE.\n",
    "\n",
    "This paper aims to show that classifying the sentiment of movie reviews is a more challenging problem to develop machine learning techniques on than the existing topic classification problems, and motivate further work (in which they were successful!)\n",
    "They do this by building a self-labelled dataset of polar movie reviews from Usenet and then show baseline classifiers don't work as well as existing topic classification datasets.\n",
    "\n",
    "This notebook aims to explore the paper and its methods in more detail, and the headings follow the paper section by section.\n",
    "We go much deeper into the data than the paper, and reproduce their methods, and get similar (but slightly better) results.\n",
    "A good future work would be to look into applying more modern methods on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e9e96",
   "metadata": {},
   "source": [
    "# Related work\n",
    "\n",
    "A lot of the papers cited in Related Work are hard to access today, or contain almost no detail.\n",
    "There are three exceptions:\n",
    "\n",
    "* Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. [Predicting the Semantic Orientation of Adjectives](https://aclanthology.org/P97-1023/) mines positive and negative semantic lexicons for adjectives by looking at nearby conjunctions (e.g. \"fair and legitimate\")\n",
    "* Turney and Littman, 2002. [Unsupervised Learning of Semantic Orientation from a Hundred-Billion-Word Corpus](https://arxiv.org/abs/cs/0212012) determines whether a word is positive or negative based on whether it occurs near a positive word (like good or nice) or a negative word (like bad or poor) on the web. In particular it leveraged AltaVista's \"near\" search and calculates the Pointwise Mutual Information for each sentiment pair.\n",
    "* Peter Turney. 2002. [Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews](https://aclanthology.org/P02-1053/) was concurrent with this work and uses the approach of the previous paper to classify the sentiment of reviews from epinions.com. In particular it extracts certain POS tags, works out their semantic orientation (using \"good\" as the positive term and \"poor\" as the negative), and adds the log scores for all the words in each review.\n",
    "\n",
    "This paper differs in that it uses more classical machine learning approaches with 1-hot encoded word features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3f881",
   "metadata": {},
   "source": [
    "# The Movie Review Domain\n",
    "\n",
    "They took reviews from the Internet Movie Database (IMDb) archive of the `rec.arts.movies.reviews`, took the reviews with a numerical or star rating and labelled the highest scored ones positive, the lowest negative, and removed the rest.\n",
    "\n",
    "The IMDb archive no longer exists, but there are current archives of this newsgroup in [Google Groups](https://groups.google.com/g/rec.arts.movies.reviews) and the [Usenet Archives](https://www.usenetarchives.com/threads.php?id=rec.arts.movies.reviews&y=0&r=0&p=0).\n",
    "Thankfully the authors released their [original data](https://www.cs.cornell.edu/people/pabo/movie-review-data/) both the raw HTML they extracted and the extracted text they used for classification.\n",
    "\n",
    "Let's take a look at the HTML to see what they worked with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2909c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import re\n",
    "\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "source_html_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/polarity_html.zip'\n",
    "\n",
    "raw_html_path = data_dir / 'polarity_html.zip'\n",
    "if not raw_html_path.exists():\n",
    "    urlretrieve(source_html_url, raw_html_path)\n",
    "    \n",
    "raw_html_zip = ZipFile(raw_html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a85153",
   "metadata": {},
   "source": [
    "The zipfile contains a single directory `movie` containing  around 27k review files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed9d83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27887"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_html_zip.infolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4e7dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ZipInfo filename='movie/0002.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=4415 compress_size=2170>,\n",
       " <ZipInfo filename='movie/0003.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=2702 compress_size=1398>,\n",
       " <ZipInfo filename='movie/0004.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=6165 compress_size=3059>,\n",
       " <ZipInfo filename='movie/0005.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=4427 compress_size=2103>,\n",
       " <ZipInfo filename='movie/0006.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=6423 compress_size=3225>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_html_zip.infolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7966a930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ZipInfo filename='movie/9995.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=5232 compress_size=2643>,\n",
       " <ZipInfo filename='movie/9997.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=10113 compress_size=4812>,\n",
       " <ZipInfo filename='movie/9998.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=3868 compress_size=1935>,\n",
       " <ZipInfo filename='movie/9999.html' compress_type=deflate filemode='-rw-rw-rw-' file_size=3081 compress_size=1605>,\n",
       " <ZipInfo filename='movie/' filemode='drwxrwxrwx' external_attr=0x10>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_html_zip.infolist()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769943d",
   "metadata": {},
   "source": [
    "Let's have a look at one of them (that's not too long, and recent enough to be in other archives); you could also see it on the [Usenet Archives](https://www.usenetarchives.com/view.php?id=rec.arts.movies.reviews&mid=PDE5OTFGZWIxLjE1MjA0My4yNDE2MUBjYm5ld3NqLmF0dC5jb20%2B) or [Google Groups](https://groups.google.com/g/rec.arts.movies.reviews/c/Itr4BZd_EKk/m/QE0gZFPUU50J).\n",
    "\n",
    "Note that the original was almost certainly a plaintext email; some the HTML markup (in particular the footer) would have been added by IMDB.\n",
    "Note that the rating is stated *twice* in the review as a \"low 0\" on a scale from -4 to 4; this begins to indicate the difficulty of automatically extracting the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73e24ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD>\r\n",
      "<TITLE>Review for Flight of the Intruder (1990)</TITLE>\r\n",
      "<LINK REL=\"STYLESHEET\" TYPE=\"text/css\" HREF=\"/ramr.css\">\r\n",
      "</HEAD>\r\n",
      "<BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\r\n",
      "<H1 ALIGN=\"CENTER\" CLASS=\"title\"><A HREF=\"/Title?0099587\">Flight of the Intruder (1990)</A></H1><H3 ALIGN=CENTER>reviewed by<BR><A HREF=\"/ReviewsBy?Mark+R.+Leeper\">Mark R. Leeper</A></H3><HR WIDTH=\"40%\" SIZE=\"4\">\r\n",
      "<PRE>                            FLIGHT OF THE INTRUDER\r\n",
      "                       A film review by Mark R. Leeper\r\n",
      "                        Copyright 1991 Mark R. Leeper</PRE>\r\n",
      "<P>          Capsule review:  Pretty pictures, stupid story.  The\r\n",
      "     air-war of a previous conflict is occasionally entertaining\r\n",
      "     to watch but the plot is cliched as are most of the\r\n",
      "     characters.  This film's only chance is to follow the current\r\n",
      "     wave of interest in military equipment.  Rating: low 0.</P>\r\n",
      "<P>     Had I not actually seen a copy of the book FLIGHT OF THE INTRUDER by\r\n",
      "Stephen Coonts, I would have had a hard time telling if this was a very weak\r\n",
      "story given classy military equipment photography and quality special\r\n",
      "effects treatment or if this was just a collection of classy military\r\n",
      "equipment photography and quality special effects tied together by a very\r\n",
      "weak excuse for a story.  During World War II a lot of B war movies carried\r\n",
      "stories just as good to the bottom half of double bills.  We are talking\r\n",
      "HELLCATS OF THE NAVY-level plotting here.  In 1972 Vietnam we have an\r\n",
      "aircraft carrier ruled over by a cigar-chewing, mean-as-a-junkyard-dog-but-\r\n",
      "heart-of-gold sort of commander.  Danny Glover plays the Black commander\r\n",
      "with the unlikely name Frank Camparelli.  One of his bright young pilots,\r\n",
      "Jake Grafton (played by the uninteresting Brad Johnson) agonizes over the\r\n",
      "loss of his bombardier.  The companion is lost in a raid that accomplishes\r\n",
      "nothing besides adding visual interest to the opening credits.  Grafton\r\n",
      "wants to go on a super-special raid of his own devising.  But this raid is\r\n",
      "directly contrary to orders.  His top-gun replacement bombardier Virgil Cole\r\n",
      "(played by Willem Dafoe) says absolutely not.  Does Jake get to make his\r\n",
      "super-special raid on North Vietnam?  And if he does, what is the Navy's\r\n",
      "reaction?</P>\r\n",
      "<P>     The weak story is, however, punctuated by pretty pictures of planes,\r\n",
      "helicopters, and aircraft carriers to keep the audience watching.  If this\r\n",
      "film stands any chance with audiences it is in the fortuitous timing of this\r\n",
      "film coincident with a sudden upsurge of interest in technical weaponry.\r\n",
      "Indeed many people may find events in the Middle East resonating with\r\n",
      "attitudes in this film.  On the other hand, maybe some people would prefer\r\n",
      "to stay home and watch technical weaponry on television.</P>\r\n",
      "<P>     FLIGHT OF THE INTRUDER is directed by John Milius, who is specializing\r\n",
      "in gutsy films like APOCALYPSE NOW (which he wrote), CONAN THE BARBARIAN,\r\n",
      "and RED DAWN.  The score is by Basil Poledouris, the gifted composer of the\r\n",
      "scores for the \"Conan\" films, who seems repeatedly associated with films\r\n",
      "with right-wing themes.  Poledouris scored RED DAWN, AMERIKA, and THE HUNT\r\n",
      "FOR RED OCTOBER.</P>\r\n",
      "<P>     FLIGHT OF THE INTRUDER is linked in advertising with THE HUNT FOR RED\r\n",
      "OCTOBER, but it falls well short of that film's interest value and quality.\r\n",
      "My rating is a low 0 on the -4 to +4 scale.</P>\r\n",
      "<PRE>                                        Mark R. Leeper\r\n",
      "                                        att!mtgzy!leeper\r\n",
      "                                        <A HREF=\"mailto:leeper@mtgzy.att.com\">leeper@mtgzy.att.com</A>\r\n",
      ".\r\n",
      "</PRE>\r\n",
      "<HR><P CLASS=flush><SMALL>The review above was posted to the\r\n",
      "<A HREF=\"news:rec.arts.movies.reviews\">rec.arts.movies.reviews</A> newsgroup (<A HREF=\"news:de.rec.film.kritiken\">de.rec.film.kritiken</A> for German reviews).<BR>\r\n",
      "The Internet Movie Database accepts no responsibility for the contents of the\r\n",
      "review and has no editorial control. Unless stated otherwise, the copyright\r\n",
      "belongs to the author.<BR>\r\n",
      "Please direct comments/criticisms of the review to relevant newsgroups.<BR>\r\n",
      "Broken URLs inthe reviews are the responsibility of the author.<BR>\r\n",
      "The formatting of the review is likely to differ from the original due\r\n",
      "to ASCII to HTML conversion.\r\n",
      "</SMALL></P>\r\n",
      "<P ALIGN=CENTER>Related links: <A HREF=\"/Reviews/\">index of all rec.arts.movies.reviews reviews</A></P>\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "</P></BODY></HTML>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CODEC = 'ISO-8859-1'\n",
    "movie_review_html = raw_html_zip.read('movie/0908.html').decode(CODEC)\n",
    "print(movie_review_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ce27a",
   "metadata": {},
   "source": [
    "For convenience let's define a function to read the HTML of a given movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8dd81a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_movie_html(movieid):\n",
    "    return raw_html_zip.read(f'movie/{movieid}.html').decode(CODEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d780523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD>\r\n",
      "<TITLE>Review for Flight of the Intruder (1990)</TITLE>\r\n",
      "<LINK REL=\"STYLESHEET\" TYPE=\"text/css\" HREF=\"/ramr.css\">\r\n",
      "</HEAD>\r\n",
      "<BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\r\n",
      "<H1 ALIGN=\"CENTER\" CLASS=\"title\"><A HREF=\"/Title?0099587\">Flight of the Intruder (1990)</A></H1><H3 ALIGN=CENTER>reviewed by<BR><A HREF=\"/ReviewsBy?Mark+R.+Leeper\">Mark R. Leeper</A></H3><HR WIDTH=\"40%\" SIZE=\"4\">\r\n",
      "<PRE>                            FLIGHT OF THE INTRUDER\r\n",
      "                       A film review by Mark R. Leeper\r\n",
      "                        Copyright 1991 Mark R. Leeper</PRE>\r\n",
      "<P>          Capsule review:  Pretty pictures, stupid story.  The\r\n",
      "     air-war of a previous conflict is occasionally entertaining\r\n",
      "     to watch but the plot is cliched as are most of the\r\n",
      "     characters.  This film's only chance is to follow the current\r\n",
      "     wave of interest in military equipment.  Rating: low 0.</P>\r\n",
      "<P>     Had I not actually seen a copy of the book FLIGHT OF THE INTRUDER by\r\n",
      "Stephen Coonts, I would have h\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_html('0908')[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b4beb",
   "metadata": {},
   "source": [
    "## Cleaned Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa06d834",
   "metadata": {},
   "source": [
    "And the cleaned and labelled text we'll get version 1.1 which [according to the README](https://www.cs.cornell.edu/people/pabo/movie-review-data/README.1.1) has some corrections over the version used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619b7ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_0211.tar.gz'\n",
    "sentiment_path = data_dir / 'sentiment.tar.gz'\n",
    "\n",
    "if not sentiment_path.exists():\n",
    "    urlretrieve(sentiment_url, sentiment_path)\n",
    "    \n",
    "urlretrieve(sentiment_url, sentiment_path)\n",
    "\n",
    "sentiment_fh = tarfile.open(sentiment_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7e742",
   "metadata": {},
   "source": [
    "There's:\n",
    "\n",
    "* a `diff.txt`that says what changed between versions\n",
    "* a [`README`](https://www.cs.cornell.edu/people/pabo/movie-review-data/README.1.1) describing the dataset\n",
    "* subfolders `neg/` and `pos/` containing negative and positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b5a3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diff.txt',\n",
       " 'README',\n",
       " 'tokens',\n",
       " 'tokens/neg',\n",
       " 'tokens/neg/cv303_tok-11557.txt',\n",
       " 'tokens/neg/cv000_tok-9611.txt',\n",
       " 'tokens/neg/cv001_tok-19324.txt',\n",
       " 'tokens/neg/cv002_tok-3321.txt',\n",
       " 'tokens/neg/cv003_tok-13044.txt',\n",
       " 'tokens/neg/cv004_tok-25944.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_fh.getnames()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed512d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokens/pos/cv690_tok-23617.txt',\n",
       " 'tokens/pos/cv691_tok-11491.txt',\n",
       " 'tokens/pos/cv692_tok-24295.txt',\n",
       " 'tokens/pos/cv693_tok-16307.txt',\n",
       " 'tokens/pos/cv694_tok-18628.txt',\n",
       " 'tokens/pos/cv695_tok-12873.txt',\n",
       " 'tokens/pos/cv696_tok-10835.txt',\n",
       " 'tokens/pos/cv697_tok-29325.txt',\n",
       " 'tokens/pos/cv698_tok-27735.txt',\n",
       " 'tokens/pos/cv699_tok-10425.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_fh.getnames()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5338a",
   "metadata": {},
   "source": [
    "We can extract the label (pos or neg) the cross-validation id (`cvid`), and the movie id from the filename with a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061d6e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neg', 'cvid': '303', 'movieid': '11557'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'^tokens/(?P<label>[^/]+)/cv(?P<cvid>[0-9]+)_tok-(?P<movieid>[0-9]+).txt$')\n",
    "\n",
    "pattern.match('tokens/neg/cv303_tok-11557.txt').groupdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b850a",
   "metadata": {},
   "source": [
    "Let's extract all the data into aligned lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ede027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping diff.txt\n",
      "Skipping README\n",
      "Skipping tokens\n",
      "Skipping tokens/neg\n",
      "Skipping tokens/pos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 1386, 'cvid': 1386, 'movieid': 1386, 'text': 1386}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'label': [],\n",
    "    'cvid': [],\n",
    "    'movieid': [],\n",
    "    'text': []\n",
    "}\n",
    "    \n",
    "\n",
    "for member in sentiment_fh:\n",
    "    match = pattern.match(member.name)\n",
    "    if not match:\n",
    "        print('Skipping %s' % member.name)\n",
    "        continue\n",
    "    for k, v in match.groupdict().items():\n",
    "        data[k].append(v)\n",
    "    data['text'].append(sentiment_fh.extractfile(member).read().decode(CODEC).rstrip())\n",
    "    \n",
    "{k: len(v) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9194ca0d",
   "metadata": {},
   "source": [
    "> To avoid domination of the corpus\n",
    "by a small number of prolific reviewers, we imposed\n",
    "a limit of fewer than 20 reviews per author per sentiment category, yielding a corpus of 752 negative\n",
    "and 1301 positive reviews, with a total of 144 reviewers represented. \n",
    ">\n",
    "> ...\n",
    ">\n",
    "> we randomly selected 700 positive-sentiment and 700\n",
    "negative-sentiment documents\n",
    "\n",
    "We get slightly fewer likely due to the updates since it was first released"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9267466e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neg': 692, 'pos': 694})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f04406",
   "metadata": {},
   "source": [
    "### Data Lengths\n",
    "\n",
    "These reviews can be quite long, and the tokenization of punctuation is quite aggressive; how long are the actual tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5bbab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(text.split()) for text in data['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c7cd8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(l):\n",
    "    return sorted(l)[len(l)//2]\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fbf006",
   "metadata": {},
   "source": [
    "The median length of reviews is around 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72a36b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "median(lengths), mean(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce927d",
   "metadata": {},
   "source": [
    "Note that negative reviews tend to be a little shorter than positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f82a4c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(681, 715.5664739884393)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_lengths = [x for x, l in zip(lengths, data['label']) if l == 'neg']\n",
    "median(neg_lengths), mean(neg_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256e4bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 798.0345821325649)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lengths = [x for x, l in zip(lengths, data['label']) if l == 'pos']\n",
    "median(pos_lengths), mean(pos_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43c1fc",
   "metadata": {},
   "source": [
    "We can use this to get a few percentage points without even looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9218ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ['pos' if l > 730 else 'neg' for l in lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa91e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5440115440115441"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(preds, actuals):\n",
    "    if len(preds) != len(actuals):\n",
    "        raise ValueError('Expected same length input')\n",
    "    return mean([p==a for p,a in zip(preds, actuals)])\n",
    "\n",
    "accuracy(preds, data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc9e50",
   "metadata": {},
   "source": [
    "# A Closer Look at the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81baf4",
   "metadata": {},
   "source": [
    "From the paper they set a benchmark using manual lists of positive and negative words:\n",
    "\n",
    "> One might also suspect that there are certain words\n",
    "people tend to use to express strong sentiments, so\n",
    "that it might suffice to simply produce a list of such\n",
    "words by introspection and rely on them alone to\n",
    "classify the texts.\n",
    ">\n",
    ">To test this latter hypothesis, we asked two graduate students in computer science to (independently)\n",
    "choose good indicator words for positive and negative sentiments in movie reviews.\n",
    "\n",
    "Extracting these from Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d27967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dazzling', 'brilliant', 'phenomenal', 'excellent', 'fantastic']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive1 = 'dazzling, brilliant, phenomenal, excellent, fantastic'.split(', ')\n",
    "positive1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383cf2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suck', 'terrible', 'awful', 'unwatchable', 'hideous']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative1 = 'suck, terrible, awful, unwatchable, hideous'.split(', ')\n",
    "negative1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2938c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gripping',\n",
       " 'mesmerizing',\n",
       " 'riveting',\n",
       " 'spectacular',\n",
       " 'cool',\n",
       " 'awesome',\n",
       " 'thrilling',\n",
       " 'badass',\n",
       " 'excellent',\n",
       " 'moving',\n",
       " 'exciting']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive2 = 'gripping, mesmerizing, riveting, spectacular, cool, ' \\\n",
    "            'awesome, thrilling, badass, excellent, moving, exciting'.split(', ')\n",
    "positive2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cad566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'cliched', 'sucks', 'boring', 'stupid', 'slow']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative2 = 'bad, cliched, sucks, boring, stupid, slow'.split(', ')\n",
    "negative2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9423ff5",
   "metadata": {},
   "source": [
    "> We then converted their responses into simple decision\n",
    "procedures that essentially count the number of the\n",
    "proposed positive and negative words in a given document. \n",
    "\n",
    "We can build a small classifier to do this.\n",
    "When there's a tie we need to decide how to break it with a \"default\".\n",
    "\n",
    "From the paper\n",
    "\n",
    "> Note that the tie rates — percentage of documents where the two sentiments\n",
    "were rated equally likely — are quite high (we chose\n",
    "a tie breaking policy that maximized the accuracy of\n",
    "the baselines)\n",
    "\n",
    "We need to look at the data for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad3d8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2cat = ['neg', 'pos']\n",
    "cat2idx = {'neg': 0, 'pos': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fdb5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotFittedException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class MatchCountClassifier:\n",
    "    def __init__(self, positive, negative):\n",
    "        self.positive = positive\n",
    "        self.negative = negative\n",
    "        self.default = None\n",
    "        self.ties = None\n",
    "        \n",
    "    def _score(self, tokens):\n",
    "        \"\"\"Return number of positive words - number of negative words in token\"\"\"\n",
    "        pos_count = len([t for t in tokens if t in self.positive])\n",
    "        neg_count = len([t for t in tokens if t in self.negative])\n",
    "        return pos_count - neg_count\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Find default that maximises \"\"\"\n",
    "        scores = [self._score(tokens) for tokens in X]\n",
    "        self.ties = len([x for x in scores if x==0]) / len(scores)\n",
    "        \n",
    "        pred_pos_default = [1 if x >= 0 else 1 for x in scores]\n",
    "        pred_neg_default = [0 if x <= 0 else 0 for x in scores]\n",
    "        \n",
    "        if accuracy(pred_pos_default, y) >= accuracy(pred_neg_default, y):\n",
    "            self.default = 1\n",
    "        else:\n",
    "            self.default = 0\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.default is None:\n",
    "            raise NotFittedException()\n",
    "        scores = [self._score(tokens) for tokens in X]\n",
    "        \n",
    "        return [1 if score > 0 else 0 if score < 0 else self.default for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e20c6",
   "metadata": {},
   "source": [
    "Let's test our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c51c6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_test = MatchCountClassifier(['happy'], ['sad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef9421db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [['happy'], ['sad'], ['happy', 'sad']]\n",
    "y_test_1 = [1, 0, 1]\n",
    "y_test_2 = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e65ddaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_test.fit(X_test, y_test_1)\n",
    "assert mcc_test.default == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "953161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mcc_test.predict([['sad'], []]) == [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "770e6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mcc_test.ties == 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec61b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_test.fit(X_test, y_test_2)\n",
    "assert mcc_test.default == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a914abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mcc_test.predict([['sad'], []]) == [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52c2d3",
   "metadata": {},
   "source": [
    "## Human Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cc59c",
   "metadata": {},
   "source": [
    "Let's get the tokens and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "965673a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [text.split() for text in data['text']]\n",
    "y = [cat2idx[l] for l in data['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e7dfe",
   "metadata": {},
   "source": [
    "The ties and accuracy matches table 1 for Human 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c72a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human 1\n",
      "Ties: 75%\n",
      "Accuracy: 56%\n"
     ]
    }
   ],
   "source": [
    "mcc1 = MatchCountClassifier(positive1, negative1)\n",
    "mcc1.fit(X, y)\n",
    "\n",
    "print(f'''Human 1\n",
    "Ties: {mcc1.ties:0.0%}\n",
    "Accuracy: {accuracy(mcc1.predict(X), y):0.0%}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afe393",
   "metadata": {},
   "source": [
    "For Human 2 we get accuracy 1 percentage point higher than the paper; likely due to corrections to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ae01cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human 2\n",
      "Ties: 39%\n",
      "Accuracy: 65%\n"
     ]
    }
   ],
   "source": [
    "mcc2 = MatchCountClassifier(positive2, negative2)\n",
    "mcc2.fit(X, y)\n",
    "\n",
    "print(f'''Human 2\n",
    "Ties: {mcc2.ties:0.0%}\n",
    "Accuracy: {accuracy(mcc2.predict(X), y):0.0%}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ed1f3",
   "metadata": {},
   "source": [
    "They also provide a third baseline in table 2 using statistics from the dataset\n",
    "\n",
    "> Based on a very\n",
    "preliminary examination of frequency counts in the\n",
    "entire corpus (including test data) plus introspection,\n",
    "we created a list of seven positive and seven negative\n",
    "words (including punctuation), shown in Figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2535bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'wonderful', 'best', 'great', 'superb', 'still', 'beautiful']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive3 = 'love, wonderful, best, great, superb, still, beautiful'.split(', ')\n",
    "positive3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c9c9d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'worst', 'stupid', 'waste', 'boring', '?', '!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative3 = 'bad, worst, stupid, waste, boring, ?, !'.split(', ')\n",
    "negative3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182a064",
   "metadata": {},
   "source": [
    "Again we get an accuracy 1% higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4811f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human 3 + stats\n",
      "Ties: 15%\n",
      "Accuracy: 70%\n"
     ]
    }
   ],
   "source": [
    "mcc3 = MatchCountClassifier(positive3, negative3)\n",
    "mcc3.fit(X, y)\n",
    "\n",
    "print(f'''Human 3 + stats\n",
    "Ties: {mcc3.ties:0.0%}\n",
    "Accuracy: {accuracy(mcc3.predict(X), y):0.0%}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b903f",
   "metadata": {},
   "source": [
    "## Could we do better?\n",
    "\n",
    "An obvious strategy would be to combine the lists; they are mostly disjoint.\n",
    "\n",
    "However that doesn't improve our accuracy at all over Human 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e783f215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Humans\n",
      "Ties: 13%\n",
      "Accuracy: 70%\n"
     ]
    }
   ],
   "source": [
    "mcc_all = MatchCountClassifier(set(positive1 + positive2 + positive3),\n",
    "                               set(negative1 + negative2 + negative3))\n",
    "\n",
    "mcc_all.fit(X, y)\n",
    "\n",
    "print(f'''Combined Humans\n",
    "Ties: {mcc_all.ties:0.0%}\n",
    "Accuracy: {accuracy(mcc_all.predict(X), y):0.0%}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d97e7a9",
   "metadata": {},
   "source": [
    "Another resource that was available at the time was the [Harvard General Inquirer](https://inquirer.sites.fas.harvard.edu/) lexicon which tags words with a `positiv` or `negativ` sentiment, among many other classifications.\n",
    "\n",
    "I can't find an official source for the lexicon, but there's a version inside the [pysentiment library](https://github.com/nickderobertis/pysentiment/) (which may be different to what was available at the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e6c85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "harvard_inquirer_url = 'https://raw.githubusercontent.com/nickderobertis/pysentiment/master/pysentiment2/static/HIV-4.csv'\n",
    "harvard_inquirer_path = data_dir / 'HIV-4.csv'\n",
    "if not harvard_inquirer_path.exists():\n",
    "    urlretrieve(harvard_inquirer_url, harvard_inquirer_path)\n",
    "\n",
    "with open(harvard_inquirer_path) as f:\n",
    "    harvard_inquirer_data = list(csv.DictReader(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179a33e",
   "metadata": {},
   "source": [
    "We can extract the positive and negative entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfee8149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['abide', 'ability', 'able', 'abound', 'absolve'],\n",
       " ['worth-while', 'worthiness', 'worthy', 'zenith', 'zest'],\n",
       " 1915)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_hi = [i['Entry'].lower() for i in harvard_inquirer_data if i['Positiv']]\n",
    "positive_hi[:5], positive_hi[-5:], len(positive_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fd91e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['abandon', 'abandonment', 'abate', 'abdicate', 'abhor'],\n",
       " ['wrongful', 'wrought', 'yawn', 'yearn', 'yelp'],\n",
       " 2291)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_hi = [i['Entry'].lower() for i in harvard_inquirer_data if i['Negativ']]\n",
    "negative_hi[:5], negative_hi[-5:], len(negative_hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22407cab",
   "metadata": {},
   "source": [
    "This has fewer ties but actually a lower accuracy than human 3.\n",
    "\n",
    "(Technically we should use stemming with Harvard Inquirer but it won't improve matters here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad0e86a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard Inquirer\n",
      "Ties: 6%\n",
      "Accuracy: 63%\n"
     ]
    }
   ],
   "source": [
    "mcc_hi = MatchCountClassifier(set(positive_hi), set(negative_hi))\n",
    "\n",
    "mcc_hi.fit(X, y)\n",
    "\n",
    "print(f'''Harvard Inquirer\n",
    "Ties: {mcc_hi.ties:0.0%}\n",
    "Accuracy: {accuracy(mcc_hi.predict(X), y):0.0%}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b40c7",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75664451",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mcc3.predict(X)\n",
    "scores = [mcc3._score(row) for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75f78fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6984126984126984"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = [yi==yhati for yi, yhati in zip(y, yhat)]\n",
    "mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82645944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_idx = [i for i, c in enumerate(correct) if not c]\n",
    "len(incorrect_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21c8c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 84 \t 20.10%\n",
      "-1 \t 67 \t 16.03%\n",
      "1 \t 61 \t 14.59%\n",
      "-2 \t 51 \t 12.20%\n",
      "2 \t 37 \t 8.85%\n",
      "-3 \t 30 \t 7.18%\n",
      "-4 \t 16 \t 3.83%\n",
      "3 \t 13 \t 3.11%\n",
      "-5 \t 11 \t 2.63%\n",
      "-6 \t 10 \t 2.39%\n",
      "4 \t 8 \t 1.91%\n",
      "5 \t 7 \t 1.67%\n",
      "6 \t 6 \t 1.44%\n",
      "-9 \t 4 \t 0.96%\n",
      "-7 \t 4 \t 0.96%\n",
      "-10 \t 3 \t 0.72%\n",
      "-8 \t 2 \t 0.48%\n",
      "17 \t 1 \t 0.24%\n",
      "-15 \t 1 \t 0.24%\n",
      "-12 \t 1 \t 0.24%\n",
      "-11 \t 1 \t 0.24%\n"
     ]
    }
   ],
   "source": [
    "for score, count in Counter(scores[i] for i in incorrect_idx).most_common():\n",
    "    print(score, '\\t', count, '\\t', f'{count/len(incorrect_idx):0.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dda13",
   "metadata": {},
   "source": [
    "Look at most extreme cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9970482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "very_wrong_idx = [i for i in incorrect_idx if abs(scores[i]) >= 7]\n",
    "\n",
    "len(very_wrong_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd63d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def mark_span(text, color):\n",
    "        return f'<span style=\"background: {color};\">{html.escape(text)}</span>'\n",
    "\n",
    "def markup_html_words(text, words, color):\n",
    "    word_pattern = '|'.join([r'\\b' + re.escape(word) + r'\\b' if len(word) > 1 else re.escape(word) for word in words])\n",
    "    return re.sub(fr\"({word_pattern})(?![^<]*>)\", lambda match: mark_span(match.group(1), color), text, flags=re.IGNORECASE)\n",
    "    \n",
    "def markup_sentiment(text, positive=positive3, negative=negative3):\n",
    "    text = markup_html_words(text, positive, \"lightgreen\")\n",
    "    text = markup_html_words(text, negative, \"orange\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6428e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def show_index(idx):\n",
    "    movieid = data['movieid'][idx]\n",
    "    print(f'Movie: {movieid}, Label: {data[\"label\"][idx]}, Score: {scores[idx]}')\n",
    "    display(HTML(markup_sentiment(get_movie_html(movieid))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cc046",
   "metadata": {},
   "source": [
    "This movie is labelled as negative, despite being 3 out of 4 stars.\n",
    "\n",
    "The author is a Woody Allen fan and it wasn't his favorite Woody Allen film but it's still pretty good.\n",
    "\n",
    "This is a mislabelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6dca4f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: 15970, Label: neg, Score: 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<HTML><HEAD>\r\n",
       "<TITLE>Review for Celebrity (1998)</TITLE>\r\n",
       "<LINK REL=\"STYLESHEET\" TYPE=\"text/css\" HREF=\"/ramr.css\">\r\n",
       "</HEAD>\r\n",
       "<BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\r\n",
       "<H1 ALIGN=\"CENTER\" CLASS=\"title\"><A HREF=\"/Title?0120533\">Celebrity (1998)</A></H1><H3 ALIGN=CENTER>reviewed by<BR><A HREF=\"/ReviewsBy?Matt+Prigge\">Matt Prigge</A></H3><HR WIDTH=\"40%\" SIZE=\"4\">\r\n",
       "<PRE>CELEBRITY (1998)\r\n",
       "A Film Review by Ted Prigge\r\n",
       "Copyright 1998 Ted Prigge</PRE>\r\n",
       "<P>Writer/Director: Woody Allen\r\n",
       "Starring: Kenneth Branagh, Judy Davis, Joe Mantegna, Charlize Theron,\r\n",
       "Leonardo DiCaprio, Famke Janssen, Winona Ryder, Melanie Griffith, Bebe\r\n",
       "Neuwirth, Michael Lerner, Hank Azaria, Gretchen Mol, Dylan Baker,\r\n",
       "Jeffrey Wright, Greg Mottola, Andre Gregory, Saffron Burrows, Alfred\r\n",
       "Molina, Vanessa Redgrave, Joey Buttafuoco, Mary Jo Buttafuoco, Donald\r\n",
       "Trump</P>\r\n",
       "<P>After hearing reviews for Woody Allen's upteenth movie in history,\r\n",
       "\"Celebrity,\" range from terribly <span style=\"background: orange;\">boring</span> to just so-so, my heart lept\r\n",
       "when the opening images of the film closely resembled that of\r\n",
       "\"Manhattan,\" my personal favorite from my personal favorite director of\r\n",
       "all time.  Woody Allen's films almost never rely on visual flair over\r\n",
       "textual flair, so when one of his films closely resembles the one time\r\n",
       "that these two entities fit hand-in-hand (\"Manhattan\" really is one of\r\n",
       "the <span style=\"background: lightgreen;\">best</span>-looking films I've ever seen, <span style=\"background: lightgreen;\">beautiful</span> black and white\r\n",
       "photography of the city's <span style=\"background: lightgreen;\">best</span> areas, etc.), a fan can't help but feel\r\n",
       "visibly moved.  The film opens up, with the usual credits with plain\r\n",
       "white font over black backgrounds, and an old ironic standard playing on\r\n",
       "the soundtrack, but then the screen fills with a gorgeous dull gray sky,\r\n",
       "with the word \"Help\" being spelled with an airplane.  Beethoven's 5th\r\n",
       "blasts on the soundtrack.  The city seems to stop to take notice of this\r\n",
       "moment, and it's all rather lovely to look at.</P>\r\n",
       "<P>And then we cut to a film crew, shooting this as the film's hilariously\r\n",
       "banal key moment in the film, where the lead actress in the film\r\n",
       "(Melanie Griffith, looking as buxom and <span style=\"background: lightgreen;\">beautiful</span> as ever) has to\r\n",
       "realize something's wrong with her life or whatever.  It's a terribly\r\n",
       "stale scene for a Woody Allen film, with the <span style=\"background: lightgreen;\">great</span> opening shots or\r\n",
       "without, and my heart sank and I soon got used to the fact that once\r\n",
       "again, a new film of his was not going to be as <span style=\"background: lightgreen;\">great</span> as his past works\r\n",
       "(though, for the record, last year's \"Deconstructing Harry\" came awfully\r\n",
       "close).</P>\r\n",
       "<P>What the hell has happened to him<span style=\"background: orange;\">?</span>  The man who once could be relied on\r\n",
       "for neurotic freshness in cinema has not become less funny, but his\r\n",
       "films have become less insightful and more like he tossed them together\r\n",
       "out of unfinished ideas.  \"Bullets Over Broadway,\" though <span style=\"background: lightgreen;\">wonderful</span>,\r\n",
       "relies on irony to pull a farce that just never totally takes off.\r\n",
       "\"Mighty Aphrodite\" is more full of <span style=\"background: lightgreen;\">great</span> moments and lines than a really\r\n",
       "<span style=\"background: lightgreen;\">great</span> story.  \"Everyone Says I <span style=\"background: lightgreen;\">Love</span> You\" was more of a <span style=\"background: lightgreen;\">great</span> idea than a\r\n",
       "<span style=\"background: lightgreen;\">great</span> film.  Even \"Deconstructing Harry\" is admittingly cheap in a way,\r\n",
       "even if it does top as one of his most truly hilarious films.</P>\r\n",
       "<P>If anything, the reception of \"Celebrity\" by everyone should tip Allen\r\n",
       "off to the fact that this time, it's not the audience and critics who\r\n",
       "are wrong about how <span style=\"background: lightgreen;\">wonderful</span> his film is: it's him.  \"Celebrity\" is,\r\n",
       "yes, a good film, but it's only marginally satisfying as a Woody Allen\r\n",
       "film.  Instead of creating the <span style=\"background: lightgreen;\">great</span> Woody Allen world, he's created a\r\n",
       "world out of a subject he knows only a bit about.  And he's fashioned a\r\n",
       "film that is based almost entirely on his uninformed philosophy of\r\n",
       "celebrities, so that it plays like a series of skits with minor\r\n",
       "connections.  It's like \"La Dolce Vita\" without the accuracy, the right\r\n",
       "amount of wit, and the correct personal crisis.</P>\r\n",
       "<P>Woody, becoming more insecure in his old age, choses to drop the Woody\r\n",
       "Allen character in on the world of celebrities, and then hang him and\r\n",
       "all his flaws up for scrutiny, and does this by casting not himself but\r\n",
       "Brit actor Kenneth Branagh in the lead.  Much has been said about his\r\n",
       "performance - dead on but irritating, makes one yearn for the real\r\n",
       "thing, blah blah blah - but to anyone who actually knows the Woody Allen\r\n",
       "character knows that Branagh's performance, though featuring some of the\r\n",
       "same mannerisms (stuttering, whining, lots o' hand gestures), is hardly\r\n",
       "a warts-and-all impersonation.  Branagh brings along with him little of\r\n",
       "the Woody Allen charm, which actually allows for his character's flaws\r\n",
       "to be more apparent.  Woody's a flawed guy, and we know it, but we <span style=\"background: lightgreen;\">love</span>\r\n",
       "him anyway, because he's really funny and really witty and really\r\n",
       "intelligent.  Branagh's Allen is a bit more flat-out <span style=\"background: orange;\">bad</span>, but with the\r\n",
       "same charm so that, yes, we like him, but we're <span style=\"background: lightgreen;\">still</span> not sure if he's\r\n",
       "really a good person or not.</P>\r\n",
       "<P>His character, Lee Simon, is first seen on the set of the aforementioned\r\n",
       "movie, hits on extra actress Winona Ryder, then goes off to interview\r\n",
       "Griffith, who takes him to her childhood home where he makes a pass at\r\n",
       "her, and she denies him...sorta.  We then learn, through flashbacks,\r\n",
       "that Lee has been sucked into trying to be a celebrity thanks to a\r\n",
       "mid-life crisis and an appearance at his high school reunion.  He has\r\n",
       "since quit his job as a travel journalist and become a gossip journalist\r\n",
       "of sorts, covering movie sets and places where celebrities congregate,\r\n",
       "so that he can meet them, and maybe sell his script (a bank robbery\r\n",
       "movie \"but with a deep personal crisis\").  As such, he has divorced his\r\n",
       "wife of several years (Allen regular Judy Davis), and continues on a\r\n",
       "quest for sexual happiness, boucing from girlfriend to girlfriend and\r\n",
       "fling to fling over the course of the film.</P>\r\n",
       "<P>After Griffith comes his escapades with a model (Charlize Theron) who is\r\n",
       "\"polymorphously perverse\" (glad to see Allen is using new jokes, ha ha),\r\n",
       "who takes him for a wild ride not different from that of the Anita\r\n",
       "Ekberg segment of \"La Dolce Vita.\"  Following are his safe relationship\r\n",
       "with smart working woman Famke Janssen, a relationship that almost\r\n",
       "assures him success, and his continued escapades with Ryder, whom he\r\n",
       "fancies most of all.  His story is juxtaposed with that of Davis, who\r\n",
       "flips out, but stumbles onto happiness when she runs into a handsome,\r\n",
       "friendly TV exec (Joe Mantegna) who lands her a job that furthers her\r\n",
       "career to national status.  While Lee is fumbling about, selfishly\r\n",
       "trying to ensure his own happiness, Davis becomes happy (\"I've become\r\n",
       "the kind of woman I've always hated...and I'm loving it.\") without doing\r\n",
       "a thing.</P>\r\n",
       "<P>The result is a film of highs and mediums.  The mediums are what take up\r\n",
       "most of the film, with sitations and scenes which don't exactly work but\r\n",
       "you can't help but pat Allen on the back for trying.  But other places\r\n",
       "are really <span style=\"background: lightgreen;\">great</span> scenes.  The opening.  The sequence with Theron, which\r\n",
       "is so good that I wished it hadn't ended.  A banana scene with Bebe\r\n",
       "Neuwirth (droll as ever).  And, perhaps the <span style=\"background: lightgreen;\">best</span> sequence: a romp with\r\n",
       "hot-as-hell teen idol, Brandon Darrow, played by none other than Leo\r\n",
       "DiCaprio, who is so un-DiCaprio-esque that if any of this fans could sit\r\n",
       "through this film, they'd never look at him the same way.  He ignites\r\n",
       "the screen with intensity, and spares nothing in showing his character\r\n",
       "as narcissistically tyrannical, and totally heartbreaking for Lee, who\r\n",
       "comes to him to talk about his script that he has read, and finds\r\n",
       "himself on a wild all-day ride with him.  They go to Atlantic City to\r\n",
       "watch a fight, they gamble, and they wind up in his hotel room, where\r\n",
       "Darrow gets it on with his flame (Gretchen Mol) and he lends him one of\r\n",
       "the leftover groupies.  Allen's writing in these scenes are so good that\r\n",
       "just for them, I'd almost recommend the film.  Almost.</P>\r\n",
       "<P>But what I really liked about this film is despite the fact that it's a\r\n",
       "mess, despite the fact that what this film really needs is a good old\r\n",
       "fashioned rewrite by Allen himself, it's <span style=\"background: lightgreen;\">still</span> a smart and insightful\r\n",
       "film.  Though some of the jokes are either stale or misplaced (some seem\r\n",
       "too cartoonish, even for this environment), Allen <span style=\"background: lightgreen;\">still</span> manages to get\r\n",
       "across that this film is not exactly about celebrities, as it may seem\r\n",
       "to be (if it were, it'd be extremely out-of-touch), but about those who\r\n",
       "want to be celebrities, and how they equate celebrity-hood with\r\n",
       "happiness.  We never get close enough to the actual celebrities to see\r\n",
       "if they're really happy (they may appear to be on the surface...), but\r\n",
       "we do get close enough to Lee and Davis' character.  Lee is obsessed\r\n",
       "with the phenomenon, while Davis takes is at arm's length, and never\r\n",
       "gets too involved in what it is, and soon becomes one herself.</P>\r\n",
       "<P>Besides, it's witty, and it does have the one thing that no other film\r\n",
       "has but Allen's: that <span style=\"background: lightgreen;\">great</span> Woody Allen feel.  It may be not exactly\r\n",
       "fresh and lively or totally brilliant in its depiction of its subject,\r\n",
       "and yes, as a part of Woody Allen's oeuvre, it's merely a blip (no\r\n",
       "\"Annie Hall\" but it's no \"Shadows and Fog\" either), but it goes to prove\r\n",
       "that no one can make a film like him, and only he and maybe Godard could\r\n",
       "possibly take a totally horrible metaphor, like the one in the\r\n",
       "beginning, and make it work not once but twice.</P>\r\n",
       "<PRE>MY RATING (out of 4): ***</PRE>\r\n",
       "<P>Homepage at: <A HREF=\"http://www.geocities.com/Hollywood/Hills/8335/\">http://www.geocities.com/Hollywood/Hills/8335/</A></P>\r\n",
       "<HR><P CLASS=flush><SMALL>The review above was posted to the\r\n",
       "<A HREF=\"news:rec.arts.movies.reviews\">rec.arts.movies.reviews</A> newsgroup (<A HREF=\"news:de.rec.film.kritiken\">de.rec.film.kritiken</A> for German reviews).<BR>\r\n",
       "The Internet Movie Database accepts no responsibility for the contents of the\r\n",
       "review and has no editorial control. Unless stated otherwise, the copyright\r\n",
       "belongs to the author.<BR>\r\n",
       "Please direct comments/criticisms of the review to relevant newsgroups.<BR>\r\n",
       "Broken URLs inthe reviews are the responsibility of the author.<BR>\r\n",
       "The formatting of the review is likely to differ from the original due\r\n",
       "to ASCII to HTML conversion.\r\n",
       "</SMALL></P>\r\n",
       "<P ALIGN=CENTER>Related links: <A HREF=\"/Reviews/\">index of all rec.arts.movies.reviews reviews</A></P>\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "</P></BODY></HTML>\r\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_index(very_wrong_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31461f9f",
   "metadata": {},
   "source": [
    "Interestingly 15970 one of the \"corrections\" pos->neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bce91faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Changes made ==\n",
      "\n",
      "mix20_rand700_tokens_cleaned.zip \n",
      "-> mix20_rand700_tokens_0211.tar.gz\n",
      "\n",
      "Removed : (non-English/incomplete reviews)\n",
      "\n",
      "pos/cv037_tok-11720.txt\n",
      "pos/cv206_tok-12590.txt\n",
      "pos/cv263_tok-10033.txt\n",
      "pos/cv365_tok-21785.txt\n",
      "pos/cv400_tok-11748.txt\n",
      "pos/cv528_tok-12960.txt\n",
      "pos/cv627_tok-14423.txt\n",
      "\n",
      "neg/cv059_tok-8583.txt\n",
      "neg/cv111_tok-11625.txt\n",
      "neg/cv193_tok-28093.txt\n",
      "neg/cv216_tok-27832.txt\n",
      "neg/cv219_tok-11130.txt\n",
      "neg/cv423_tok-10742.txt\n",
      "neg/cv592_tok-10894.txt\n",
      "\n",
      "\n",
      "Moved: (based on Nathan's judgement when he read the review,\n",
      "sometimes different from the original author's own rating,\n",
      "as listed below)\n",
      "\n",
      "neg -> pos:\n",
      "cv279_tok-23947.txt\t*1/2, but reads positive\n",
      "cv346_tok-24609.txt \tmisclassification\n",
      "cv375_tok-0514.txt  \tmisclassification\n",
      "cv389_tok-8969.txt \tmisclassification\n",
      "cv425_tok-8417.txt \tseveral reviews together\n",
      "cv518_tok-11610.txt \tmisclassification\n",
      "\n",
      "pos -> neg:\n",
      "cv017_tok-29801.txt \t*** Average, hits and misses \n",
      "cv352_tok-15970.txt \t(out of 4): *** \n",
      "cv375_tok-21437.txt \t* * * - Okay movie, hits and misses\n",
      "cv377_tok-7572.txt \t*** Pretty good, bring a friend\n",
      "cv546_tok-23965.txt  \t* * * - Okay movie, hits and misses\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "diff_txt = sentiment_fh.extractfile(sentiment_fh.getmember('diff.txt')).read().decode(CODEC).rstrip()\n",
    "print(diff_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068630f1",
   "metadata": {},
   "source": [
    "A lot of the other examples were based on repeated punctuation (exclamation marks and question marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1452e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bad': 1,\n",
       "         'stupid': 1,\n",
       "         'worst': 2,\n",
       "         '!': 9,\n",
       "         'great': 2,\n",
       "         '?': 6,\n",
       "         'still': 2})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(word for word in data['text'][very_wrong_idx[1]].split() if word in positive3 + negative3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e72bc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'!': 3, '?': 6, 'bad': 1, 'great': 1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(word for word in data['text'][very_wrong_idx[2]].split() if word in positive3 + negative3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5458f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'?': 9, 'bad': 1})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(word for word in data['text'][very_wrong_idx[3]].split() if word in positive3 + negative3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5969e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'?': 11, 'love': 2, 'bad': 1})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(word for word in data['text'][very_wrong_idx[4]].split() if word in positive3 + negative3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09201ea3",
   "metadata": {},
   "source": [
    "# Machine Learning Methods\n",
    "\n",
    "We'll now use traditional machine learning methods.\n",
    "For showing the different methods we'll use a small vocabulary from the human baseline.\n",
    "\n",
    "To keep this section reasonable length we'll use `sklearn` implementations of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d638d62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'wonderful',\n",
       " 'best',\n",
       " 'great',\n",
       " 'superb',\n",
       " 'still',\n",
       " 'beautiful',\n",
       " 'bad',\n",
       " 'worst',\n",
       " 'stupid',\n",
       " 'waste',\n",
       " 'boring',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = positive3 + negative3\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a242973",
   "metadata": {},
   "source": [
    "We'll create a Feature vector from this vocabulary for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff886d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0,\n",
       " 'wonderful': 0,\n",
       " 'best': 0,\n",
       " 'great': 1,\n",
       " 'superb': 0,\n",
       " 'still': 1,\n",
       " 'beautiful': 0,\n",
       " 'bad': 0,\n",
       " 'worst': 0,\n",
       " 'stupid': 0,\n",
       " 'waste': 0,\n",
       " 'boring': 0,\n",
       " '?': 1,\n",
       " '!': 0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = [Counter(word for word in doc if word in vocab) for doc in X]\n",
    "\n",
    "X_feature = [[row[word] for word in vocab] for row in word_counts]\n",
    "\n",
    "dict(zip(vocab, X_feature[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeb1fb",
   "metadata": {},
   "source": [
    "And split it into train and test sets by the cvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "657e8b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 465, 921, 465)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [row for row, cvid in zip(X_feature, data['cvid']) if int(cvid) // 233 < 2]\n",
    "X_test = [row for row, cvid in zip(X_feature, data['cvid']) if int(cvid) // 233 >= 2]\n",
    "\n",
    "y_train = [row for row, cvid in zip(y, data['cvid']) if int(cvid) // 233 < 2]\n",
    "y_test = [row for row, cvid in zip(y, data['cvid']) if int(cvid) // 233 >= 2]\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5dcf2",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The text states they use Naive Bayes with add-1 smoothing (so in sklearn `alpha=1.0`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff8acd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7161290322580646"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "accuracy(nb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7748736",
   "metadata": {},
   "source": [
    "There is another way to do Naive Bayes where each word is taken as an independent feature, but it tends to be worse for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "763963ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "nb = BernoulliNB(binarize=1.0, alpha=1.0)\n",
    "nb.fit(X_train, y_train)\n",
    "accuracy(nb.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42689e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98abaea4",
   "metadata": {},
   "source": [
    "## Maximum Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132660a1",
   "metadata": {},
   "source": [
    "Maximum Entropy is an old NLP term for Logistic Regression\n",
    "\n",
    "> We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000).\n",
    "\n",
    "A Gaussian Prior is equivalent to an L2 penalty, but they don't specify the size of the prior and I can't access the referenced paper.\n",
    "I'll stick to the default in sklearn of `1.0` (the solver shouldn't matter much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b33ccf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6946236559139785"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "me = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)\n",
    "\n",
    "me.fit(X_train, y_train)\n",
    "accuracy(me.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebc5b0",
   "metadata": {},
   "source": [
    "Note that the amount of regularization can actually matter.\n",
    "Ideally we'd keep a small holdout set for hyperparameter tuning, but I'll stick to the methods in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "863957da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7075268817204301"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me = LogisticRegression(penalty='l2', solver='liblinear', C=0.5)\n",
    "\n",
    "me.fit(X_train, y_train)\n",
    "accuracy(me.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d3304fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "me = LogisticRegression(penalty='l2', solver='liblinear', C=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd1aef",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba0c7a0",
   "metadata": {},
   "source": [
    ">  We used Joachim’s (1999) SVM light package for training and testing, with all parameters set to their default values, after first length-normalizing the document vectors, as is standard (neglecting to normalize generally hurt performance slightly).\n",
    "\n",
    "There's little detail as to the hyperparameters again, so I'll use the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61e11fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290322580645161"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svm = Pipeline([('norm', Normalizer(norm='l2')),\n",
    "                ('svc', LinearSVC(penalty='l2', loss='squared_hinge', C=1.0))])\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "accuracy(svm.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26156d8",
   "metadata": {},
   "source": [
    "I'm not clear on what length-normalizing is, but L2 normalizing looks like it works better than L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d86d77c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7118279569892473"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = Pipeline([('norm', Normalizer(norm='l1')),\n",
    "                ('svc', LinearSVC(penalty='l2', loss='squared_hinge', C=1.0))])\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "accuracy(svm.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b96dca",
   "metadata": {},
   "source": [
    "And this works better than not normalizing it at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eb1a4f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eross/mambaforge/envs/pang_lee_2003/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6967741935483871"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', loss='squared_hinge', C=1.0)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "accuracy(svm.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20f201",
   "metadata": {},
   "source": [
    "As with Maximum Entropy it's sensitive to the amount of regularizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9402c1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311827956989247"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = Pipeline([('norm', Normalizer(norm='l2')),\n",
    "                ('svc', LinearSVC(penalty='l2', loss='squared_hinge', C=2.0))])\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "accuracy(svm.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c21c4f",
   "metadata": {},
   "source": [
    "We'll reset everything to the defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c5dbe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = Pipeline([('norm', Normalizer(norm='l2')),\n",
    "                ('svc', LinearSVC(penalty='l2', loss='squared_hinge', C=1.0))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d8d9d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693b23",
   "metadata": {},
   "source": [
    "## Experimental Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9fc60a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc721fac",
   "metadata": {},
   "source": [
    "> we randomly selected 700 positive-sentiment and 700 negative-sentiment documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59117292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 692, 1: 694})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1caee",
   "metadata": {},
   "source": [
    "> We then divided this data into three equal-sized folds, maintaining balanced class distributions in each fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cf80f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[459, 462, 463]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = [[idx for idx, cvid in enumerate(data['cvid']) if int(cvid) // 233 == i] for i in range(3)]\n",
    "\n",
    "[len(f) for f in folds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f46acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = [(folds[0] + folds[1], folds[2]),\n",
    "      (folds[0] + folds[2], folds[1]),\n",
    "      (folds[1] + folds[2], folds[0]),\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd8509",
   "metadata": {},
   "source": [
    "> One unconventional step we took was to attempt\n",
    "to model the potentially important contextual effect\n",
    "of negation: clearly “good” and “not very good” indicate opposite sentiment orientations.\n",
    "> Adapting a\n",
    "technique of Das and Chen (2001), we added the tag\n",
    "NOT to every word between a negation word (“not”,\n",
    "“isn’t”, “didn’t”, etc.) and the first punctuation\n",
    "mark following the negation word.\n",
    "\n",
    "To do this we can first get the negation words with a surprisingly effective heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c198b02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " \"isn't\",\n",
       " \"can't\",\n",
       " \"didn't\",\n",
       " \"wasn't\",\n",
       " \"aren't\",\n",
       " \"won't\",\n",
       " \"couldn't\",\n",
       " \"wouldn't\",\n",
       " 'cannot',\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"weren't\",\n",
       " \"shouldn't\",\n",
       " \"ain't\",\n",
       " \"hadn't\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = Counter(word for text in X for word in text)\n",
    "\n",
    "negation_words = [w for (w,c) in words.most_common(10_000) if re.match(\".*n[o']t$\", w)]\n",
    "negation_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398f546",
   "metadata": {},
   "source": [
    "Then we can use a simple finite state machine to add the negation tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aeabb954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this isn't NOT_a NOT_great NOT_movie , it is terrible\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = '!?.,()[];:,\"'\n",
    "\n",
    "def negation_mark(x):\n",
    "    return 'NOT_' + x\n",
    "\n",
    "def add_negation_tag(tokens, negation_words=negation_words, punctuation=punctuation, negation_mark=negation_mark):\n",
    "    in_negation = False\n",
    "    tagged_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in negation_words:\n",
    "            in_negation = not in_negation\n",
    "        elif token in punctuation:\n",
    "            in_negation = False\n",
    "        elif in_negation:\n",
    "            token = negation_mark(token)\n",
    "            \n",
    "        tagged_tokens.append(token)\n",
    "        \n",
    "    return tagged_tokens\n",
    "\n",
    "def text_add_negation_tag(s: str, **kwargs) -> str:\n",
    "    return ' '.join(add_negation_tag(tokens=s.split(), **kwargs))\n",
    "\n",
    "text_add_negation_tag(\"this isn't a great movie , it is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c97ef",
   "metadata": {},
   "source": [
    "> For this study, we focused on features based on unigrams (with negation tagging) and bigrams.\n",
    "> Because training MaxEnt is expensive in the number of features, we limited consideration to (1) the 16165 unigrams appearing at least four times in our 1400-document corpus\n",
    "> (lower count cutoffs did not yield significantly different results), and (2) the 16165 bigrams occurring most often in the same data (the selected bigrams all occurred at least seven times).\n",
    "> Note that we did not add negation tags to the bigrams, since we consider bigrams (and n-grams in general) to be an orthogonal way to incorporate context.\n",
    "\n",
    "There's a slight issue here in using the vocabulary based on the entire dataset; the features should only be selected by the data in each fold otherwise you could be overfitting.\n",
    "\n",
    "We'll be making lots of this so we'll make a factory function to remove some of the boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef8c060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_vectorizer(\n",
    "    max_features=16165,\n",
    "    input='content',\n",
    "    token_pattern=r\"[^ ]+\",\n",
    "    ngram_range=(1,1),\n",
    "    preprocessor=None,\n",
    "    binary=True):\n",
    "    return CountVectorizer(input=input,\n",
    "                          token_pattern=token_pattern,\n",
    "                          ngram_range=ngram_range,\n",
    "                          preprocessor=preprocessor,\n",
    "                          max_features=max_features,\n",
    "                          binary=binary)\n",
    "\n",
    "unigram_freq_vectorizer = make_count_vectorizer(preprocessor=text_add_negation_tag, binary=False)\n",
    "\n",
    "unigram_vectorizer = make_count_vectorizer(preprocessor=text_add_negation_tag)\n",
    "\n",
    "bigram_vectorizer = make_count_vectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23fec8c",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "From the README there's an updated Figure 3\n",
    "\n",
    "| | Features          | # features | NB   | ME   | SVM  |\n",
    "|-|-------------------|------------|------|------|------|\n",
    "|1| unigrams (freq.)  | 16162      | 79.0 | n/a  | 73.0 |\n",
    "|2| unigrams          | 16162      | 81.0 | 80.2 | 82.9 |\n",
    "|3| unigrams+bigrams  | 32324      | 80.7 | 80.7 | 82.8 |\n",
    "|4| bigrams           | 16162      | 77.3 | 77.5 | 76.5 |\n",
    "|5| unigrams+POS      | 16688      | 81.3 | 80.3 | 82.0 |\n",
    "|6| adjectives        | 2631       | 76.6 | 77.6 | 75.3 |\n",
    "|7| top 2631 unigrams | 2631       | 80.9 | 81.3 | 81.2 |\n",
    "|8| unigrams+position | 22407      | 80.8 | 79.8 | 81.8 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45cb8a",
   "metadata": {},
   "source": [
    "For each feature function (row in the table) we want to apply it followed by each of the 3 types of classifiers\n",
    "\n",
    "> All results reported below, as well as the baseline results from Section 4,\n",
    "are the average three-fold cross-validation results on this data.\n",
    "\n",
    "We can use `cross_val_score` to do the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "086c5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'NB': nb,\n",
    "    'ME': me,\n",
    "    'SVM': svm\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "def evaluate(vectorizer):\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline(steps=[('Vectorizer', vectorizer), ('model', model)])\n",
    "        cv_score = cross_val_score(pipeline, data['text'], y, cv=cv, scoring='accuracy')\n",
    "        results[model_name] = cv_score.mean()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5463fc0",
   "metadata": {},
   "source": [
    "### Initial Unigram Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3acc5",
   "metadata": {},
   "source": [
    "We can show the results to compare with the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a271b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    print('\\t'.join(results))\n",
    "    print('\\t'.join([f'{v:0.1%}' for v in results.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "248bf05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    result_html = '<table><tr>' + ''.join([f'<th>{r}</th>' for r in results]) + '</tr><tr>' + \\\n",
    "                  ''.join([f'<td>{v:0.1%}</td>' for v in results.values()]) + '</tr></table>'\n",
    "    \n",
    "    display(HTML(result_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e972d",
   "metadata": {},
   "source": [
    "This does a little better than the table from the README, which is surprising especially for Naive Bayes which is deterministic with no tunable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "070e7f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>80.3%</td><td>80.5%</td><td>77.0%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['unigram_freq'] = evaluate(unigram_freq_vectorizer)\n",
    "\n",
    "display_results(results['unigram_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1a01d0",
   "metadata": {},
   "source": [
    "### Feature frequency vs. presence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f97112",
   "metadata": {},
   "source": [
    "As in the paper using presence features gives a significant lift, but again all the accuracies are much higher here.\n",
    "In particular Maximum Entropy (Logistic Regression) is higher than Naive Bayes here and much closer to SVM, which suggests there could be poor regularisation in the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba1c26a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>81.9%</td><td>84.2%</td><td>84.4%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['unigram'] = evaluate(unigram_vectorizer)\n",
    "\n",
    "display_results(results['unigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb1dd2",
   "metadata": {},
   "source": [
    "### Bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131061eb",
   "metadata": {},
   "source": [
    "> Line (3) of the results table shows that bigram\n",
    "information does not improve performance beyond\n",
    "that of unigram presence, although adding in the bigrams does not seriously impact the results, even for\n",
    "Naive Bayes\n",
    "\n",
    "This is still observed here, but again our results are slightly higher than the authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "927e554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>80.7%</td><td>83.8%</td><td>84.4%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['unigram_bigram'] = evaluate(FeatureUnion([('unigram', unigram_vectorizer), ('bigram', bigram_vectorizer)]))\n",
    "\n",
    "display_results(results['unigram_bigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13bb78",
   "metadata": {},
   "source": [
    "We have similar observations for bigrams:\n",
    "\n",
    "> However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f97fb83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>77.6%</td><td>78.0%</td><td>78.3%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['bigram'] = evaluate(bigram_vectorizer)\n",
    "\n",
    "display_results(results['bigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c69cc2",
   "metadata": {},
   "source": [
    "We see a marginally larger decline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de3ee402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NB': '-4.26%', 'ME': '-6.14%', 'SVM': '-6.07%'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: f\"{results['bigram'][k] - results['unigram'][k] :0.2%}\" for k in results['bigram']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df476da",
   "metadata": {},
   "source": [
    "### Parts of speech\n",
    "\n",
    "> We also experimented with appending POS tags to every word via Oliver Mason’s Qtag program\n",
    "\n",
    "Unfortunately I can't access [QTag](https://web.archive.org/web/20071005185742/http://www.english.bham.ac.uk/staff/omason/software/qtag.html), but instead will use a much more modern (and likely more accurate) [Averaged Perceptron Tagger](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python) from NLTK.\n",
    "It's relatively expensive so I'll cache the calculations across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e8c2459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/eross/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog.', 'NN')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from functools import lru_cache\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def pos_tag(doc):\n",
    "    return nltk.pos_tag(doc.split())\n",
    "\n",
    "pos_tag('The quick brown fox jumped over the lazy dog.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa10ab8",
   "metadata": {},
   "source": [
    "We can then append the tags as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3bcf449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united_JJ',\n",
       " 'states_NNS',\n",
       " ',_,',\n",
       " '1998_CD',\n",
       " 'u_NN',\n",
       " '._.',\n",
       " 's_NN',\n",
       " '._.',\n",
       " 'release_NN',\n",
       " 'date_NN']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_marker(doc):\n",
    "    return ' '.join([token + '_' + tag for token, tag in pos_tag(doc)])\n",
    "\n",
    "pos_marker(data['text'][0]).split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a004c53f",
   "metadata": {},
   "source": [
    "\n",
    "> However, the effect of this information seems to be a wash: as depicted in\n",
    "line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged.\n",
    "\n",
    "We actually get worse results across the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "441e7a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>80.6%</td><td>82.4%</td><td>82.5%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_pos_vectorizer = make_count_vectorizer(preprocessor=pos_marker)\n",
    "\n",
    "results['unigram_pos'] = evaluate(unigram_pos_vectorizer)\n",
    "\n",
    "display_results(results['unigram_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "579280e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>81.9%</td><td>84.2%</td><td>84.4%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_results(results['unigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b79666",
   "metadata": {},
   "source": [
    "It's not the effect of dropping negations either(in fact that makes Naive Bayes and SVM do slightly better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23cf3018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>82.2%</td><td>82.9%</td><td>83.3%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_no_negation_vectorizer = make_count_vectorizer(preprocessor=None)\n",
    "\n",
    "\n",
    "display_results(evaluate(unigram_no_negation_vectorizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46259c",
   "metadata": {},
   "source": [
    "> Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe,\n",
    "2000; Turney, 2002), we looked at the performance of using adjectives alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2768dd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"united wide mpaa male full-frontal female graphic sexual frequent theatrical daphne murray liber kimball wild dreary early frontal late early lucrative mtv fast-paced slick flashy mindless wild first eleven convincing much flesh narrative wild real increasingly- improbable predictable serpentine idiotic steven seagal easy unlikely right wrong good next hot young old screen i'll film's erotic impressive wild risqué soft-core generic much lesbian token iron-clad film's full kevin few fully-clothed thirteenth titanic familiar wild john last finely-tuned psychological normal copious real powerful difficult same i mainstream previous wide-release mad box-office quick pretty main wild occasional sam matt florida's blue high curvaceous van theresa daphne skeptical suzie neve similar wild isn't good much ludicrous triple-digit on-screen nice slutty see-through one-piece kevin only interesting right secret wild absurd basic kinetic wild great superficial trash\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjective_extractor(doc):\n",
    "    adjectives = [token for token, tag in pos_tag(doc) if tag == 'JJ']\n",
    "    return ' '.join(adjectives)\n",
    "\n",
    "adjective_extractor(data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d6917",
   "metadata": {},
   "source": [
    ">  Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than unigram presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c679595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>77.2%</td><td>76.2%</td><td>76.2%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adjective_vectorizer = make_count_vectorizer(preprocessor=lambda x: adjective_extractor(text_add_negation_tag(x)),\n",
    "                                             max_features=2633)\n",
    "\n",
    "results['adjective'] = evaluate(adjective_vectorizer)\n",
    "\n",
    "display_results(results['adjective'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702c9b1",
   "metadata": {},
   "source": [
    "> Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "777be5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>81.0%</td><td>81.9%</td><td>83.2%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram_2633_vectorizer = make_count_vectorizer(preprocessor=text_add_negation_tag, max_features=2633)\n",
    "\n",
    "results['unigram_2633'] = evaluate(unigram_2633_vectorizer)\n",
    "\n",
    "display_results(results['unigram_2633'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507bda4",
   "metadata": {},
   "source": [
    "### Position\n",
    "\n",
    "> As a rough approximation to determining this kind of structure, we tagged each word\n",
    "according to whether it appeared in the first quarter, last quarter, or middle half of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f5b565a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this_0 is_0 an_1 example_1 text_2 with_2 several_3 words_3 in_4 it_4'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quartile_marker(doc):\n",
    "    tokens = doc.split()\n",
    "    quartile = len(tokens) // 4\n",
    "    output_tokens = [ token + f'_{i // quartile}' for i, token in enumerate(tokens)]\n",
    "    return ' '.join(output_tokens)\n",
    "\n",
    "quartile_marker('this is an example text with several words in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac2750",
   "metadata": {},
   "source": [
    "I suspect they kept the top vocabulary across the whole document, but we will generate it by quartile.\n",
    "\n",
    "> The results (line (8)) didn’t differ greatly from using unigrams alone, but more refined notions of position might be more successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26c8f5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>NB</th><th>ME</th><th>SVM</th></tr><tr><td>78.7%</td><td>79.8%</td><td>80.1%</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "position_vectorizer =  make_count_vectorizer(preprocessor=quartile_marker, max_features=22430)\n",
    "\n",
    "results['position'] = evaluate(position_vectorizer)\n",
    "\n",
    "display_results(results['position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964dc036",
   "metadata": {},
   "source": [
    "Let's see all our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e557e4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c5aff\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c5aff_level0_col0\" class=\"col_heading level0 col0\" >NB</th>\n",
       "      <th id=\"T_c5aff_level0_col1\" class=\"col_heading level0 col1\" >ME</th>\n",
       "      <th id=\"T_c5aff_level0_col2\" class=\"col_heading level0 col2\" >SVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row0\" class=\"row_heading level0 row0\" >unigram_freq</th>\n",
       "      <td id=\"T_c5aff_row0_col0\" class=\"data row0 col0\" >80.3%</td>\n",
       "      <td id=\"T_c5aff_row0_col1\" class=\"data row0 col1\" >80.5%</td>\n",
       "      <td id=\"T_c5aff_row0_col2\" class=\"data row0 col2\" >77.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row1\" class=\"row_heading level0 row1\" >unigram</th>\n",
       "      <td id=\"T_c5aff_row1_col0\" class=\"data row1 col0\" >81.9%</td>\n",
       "      <td id=\"T_c5aff_row1_col1\" class=\"data row1 col1\" >84.2%</td>\n",
       "      <td id=\"T_c5aff_row1_col2\" class=\"data row1 col2\" >84.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row2\" class=\"row_heading level0 row2\" >unigram_bigram</th>\n",
       "      <td id=\"T_c5aff_row2_col0\" class=\"data row2 col0\" >80.7%</td>\n",
       "      <td id=\"T_c5aff_row2_col1\" class=\"data row2 col1\" >83.8%</td>\n",
       "      <td id=\"T_c5aff_row2_col2\" class=\"data row2 col2\" >84.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row3\" class=\"row_heading level0 row3\" >bigram</th>\n",
       "      <td id=\"T_c5aff_row3_col0\" class=\"data row3 col0\" >77.6%</td>\n",
       "      <td id=\"T_c5aff_row3_col1\" class=\"data row3 col1\" >78.0%</td>\n",
       "      <td id=\"T_c5aff_row3_col2\" class=\"data row3 col2\" >78.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row4\" class=\"row_heading level0 row4\" >unigram_pos</th>\n",
       "      <td id=\"T_c5aff_row4_col0\" class=\"data row4 col0\" >80.6%</td>\n",
       "      <td id=\"T_c5aff_row4_col1\" class=\"data row4 col1\" >82.4%</td>\n",
       "      <td id=\"T_c5aff_row4_col2\" class=\"data row4 col2\" >82.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row5\" class=\"row_heading level0 row5\" >adjective</th>\n",
       "      <td id=\"T_c5aff_row5_col0\" class=\"data row5 col0\" >77.2%</td>\n",
       "      <td id=\"T_c5aff_row5_col1\" class=\"data row5 col1\" >76.2%</td>\n",
       "      <td id=\"T_c5aff_row5_col2\" class=\"data row5 col2\" >76.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row6\" class=\"row_heading level0 row6\" >unigram_2633</th>\n",
       "      <td id=\"T_c5aff_row6_col0\" class=\"data row6 col0\" >81.0%</td>\n",
       "      <td id=\"T_c5aff_row6_col1\" class=\"data row6 col1\" >81.9%</td>\n",
       "      <td id=\"T_c5aff_row6_col2\" class=\"data row6 col2\" >83.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5aff_level0_row7\" class=\"row_heading level0 row7\" >position</th>\n",
       "      <td id=\"T_c5aff_row7_col0\" class=\"data row7 col0\" >78.7%</td>\n",
       "      <td id=\"T_c5aff_row7_col1\" class=\"data row7 col1\" >79.8%</td>\n",
       "      <td id=\"T_c5aff_row7_col2\" class=\"data row7 col2\" >80.1%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff596e98a90>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(results).T.style.format('{:0.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe97e23",
   "metadata": {},
   "source": [
    "Which is comparable to the original table\n",
    "\n",
    "\n",
    "| | Features          | # features | NB   | ME   | SVM  |\n",
    "|-|-------------------|------------|------|------|------|\n",
    "|1| unigrams (freq.)  | 16162      | 79.0 | n/a  | 73.0 |\n",
    "|2| unigrams          | 16162      | 81.0 | 80.2 | 82.9 |\n",
    "|3| unigrams+bigrams  | 32324      | 80.7 | 80.7 | 82.8 |\n",
    "|4| bigrams           | 16162      | 77.3 | 77.5 | 76.5 |\n",
    "|5| unigrams+POS      | 16688      | 81.3 | 80.3 | 82.0 |\n",
    "|6| adjectives        | 2631       | 76.6 | 77.6 | 75.3 |\n",
    "|7| top 2631 unigrams | 2631       | 80.9 | 81.3 | 81.2 |\n",
    "|8| unigrams+position | 22407      | 80.8 | 79.8 | 81.8 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91ca24",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "The conclustions from the original still hold up:\n",
    "\n",
    "> The results produced via machine learning techniques are quite good in comparison to the human-generated baselines discussed in Section 4.\n",
    "> In terms of relative performance, Naive Bayes tends to do the worst and SVMs tend to do the best, although the differences aren’t very large.\n",
    "> On the other hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topic-based categorization, despite the several different types of features we tried.\n",
    "> Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. \n",
    "\n",
    "They also give this analysis:\n",
    "\n",
    "> As it turns out, a common phenomenon in the documents was a kind of “thwarted expectations” narrative, where the author sets up a deliberate contrast to earlier discussion.\n",
    "\n",
    "We now have much more advanced methods, in particular neural methods that can take the context into account.\n",
    "\n",
    "## On compute\n",
    "\n",
    "This paper was relatively easy to reproduce because computers are so much faster than when they wrote it, and software is so much better.\n",
    "\n",
    "It's worth keeping in mind the change in technology in the last 20 years; the fastest supercomputer in the world at the time, the [Earth Simulator](https://en.wikipedia.org/wiki/Earth_Simulator) could perform just under 36 TFLOPS, about the same as 4 NVIDIA A100's that could be rented today in the cloud for under $5 an hour (AWS was just starting up around this time).\n",
    "\n",
    "For a more grounded comparison in 2003 Apple released the [Power Mac G5](https://en.wikipedia.org/wiki/Power_Mac_G5) which at the time was a powerful consumer computer, but some [benchmarks from 2017](https://www.phoronix.com/news/PowerMac-Intel-KBL) show it's around 10-100 times slower and a 7th Generation Intel i7. It had 256MB of RAM, where a mid-range laptop today would have at least 8GB, about 30 times more.\n",
    "This meant recalculating the features 9 times (once per split and per model) was a reasonable thing to do, but would have been very unreasonable at the time.\n",
    "\n",
    "Also software has come a long way, [Scikit-Learn](https://scikit-learn.org/stable/about.html) started in 2007 and made all of the feature fitting trivial, that at the time would have involved plugging together different systems (likely in C and Java).\n",
    "\n",
    "It's interesting to think what things may be like in another 20 years."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "364px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
